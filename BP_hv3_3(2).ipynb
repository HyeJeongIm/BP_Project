{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/%08BP_hv3_3(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# batch_size"
      ],
      "metadata": {
        "id": "XiiiBla2-j1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WsCoux5AOZnK",
        "outputId": "a8caa588-e275-40ed-9197-e1a3381e54f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "TensorFlow version :  2.8.2\n",
            "Keras version :  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlHICkovd809",
        "outputId": "e0350e10-3561-4bff-ebd9-15027e9865f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러z오기\n",
        "train_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ],
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "9e1e8c4f-401f-4c5b-8b2f-6a14d77f2163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "795e93a2-7ed1-4b77-82f1-8f73bcaafff9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47456c01-c316-492a-98ac-57fd16c1c233\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47456c01-c316-492a-98ac-57fd16c1c233')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-47456c01-c316-492a-98ac-57fd16c1c233 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-47456c01-c316-492a-98ac-57fd16c1c233');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "25eed2da-eb3e-4108-dd12-1ae9f1fa1159"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7d03f07-598f-4bed-9f01-72fd8cf48d8e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7d03f07-598f-4bed-9f01-72fd8cf48d8e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c7d03f07-598f-4bed-9f01-72fd8cf48d8e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c7d03f07-598f-4bed-9f01-72fd8cf48d8e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ],
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ptBRJtSOZnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EI8SHBwBOZnO",
        "outputId": "28a0961a-72d1-40cb-bc80-8f38736373c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 16)               64        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,489\n",
            "Trainable params: 5,137\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "id": "dGT6-7NcOZnO",
        "outputId": "d7572dcf-11ae-4491-a22f-181652b5a2ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 7s 12ms/step - loss: 12018.7568 - val_loss: 11987.5098\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 11283.2910 - val_loss: 11504.4473\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 10284.0635 - val_loss: 9250.2627\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 9004.9746 - val_loss: 9303.3203\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 7553.2856 - val_loss: 6644.5654\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 6069.6079 - val_loss: 4045.8865\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 4648.3892 - val_loss: 2388.2642\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 3377.7063 - val_loss: 3157.8364\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2305.3330 - val_loss: 3222.4575\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1461.3849 - val_loss: 587.0613\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 855.6591 - val_loss: 757.7213\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 465.4413 - val_loss: 375.4196\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 259.3607 - val_loss: 295.0258\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 161.9164 - val_loss: 136.6505\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 122.2038 - val_loss: 148.9538\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 108.4462 - val_loss: 156.1256\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 103.1167 - val_loss: 144.8389\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 100.6028 - val_loss: 112.4082\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 99.0438 - val_loss: 117.5545\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 98.7264 - val_loss: 125.1832\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.5403 - val_loss: 111.9292\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.6385 - val_loss: 123.3180\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 95.5254 - val_loss: 111.6703\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 95.2387 - val_loss: 113.1557\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.4366 - val_loss: 122.4196\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 93.9937 - val_loss: 124.5656\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 93.2797 - val_loss: 125.3834\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.3887 - val_loss: 157.0292\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.4010 - val_loss: 118.1218\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 91.1923 - val_loss: 174.1026\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 90.5200 - val_loss: 117.3082\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 90.1276 - val_loss: 108.0892\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 89.7337 - val_loss: 119.7969\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.8685 - val_loss: 105.8088\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 88.4671 - val_loss: 124.0962\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 87.9617 - val_loss: 117.0105\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.3460 - val_loss: 105.4509\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.1759 - val_loss: 117.8634\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 86.8838 - val_loss: 126.3495\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.4932 - val_loss: 138.5973\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.2274 - val_loss: 154.9930\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.7107 - val_loss: 111.9073\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.9512 - val_loss: 108.9689\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.8379 - val_loss: 110.4993\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.4645 - val_loss: 124.1659\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.8846 - val_loss: 124.4965\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 84.0306 - val_loss: 105.3713\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.2855 - val_loss: 134.4733\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 82.8307 - val_loss: 117.0426\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.4660 - val_loss: 105.6805\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.1968 - val_loss: 108.6128\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 81.9589 - val_loss: 102.7702\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.8450 - val_loss: 108.8520\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.6075 - val_loss: 170.6328\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 81.3525 - val_loss: 137.9045\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 80.7257 - val_loss: 140.0878\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 80.2784 - val_loss: 104.4115\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 80.1895 - val_loss: 130.1388\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 79.6104 - val_loss: 109.6900\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 79.6705 - val_loss: 107.3903\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 79.7265 - val_loss: 107.7013\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.3780 - val_loss: 128.1280\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.2490 - val_loss: 110.3888\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 78.8425 - val_loss: 94.4362\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 78.8209 - val_loss: 100.9920\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 78.7012 - val_loss: 100.1135\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 78.4234 - val_loss: 148.5212\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.9674 - val_loss: 97.0341\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.7503 - val_loss: 95.8209\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.6664 - val_loss: 129.7313\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 77.4883 - val_loss: 96.4498\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.3934 - val_loss: 112.2704\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.4568 - val_loss: 101.7000\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 77.0238 - val_loss: 115.1010\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 76.9265 - val_loss: 105.0276\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.0148 - val_loss: 130.0827\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 76.5420 - val_loss: 124.8090\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.4470 - val_loss: 133.2879\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.4025 - val_loss: 100.5361\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 76.1958 - val_loss: 103.8210\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.0208 - val_loss: 98.0219\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 76.0549 - val_loss: 103.6206\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 75.8011 - val_loss: 168.6650\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.7525 - val_loss: 103.8833\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.4037 - val_loss: 127.4692\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.4350 - val_loss: 122.9532\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 75.4947 - val_loss: 93.4650\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 75.0266 - val_loss: 94.1018\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.8908 - val_loss: 93.5022\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.9973 - val_loss: 106.8872\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 75.0348 - val_loss: 104.8873\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.7858 - val_loss: 107.4302\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.6814 - val_loss: 103.3965\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.6805 - val_loss: 97.0683\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.5909 - val_loss: 95.1819\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.4110 - val_loss: 97.9439\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.1135 - val_loss: 96.8296\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.1586 - val_loss: 100.1637\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.2361 - val_loss: 99.2147\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 73.7704 - val_loss: 109.1841\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8996 - val_loss: 120.3473\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 73.7780 - val_loss: 92.9735\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4860 - val_loss: 111.2541\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.6769 - val_loss: 96.1878\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3622 - val_loss: 101.7496\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3512 - val_loss: 108.9279\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2372 - val_loss: 110.6727\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 73.1307 - val_loss: 88.9603\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 72.9291 - val_loss: 96.3057\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1675 - val_loss: 100.1728\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.8237 - val_loss: 101.1602\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 72.8624 - val_loss: 101.6199\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7799 - val_loss: 121.7571\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 72.6583 - val_loss: 106.1204\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 72.6521 - val_loss: 116.0364\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.5992 - val_loss: 93.9673\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.2971 - val_loss: 114.3356\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.5793 - val_loss: 110.3951\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 72.3430 - val_loss: 108.3261\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.2562 - val_loss: 100.7248\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.3983 - val_loss: 98.4732\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.9793 - val_loss: 101.8041\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.0061 - val_loss: 99.2051\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.9069 - val_loss: 104.0017\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.9353 - val_loss: 95.7259\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8154 - val_loss: 92.8452\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.9475 - val_loss: 94.6552\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8721 - val_loss: 98.7453\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.8627 - val_loss: 106.1201\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.7540 - val_loss: 107.0550\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.7519 - val_loss: 152.4655\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.7759 - val_loss: 145.5437\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.4294 - val_loss: 112.5455\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.4700 - val_loss: 114.4512\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.3853 - val_loss: 163.0361\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.4232 - val_loss: 97.0853\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.3687 - val_loss: 113.3727\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.1768 - val_loss: 113.8745\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.9530 - val_loss: 101.0002\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.9835 - val_loss: 115.9373\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.2839 - val_loss: 111.3745\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.9887 - val_loss: 92.3913\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.1823 - val_loss: 95.8030\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.9342 - val_loss: 124.8129\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.7067 - val_loss: 96.0671\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.9192 - val_loss: 103.6282\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.9075 - val_loss: 95.0103\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.7118 - val_loss: 93.4125\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.4547 - val_loss: 89.5695\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.5672 - val_loss: 109.7267\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.8016 - val_loss: 118.4978\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.7388 - val_loss: 102.4564\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.4266 - val_loss: 115.0235\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.7603 - val_loss: 106.7918\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.3410 - val_loss: 94.8098\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.5639 - val_loss: 96.2258\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.3745 - val_loss: 93.7424\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.2895 - val_loss: 97.8313\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.1806 - val_loss: 161.6772\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.1909 - val_loss: 112.5382\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.9591 - val_loss: 107.6508\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.0111 - val_loss: 93.6385\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.0573 - val_loss: 96.3553\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.9763 - val_loss: 105.6706\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.8729 - val_loss: 131.0100\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.0687 - val_loss: 91.9308\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.8896 - val_loss: 97.5208\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.7552 - val_loss: 92.2851\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.8366 - val_loss: 98.3873\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.8230 - val_loss: 95.6654\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.7325 - val_loss: 104.6368\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.7905 - val_loss: 92.8989\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.6390 - val_loss: 110.3794\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.4384 - val_loss: 99.1667\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.5499 - val_loss: 92.7804\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.2750 - val_loss: 105.8053\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.2411 - val_loss: 90.7830\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.3092 - val_loss: 145.7268\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.4655 - val_loss: 122.3642\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.4074 - val_loss: 96.1189\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.2695 - val_loss: 103.3634\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.4707 - val_loss: 94.2495\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.1814 - val_loss: 105.1479\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 69.2861 - val_loss: 95.7777\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.1348 - val_loss: 91.3812\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.9809 - val_loss: 98.2803\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.0193 - val_loss: 93.4637\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.8024 - val_loss: 91.4322\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.2601 - val_loss: 91.1071\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.0636 - val_loss: 91.0538\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.9313 - val_loss: 106.0150\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.9679 - val_loss: 91.1166\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.9334 - val_loss: 95.1660\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.0908 - val_loss: 89.4248\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.8226 - val_loss: 88.2597\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.8095 - val_loss: 129.6890\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.8345 - val_loss: 104.3731\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.8878 - val_loss: 97.3890\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5092 - val_loss: 99.3546\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.8057 - val_loss: 95.5280\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.9610 - val_loss: 109.4360\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.4284 - val_loss: 95.8154\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3839 - val_loss: 101.6109\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5189 - val_loss: 94.4173\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.6516 - val_loss: 94.2369\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.4330 - val_loss: 97.0926\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5895 - val_loss: 120.2615\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.4246 - val_loss: 102.5591\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5303 - val_loss: 105.6966\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2311 - val_loss: 107.0232\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2975 - val_loss: 106.1012\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.6057 - val_loss: 130.2884\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2397 - val_loss: 105.5863\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.4548 - val_loss: 91.0433\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2921 - val_loss: 101.1725\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2887 - val_loss: 108.6564\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2092 - val_loss: 96.2518\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3591 - val_loss: 96.0552\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2309 - val_loss: 92.4532\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3361 - val_loss: 110.2052\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.1931 - val_loss: 93.1112\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.2304 - val_loss: 120.3066\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.2367 - val_loss: 94.8612\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.1491 - val_loss: 101.7618\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2008 - val_loss: 90.6810\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9072 - val_loss: 92.1738\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.2037 - val_loss: 132.6963\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.2010 - val_loss: 97.6986\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 67.8271 - val_loss: 114.2212\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0660 - val_loss: 98.1669\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9130 - val_loss: 132.3195\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7711 - val_loss: 97.9974\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.8726 - val_loss: 98.8770\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 67.7513 - val_loss: 93.3459\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.1028 - val_loss: 141.6559\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9361 - val_loss: 93.2561\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 67.9298 - val_loss: 90.6152\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6782 - val_loss: 115.1980\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.8766 - val_loss: 99.8925\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7655 - val_loss: 108.7250\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6235 - val_loss: 89.8641\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7815 - val_loss: 90.6262\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7865 - val_loss: 93.1871\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5413 - val_loss: 93.2434\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7837 - val_loss: 94.5431\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6138 - val_loss: 130.9770\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6709 - val_loss: 89.6502\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6932 - val_loss: 95.1085\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7510 - val_loss: 116.4870\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6859 - val_loss: 97.3396\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6955 - val_loss: 111.1878\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6370 - val_loss: 106.9311\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6416 - val_loss: 90.5206\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5123 - val_loss: 95.8992\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.4310 - val_loss: 93.4549\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5627 - val_loss: 99.7989\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 67.4378 - val_loss: 92.9391\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.4094 - val_loss: 93.9764\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3805 - val_loss: 99.2523\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.4109 - val_loss: 124.5068\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5241 - val_loss: 93.2162\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6366 - val_loss: 116.0896\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3335 - val_loss: 88.7746\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2468 - val_loss: 89.0364\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1874 - val_loss: 92.1215\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3260 - val_loss: 88.5135\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2412 - val_loss: 114.0383\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5752 - val_loss: 95.4842\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2899 - val_loss: 92.9698\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3642 - val_loss: 98.6250\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3202 - val_loss: 88.1869\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2469 - val_loss: 93.9382\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2061 - val_loss: 125.7655\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 67.3004 - val_loss: 115.5422\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1325 - val_loss: 102.2273\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0569 - val_loss: 103.5612\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2247 - val_loss: 94.6507\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1359 - val_loss: 208.7007\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3151 - val_loss: 120.0905\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1697 - val_loss: 119.8200\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2241 - val_loss: 98.6669\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2291 - val_loss: 118.8142\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0413 - val_loss: 121.2019\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2870 - val_loss: 88.4250\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1138 - val_loss: 101.7627\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0594 - val_loss: 97.2692\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1451 - val_loss: 109.6672\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9376 - val_loss: 90.4851\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8551 - val_loss: 98.6421\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1101 - val_loss: 87.0346\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8582 - val_loss: 91.3583\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8611 - val_loss: 89.2384\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8740 - val_loss: 87.8912\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8576 - val_loss: 92.7914\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0874 - val_loss: 100.9192\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9727 - val_loss: 86.0842\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9164 - val_loss: 98.6111\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8058 - val_loss: 102.7731\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8126 - val_loss: 101.1449\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0323 - val_loss: 85.7922\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6309 - val_loss: 94.7168\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7788 - val_loss: 142.0271\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9068 - val_loss: 111.3313\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9061 - val_loss: 102.1517\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8694 - val_loss: 96.4244\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7829 - val_loss: 99.9691\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7063 - val_loss: 100.9387\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6769 - val_loss: 94.0371\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7340 - val_loss: 100.1762\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8386 - val_loss: 92.0372\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6796 - val_loss: 104.9267\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8402 - val_loss: 96.7249\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5029 - val_loss: 108.4324\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9078 - val_loss: 96.2791\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5230 - val_loss: 85.6830\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6874 - val_loss: 111.5336\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9786 - val_loss: 111.2196\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6778 - val_loss: 93.2874\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3462 - val_loss: 91.0337\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5878 - val_loss: 100.2586\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8270 - val_loss: 90.8615\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5334 - val_loss: 93.4621\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5482 - val_loss: 91.5149\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6410 - val_loss: 87.7921\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3504 - val_loss: 119.8572\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5600 - val_loss: 88.7705\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4997 - val_loss: 86.0227\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4401 - val_loss: 87.9050\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 66.5380 - val_loss: 92.2878\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5804 - val_loss: 107.2997\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7149 - val_loss: 92.4453\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4733 - val_loss: 147.0631\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2866 - val_loss: 100.1632\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3472 - val_loss: 88.6079\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3891 - val_loss: 94.3976\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4686 - val_loss: 90.2255\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4703 - val_loss: 93.5396\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5436 - val_loss: 90.9992\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2999 - val_loss: 93.1557\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3889 - val_loss: 93.1328\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2037 - val_loss: 109.4748\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3575 - val_loss: 104.6848\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4233 - val_loss: 102.2593\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3970 - val_loss: 118.5365\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3517 - val_loss: 96.5470\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2873 - val_loss: 94.0192\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2796 - val_loss: 107.6008\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 66.3527 - val_loss: 93.9844\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 66.2876 - val_loss: 102.9410\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4360 - val_loss: 122.5100\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1268 - val_loss: 94.4291\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2903 - val_loss: 95.4254\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2977 - val_loss: 104.2615\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0987 - val_loss: 90.6201\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1080 - val_loss: 90.8430\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2431 - val_loss: 105.8390\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9694 - val_loss: 98.8485\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0928 - val_loss: 87.8494\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3929 - val_loss: 92.1738\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1684 - val_loss: 109.0678\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1938 - val_loss: 89.1318\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2544 - val_loss: 99.6192\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3662 - val_loss: 88.4891\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9365 - val_loss: 90.0594\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0161 - val_loss: 89.8855\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1760 - val_loss: 99.6233\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1444 - val_loss: 95.4197\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9675 - val_loss: 93.7005\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1996 - val_loss: 89.9134\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0201 - val_loss: 102.2590\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9653 - val_loss: 92.8784\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0180 - val_loss: 100.0488\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0509 - val_loss: 103.5065\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0278 - val_loss: 103.5168\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8622 - val_loss: 87.1400\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0599 - val_loss: 93.5852\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1974 - val_loss: 93.4929\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9426 - val_loss: 95.8806\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 66.0267 - val_loss: 98.7781\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0572 - val_loss: 87.7849\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9292 - val_loss: 93.5809\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0894 - val_loss: 89.5379\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8895 - val_loss: 101.5909\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9881 - val_loss: 103.9693\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9818 - val_loss: 95.7703\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0441 - val_loss: 89.1661\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9842 - val_loss: 86.4701\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.9644 - val_loss: 101.6444\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8891 - val_loss: 97.6770\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9273 - val_loss: 91.8288\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0625 - val_loss: 128.4530\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0604 - val_loss: 96.2238\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9247 - val_loss: 97.8010\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7868 - val_loss: 95.3735\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8156 - val_loss: 125.3330\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0250 - val_loss: 94.0952\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9797 - val_loss: 97.4528\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8541 - val_loss: 92.2038\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0438 - val_loss: 101.4278\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0381 - val_loss: 117.8464\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7801 - val_loss: 94.4616\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7126 - val_loss: 100.1307\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8764 - val_loss: 95.5593\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.7038 - val_loss: 89.4917\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5688 - val_loss: 90.1776\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9200 - val_loss: 102.4708\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8872 - val_loss: 91.6078\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7227 - val_loss: 96.4280\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8077 - val_loss: 94.1836\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6646 - val_loss: 98.1799\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7038 - val_loss: 92.5289\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6507 - val_loss: 98.2616\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6948 - val_loss: 95.7370\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7745 - val_loss: 102.5819\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6493 - val_loss: 93.5103\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6665 - val_loss: 94.5890\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5705 - val_loss: 98.1021\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4725 - val_loss: 100.2001\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6665 - val_loss: 117.9009\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4873 - val_loss: 134.4898\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3854 - val_loss: 87.8003\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6674 - val_loss: 109.4388\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6474 - val_loss: 92.9016\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5655 - val_loss: 99.6972\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6410 - val_loss: 108.1424\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5417 - val_loss: 98.8450\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.5881 - val_loss: 90.4473\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4690 - val_loss: 93.2779\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7269 - val_loss: 86.3422\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7573 - val_loss: 90.8342\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5811 - val_loss: 109.2137\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3572 - val_loss: 98.3767\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.5134 - val_loss: 103.2521\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.5764 - val_loss: 95.5254\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5289 - val_loss: 99.9658\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7007 - val_loss: 91.9844\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5839 - val_loss: 91.3714\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5259 - val_loss: 98.0175\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4035 - val_loss: 97.7406\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5698 - val_loss: 94.6298\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6287 - val_loss: 114.2500\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5946 - val_loss: 117.1142\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5298 - val_loss: 91.6824\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4242 - val_loss: 96.8081\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4486 - val_loss: 98.7045\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5129 - val_loss: 88.5657\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4852 - val_loss: 98.4239\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5878 - val_loss: 89.2083\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6462 - val_loss: 96.4327\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6099 - val_loss: 105.2412\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3428 - val_loss: 103.6863\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4651 - val_loss: 93.4655\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2813 - val_loss: 100.8001\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4992 - val_loss: 87.6327\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4452 - val_loss: 90.0799\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2988 - val_loss: 94.0045\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3963 - val_loss: 120.2065\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3371 - val_loss: 89.9215\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6100 - val_loss: 90.1059\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3865 - val_loss: 93.5030\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3306 - val_loss: 94.7991\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2543 - val_loss: 93.2457\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3304 - val_loss: 104.5460\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4333 - val_loss: 97.0944\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4454 - val_loss: 115.6217\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3671 - val_loss: 91.6281\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3490 - val_loss: 90.3077\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3587 - val_loss: 92.1737\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3073 - val_loss: 87.6139\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4194 - val_loss: 106.2395\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4544 - val_loss: 87.4700\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3616 - val_loss: 100.2608\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3257 - val_loss: 97.1263\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.3480 - val_loss: 95.1621\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3814 - val_loss: 99.8806\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1471 - val_loss: 108.2310\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2982 - val_loss: 89.2069\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.1719 - val_loss: 88.3170\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0436 - val_loss: 91.7598\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1851 - val_loss: 93.7340\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2191 - val_loss: 102.0495\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1531 - val_loss: 90.8120\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0242 - val_loss: 109.4628\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0548 - val_loss: 92.4500\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1978 - val_loss: 117.3293\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2457 - val_loss: 98.3772\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3006 - val_loss: 108.5273\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2372 - val_loss: 96.6511\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0374 - val_loss: 86.8059\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1984 - val_loss: 86.8145\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2008 - val_loss: 95.2399\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 64.9941 - val_loss: 86.6475\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 64.9360 - val_loss: 98.1675\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1890 - val_loss: 89.7965\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0329 - val_loss: 97.6608\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 64.9691 - val_loss: 105.9298\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1213 - val_loss: 91.1775\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2002 - val_loss: 89.3225\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0201 - val_loss: 97.9948\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 64.9736 - val_loss: 131.0921\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "e0f09bb4-95b8-45ee-8cdf-875efd58a824",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -5.8865792952920115 \n",
            "MAE:  8.94071121272786 \n",
            "SD:  9.820400785962152\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "125e57aa-3a16-436b-eb4d-54f97b38209f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "outputId": "c710f8b0-b546-4f64-93f7-34f3db4a9180",
        "id": "su2Sj5jZCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,489\n",
            "Trainable params: 5,137\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "scrolled": true,
        "outputId": "d7d8c971-c8b1-4868-bc91-6e265ac9021e",
        "id": "kPRh6v-mCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 4s 12ms/step - loss: 12175.9492 - val_loss: 12079.5020\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 11524.7559 - val_loss: 11233.8682\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 10620.3789 - val_loss: 10328.0811\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 9449.0889 - val_loss: 7818.0420\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 8097.4067 - val_loss: 7282.0015\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 6599.8447 - val_loss: 5107.0269\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 5104.7026 - val_loss: 4737.2119\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 3704.7361 - val_loss: 2823.8049\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2461.3613 - val_loss: 1714.1395\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 1534.7583 - val_loss: 2048.4922\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 900.5815 - val_loss: 450.7146\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 514.6877 - val_loss: 575.4778\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 297.6981 - val_loss: 186.6767\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 187.1213 - val_loss: 150.5419\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 141.7123 - val_loss: 188.9067\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 121.5961 - val_loss: 158.9948\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 112.7764 - val_loss: 125.2454\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.3043 - val_loss: 139.3511\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.3873 - val_loss: 134.1659\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 106.9534 - val_loss: 135.8004\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.7527 - val_loss: 151.2705\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 104.4552 - val_loss: 124.3599\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.2314 - val_loss: 142.8487\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 104.8244 - val_loss: 131.5965\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.6094 - val_loss: 144.0553\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 102.3444 - val_loss: 121.9372\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 101.2862 - val_loss: 123.7094\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 100.3451 - val_loss: 181.8764\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.2592 - val_loss: 107.6654\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.9673 - val_loss: 167.2542\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.0313 - val_loss: 136.7391\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 96.4609 - val_loss: 108.8199\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 95.5028 - val_loss: 108.7885\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 94.1648 - val_loss: 106.1869\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.0218 - val_loss: 121.3364\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 93.5569 - val_loss: 111.3967\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.7863 - val_loss: 112.7098\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.9415 - val_loss: 127.1200\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 91.3710 - val_loss: 135.4586\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.6078 - val_loss: 115.3556\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.8445 - val_loss: 164.0008\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 89.6127 - val_loss: 104.1602\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.6217 - val_loss: 108.4044\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.1466 - val_loss: 125.8992\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.6361 - val_loss: 118.1082\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.6155 - val_loss: 100.0676\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.0071 - val_loss: 130.5047\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 86.4334 - val_loss: 110.0758\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 85.9968 - val_loss: 126.2365\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.5601 - val_loss: 102.4885\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.5874 - val_loss: 117.2258\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.7716 - val_loss: 108.4645\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.4403 - val_loss: 121.3276\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.1113 - val_loss: 120.8751\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.8674 - val_loss: 103.9794\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.4109 - val_loss: 99.8444\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.9801 - val_loss: 100.3814\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.7224 - val_loss: 108.3231\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.5789 - val_loss: 102.5212\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 82.0336 - val_loss: 100.2715\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.0386 - val_loss: 103.1564\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.6150 - val_loss: 103.7607\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.1486 - val_loss: 96.8528\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.7618 - val_loss: 112.6271\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 80.6721 - val_loss: 106.5486\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.3271 - val_loss: 109.9473\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.7933 - val_loss: 104.2186\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 79.6535 - val_loss: 104.6496\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.4817 - val_loss: 93.1699\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.2989 - val_loss: 117.9695\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 78.9019 - val_loss: 97.0151\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.8672 - val_loss: 93.8266\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.8091 - val_loss: 101.5517\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 78.0391 - val_loss: 119.2416\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.0943 - val_loss: 93.1799\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 77.6488 - val_loss: 96.2960\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.8032 - val_loss: 106.4830\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.6322 - val_loss: 117.1691\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.1254 - val_loss: 115.9081\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.9524 - val_loss: 144.5481\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.1670 - val_loss: 91.1993\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.3163 - val_loss: 109.5638\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.6497 - val_loss: 99.6742\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.6325 - val_loss: 94.7080\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.3729 - val_loss: 98.5568\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.4148 - val_loss: 97.9435\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.1486 - val_loss: 96.2231\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.0620 - val_loss: 128.0807\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.7659 - val_loss: 102.4953\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.4822 - val_loss: 109.8123\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.4154 - val_loss: 108.0429\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.6100 - val_loss: 94.3002\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.1875 - val_loss: 126.3173\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.8562 - val_loss: 153.2711\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.1070 - val_loss: 105.3619\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.6681 - val_loss: 100.8489\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.7259 - val_loss: 113.5495\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.3856 - val_loss: 98.8824\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.4778 - val_loss: 93.2939\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.3578 - val_loss: 95.8728\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.0898 - val_loss: 91.0577\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.0733 - val_loss: 110.9699\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7240 - val_loss: 95.2888\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7122 - val_loss: 92.4012\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5209 - val_loss: 96.0189\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8071 - val_loss: 93.4949\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4534 - val_loss: 93.2385\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.6468 - val_loss: 101.9892\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2840 - val_loss: 94.5343\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2197 - val_loss: 97.2045\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9047 - val_loss: 101.6073\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1728 - val_loss: 102.6741\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.8533 - val_loss: 122.1017\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9436 - val_loss: 123.6445\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7303 - val_loss: 95.7868\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7952 - val_loss: 91.6401\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4143 - val_loss: 90.7644\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4501 - val_loss: 118.0960\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4828 - val_loss: 104.0302\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.3740 - val_loss: 95.6533\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.2543 - val_loss: 128.2361\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.2040 - val_loss: 98.9702\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 72.2603 - val_loss: 94.4700\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.0356 - val_loss: 97.5012\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.9237 - val_loss: 114.3439\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.7659 - val_loss: 100.5142\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.9841 - val_loss: 125.6651\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.7340 - val_loss: 91.8267\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6283 - val_loss: 100.0433\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6863 - val_loss: 109.4233\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.7702 - val_loss: 94.0676\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6721 - val_loss: 108.3006\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.4068 - val_loss: 98.2143\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.4441 - val_loss: 97.0480\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.1910 - val_loss: 93.1204\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.0082 - val_loss: 104.7763\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.2605 - val_loss: 152.3051\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.1789 - val_loss: 138.3083\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.9835 - val_loss: 89.8755\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.9900 - val_loss: 91.6943\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.2375 - val_loss: 101.3114\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.9028 - val_loss: 103.7144\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.8671 - val_loss: 143.6010\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.9911 - val_loss: 163.8810\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.7184 - val_loss: 104.4743\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.7043 - val_loss: 93.9537\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.7450 - val_loss: 143.1894\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.7108 - val_loss: 90.9193\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.4890 - val_loss: 108.2098\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.7165 - val_loss: 92.8095\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.6691 - val_loss: 93.6663\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.3914 - val_loss: 96.0559\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.6060 - val_loss: 96.5096\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.2432 - val_loss: 103.5359\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.5278 - val_loss: 95.4889\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.5098 - val_loss: 90.9348\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.4482 - val_loss: 96.8531\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.0725 - val_loss: 106.2092\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.1598 - val_loss: 96.4686\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.0647 - val_loss: 97.8533\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.0279 - val_loss: 104.8983\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.9293 - val_loss: 118.1411\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.9919 - val_loss: 95.7603\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.0008 - val_loss: 95.5663\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.9183 - val_loss: 102.3301\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.8491 - val_loss: 94.0266\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.8473 - val_loss: 104.8068\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.8162 - val_loss: 91.4378\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.7167 - val_loss: 115.2969\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.9420 - val_loss: 99.5799\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.6674 - val_loss: 95.6165\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.4804 - val_loss: 101.2691\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.6187 - val_loss: 95.5889\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.4516 - val_loss: 134.8339\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.6758 - val_loss: 91.6859\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.4993 - val_loss: 93.5264\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.1432 - val_loss: 98.1151\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.5357 - val_loss: 98.1144\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.4554 - val_loss: 94.0946\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.4087 - val_loss: 99.3848\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.5583 - val_loss: 116.4826\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.4127 - val_loss: 134.9247\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.1983 - val_loss: 119.0079\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.5922 - val_loss: 103.5506\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.3612 - val_loss: 92.8758\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.2803 - val_loss: 95.4821\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.2972 - val_loss: 97.3295\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.4902 - val_loss: 110.1357\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.0854 - val_loss: 103.5506\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.2187 - val_loss: 95.1328\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.1091 - val_loss: 116.9897\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.3568 - val_loss: 106.5426\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.2107 - val_loss: 94.1779\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.0163 - val_loss: 137.0665\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.0370 - val_loss: 95.0839\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.9792 - val_loss: 108.2735\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.8444 - val_loss: 96.2230\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.9865 - val_loss: 131.8022\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.0945 - val_loss: 139.5504\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.8525 - val_loss: 97.3339\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.7449 - val_loss: 105.2699\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.8412 - val_loss: 108.3291\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.9013 - val_loss: 94.9735\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.7936 - val_loss: 95.5006\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.6828 - val_loss: 92.5932\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.8250 - val_loss: 94.3275\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.8498 - val_loss: 103.0139\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5705 - val_loss: 90.8623\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5211 - val_loss: 114.4330\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5206 - val_loss: 99.8826\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.6345 - val_loss: 125.3070\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.4723 - val_loss: 91.4903\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.6334 - val_loss: 108.3161\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3934 - val_loss: 104.6268\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5365 - val_loss: 105.7318\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.4431 - val_loss: 98.2221\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5518 - val_loss: 87.7901\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5773 - val_loss: 93.5394\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.6904 - val_loss: 106.4882\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.4662 - val_loss: 90.1975\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3610 - val_loss: 95.1540\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.4428 - val_loss: 102.3082\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3234 - val_loss: 97.5279\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3369 - val_loss: 96.2107\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5757 - val_loss: 123.5901\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3873 - val_loss: 98.8326\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2571 - val_loss: 104.7218\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3146 - val_loss: 97.6508\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0403 - val_loss: 91.0959\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.1592 - val_loss: 99.7345\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0357 - val_loss: 92.5396\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0447 - val_loss: 90.5593\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.4105 - val_loss: 94.2319\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2410 - val_loss: 113.5549\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2108 - val_loss: 93.1721\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0742 - val_loss: 100.4126\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2393 - val_loss: 114.5448\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9709 - val_loss: 102.8087\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0902 - val_loss: 127.6838\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3129 - val_loss: 118.2796\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0844 - val_loss: 105.4317\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9748 - val_loss: 96.2384\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.8965 - val_loss: 101.0529\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9814 - val_loss: 94.2019\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.1141 - val_loss: 93.8213\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9354 - val_loss: 89.7588\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7418 - val_loss: 107.2863\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9424 - val_loss: 96.9690\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9464 - val_loss: 97.2396\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9821 - val_loss: 98.3795\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6165 - val_loss: 96.9487\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7759 - val_loss: 89.7245\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9162 - val_loss: 103.6225\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5653 - val_loss: 90.8315\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.8549 - val_loss: 90.0982\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7239 - val_loss: 112.9893\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.8288 - val_loss: 98.0424\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.8241 - val_loss: 96.3867\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9718 - val_loss: 90.7516\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7176 - val_loss: 107.0230\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7488 - val_loss: 90.8686\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7444 - val_loss: 107.2671\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6359 - val_loss: 97.1700\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6442 - val_loss: 92.4573\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5745 - val_loss: 94.2248\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.4602 - val_loss: 87.8899\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5562 - val_loss: 87.0524\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6178 - val_loss: 92.4411\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5699 - val_loss: 90.6680\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6728 - val_loss: 111.2110\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6621 - val_loss: 106.6952\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6510 - val_loss: 89.6045\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3588 - val_loss: 89.0432\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.4013 - val_loss: 90.3894\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6157 - val_loss: 94.6110\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3352 - val_loss: 92.0885\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6270 - val_loss: 110.8551\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3918 - val_loss: 95.8548\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3248 - val_loss: 94.2555\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.4354 - val_loss: 97.4137\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.4829 - val_loss: 111.1818\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.4022 - val_loss: 96.4297\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3886 - val_loss: 88.7345\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3687 - val_loss: 99.2773\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3744 - val_loss: 131.7948\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1857 - val_loss: 104.5181\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2943 - val_loss: 97.6804\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2645 - val_loss: 112.9718\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5097 - val_loss: 106.8250\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5144 - val_loss: 94.7325\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0767 - val_loss: 97.2781\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.4211 - val_loss: 135.8008\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0585 - val_loss: 105.7548\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1621 - val_loss: 138.1858\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3222 - val_loss: 96.4827\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0805 - val_loss: 89.0834\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1019 - val_loss: 96.9981\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9946 - val_loss: 95.8526\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9641 - val_loss: 93.7370\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2949 - val_loss: 99.9933\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3072 - val_loss: 94.0583\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1587 - val_loss: 92.1452\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2476 - val_loss: 91.6250\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1719 - val_loss: 94.8051\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1188 - val_loss: 106.0844\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8983 - val_loss: 97.6213\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9247 - val_loss: 102.8279\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1389 - val_loss: 94.6868\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1078 - val_loss: 105.8750\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9078 - val_loss: 104.1085\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9535 - val_loss: 100.5547\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1564 - val_loss: 93.6272\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9201 - val_loss: 167.1998\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2362 - val_loss: 114.4170\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8487 - val_loss: 94.8165\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 66.9408 - val_loss: 88.2399\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8380 - val_loss: 106.6455\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8319 - val_loss: 98.5067\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9013 - val_loss: 101.0564\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6696 - val_loss: 86.6509\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1637 - val_loss: 99.3351\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7876 - val_loss: 90.8821\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9691 - val_loss: 94.7796\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0355 - val_loss: 99.1459\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7767 - val_loss: 99.8099\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 66.8904 - val_loss: 94.3820\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.7938 - val_loss: 104.0245\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0151 - val_loss: 100.9081\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7868 - val_loss: 116.9154\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0263 - val_loss: 89.7309\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6557 - val_loss: 89.1529\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8958 - val_loss: 113.5872\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5587 - val_loss: 143.9390\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8401 - val_loss: 92.3795\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7557 - val_loss: 122.4391\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7590 - val_loss: 110.6728\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7624 - val_loss: 93.8240\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9415 - val_loss: 106.5312\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7352 - val_loss: 97.2757\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6718 - val_loss: 110.4363\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6685 - val_loss: 92.2787\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6649 - val_loss: 89.2494\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6033 - val_loss: 118.4679\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5918 - val_loss: 121.9420\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5666 - val_loss: 98.7095\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6219 - val_loss: 90.7706\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4645 - val_loss: 91.1090\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6749 - val_loss: 136.7242\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4752 - val_loss: 88.1833\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 66.5795 - val_loss: 88.9850\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4060 - val_loss: 102.3400\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5497 - val_loss: 97.0352\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7222 - val_loss: 105.6364\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5475 - val_loss: 102.9569\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6657 - val_loss: 97.5523\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3920 - val_loss: 96.2727\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4689 - val_loss: 96.3994\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5280 - val_loss: 94.9252\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4776 - val_loss: 100.1221\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5241 - val_loss: 95.8984\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4329 - val_loss: 89.4174\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 66.5680 - val_loss: 89.3341\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2554 - val_loss: 91.9652\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 66.2744 - val_loss: 90.8961\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5379 - val_loss: 110.0259\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3831 - val_loss: 89.5892\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3729 - val_loss: 96.1416\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3756 - val_loss: 107.8171\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5220 - val_loss: 90.0641\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6077 - val_loss: 95.4049\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5095 - val_loss: 87.8509\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3425 - val_loss: 90.4209\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3863 - val_loss: 91.1859\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2894 - val_loss: 97.6627\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4414 - val_loss: 99.8724\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5952 - val_loss: 115.9390\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3482 - val_loss: 89.4129\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3020 - val_loss: 95.7244\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3902 - val_loss: 103.3304\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3318 - val_loss: 92.4930\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3286 - val_loss: 95.5996\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2168 - val_loss: 94.7305\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2263 - val_loss: 90.3695\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2400 - val_loss: 97.2069\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2437 - val_loss: 95.2965\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2191 - val_loss: 120.7778\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2326 - val_loss: 91.1343\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2661 - val_loss: 90.9857\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2791 - val_loss: 111.9409\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5720 - val_loss: 88.0728\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2787 - val_loss: 105.8963\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2011 - val_loss: 93.0005\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2131 - val_loss: 96.5637\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2732 - val_loss: 95.3612\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 66.0318 - val_loss: 127.1458\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5414 - val_loss: 91.8065\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1899 - val_loss: 91.5958\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1845 - val_loss: 109.8047\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2887 - val_loss: 93.9038\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1431 - val_loss: 107.9864\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1156 - val_loss: 96.4216\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2257 - val_loss: 171.6008\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2921 - val_loss: 93.7631\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2531 - val_loss: 93.9873\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0576 - val_loss: 99.0219\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0695 - val_loss: 89.1256\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0260 - val_loss: 93.0166\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1792 - val_loss: 96.2001\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0724 - val_loss: 95.6488\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1856 - val_loss: 96.4350\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9685 - val_loss: 91.2999\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0370 - val_loss: 88.1143\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0757 - val_loss: 92.3636\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0782 - val_loss: 101.5096\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0049 - val_loss: 100.7270\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 66.2275 - val_loss: 92.7929\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0582 - val_loss: 90.2862\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 66.0268 - val_loss: 91.9535\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 66.1906 - val_loss: 88.9228\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0192 - val_loss: 96.1211\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.9741 - val_loss: 97.7460\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8761 - val_loss: 93.3448\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0582 - val_loss: 91.9701\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1573 - val_loss: 95.4286\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0709 - val_loss: 97.3331\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9412 - val_loss: 98.5024\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0227 - val_loss: 94.6906\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9544 - val_loss: 89.9380\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0405 - val_loss: 119.7590\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0702 - val_loss: 93.8913\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9199 - val_loss: 90.7091\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9860 - val_loss: 111.4323\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9969 - val_loss: 101.6134\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0623 - val_loss: 90.7022\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9688 - val_loss: 97.1596\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.8483 - val_loss: 94.0912\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9157 - val_loss: 96.3520\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0385 - val_loss: 92.2932\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0679 - val_loss: 114.0140\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 66.1001 - val_loss: 90.5135\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0039 - val_loss: 92.4182\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7808 - val_loss: 92.1158\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.7952 - val_loss: 95.3506\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7629 - val_loss: 89.8351\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8580 - val_loss: 90.7084\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9964 - val_loss: 97.7751\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9782 - val_loss: 92.1794\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8971 - val_loss: 90.5736\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8614 - val_loss: 110.4425\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8401 - val_loss: 94.7582\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9513 - val_loss: 97.1387\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.8036 - val_loss: 94.2552\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1632 - val_loss: 92.1631\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9748 - val_loss: 94.1473\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8461 - val_loss: 87.9827\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9316 - val_loss: 93.6696\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9179 - val_loss: 141.9298\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.7388 - val_loss: 88.6811\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.8771 - val_loss: 97.3975\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9165 - val_loss: 90.9220\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.9075 - val_loss: 90.9224\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9035 - val_loss: 90.0298\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.8732 - val_loss: 93.6171\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.9352 - val_loss: 97.9667\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7056 - val_loss: 96.0227\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.6694 - val_loss: 87.1364\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.9599 - val_loss: 122.4375\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.9098 - val_loss: 103.4161\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6620 - val_loss: 96.5920\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8542 - val_loss: 92.8928\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7986 - val_loss: 88.9636\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8497 - val_loss: 91.0608\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7550 - val_loss: 101.1626\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9413 - val_loss: 90.5954\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7937 - val_loss: 87.1560\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7980 - val_loss: 91.8723\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7793 - val_loss: 91.3095\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.7077 - val_loss: 90.3049\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9770 - val_loss: 107.2588\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6410 - val_loss: 99.9398\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7329 - val_loss: 100.6417\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7124 - val_loss: 91.3777\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.7206 - val_loss: 86.4768\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9338 - val_loss: 127.4382\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 65.8013 - val_loss: 97.2951\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 65.7486 - val_loss: 112.2260\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.5748 - val_loss: 103.4400\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8355 - val_loss: 96.1600\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7511 - val_loss: 106.9296\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6656 - val_loss: 88.7782\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.8949 - val_loss: 130.4203\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.7083 - val_loss: 98.6494\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7172 - val_loss: 103.1715\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.6544 - val_loss: 106.4383\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.5917 - val_loss: 89.3365\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7718 - val_loss: 94.1471\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.6259 - val_loss: 98.4893\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6301 - val_loss: 97.8556\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.8399 - val_loss: 93.6357\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5088 - val_loss: 100.3478\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4415ac30-4eee-4378-cb8b-3abdf4403a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -3.197892390739001 \n",
            "MAE:  7.699712525147856 \n",
            "SD:  9.493224214963043\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "0dde8894-6ca0-4f5d-8712-517356a639a7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89080f5b-48b6-4ad5-9529-897c6283aa1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_24 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_24 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_24 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_25 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_25 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_26 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_26 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_27 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_27 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_28 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_28 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_29 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_29 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_30 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_31 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_32 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,489\n",
            "Trainable params: 5,137\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "scrolled": true,
        "id": "dcXAOqd2CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6432d5b-5945-417c-f857-2b8b833689ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 4s 12ms/step - loss: 12250.8135 - val_loss: 12094.7393\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 11687.1455 - val_loss: 11550.9805\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 10958.0498 - val_loss: 10075.2100\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 10002.9346 - val_loss: 9394.7500\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 8848.9834 - val_loss: 6211.3345\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 7571.1221 - val_loss: 4478.6250\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 6270.5400 - val_loss: 4901.0825\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 4900.8345 - val_loss: 3442.7656\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 3701.6580 - val_loss: 1881.4698\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2692.9729 - val_loss: 2777.8599\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 1875.1788 - val_loss: 2065.4517\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 1253.5829 - val_loss: 639.0089\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 805.9351 - val_loss: 807.6402\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 502.0604 - val_loss: 202.2052\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 315.8048 - val_loss: 389.8730\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 210.0613 - val_loss: 157.5111\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 154.6098 - val_loss: 164.4859\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 128.4417 - val_loss: 128.3218\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 116.4932 - val_loss: 115.2584\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.9238 - val_loss: 245.8875\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.9474 - val_loss: 131.5169\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.3466 - val_loss: 135.9150\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.7891 - val_loss: 183.4499\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.5733 - val_loss: 115.0072\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.9530 - val_loss: 213.2658\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.1957 - val_loss: 114.7717\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.7866 - val_loss: 115.1357\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.1005 - val_loss: 122.6366\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.8771 - val_loss: 114.4938\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 98.2230 - val_loss: 110.7894\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.5395 - val_loss: 164.7401\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.7920 - val_loss: 112.0010\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.0507 - val_loss: 115.6973\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.2999 - val_loss: 126.0367\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.7004 - val_loss: 124.3936\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.7169 - val_loss: 116.6746\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 94.1380 - val_loss: 134.8672\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.6182 - val_loss: 115.6680\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.8568 - val_loss: 144.5358\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.0786 - val_loss: 148.9109\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.2406 - val_loss: 123.2873\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.6015 - val_loss: 113.5332\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.5785 - val_loss: 235.2274\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.7889 - val_loss: 115.1743\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.2329 - val_loss: 123.9742\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.8243 - val_loss: 114.1468\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 89.2051 - val_loss: 109.4340\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 88.6787 - val_loss: 110.2774\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 87.8193 - val_loss: 143.7507\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.7379 - val_loss: 102.2172\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.3353 - val_loss: 131.9983\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.2225 - val_loss: 132.5723\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 87.0305 - val_loss: 148.9485\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.3325 - val_loss: 213.9656\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.6834 - val_loss: 105.0034\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.5643 - val_loss: 114.5900\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 84.9858 - val_loss: 101.2596\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.5579 - val_loss: 110.0724\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 84.5442 - val_loss: 150.0665\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 84.4305 - val_loss: 117.2518\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.7925 - val_loss: 103.7510\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.5023 - val_loss: 135.7713\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 83.8352 - val_loss: 116.9977\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.1549 - val_loss: 104.5945\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.8579 - val_loss: 114.5986\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.3418 - val_loss: 138.3843\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 82.3913 - val_loss: 115.0384\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.0283 - val_loss: 102.7775\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 81.7546 - val_loss: 107.2020\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 81.5270 - val_loss: 101.8375\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.4318 - val_loss: 97.0036\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.8347 - val_loss: 108.0840\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 80.7969 - val_loss: 142.9922\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.0078 - val_loss: 97.5891\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 80.8742 - val_loss: 510.0306\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 80.8216 - val_loss: 110.1869\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 80.1632 - val_loss: 98.1176\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.8695 - val_loss: 129.1323\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.8029 - val_loss: 138.3484\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.6013 - val_loss: 99.4085\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 79.6712 - val_loss: 101.3810\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 79.4368 - val_loss: 162.0160\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.7431 - val_loss: 140.1644\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 79.1631 - val_loss: 99.2278\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.1401 - val_loss: 98.6949\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 78.4538 - val_loss: 111.6119\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 78.4382 - val_loss: 122.1466\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.3314 - val_loss: 135.4520\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.1701 - val_loss: 107.0436\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.1089 - val_loss: 104.9378\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.7188 - val_loss: 162.6559\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 77.5038 - val_loss: 105.3547\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 77.5252 - val_loss: 100.4044\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 77.4233 - val_loss: 94.4635\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 77.1848 - val_loss: 91.8101\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.8938 - val_loss: 102.3382\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 77.0262 - val_loss: 103.4068\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 76.5379 - val_loss: 103.9095\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.6466 - val_loss: 118.9050\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.5358 - val_loss: 113.7211\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.1597 - val_loss: 104.0599\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.3108 - val_loss: 121.7724\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.2429 - val_loss: 97.8340\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.5783 - val_loss: 99.8493\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 75.7349 - val_loss: 98.5577\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.7624 - val_loss: 122.6965\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 75.5764 - val_loss: 94.1077\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.1905 - val_loss: 104.4642\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.0100 - val_loss: 105.9478\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.9375 - val_loss: 91.3753\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.6666 - val_loss: 94.7699\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.0941 - val_loss: 98.1465\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.9622 - val_loss: 97.2811\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.1726 - val_loss: 100.1069\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.4901 - val_loss: 99.0073\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.5363 - val_loss: 96.6693\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.6968 - val_loss: 107.1327\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.9356 - val_loss: 97.6197\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.1493 - val_loss: 96.1066\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.9660 - val_loss: 98.6309\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.0711 - val_loss: 103.7386\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5762 - val_loss: 129.6197\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7946 - val_loss: 101.7151\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7738 - val_loss: 107.1302\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8941 - val_loss: 96.3802\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3125 - val_loss: 97.4394\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2469 - val_loss: 134.2605\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.6041 - val_loss: 96.6231\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9927 - val_loss: 102.1533\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.6272 - val_loss: 98.2113\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4114 - val_loss: 197.6258\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3689 - val_loss: 104.2243\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6115 - val_loss: 103.8204\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.8387 - val_loss: 97.5423\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0055 - val_loss: 99.6478\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6013 - val_loss: 98.8002\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4817 - val_loss: 92.1623\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.3080 - val_loss: 92.5472\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.3610 - val_loss: 102.3411\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4378 - val_loss: 112.2900\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.1350 - val_loss: 110.1339\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8944 - val_loss: 134.4538\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.2617 - val_loss: 106.5194\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.1462 - val_loss: 95.4492\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8251 - val_loss: 101.2498\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.7119 - val_loss: 95.3012\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8507 - val_loss: 115.3830\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.1655 - val_loss: 99.6702\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6104 - val_loss: 99.7964\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8007 - val_loss: 96.0345\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.7426 - val_loss: 106.2100\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.7423 - val_loss: 105.0294\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 71.6068 - val_loss: 129.5980\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.4282 - val_loss: 90.9442\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.2734 - val_loss: 95.1641\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.1775 - val_loss: 93.7463\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.3532 - val_loss: 88.9226\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8040 - val_loss: 146.1475\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.2611 - val_loss: 95.8649\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.2843 - val_loss: 96.7446\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.1943 - val_loss: 97.8364\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.3599 - val_loss: 99.9869\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.1421 - val_loss: 103.5526\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.0060 - val_loss: 88.8886\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.9089 - val_loss: 94.3186\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.1266 - val_loss: 94.2056\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.6926 - val_loss: 96.3908\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.8852 - val_loss: 93.1297\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.6490 - val_loss: 111.3719\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.7407 - val_loss: 101.0969\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.5451 - val_loss: 98.5107\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.4554 - val_loss: 151.1252\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.6462 - val_loss: 96.7376\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.9433 - val_loss: 106.9793\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.1937 - val_loss: 116.8394\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.8189 - val_loss: 143.1328\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.9744 - val_loss: 96.4524\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.3513 - val_loss: 113.6204\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.5197 - val_loss: 115.1861\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 70.4142 - val_loss: 139.5424\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.4518 - val_loss: 104.1215\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.2859 - val_loss: 88.1975\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.1440 - val_loss: 153.1586\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 70.2335 - val_loss: 99.2586\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.8816 - val_loss: 115.9044\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.8290 - val_loss: 89.7144\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.8641 - val_loss: 107.1875\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.8583 - val_loss: 100.2055\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.7953 - val_loss: 120.6937\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.8337 - val_loss: 93.6625\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.9551 - val_loss: 103.0018\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.8051 - val_loss: 116.7753\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.6387 - val_loss: 96.1748\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.5298 - val_loss: 88.9130\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.5311 - val_loss: 99.2901\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.7020 - val_loss: 94.7016\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.7251 - val_loss: 112.9455\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.4910 - val_loss: 90.3004\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.4674 - val_loss: 94.2003\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.4508 - val_loss: 116.1142\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.3840 - val_loss: 133.8936\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.1870 - val_loss: 88.7756\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.0421 - val_loss: 120.4407\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.3433 - val_loss: 97.0774\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.2665 - val_loss: 96.4590\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.3864 - val_loss: 108.5544\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.2435 - val_loss: 89.0910\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.2398 - val_loss: 114.0548\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.1561 - val_loss: 88.0740\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.2745 - val_loss: 90.7068\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.3155 - val_loss: 94.2619\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.2472 - val_loss: 101.5394\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 69.1618 - val_loss: 111.1446\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.9897 - val_loss: 88.1280\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.9883 - val_loss: 91.8976\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.9637 - val_loss: 94.0824\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.6792 - val_loss: 103.0060\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.8079 - val_loss: 97.2938\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5928 - val_loss: 87.9951\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.6469 - val_loss: 95.8658\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.7094 - val_loss: 91.0506\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.5473 - val_loss: 92.7525\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.7346 - val_loss: 97.8813\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.7288 - val_loss: 121.6264\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.9006 - val_loss: 107.1834\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.7779 - val_loss: 95.8271\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.6446 - val_loss: 89.6998\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5947 - val_loss: 107.9813\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5023 - val_loss: 110.9543\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.4563 - val_loss: 95.3667\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.6691 - val_loss: 87.7795\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.5528 - val_loss: 85.9482\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3718 - val_loss: 86.0649\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.1220 - val_loss: 94.4453\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3753 - val_loss: 96.9852\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2734 - val_loss: 90.6936\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0481 - val_loss: 105.9423\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.1997 - val_loss: 92.5001\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3492 - val_loss: 96.0199\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3631 - val_loss: 93.8988\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2365 - val_loss: 88.1007\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0366 - val_loss: 138.0874\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2881 - val_loss: 89.9793\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3087 - val_loss: 89.1806\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.3687 - val_loss: 100.2250\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.1677 - val_loss: 84.6941\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0599 - val_loss: 92.2323\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.1226 - val_loss: 99.2140\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.1325 - val_loss: 90.3257\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.2517 - val_loss: 96.0220\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.1792 - val_loss: 117.9651\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.8418 - val_loss: 114.7533\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.8506 - val_loss: 87.2392\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9683 - val_loss: 92.8259\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0845 - val_loss: 116.0906\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 67.9034 - val_loss: 103.7685\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7640 - val_loss: 93.2765\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0226 - val_loss: 106.0014\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.7997 - val_loss: 98.9352\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5365 - val_loss: 90.4651\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.0696 - val_loss: 90.5066\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9075 - val_loss: 98.9815\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.8126 - val_loss: 94.7220\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.9572 - val_loss: 94.5613\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.4372 - val_loss: 93.3450\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5270 - val_loss: 92.8780\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6676 - val_loss: 94.1216\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6919 - val_loss: 86.6654\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6666 - val_loss: 96.8264\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.8653 - val_loss: 92.0579\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5406 - val_loss: 117.3151\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.4212 - val_loss: 96.9373\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5928 - val_loss: 94.8836\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.8762 - val_loss: 86.4076\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6067 - val_loss: 87.7760\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 67.5201 - val_loss: 94.7233\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5141 - val_loss: 87.9408\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3179 - val_loss: 107.4480\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6702 - val_loss: 93.6197\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2984 - val_loss: 87.0955\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3725 - val_loss: 92.6245\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3170 - val_loss: 93.4497\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1789 - val_loss: 106.9745\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.5855 - val_loss: 104.3547\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3867 - val_loss: 95.9719\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.4024 - val_loss: 88.9338\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3877 - val_loss: 105.3480\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3525 - val_loss: 88.1403\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2043 - val_loss: 85.9673\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2742 - val_loss: 85.2339\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1398 - val_loss: 126.1690\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1687 - val_loss: 88.1442\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1894 - val_loss: 85.8393\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.3256 - val_loss: 128.7643\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1511 - val_loss: 102.2899\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1785 - val_loss: 168.6122\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0838 - val_loss: 106.4955\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1025 - val_loss: 91.3156\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.2637 - val_loss: 132.3531\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1553 - val_loss: 89.8699\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1685 - val_loss: 91.1512\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1627 - val_loss: 195.6734\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.1216 - val_loss: 112.8413\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0746 - val_loss: 89.7209\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7704 - val_loss: 92.9783\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7583 - val_loss: 98.7617\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8900 - val_loss: 94.6072\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8936 - val_loss: 105.9139\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0403 - val_loss: 88.5020\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8633 - val_loss: 86.3562\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9489 - val_loss: 94.7352\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6316 - val_loss: 93.9772\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6803 - val_loss: 93.6479\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5684 - val_loss: 92.7155\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6497 - val_loss: 98.7781\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8713 - val_loss: 90.2605\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8648 - val_loss: 100.7407\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8805 - val_loss: 94.5857\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.0332 - val_loss: 92.8311\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7755 - val_loss: 127.9191\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7709 - val_loss: 128.1800\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6261 - val_loss: 91.5756\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 66.7211 - val_loss: 95.8662\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.7560 - val_loss: 109.4450\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5540 - val_loss: 100.9166\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6317 - val_loss: 101.5269\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6859 - val_loss: 133.1160\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.7473 - val_loss: 92.9149\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5664 - val_loss: 105.9530\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5208 - val_loss: 99.3754\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5293 - val_loss: 118.1003\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5431 - val_loss: 91.0685\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5489 - val_loss: 89.8112\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4470 - val_loss: 100.6152\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5677 - val_loss: 119.0258\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5066 - val_loss: 95.3063\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.8384 - val_loss: 92.4649\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.6547 - val_loss: 150.6726\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5378 - val_loss: 90.4657\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5273 - val_loss: 86.3327\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2057 - val_loss: 88.7254\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3427 - val_loss: 85.8218\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2833 - val_loss: 90.2512\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3479 - val_loss: 97.5442\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4060 - val_loss: 88.2618\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4791 - val_loss: 103.1459\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2170 - val_loss: 87.6836\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 66.3762 - val_loss: 94.0136\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1479 - val_loss: 95.0681\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2053 - val_loss: 105.9522\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1702 - val_loss: 119.5141\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4662 - val_loss: 94.8098\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2208 - val_loss: 101.9637\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2210 - val_loss: 87.5491\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.4427 - val_loss: 93.1630\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0206 - val_loss: 96.0440\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.5064 - val_loss: 89.1394\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2371 - val_loss: 88.6885\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 66.3821 - val_loss: 86.9613\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1534 - val_loss: 131.0969\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1137 - val_loss: 87.8173\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1600 - val_loss: 112.6386\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1329 - val_loss: 132.1365\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9390 - val_loss: 90.3154\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1686 - val_loss: 104.9877\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0785 - val_loss: 96.3365\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1131 - val_loss: 93.5970\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.2084 - val_loss: 91.5662\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.3472 - val_loss: 111.2505\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0211 - val_loss: 87.8637\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8766 - val_loss: 117.9810\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0310 - val_loss: 91.6129\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8881 - val_loss: 152.5431\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9797 - val_loss: 111.7807\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1900 - val_loss: 107.8226\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7549 - val_loss: 101.6841\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8872 - val_loss: 102.7240\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9068 - val_loss: 93.6142\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.9623 - val_loss: 86.3328\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8293 - val_loss: 110.3695\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9078 - val_loss: 94.5664\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1334 - val_loss: 90.7918\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7977 - val_loss: 95.0022\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9652 - val_loss: 94.3199\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7967 - val_loss: 87.8514\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9065 - val_loss: 129.0456\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9191 - val_loss: 107.3205\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9642 - val_loss: 103.7785\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8624 - val_loss: 110.4439\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8377 - val_loss: 147.0678\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9451 - val_loss: 88.4265\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8389 - val_loss: 90.6803\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0734 - val_loss: 96.5722\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.6541 - val_loss: 83.4993\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6999 - val_loss: 91.7464\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.8850 - val_loss: 87.5000\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.9360 - val_loss: 105.4006\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.0883 - val_loss: 140.0189\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.7196 - val_loss: 113.7694\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7208 - val_loss: 126.8028\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7475 - val_loss: 87.6418\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6105 - val_loss: 97.5711\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6624 - val_loss: 115.2227\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.1257 - val_loss: 90.2420\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7221 - val_loss: 89.2429\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5190 - val_loss: 99.1652\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5986 - val_loss: 100.7996\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5365 - val_loss: 92.7668\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.7105 - val_loss: 128.4600\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7765 - val_loss: 85.2179\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5563 - val_loss: 94.8502\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5380 - val_loss: 103.2117\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.6602 - val_loss: 91.9972\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5824 - val_loss: 125.2057\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6120 - val_loss: 87.1291\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6206 - val_loss: 88.3057\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5093 - val_loss: 88.5332\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.6795 - val_loss: 91.1084\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.3915 - val_loss: 87.7869\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4551 - val_loss: 83.7456\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7939 - val_loss: 88.8186\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3302 - val_loss: 93.1625\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5450 - val_loss: 86.7702\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4193 - val_loss: 89.7662\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6060 - val_loss: 97.4853\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5134 - val_loss: 90.2779\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6528 - val_loss: 100.3520\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4188 - val_loss: 106.1478\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3291 - val_loss: 88.2281\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.7380 - val_loss: 95.1552\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5233 - val_loss: 105.3029\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3694 - val_loss: 89.5481\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4591 - val_loss: 108.7243\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4394 - val_loss: 92.4434\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5843 - val_loss: 109.9080\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.5156 - val_loss: 107.1023\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4859 - val_loss: 89.8174\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4212 - val_loss: 92.5813\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.4510 - val_loss: 90.6818\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4214 - val_loss: 93.0878\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1959 - val_loss: 98.5959\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3508 - val_loss: 95.7225\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3027 - val_loss: 102.4405\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3059 - val_loss: 93.6217\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.6589 - val_loss: 92.1371\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3080 - val_loss: 86.9362\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3160 - val_loss: 94.4417\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3229 - val_loss: 83.9421\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3643 - val_loss: 93.5092\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1142 - val_loss: 99.5766\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1195 - val_loss: 91.8481\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3169 - val_loss: 94.7760\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.2905 - val_loss: 100.1935\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.2610 - val_loss: 114.3535\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3660 - val_loss: 90.9142\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2964 - val_loss: 90.8716\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1586 - val_loss: 109.8663\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1376 - val_loss: 92.1075\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3817 - val_loss: 90.5508\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.3957 - val_loss: 90.9313\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3014 - val_loss: 123.7785\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1470 - val_loss: 85.7122\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4107 - val_loss: 89.5441\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1406 - val_loss: 89.3121\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0458 - val_loss: 106.2149\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1827 - val_loss: 102.4638\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.4127 - val_loss: 89.6386\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.2730 - val_loss: 85.7903\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2729 - val_loss: 92.9437\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1294 - val_loss: 93.1016\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.3330 - val_loss: 88.0010\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2158 - val_loss: 91.9059\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0588 - val_loss: 89.6014\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 64.8883 - val_loss: 95.4492\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0648 - val_loss: 104.3939\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1564 - val_loss: 87.9298\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 65.0601 - val_loss: 101.1842\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 64.9846 - val_loss: 134.7088\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0578 - val_loss: 117.4194\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1835 - val_loss: 87.7579\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1097 - val_loss: 91.6097\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 64.8496 - val_loss: 89.9811\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1095 - val_loss: 92.4119\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1547 - val_loss: 96.8587\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0750 - val_loss: 95.0462\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1610 - val_loss: 92.6126\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0835 - val_loss: 88.9307\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 64.9244 - val_loss: 85.9996\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 64.9421 - val_loss: 94.5163\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0971 - val_loss: 91.1440\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0294 - val_loss: 96.8351\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1459 - val_loss: 107.5818\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.1060 - val_loss: 97.7075\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.2088 - val_loss: 89.2586\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0530 - val_loss: 87.2217\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 64.9939 - val_loss: 100.5700\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0521 - val_loss: 95.8247\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.0055 - val_loss: 89.4171\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 64.6954 - val_loss: 91.4089\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 65.1261 - val_loss: 90.2510\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "696v_fuFCTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922f5a96-62e2-43c0-b0f4-da63e5d7e995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  1.866154210521291 \n",
            "MAE:  7.079120760240547 \n",
            "SD:  9.314957256948327\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "aaef913d-f80c-423a-c49d-63cabd432bb8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "mdZF2osWCUQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "849a2992-a0c7-48c3-cdeb-5c4f69437f0b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -2.4061058251699072 \n",
            "Ensemble_std:  9.542860752624506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174895d6-01f7-4f58-c7c7-d05c6d611f08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_36 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_33 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_33 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_34 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_35 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_36 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_37 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_38 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_38 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_39 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_39 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_40 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_40 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_41 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_41 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_42 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_42 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_43 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_43 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,489\n",
            "Trainable params: 5,137\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f897bd47-ab7e-4601-8405-7de57ead0ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 4s 12ms/step - loss: 3734.6724 - val_loss: 3657.0261\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 3493.9980 - val_loss: 3364.2231\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 3216.6719 - val_loss: 2971.2102\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 2840.4749 - val_loss: 2504.4546\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 2363.9355 - val_loss: 2731.1514\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1881.8796 - val_loss: 1777.2976\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 1435.4716 - val_loss: 459.0269\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1043.3340 - val_loss: 888.0333\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 723.7893 - val_loss: 340.6480\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 474.2471 - val_loss: 379.7660\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 296.8672 - val_loss: 260.5323\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 183.1230 - val_loss: 160.9640\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.7420 - val_loss: 229.7869\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 77.3674 - val_loss: 58.8236\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 59.4208 - val_loss: 160.3038\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.6618 - val_loss: 54.9803\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 46.1075 - val_loss: 48.8723\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 43.9120 - val_loss: 44.3975\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 42.2656 - val_loss: 47.8599\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 41.3966 - val_loss: 50.6343\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 40.8762 - val_loss: 43.9788\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 39.6453 - val_loss: 44.7814\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 38.9148 - val_loss: 45.4590\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 38.2870 - val_loss: 44.1828\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 37.8633 - val_loss: 43.2894\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 37.3391 - val_loss: 44.5353\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 36.9847 - val_loss: 40.2947\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 36.4608 - val_loss: 41.8199\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 36.1759 - val_loss: 42.8490\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 35.8639 - val_loss: 39.6451\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 35.6141 - val_loss: 47.4067\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 35.2069 - val_loss: 52.9265\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.1063 - val_loss: 51.4150\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 34.7704 - val_loss: 40.2198\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.5508 - val_loss: 40.5196\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 34.2528 - val_loss: 58.1444\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.9644 - val_loss: 41.5452\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.7238 - val_loss: 44.7120\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 33.4581 - val_loss: 53.2303\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 33.4011 - val_loss: 91.7370\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.4156 - val_loss: 42.8975\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 32.9090 - val_loss: 40.9935\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5108 - val_loss: 38.4481\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 32.4275 - val_loss: 44.2631\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 32.3511 - val_loss: 41.6479\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 32.2168 - val_loss: 41.3656\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 31.9858 - val_loss: 40.4059\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 31.7717 - val_loss: 42.0148\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 32.0449 - val_loss: 49.3998\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 31.6240 - val_loss: 40.4501\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6051 - val_loss: 41.5087\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3175 - val_loss: 41.8427\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 31.3159 - val_loss: 42.3100\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0508 - val_loss: 38.8072\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 31.2298 - val_loss: 38.9929\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.9380 - val_loss: 38.5896\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.7599 - val_loss: 46.7765\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.6031 - val_loss: 37.0463\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.5831 - val_loss: 42.2810\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.4696 - val_loss: 43.5305\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.3406 - val_loss: 46.9209\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.1820 - val_loss: 46.9090\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.0955 - val_loss: 39.4911\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.1479 - val_loss: 38.4528\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.0451 - val_loss: 42.5338\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.9769 - val_loss: 39.2466\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.9118 - val_loss: 42.6619\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.7855 - val_loss: 38.5263\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.7050 - val_loss: 37.4971\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.6031 - val_loss: 55.1690\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6091 - val_loss: 34.6245\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.5386 - val_loss: 51.3754\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.4421 - val_loss: 40.1996\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.3402 - val_loss: 38.8047\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.4786 - val_loss: 56.3607\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.5697 - val_loss: 44.1799\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.3822 - val_loss: 34.0033\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.3913 - val_loss: 37.7040\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.1804 - val_loss: 45.2217\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.1525 - val_loss: 36.3999\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.1786 - val_loss: 39.9276\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.2439 - val_loss: 39.1789\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.9810 - val_loss: 39.1492\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9693 - val_loss: 43.7537\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.8883 - val_loss: 43.7130\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.9235 - val_loss: 37.6798\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.7377 - val_loss: 40.7978\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.7028 - val_loss: 40.9258\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.6731 - val_loss: 35.2236\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.7422 - val_loss: 41.1050\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.5613 - val_loss: 37.9484\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.5510 - val_loss: 42.3217\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4561 - val_loss: 33.6746\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5912 - val_loss: 40.9367\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5550 - val_loss: 37.4725\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.3733 - val_loss: 36.2128\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3608 - val_loss: 40.7084\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3181 - val_loss: 33.9833\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3322 - val_loss: 40.2052\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.2254 - val_loss: 41.0004\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2910 - val_loss: 43.9006\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3588 - val_loss: 35.9136\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.2177 - val_loss: 39.0933\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.2272 - val_loss: 34.8281\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2452 - val_loss: 36.9655\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2011 - val_loss: 50.5424\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.1395 - val_loss: 36.0866\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.1666 - val_loss: 41.6458\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0155 - val_loss: 34.0102\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0196 - val_loss: 42.2815\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.0562 - val_loss: 41.5517\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.0330 - val_loss: 35.8299\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0109 - val_loss: 34.3395\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.9630 - val_loss: 51.0660\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.8153 - val_loss: 35.5508\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8030 - val_loss: 41.8844\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8208 - val_loss: 43.5781\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.8500 - val_loss: 35.0506\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.7513 - val_loss: 34.4360\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.7886 - val_loss: 35.6208\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.7420 - val_loss: 34.7507\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.7365 - val_loss: 39.4388\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.7034 - val_loss: 36.7690\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6450 - val_loss: 40.2605\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7086 - val_loss: 36.9967\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.6701 - val_loss: 35.1755\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7073 - val_loss: 37.2758\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7984 - val_loss: 35.5634\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5801 - val_loss: 40.0407\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.5042 - val_loss: 52.1916\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4590 - val_loss: 36.1839\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4567 - val_loss: 34.4672\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.4834 - val_loss: 39.9161\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4061 - val_loss: 35.0111\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.3809 - val_loss: 36.0627\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3896 - val_loss: 41.4309\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 27.3140 - val_loss: 33.1922\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3345 - val_loss: 38.1905\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2976 - val_loss: 38.0302\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.3294 - val_loss: 33.2004\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.2952 - val_loss: 36.9726\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2995 - val_loss: 48.4485\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3500 - val_loss: 36.3575\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2866 - val_loss: 32.6075\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2355 - val_loss: 32.6221\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2292 - val_loss: 46.0720\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1461 - val_loss: 32.8653\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1635 - val_loss: 35.4998\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1844 - val_loss: 33.9157\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2454 - val_loss: 34.4898\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1229 - val_loss: 43.9853\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1130 - val_loss: 38.2566\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0832 - val_loss: 34.6334\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1512 - val_loss: 36.3526\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0149 - val_loss: 35.1463\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0003 - val_loss: 33.0791\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0533 - val_loss: 33.5567\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.9791 - val_loss: 36.6138\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9865 - val_loss: 34.8734\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.0381 - val_loss: 36.9335\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9987 - val_loss: 38.9280\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8723 - val_loss: 39.0796\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9514 - val_loss: 44.5367\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9008 - val_loss: 33.7155\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.8535 - val_loss: 40.5964\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8890 - val_loss: 35.7969\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8442 - val_loss: 36.5562\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8567 - val_loss: 39.2834\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.7790 - val_loss: 35.6295\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8458 - val_loss: 36.6420\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7339 - val_loss: 34.3116\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.7974 - val_loss: 42.3580\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.7756 - val_loss: 34.2876\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8246 - val_loss: 37.7845\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7812 - val_loss: 35.3471\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7462 - val_loss: 35.6341\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6596 - val_loss: 36.7847\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6796 - val_loss: 33.9482\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6212 - val_loss: 34.3846\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.6550 - val_loss: 41.8184\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.6736 - val_loss: 38.6892\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5888 - val_loss: 37.1194\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.5759 - val_loss: 35.9944\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.6100 - val_loss: 36.3991\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5731 - val_loss: 38.5914\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5778 - val_loss: 34.8828\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.6262 - val_loss: 34.3406\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5707 - val_loss: 32.8418\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5644 - val_loss: 51.3424\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6401 - val_loss: 37.9765\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5413 - val_loss: 32.9335\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5056 - val_loss: 33.4505\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5760 - val_loss: 41.1397\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4356 - val_loss: 47.3619\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.5529 - val_loss: 34.7877\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.4225 - val_loss: 35.8655\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5154 - val_loss: 38.6259\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3634 - val_loss: 44.3212\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.4432 - val_loss: 46.4289\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3963 - val_loss: 36.8937\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3538 - val_loss: 34.0608\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3994 - val_loss: 35.4997\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.3232 - val_loss: 32.1468\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.3422 - val_loss: 33.9216\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3905 - val_loss: 37.0173\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3396 - val_loss: 33.9500\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.3672 - val_loss: 36.3290\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2937 - val_loss: 33.7527\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3526 - val_loss: 37.1740\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3918 - val_loss: 36.0004\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.3040 - val_loss: 37.0261\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3144 - val_loss: 33.8457\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2510 - val_loss: 43.2451\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2914 - val_loss: 35.5212\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2253 - val_loss: 35.7877\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1996 - val_loss: 41.9703\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.2209 - val_loss: 39.4332\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.3195 - val_loss: 37.5409\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.2188 - val_loss: 37.5921\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.2386 - val_loss: 36.3151\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2976 - val_loss: 36.7572\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.1742 - val_loss: 34.3173\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2033 - val_loss: 36.1003\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.2453 - val_loss: 34.3147\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2699 - val_loss: 34.2482\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.2010 - val_loss: 37.4841\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2596 - val_loss: 55.8536\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3515 - val_loss: 36.6347\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3212 - val_loss: 33.2420\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1322 - val_loss: 48.1256\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1103 - val_loss: 39.9224\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1022 - val_loss: 35.4213\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0848 - val_loss: 36.8450\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1115 - val_loss: 32.3563\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0474 - val_loss: 40.9454\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1491 - val_loss: 36.2423\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0500 - val_loss: 33.4730\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1136 - val_loss: 35.5537\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1240 - val_loss: 42.8813\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0822 - val_loss: 33.4695\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1121 - val_loss: 34.7679\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0611 - val_loss: 38.6571\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.1175 - val_loss: 38.5682\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1239 - val_loss: 38.7458\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0947 - val_loss: 32.7577\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9852 - val_loss: 32.8392\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9917 - val_loss: 32.0086\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0000 - val_loss: 33.2730\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 25.9519 - val_loss: 34.3565\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9531 - val_loss: 34.6983\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9101 - val_loss: 34.0685\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8999 - val_loss: 35.1214\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9597 - val_loss: 35.5677\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9461 - val_loss: 34.6974\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9367 - val_loss: 34.1597\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9661 - val_loss: 37.3903\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9690 - val_loss: 32.5270\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9693 - val_loss: 33.1764\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9388 - val_loss: 37.6869\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9569 - val_loss: 36.4225\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8957 - val_loss: 33.4982\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8492 - val_loss: 36.9117\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0092 - val_loss: 32.3436\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9334 - val_loss: 34.3503\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8781 - val_loss: 35.4366\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9377 - val_loss: 41.0186\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9133 - val_loss: 34.4737\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8559 - val_loss: 33.2345\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8913 - val_loss: 36.0786\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8128 - val_loss: 33.8626\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8659 - val_loss: 38.2180\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8102 - val_loss: 33.1300\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9369 - val_loss: 37.8464\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7911 - val_loss: 33.0465\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8471 - val_loss: 32.0420\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 25.8603 - val_loss: 37.5931\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9107 - val_loss: 33.9123\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7737 - val_loss: 40.2075\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8226 - val_loss: 35.7700\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8282 - val_loss: 32.9357\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8187 - val_loss: 39.4382\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 25.7956 - val_loss: 33.3501\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8429 - val_loss: 38.6896\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9000 - val_loss: 34.1395\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7352 - val_loss: 34.7759\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8003 - val_loss: 35.1687\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8129 - val_loss: 33.5184\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7780 - val_loss: 31.9091\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7103 - val_loss: 33.9949\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7131 - val_loss: 33.8099\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7386 - val_loss: 36.4867\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7441 - val_loss: 35.7706\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7029 - val_loss: 33.9429\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7279 - val_loss: 32.4037\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7401 - val_loss: 35.2656\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7232 - val_loss: 34.2711\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7161 - val_loss: 32.4765\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7215 - val_loss: 42.7030\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6599 - val_loss: 33.2074\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7506 - val_loss: 33.7975\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6442 - val_loss: 33.3988\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.7159 - val_loss: 34.9584\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6159 - val_loss: 35.4991\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6055 - val_loss: 42.9166\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6126 - val_loss: 34.6511\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6230 - val_loss: 32.8278\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6565 - val_loss: 33.5227\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6794 - val_loss: 33.1944\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6543 - val_loss: 41.8922\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6520 - val_loss: 32.9076\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6171 - val_loss: 32.6509\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5238 - val_loss: 36.2261\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6609 - val_loss: 34.3156\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5804 - val_loss: 33.6375\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6541 - val_loss: 38.3860\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5812 - val_loss: 33.8341\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6325 - val_loss: 34.7226\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6645 - val_loss: 39.7222\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7070 - val_loss: 34.8920\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6119 - val_loss: 36.1522\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6013 - val_loss: 41.6032\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4759 - val_loss: 32.2440\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6510 - val_loss: 36.4056\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6261 - val_loss: 33.6452\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5637 - val_loss: 36.0943\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6471 - val_loss: 38.5515\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5750 - val_loss: 34.7836\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5523 - val_loss: 36.7901\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5684 - val_loss: 34.2355\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6022 - val_loss: 32.8233\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6101 - val_loss: 39.4832\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5521 - val_loss: 41.0359\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6216 - val_loss: 31.2372\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5944 - val_loss: 38.2451\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5360 - val_loss: 38.7138\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5480 - val_loss: 37.2993\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5220 - val_loss: 35.9294\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5459 - val_loss: 32.4902\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4922 - val_loss: 36.8688\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4982 - val_loss: 34.0330\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5938 - val_loss: 33.3819\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5088 - val_loss: 36.9785\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4501 - val_loss: 35.0424\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4503 - val_loss: 33.6691\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4861 - val_loss: 38.1413\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5313 - val_loss: 33.0761\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5242 - val_loss: 35.1960\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4585 - val_loss: 38.4689\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5122 - val_loss: 34.0865\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4579 - val_loss: 36.8932\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5515 - val_loss: 35.7759\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4640 - val_loss: 32.5450\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4351 - val_loss: 35.6001\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4343 - val_loss: 34.0476\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4550 - val_loss: 33.7787\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5072 - val_loss: 33.1352\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4542 - val_loss: 32.5210\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4283 - val_loss: 33.0684\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4530 - val_loss: 33.9300\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4609 - val_loss: 46.3480\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4412 - val_loss: 34.0232\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4668 - val_loss: 32.7644\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4533 - val_loss: 35.3044\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5119 - val_loss: 39.9685\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5146 - val_loss: 33.4339\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4261 - val_loss: 32.8381\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4064 - val_loss: 36.3887\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4398 - val_loss: 32.1548\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4316 - val_loss: 40.4360\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4067 - val_loss: 32.3794\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4056 - val_loss: 46.1427\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4768 - val_loss: 36.2519\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4094 - val_loss: 34.9089\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4136 - val_loss: 34.0335\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5201 - val_loss: 33.4378\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4073 - val_loss: 33.6607\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4228 - val_loss: 38.6189\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3600 - val_loss: 35.3828\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3972 - val_loss: 38.3781\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4153 - val_loss: 34.3500\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2813 - val_loss: 35.2414\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4166 - val_loss: 36.8108\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3855 - val_loss: 43.7479\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3326 - val_loss: 34.8779\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4216 - val_loss: 40.8136\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3479 - val_loss: 36.5130\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4288 - val_loss: 35.5194\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4400 - val_loss: 33.9815\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4209 - val_loss: 37.5803\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3990 - val_loss: 45.4063\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3553 - val_loss: 32.7037\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3333 - val_loss: 32.9489\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2867 - val_loss: 34.0434\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4186 - val_loss: 35.7244\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2990 - val_loss: 34.4859\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3266 - val_loss: 34.1302\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3516 - val_loss: 35.2939\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3564 - val_loss: 34.0159\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3782 - val_loss: 38.4332\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3661 - val_loss: 36.5557\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3606 - val_loss: 34.2836\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3491 - val_loss: 47.2534\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3078 - val_loss: 32.5473\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3403 - val_loss: 36.6838\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2873 - val_loss: 32.0686\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2627 - val_loss: 34.9948\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3584 - val_loss: 36.3732\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2974 - val_loss: 33.3234\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3087 - val_loss: 35.3281\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2854 - val_loss: 32.7455\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3026 - val_loss: 39.0216\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3329 - val_loss: 32.6730\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3621 - val_loss: 33.7848\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3190 - val_loss: 34.5760\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3250 - val_loss: 32.1763\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3243 - val_loss: 33.0633\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.3464 - val_loss: 37.1335\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2645 - val_loss: 37.3875\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3290 - val_loss: 35.1861\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3174 - val_loss: 33.5192\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1916 - val_loss: 38.9523\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2408 - val_loss: 34.0750\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2245 - val_loss: 32.6587\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2857 - val_loss: 31.8031\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2447 - val_loss: 31.9934\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2183 - val_loss: 35.6180\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2641 - val_loss: 33.5686\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2932 - val_loss: 35.0170\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2762 - val_loss: 32.0397\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2513 - val_loss: 32.1557\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2399 - val_loss: 35.8432\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.3145 - val_loss: 38.5873\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2429 - val_loss: 32.4115\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2795 - val_loss: 32.9398\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.1787 - val_loss: 32.8314\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2120 - val_loss: 34.5027\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1892 - val_loss: 33.2711\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2390 - val_loss: 32.7808\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.2079 - val_loss: 35.4220\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.2340 - val_loss: 35.4810\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1706 - val_loss: 34.4923\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1421 - val_loss: 36.5395\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2664 - val_loss: 34.1503\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1816 - val_loss: 37.4539\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2384 - val_loss: 33.9701\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2592 - val_loss: 35.9113\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.1664 - val_loss: 32.7621\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1568 - val_loss: 42.0828\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1587 - val_loss: 36.8581\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1914 - val_loss: 31.6939\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1480 - val_loss: 34.8541\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1631 - val_loss: 33.0103\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2225 - val_loss: 35.1380\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1778 - val_loss: 38.1340\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.1710 - val_loss: 31.9669\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1327 - val_loss: 38.1500\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1845 - val_loss: 33.8512\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1394 - val_loss: 34.3647\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1294 - val_loss: 33.4712\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0953 - val_loss: 32.3959\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.2355 - val_loss: 36.3181\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1357 - val_loss: 37.6481\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0876 - val_loss: 33.3022\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1840 - val_loss: 34.0942\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1161 - val_loss: 33.0903\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1809 - val_loss: 35.3391\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0956 - val_loss: 36.8362\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1215 - val_loss: 34.3449\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0794 - val_loss: 35.7314\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1424 - val_loss: 42.8254\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0842 - val_loss: 37.6161\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1704 - val_loss: 33.2690\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.1359 - val_loss: 36.1455\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.1182 - val_loss: 33.4699\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.0820 - val_loss: 33.9102\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.0461 - val_loss: 33.1645\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1405 - val_loss: 36.9741\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0949 - val_loss: 46.6061\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0669 - val_loss: 35.8410\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0546 - val_loss: 42.0653\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0652 - val_loss: 32.5816\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0931 - val_loss: 35.7931\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0928 - val_loss: 32.7724\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0746 - val_loss: 33.3755\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1162 - val_loss: 35.1567\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.1382 - val_loss: 32.4572\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0581 - val_loss: 33.3517\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0839 - val_loss: 32.1870\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0209 - val_loss: 36.0156\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0063 - val_loss: 35.8939\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0616 - val_loss: 37.0291\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.1219 - val_loss: 33.5281\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0678 - val_loss: 32.4313\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0568 - val_loss: 34.4390\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0998 - val_loss: 33.9874\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.0744 - val_loss: 33.3316\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0808 - val_loss: 33.8048\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0698 - val_loss: 33.9269\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0442 - val_loss: 38.4248\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.0391 - val_loss: 32.4336\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c14de24-9359-4b72-a734-b27908a3bb45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.8134971974868752 \n",
            "MAE:  4.227554743785838 \n",
            "SD:  5.636652629315452\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0cip38xZOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "1c1baf3b-776f-486d-ead0-74e70a722c79"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac1c3a3-e905-45c5-ec09-8c4b298e0188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_48 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_44 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_44 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_45 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_45 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_46 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_46 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_47 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_47 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_48 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_48 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_49 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_49 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_50 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_50 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_51 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_51 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_52 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_52 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_53 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_53 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_54 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_54 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,489\n",
            "Trainable params: 5,137\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35557f8-98a6-4445-af82-e0fee1e3379a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 4s 11ms/step - loss: 3665.3977 - val_loss: 3577.5532\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 3340.2673 - val_loss: 2967.7810\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2895.5776 - val_loss: 2584.0063\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2344.5403 - val_loss: 1748.9192\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 1733.8467 - val_loss: 1175.3387\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 1172.0509 - val_loss: 957.5014\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 703.5297 - val_loss: 666.5156\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 378.2401 - val_loss: 560.3057\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 178.9384 - val_loss: 398.5756\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.2965 - val_loss: 50.6560\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.1239 - val_loss: 52.6387\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 42.2178 - val_loss: 48.8512\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 39.1004 - val_loss: 43.7750\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 37.8647 - val_loss: 44.1338\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.9217 - val_loss: 44.6022\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.3078 - val_loss: 45.7378\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.1533 - val_loss: 43.8588\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 35.4058 - val_loss: 47.4368\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.9432 - val_loss: 39.4227\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.6279 - val_loss: 40.0504\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.3927 - val_loss: 42.9419\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.0569 - val_loss: 41.6003\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 33.6898 - val_loss: 44.4876\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.4793 - val_loss: 42.0247\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 33.1527 - val_loss: 38.2269\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.9307 - val_loss: 49.0528\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6571 - val_loss: 38.8350\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4924 - val_loss: 40.9889\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1379 - val_loss: 41.1573\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 32.0282 - val_loss: 38.5241\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0083 - val_loss: 41.8004\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6348 - val_loss: 48.4166\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5931 - val_loss: 51.1199\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2561 - val_loss: 40.4210\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2052 - val_loss: 46.2310\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 31.0588 - val_loss: 49.5914\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9263 - val_loss: 40.8577\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7190 - val_loss: 40.8674\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.6452 - val_loss: 38.1975\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.4484 - val_loss: 41.2506\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.3872 - val_loss: 55.8472\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.2846 - val_loss: 39.0021\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.1824 - val_loss: 40.6549\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.0783 - val_loss: 35.8621\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.0494 - val_loss: 35.5413\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.0133 - val_loss: 49.2859\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.0029 - val_loss: 45.8788\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.8592 - val_loss: 36.8105\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.7497 - val_loss: 44.6678\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6964 - val_loss: 39.1319\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.5828 - val_loss: 49.2722\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6079 - val_loss: 37.8322\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.4236 - val_loss: 41.5049\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.3932 - val_loss: 54.0781\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.3823 - val_loss: 40.8956\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.3401 - val_loss: 45.9313\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.2024 - val_loss: 39.0582\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.1170 - val_loss: 36.7591\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.0926 - val_loss: 43.8974\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.1054 - val_loss: 37.8208\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9202 - val_loss: 39.6899\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9105 - val_loss: 36.2185\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9203 - val_loss: 39.6488\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.8153 - val_loss: 42.1493\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7339 - val_loss: 40.1603\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.6511 - val_loss: 37.9847\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5343 - val_loss: 38.7322\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.6242 - val_loss: 38.0418\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4904 - val_loss: 36.4544\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5383 - val_loss: 65.4210\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4508 - val_loss: 51.6317\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4869 - val_loss: 36.7760\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3551 - val_loss: 42.4518\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3953 - val_loss: 46.0322\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.3954 - val_loss: 41.8840\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.3017 - val_loss: 38.3400\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2424 - val_loss: 38.1573\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1906 - val_loss: 34.4839\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2334 - val_loss: 39.1565\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2025 - val_loss: 36.0994\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.1326 - val_loss: 35.9978\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.1057 - val_loss: 54.0026\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.0483 - val_loss: 37.4511\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.1215 - val_loss: 39.2648\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.9263 - val_loss: 43.2796\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0176 - val_loss: 34.9939\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9081 - val_loss: 35.8205\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9153 - val_loss: 37.7253\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.9151 - val_loss: 43.3822\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8686 - val_loss: 36.5428\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8391 - val_loss: 43.2708\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.8418 - val_loss: 36.0183\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8375 - val_loss: 37.0229\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8240 - val_loss: 34.9894\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.7259 - val_loss: 40.4815\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7565 - val_loss: 35.1845\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7403 - val_loss: 38.2559\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6816 - val_loss: 37.5197\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7058 - val_loss: 35.6496\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6799 - val_loss: 37.8991\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.6505 - val_loss: 35.3807\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.5134 - val_loss: 35.6214\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5685 - val_loss: 34.7538\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.5452 - val_loss: 37.0218\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5460 - val_loss: 34.8172\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5206 - val_loss: 45.8208\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5182 - val_loss: 41.3567\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4816 - val_loss: 40.1334\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4834 - val_loss: 35.2401\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.4838 - val_loss: 42.4250\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.4957 - val_loss: 43.9254\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4273 - val_loss: 34.5507\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4841 - val_loss: 35.5583\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3544 - val_loss: 39.2836\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3572 - val_loss: 36.9489\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4042 - val_loss: 38.1072\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2515 - val_loss: 36.2278\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2677 - val_loss: 34.3196\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2782 - val_loss: 35.3383\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2466 - val_loss: 34.9589\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3248 - val_loss: 35.1975\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2446 - val_loss: 39.2784\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3205 - val_loss: 34.6516\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2071 - val_loss: 34.9248\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2063 - val_loss: 34.7956\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2771 - val_loss: 34.9269\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2207 - val_loss: 38.5330\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1798 - val_loss: 37.1848\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2288 - val_loss: 40.4281\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0847 - val_loss: 37.4067\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.1139 - val_loss: 35.8825\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.1238 - val_loss: 34.1009\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.1260 - val_loss: 35.6415\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.0911 - val_loss: 39.5314\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0411 - val_loss: 41.5002\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.0116 - val_loss: 36.7577\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0891 - val_loss: 36.4909\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0869 - val_loss: 42.0117\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0081 - val_loss: 34.4321\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.0286 - val_loss: 34.5681\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9965 - val_loss: 54.7056\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0495 - val_loss: 35.3487\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0200 - val_loss: 34.7259\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0350 - val_loss: 37.8387\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9742 - val_loss: 37.6194\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9594 - val_loss: 35.6699\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9899 - val_loss: 40.1925\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0080 - val_loss: 34.5208\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9156 - val_loss: 35.6163\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9708 - val_loss: 47.5344\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8988 - val_loss: 37.0356\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8303 - val_loss: 46.4361\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9128 - val_loss: 38.5315\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.8321 - val_loss: 37.2971\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8326 - val_loss: 38.5673\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.8452 - val_loss: 36.4876\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.9058 - val_loss: 38.8192\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9031 - val_loss: 38.7036\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.8706 - val_loss: 37.1918\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8678 - val_loss: 34.3320\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8535 - val_loss: 42.4144\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8166 - val_loss: 37.5784\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7721 - val_loss: 36.5113\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8410 - val_loss: 37.7121\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9109 - val_loss: 35.3216\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7380 - val_loss: 41.1046\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8020 - val_loss: 35.1235\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7506 - val_loss: 43.6558\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7425 - val_loss: 39.9212\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6872 - val_loss: 35.0900\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.6958 - val_loss: 35.3483\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.7044 - val_loss: 35.3570\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7059 - val_loss: 34.6217\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7825 - val_loss: 38.2249\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6112 - val_loss: 36.7642\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6868 - val_loss: 38.0957\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.7729 - val_loss: 35.2766\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7164 - val_loss: 56.0890\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6486 - val_loss: 53.5265\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6221 - val_loss: 36.0706\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6988 - val_loss: 34.6524\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6896 - val_loss: 38.3592\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5296 - val_loss: 34.4254\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.6409 - val_loss: 35.4306\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6298 - val_loss: 35.1210\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5883 - val_loss: 39.4032\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.5693 - val_loss: 37.9255\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7001 - val_loss: 49.6643\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5880 - val_loss: 42.5434\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4974 - val_loss: 35.6107\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5100 - val_loss: 35.0339\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.5325 - val_loss: 35.2477\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5407 - val_loss: 36.6359\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6050 - val_loss: 34.0940\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5156 - val_loss: 35.0120\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.4821 - val_loss: 36.4209\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.5579 - val_loss: 41.3378\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4890 - val_loss: 33.6820\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4622 - val_loss: 38.0546\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.5446 - val_loss: 38.0819\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4743 - val_loss: 35.1568\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4478 - val_loss: 40.3141\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5233 - val_loss: 34.6580\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5449 - val_loss: 36.0396\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5083 - val_loss: 41.8064\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4288 - val_loss: 37.3242\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5017 - val_loss: 39.3697\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4482 - val_loss: 42.5478\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4904 - val_loss: 39.5486\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.3934 - val_loss: 37.3792\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.4258 - val_loss: 46.5928\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4238 - val_loss: 35.2892\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4275 - val_loss: 35.9552\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4040 - val_loss: 40.2554\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4429 - val_loss: 38.6497\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3720 - val_loss: 37.0883\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3646 - val_loss: 42.1613\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4016 - val_loss: 38.4184\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3950 - val_loss: 38.1131\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3855 - val_loss: 42.4308\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3284 - val_loss: 34.7067\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.3230 - val_loss: 37.8916\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3580 - val_loss: 43.4375\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3287 - val_loss: 35.6465\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3432 - val_loss: 52.5074\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4096 - val_loss: 37.2369\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2928 - val_loss: 35.7739\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2785 - val_loss: 38.1976\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3782 - val_loss: 37.4828\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3204 - val_loss: 41.0473\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2901 - val_loss: 41.0355\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3484 - val_loss: 36.7654\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1604 - val_loss: 34.8106\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2980 - val_loss: 38.7389\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3131 - val_loss: 36.7351\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2652 - val_loss: 36.4961\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2031 - val_loss: 37.6746\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1966 - val_loss: 39.2073\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3062 - val_loss: 37.8086\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2592 - val_loss: 34.8192\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2329 - val_loss: 35.0376\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2618 - val_loss: 36.9648\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2643 - val_loss: 36.8348\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3606 - val_loss: 36.7778\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2695 - val_loss: 36.2496\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2617 - val_loss: 36.6787\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1980 - val_loss: 34.9942\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1848 - val_loss: 38.2320\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2675 - val_loss: 46.1559\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2189 - val_loss: 37.0353\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2034 - val_loss: 40.4955\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1877 - val_loss: 35.6144\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.1593 - val_loss: 38.8442\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1686 - val_loss: 35.0189\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1734 - val_loss: 36.6221\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2090 - val_loss: 41.5529\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1548 - val_loss: 35.9575\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2636 - val_loss: 39.3075\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1429 - val_loss: 36.7695\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1265 - val_loss: 41.1651\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1606 - val_loss: 37.8415\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1839 - val_loss: 37.1983\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0685 - val_loss: 36.7776\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0980 - val_loss: 37.2757\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2015 - val_loss: 36.4143\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1043 - val_loss: 48.8170\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1657 - val_loss: 41.6451\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1868 - val_loss: 40.5253\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1647 - val_loss: 36.4396\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1316 - val_loss: 35.4103\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1016 - val_loss: 37.2113\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1874 - val_loss: 36.3351\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1328 - val_loss: 41.6807\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1088 - val_loss: 36.4488\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0779 - val_loss: 37.8775\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1920 - val_loss: 35.9963\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0504 - val_loss: 35.1860\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0875 - val_loss: 36.3566\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1422 - val_loss: 34.8487\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1474 - val_loss: 37.1535\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1204 - val_loss: 37.4993\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0633 - val_loss: 35.7341\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0566 - val_loss: 37.8156\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1220 - val_loss: 34.9274\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0319 - val_loss: 34.3499\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0293 - val_loss: 35.6578\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1166 - val_loss: 43.4577\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0408 - val_loss: 36.8040\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0648 - val_loss: 35.7310\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0590 - val_loss: 56.5022\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0160 - val_loss: 36.6888\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0555 - val_loss: 38.5071\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0290 - val_loss: 36.3158\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9947 - val_loss: 35.7624\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0359 - val_loss: 37.9419\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9902 - val_loss: 34.6482\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 26.0422 - val_loss: 35.9815\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0195 - val_loss: 37.8814\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9402 - val_loss: 36.4681\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0659 - val_loss: 42.7112\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9653 - val_loss: 37.2339\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9581 - val_loss: 42.3904\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0942 - val_loss: 41.3803\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 25.9787 - val_loss: 36.8006\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9261 - val_loss: 39.4093\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9652 - val_loss: 42.2871\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9548 - val_loss: 43.0971\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9541 - val_loss: 38.5263\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9826 - val_loss: 35.7771\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0279 - val_loss: 35.0348\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0184 - val_loss: 37.6425\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9302 - val_loss: 40.8955\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9387 - val_loss: 43.8546\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9716 - val_loss: 34.3003\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9481 - val_loss: 36.0593\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9282 - val_loss: 42.4728\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0110 - val_loss: 37.0505\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9455 - val_loss: 34.2980\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9089 - val_loss: 36.6402\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9251 - val_loss: 37.2255\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9571 - val_loss: 36.4823\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9587 - val_loss: 38.7190\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9339 - val_loss: 38.0154\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8892 - val_loss: 45.4296\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9173 - val_loss: 37.8935\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8231 - val_loss: 36.7803\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8618 - val_loss: 37.5875\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9157 - val_loss: 36.0689\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8951 - val_loss: 36.2217\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8495 - val_loss: 36.0799\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8783 - val_loss: 36.7158\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8975 - val_loss: 37.0800\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9306 - val_loss: 38.2364\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7938 - val_loss: 36.5131\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8772 - val_loss: 35.7386\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8422 - val_loss: 36.3350\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 3s 19ms/step - loss: 25.8927 - val_loss: 38.1567\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 25.7387 - val_loss: 36.0732\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8966 - val_loss: 38.4032\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8795 - val_loss: 35.7679\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8366 - val_loss: 36.7514\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8027 - val_loss: 37.6375\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8488 - val_loss: 34.9183\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8438 - val_loss: 36.1576\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7527 - val_loss: 36.4525\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8374 - val_loss: 44.5042\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8520 - val_loss: 40.4505\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8409 - val_loss: 39.7429\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7618 - val_loss: 37.0673\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.8384 - val_loss: 35.4209\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7689 - val_loss: 41.7626\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8389 - val_loss: 35.9215\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7640 - val_loss: 39.8560\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7833 - val_loss: 45.3480\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8270 - val_loss: 37.4022\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7724 - val_loss: 48.6290\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8203 - val_loss: 36.3898\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8105 - val_loss: 35.5901\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8227 - val_loss: 39.4258\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7693 - val_loss: 41.4267\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8102 - val_loss: 38.4149\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8024 - val_loss: 37.4569\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7959 - val_loss: 35.2806\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8126 - val_loss: 35.1354\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7547 - val_loss: 37.4531\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7347 - val_loss: 36.9508\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7195 - val_loss: 39.0308\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8132 - val_loss: 36.3644\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7130 - val_loss: 38.1573\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8052 - val_loss: 36.3403\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7850 - val_loss: 36.3030\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8115 - val_loss: 47.4431\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8266 - val_loss: 35.4184\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7379 - val_loss: 36.5747\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7487 - val_loss: 43.6803\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7779 - val_loss: 35.3980\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7238 - val_loss: 37.0508\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7450 - val_loss: 37.8289\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7396 - val_loss: 39.2519\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7581 - val_loss: 48.5631\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6904 - val_loss: 37.2101\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.6703 - val_loss: 36.6496\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 25.6936 - val_loss: 39.8563\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7109 - val_loss: 37.4746\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7619 - val_loss: 39.4524\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6915 - val_loss: 37.8307\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6896 - val_loss: 35.4877\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6998 - val_loss: 37.0396\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7325 - val_loss: 36.7920\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7187 - val_loss: 36.9292\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7227 - val_loss: 35.6814\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7958 - val_loss: 35.6038\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7476 - val_loss: 35.4267\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7558 - val_loss: 39.4281\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6912 - val_loss: 37.0642\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7768 - val_loss: 38.3539\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6585 - val_loss: 34.6332\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7079 - val_loss: 36.8096\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6865 - val_loss: 39.6968\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6684 - val_loss: 38.3182\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6229 - val_loss: 38.8331\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6462 - val_loss: 37.3869\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6225 - val_loss: 35.2062\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6772 - val_loss: 38.2634\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6884 - val_loss: 35.6100\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.6447 - val_loss: 40.7971\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6719 - val_loss: 38.0771\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6279 - val_loss: 38.8042\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.6469 - val_loss: 39.7869\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6658 - val_loss: 35.8232\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6609 - val_loss: 36.2614\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6787 - val_loss: 36.6673\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6386 - val_loss: 37.2293\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7368 - val_loss: 35.7697\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.6150 - val_loss: 35.2575\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6024 - val_loss: 34.5043\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6896 - val_loss: 44.2748\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6893 - val_loss: 34.0097\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6105 - val_loss: 35.3182\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6483 - val_loss: 35.9635\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7235 - val_loss: 34.4536\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6653 - val_loss: 36.1159\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6716 - val_loss: 39.3605\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7404 - val_loss: 36.4169\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6436 - val_loss: 35.1242\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6159 - val_loss: 35.7927\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5910 - val_loss: 38.9132\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6210 - val_loss: 35.8713\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6490 - val_loss: 35.0373\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5960 - val_loss: 40.7064\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6737 - val_loss: 42.0771\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7045 - val_loss: 38.2236\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.6150 - val_loss: 36.0901\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6127 - val_loss: 38.0263\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5987 - val_loss: 40.6644\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.5702 - val_loss: 35.5438\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6120 - val_loss: 35.7653\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6149 - val_loss: 36.9195\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5883 - val_loss: 36.4480\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5770 - val_loss: 38.2634\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6105 - val_loss: 41.5159\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6241 - val_loss: 35.2338\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5535 - val_loss: 34.6774\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5912 - val_loss: 43.5100\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.6147 - val_loss: 38.5257\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5650 - val_loss: 37.1617\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5969 - val_loss: 35.7669\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5933 - val_loss: 41.6339\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6241 - val_loss: 36.6168\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6460 - val_loss: 34.3049\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.5880 - val_loss: 37.9104\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.5260 - val_loss: 35.2122\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5767 - val_loss: 35.8149\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5665 - val_loss: 36.2526\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5369 - val_loss: 37.1847\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6098 - val_loss: 35.2219\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6005 - val_loss: 38.1135\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5417 - val_loss: 35.8057\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4737 - val_loss: 34.7562\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5834 - val_loss: 39.4664\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5577 - val_loss: 36.8030\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.5864 - val_loss: 34.9144\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5237 - val_loss: 37.4434\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.4735 - val_loss: 40.0717\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5732 - val_loss: 39.9846\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5612 - val_loss: 36.1403\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5832 - val_loss: 41.3569\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5524 - val_loss: 35.2658\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.5734 - val_loss: 36.1387\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5694 - val_loss: 36.6697\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5591 - val_loss: 34.9845\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5962 - val_loss: 35.6233\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5536 - val_loss: 47.5580\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.6098 - val_loss: 37.0184\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5553 - val_loss: 38.6848\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5749 - val_loss: 34.7990\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4978 - val_loss: 37.9047\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5660 - val_loss: 34.3593\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4683 - val_loss: 35.6153\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4931 - val_loss: 34.8601\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5063 - val_loss: 37.0495\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.4686 - val_loss: 37.0283\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.4581 - val_loss: 35.7397\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5200 - val_loss: 36.5558\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.4151 - val_loss: 37.4107\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.6344 - val_loss: 36.8251\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.4948 - val_loss: 35.5431\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4848 - val_loss: 36.1219\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5997 - val_loss: 36.7211\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5699 - val_loss: 37.8318\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5082 - val_loss: 35.8729\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4936 - val_loss: 36.4803\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5440 - val_loss: 38.1247\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.5132 - val_loss: 35.8848\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.4972 - val_loss: 34.7828\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5678 - val_loss: 37.9454\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.5062 - val_loss: 36.1784\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.4331 - val_loss: 36.8238\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5366 - val_loss: 36.4914\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.4701 - val_loss: 36.8366\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26494e17-a728-4026-8c91-7c64cd4553aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.3565395818146451 \n",
            "MAE:  4.456756647706848 \n",
            "SD:  6.058837856222443\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "5a825c3d-0da7-4950-9f80-be642148470f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAFBCAYAAAAsfIegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwU5f0H8M83Bwn3LSDwE7BYFAMBgYqIVfCmKp6oeIAgtuKB2qog9aoXYr1aRFFRUFQQpaKggEhB6sFluOQUQRKRAJJAOEKS/f7+eGaY2WST7Ca7mc34eb9e+9qd+5nd2c88+8yxoqogIqLoSfC6AEREfsNgJSKKMgYrEVGUMViJiKKMwUpEFGUMViKiKItZsIpIqogsEZGVIrJWRB6x+rcVkW9FZLOITBWRGlb/FKt7szW8TazKRkQUS7GsseYD6KOqnQGkAzhfRE4FMAbAc6r6OwB7AQyxxh8CYK/V/zlrPCKiaidmwapGntWZbD0UQB8A063+kwD0t15fYnXDGt5XRCRW5SMiipWYtrGKSKKIZADIBjAPwA8AclS10BolE0BL63VLANsBwBqeC6BxLMtHRBQLSbGcuaoWAUgXkQYAZgDoUNl5isgwAMMAoHbt2qd06GBmmZ0NbN8OdG6ShaTjWpY1CyKiMi1fvny3qjat6PQxDVabquaIyAIAPQE0EJEkq1baCkCWNVoWgNYAMkUkCUB9AHtCzGsCgAkA0K1bN122bBkA4F//Au64A/i8/0g0efXJmK8TEfmXiGyrzPSxPCugqVVThYjUBHAOgHUAFgC4whrtRgAfWa9nWt2whn+hEdwhxm6N5T1liMhrsayxtgAwSUQSYQJ8mqp+IiLfA3hPRB4D8B2A163xXwfwlohsBvArgKsjWRiDlYjiRcyCVVVXAegSov8WAD1C9D8M4MqKLu/o+QNMViLyWJW0sVYlBc/QovhVUFCAzMxMHD582OuiEIDU1FS0atUKycnJUZ2vb4KVTQFUHWRmZqJu3bpo06YNeJq2t1QVe/bsQWZmJtq2bRvVefvmXgEMVqoODh8+jMaNGzNU44CIoHHjxjH59cBgJapiDNX4EavPwnfBSkTkNd8Eq00DrLISVUd16tQpddjWrVtx8sknV2FpKsc3wXq0KYBnBRCRxxisRL8xW7duRYcOHTBo0CCccMIJGDhwID7//HP06tUL7du3x5IlS7Bw4UKkp6cjPT0dXbp0wf79+wEAY8eORffu3dGpUyc89NBDpS7j/vvvx7hx4452P/zww3jmmWeQl5eHvn37omvXrkhLS8NHH31U6jxKc/jwYQwePBhpaWno0qULFixYAABYu3YtevTogfT0dHTq1AmbNm3CgQMH0K9fP3Tu3Bknn3wypk6dGvHyKsJ/p1uxKYCqixEjgIyM6M4zPR14/vlyR9u8eTPef/99TJw4Ed27d8c777yDxYsXY+bMmXjiiSdQVFSEcePGoVevXsjLy0Nqairmzp2LTZs2YcmSJVBVXHzxxVi0aBHOOOOMEvMfMGAARowYgeHDhwMApk2bhjlz5iA1NRUzZsxAvXr1sHv3bpx66qm4+OKLIzqING7cOIgIVq9ejfXr1+Pcc8/Fxo0b8fLLL+POO+/EwIEDceTIERQVFWH27Nk49thjMWvWLABAbm5u2MupDN/VWImofG3btkVaWhoSEhLQsWNH9O3bFyKCtLQ0bN26Fb169cLdd9+NF198ETk5OUhKSsLcuXMxd+5cdOnSBV27dsX69euxadOmkPPv0qULsrOz8fPPP2PlypVo2LAhWrduDVXFqFGj0KlTJ5x99tnIysrCzp07Iyr74sWLcd111wEAOnTogOOOOw4bN25Ez5498cQTT2DMmDHYtm0batasibS0NMybNw/33XcfvvzyS9SvX7/S7104fFNjtfF0K6o2wqhZxkpKSsrR1wkJCUe7ExISUFhYiPvvvx/9+vXD7Nmz0atXL8yZMweqipEjR+KWW24JaxlXXnklpk+fjl9++QUDBgwAAEyZMgW7du3C8uXLkZycjDZt2kTtPNJrr70Wf/jDHzBr1ixceOGFeOWVV9CnTx+sWLECs2fPxujRo9G3b188+OCDUVleWXwTrGxjJYqeH374AWlpaUhLS8PSpUuxfv16nHfeefj73/+OgQMHok6dOsjKykJycjKOOeaYkPMYMGAAbr75ZuzevRsLFy4EYH6KH3PMMUhOTsaCBQuwbVvkd+fr3bs3pkyZgj59+mDjxo346aef8Pvf/x5btmxBu3btcMcdd+Cnn37CqlWr0KFDBzRq1AjXXXcdGjRogNdee61S70u4/BesbGMlqrTnn38eCxYsONpUcMEFFyAlJQXr1q1Dz549AZjTo95+++1Sg7Vjx47Yv38/WrZsiRYtWgAABg4ciIsuughpaWno1q0b7BvVR+LWW2/FX/7yF6SlpSEpKQlvvvkmUlJSMG3aNLz11ltITk5G8+bNMWrUKCxduhR/+9vfkJCQgOTkZIwfP77ib0oEJIJbnsYd942uJ00CBg0CfhgwCu3ee8LbghGVYt26dTjxxBO9Lga5hPpMRGS5qnar6Dz9d/CqGu8oiMgffNMUYGMbK1HV2bNnD/r27Vui//z589G4ceT/Bbp69Wpcf/31Qf1SUlLw7bffVriMXvBNsPImLERVr3HjxsiI4rm4aWlpUZ2fV3zXFMBgJSKv+S9Y2RRARB7zXbCyykpEXvNNsNoYq0TkNd8Eq3OBgLflICKjrPur+p3/gpVtrETkMZ5uReQRr+4auHXrVpx//vk49dRT8dVXX6F79+4YPHgwHnroIWRnZ2PKlCk4dOgQ7rzzTgDmf6EWLVqEunXrYuzYsZg2bRry8/Nx6aWX4pFHHim3TKqKe++9F59++ilEBKNHj8aAAQOwY8cODBgwAPv27UNhYSHGjx+P0047DUOGDMGyZcsgIrjppptw1113ReOtqVK+C1YiKl+s78fq9uGHHyIjIwMrV67E7t270b17d5xxxhl45513cN555+GBBx5AUVERDh48iIyMDGRlZWHNmjUAgJycnKp4O6LON8Fq401YqLrw8K6BR+/HCiDk/Vivvvpq3H333Rg4cCAuu+wytGrVKuh+rACQl5eHTZs2lRusixcvxjXXXIPExEQ0a9YMf/zjH7F06VJ0794dN910EwoKCtC/f3+kp6ejXbt22LJlC26//Xb069cP5557bszfi1hgGyvRb1A492N97bXXcOjQIfTq1Qvr168/ej/WjIwMZGRkYPPmzRgyZEiFy3DGGWdg0aJFaNmyJQYNGoTJkyejYcOGWLlyJc4880y8/PLLGDp0aKXX1Qv+C1ZWWIkqzb4f63333Yfu3bsfvR/rxIkTkZeXBwDIyspCdnZ2ufPq3bs3pk6diqKiIuzatQuLFi1Cjx49sG3bNjRr1gw333wzhg4dihUrVmD37t0IBAK4/PLL8dhjj2HFihWxXtWY8E1TANtYiaInGvdjtV166aX4+uuv0blzZ4gInn76aTRv3hyTJk3C2LFjkZycjDp16mDy5MnIysrC4MGDEQiY8yaffPLJmK9rLPjmfqwffABccQWw8qLR6DTzMY9LRhQa78caf3g/1jLwHwSIKF74rimAB6+Iqk6078fqF/4LVlZYiapMtO/H6he+awoginfV+biG38Tqs/BNsNq4zVI8S01NxZ49exiucUBVsWfPHqSmpkZ93mwKIKpCrVq1QmZmJnbt2uV1UQhmR9eqVauozzdmwSoirQFMBtAM5japE1T1BRF5GMDNAOwta5SqzramGQlgCIAiAHeo6pzwl2eeGawUz5KTk9G2bVuvi0ExFssaayGAe1R1hYjUBbBcROZZw55T1WfcI4vISQCuBtARwLEAPheRE1S1KJyF8awAIooXMWtjVdUdqrrCer0fwDoALcuY5BIA76lqvqr+CGAzgB7hLo9/zUJE8aJKDl6JSBsAXQDYfw5+m4isEpGJItLQ6tcSwHbXZJkoO4hDYo2ViLwW82AVkToAPgAwQlX3ARgP4HgA6QB2APhnhPMbJiLLRGSZ+wAA21iJKF7ENFhFJBkmVKeo6ocAoKo7VbVIVQMAXoXzcz8LQGvX5K2sfkFUdYKqdlPVbk2bNnUtyx4egxUhIopAzIJVRATA6wDWqeqzrv4tXKNdCmCN9XomgKtFJEVE2gJoD2BJ+Mszz2wKICKvxfKsgF4ArgewWkTsa95GAbhGRNJhTsHaCuAWAFDVtSIyDcD3MGcUDA/3jACAB6+IKH7ELFhVdTEQsvo4u4xpHgfweKWWyxorEXnMN5e0so2ViOIFg5WIKMoYrEREUea7YCUi8ppvgtXGGisRec03wcrzWIkoXvgvWFljJSKPMViJiKLMd8FKROQ13wSrjTVWIvKab4KVB6+IKF74L1hZYyUij/kuWImIvOabYLWxxkpEXvNNsDptrERE3vJfsDJZichjPgxWNrYSkbd8F6xsDCAir/kmWG2ssRKR13wTrLxAgIjihf+ClS0BROQx/wWrt8UgIvJfsDJZichrvglWG3OViLzmm2BlGysRxQsfBivPCiAib/kwWL0tBxGR74KViMhrvglWG2usROQ13wQrr7wionjBYCUiijL/BSubAojIY74LViIir/kmWG2ssRKR13wTrLwJCxHFC/8FK6+8IiKP+TBYvS0HEVHMglVEWovIAhH5XkTWisidVv9GIjJPRDZZzw2t/iIiL4rIZhFZJSJdI1teLNaCiChysayxFgK4R1VPAnAqgOEichKA+wHMV9X2AOZb3QBwAYD21mMYgPEVWShrrETktZgFq6ruUNUV1uv9ANYBaAngEgCTrNEmAehvvb4EwGQ1vgHQQERahLs8HrwionhRJW2sItIGQBcA3wJopqo7rEG/AGhmvW4JYLtrskyrX5jLMM88eEVEXot5sIpIHQAfABihqvvcw1RVEWElU0SGicgyEVm2a9cuV/9olJaIqPJiGqwikgwTqlNU9UOr9077J771nG31zwLQ2jV5K6tfEFWdoKrdVLVb06ZNSyyTbaxE5LVYnhUgAF4HsE5Vn3UNmgngRuv1jQA+cvW/wTo74FQAua4mgzCWZ555ExYi8lpSDOfdC8D1AFaLSIbVbxSApwBME5EhALYBuMoaNhvAhQA2AzgIYHAkC+N5rEQUL2IWrKq6GCi1+tg3xPgKYHhFl8caKxHFC99decUqKxF5zTfBamONlYi85ptgZRsrEcUL/wUra6xE5DH/BStrrETkMd8FKxGR13wTrDY2BRCR13wTrGwKIKJ44b9gZY2ViDzmv2BljZWIPOa7YCUi8ppvgtXGGisRec03wco2ViKKF/4LVtZYichjvgnWBGtNWGMlIq/5JljtGmuAfyZIRB7zTbA6NVYiIm/5LlgD6ptVIqJqyjcp5AQrmwKIyFu+Cdajbaw8eEVEHvNNsB5tY2WNlYg85rtgZVMAEXmNwUpEFGW+CVanjdU3q0RE1ZRvUshpY/W2HEREvgtW1liJyGu+SSFe0kpE8cJ3wcqWACLymm+CFQASUMRLWonIc75KoQRRNgUQked8FawC5cErIvJcWCkkIrVFJMF6fYKIXCwiybEtWuQSRHm6FRF5Ltzq3SIAqSLSEsBcANcDeDNWhaqoBARYYyUiz4WbQqKqBwFcBuAlVb0SQMfYFatiRHi6FRF5L+xgFZGeAAYCmGX1S4xNkSrO1FgZrETkrXCDdQSAkQBmqOpaEWkHYEHsilUxpo2VwUpE3gorWFV1oaperKpjrINYu1X1jrKmEZGJIpItImtc/R4WkSwRybAeF7qGjRSRzSKyQUTOq9DKiLLGSkSeC/esgHdEpJ6I1AawBsD3IvK3ciZ7E8D5Ifo/p6rp1mO2Nf+TAFwN0257PoCXRCTipgaB8gIBIvJcuCl0kqruA9AfwKcA2sKcGVAqVV0E4Ncw538JgPdUNV9VfwSwGUCPMKc9KkGUl7QSkefCDdZk67zV/gBmqmoBKn5Z/m0isspqKmho9WsJYLtrnEyrX0QSEGCNlYg8F24KvQJgK4DaABaJyHEA9lVgeeMBHA8gHcAOAP+MdAYiMkxElonIsl27dgUNYxsrEcWDcA9evaiqLVX1QjW2ATgr0oWp6k5VLVLVAIBX4fzczwLQ2jVqK6tfqHlMUNVuqtqtadOmQcPYxkpE8SDcg1f1ReRZu6YoIv+Eqb1GRERauDovhTkQBgAzAVwtIiki0hZAewBLIp0/21iJKB4khTneRJgQvMrqvh7AGzBXYoUkIu8COBNAExHJBPAQgDNFJB2mfXYrgFsAwDo3dhqA7wEUAhiuqkWRroy5uxVrrETkrXCD9XhVvdzV/YiIZJQ1gapeE6L362WM/ziAx8MsT0jm7lZsYyUib4VbvTskIqfbHSLSC8Ch2BSp4njlFRHFg3BrrH8GMFlE6lvdewHcGJsiVRzvFUBE8SCsYFXVlQA6i0g9q3ufiIwAsCqWhYsU21iJKB5ElEKqus+6AgsA7o5BeSpFANZYichzlanexV2CJUiAbaxE5LnKBGvcnTLKK6+IKB6U2cYqIvsROkAFQM2YlKgSBGAbKxF5rsxgVdW6VVWQaEgQ3oSFiLznqxTiJa1EFA/8Fay8CQsRxQFfpZDw4BURxQFfBSsvaSWieOCvYOVNWIgoDvgrWEUR8NcqEVE15KsUMv8gwBorEXnLV8FqTrdisBKRt3wWrLxAgIi856sUEuHdrYjIe74K1gSwKYCIvOevYGVTABHFAV+lEG8bSETxwFfBav5BwFerRETVkK9SiJe0ElE88F2wsimAiLzmq2AVXtJKRHHAVynE+7ESUTzwVQrxHwSIKB74LljZFEBEXvNVComwKYCIvOerFOLdrYgoHvgrWPkPAkQUB/wVrGxjJaI44KsUMm2srLESkbd8FaxsYyWieOC7YGVTABF5zVcpJABPtyIiz8UshURkoohki8gaV79GIjJPRDZZzw2t/iIiL4rIZhFZJSJdK7JMXnlFRPEgltW7NwGcX6zf/QDmq2p7APOtbgC4AEB76zEMwPiKLJBNAUQUD2KWQqq6CMCvxXpfAmCS9XoSgP6u/pPV+AZAAxFpEekyGaxEFA+qOoWaqeoO6/UvAJpZr1sC2O4aL9PqFxFe0kpE8cCzFFJVBSJvEhWRYSKyTESW7dq1K2gY/6WViOJBVQfrTvsnvvWcbfXPAtDaNV4rq18JqjpBVbuparemTZsGDeM/CBBRPKjqYJ0J4Ebr9Y0APnL1v8E6O+BUALmuJoOwifDPBInIe0mxmrGIvAvgTABNRCQTwEMAngIwTUSGANgG4Cpr9NkALgSwGcBBAIMrsswECTBYichzMQtWVb2mlEF9Q4yrAIZXdpkJAv5LKxF5zlfVO1NjZbASkbd8FaxsYyWieOCrFOLpVkQUD/wVrAm88oqIvOerFEoAr7wiIu/5KoVMGyubAojIW74KVt6EhYjiga9SKCkhgMLYnZpLRBQW3wVrAIkIBLwuCRH9lvkqWJMTigAAhYUeF4SIftP8FayJDFYi8p6vgjVJTBtAQYHHBSGi3zRfBWtyIoOViLznq2BNYhsrEcUBXwVrcgJrrETkPZ8FK2usROQ9XwVrUqL5b0LWWInIS74KVrvGymAlIi/5K1itswLYFEBEXvJVsCYJa6xE5D1fBSubAogoHvgqWO2DV2wKICIv+SpYWWMlonjgr2DlwSsiigO+CtYkXnlFRHHAV8EaNzdh2bmT1Wai3zBfBatdY/U00w4fBn73O2DyZA8LQURe8lWwxkWN9eBBIC8P2L7dw0IQkZf8FazxcFbAkSPm+cABDwtBRF7yVbDGxXmsdqozWIl+s3wVrKyxElE88GWwssZKRF7yVbDGxf1YGaxEv3m+ClY2BVCVO3TI6xJQHPJVsEbl4FVhIfDvfzsBGanfYo31pZeAiRO9LkXV++QToFYtYNkyr0tCccZXwRqVGutrrwG33w4891zFpo/3GuuePcDUqdGd5/DhwJAh0Z1ndTB7tnn+5htvy0Fxx1fBmpgICAKVC9a9e4Ofi2vbFhg/vvTpy6qxfvcdcOKJQG5uJQpYSVddBVx9dWwuYMjKiv4845mIeVb1thwUdzwJVhHZKiKrRSRDRJZZ/RqJyDwR2WQ9N6zIvJNRgILMnRUvXCBgF7LksMJCYOtW4NZbS5++rBrrAw8A69cDX35Z8fJV1tat5jk/P/rzXrMm+vN027IFmDUrtssgigIva6xnqWq6qnazuu8HMF9V2wOYb3VH5scfUQd5yPvgs/Cn2bsXyM52uu1gTQjx1uTllT+/smqs9jy9rOHEspZ18GD05+nWsSPwpz/FdhmRCLXzJUJ8NQVcAmCS9XoSgP4Rz2HjRtTDPuwrrBX+NE2aAM2aOd124IT60uzfX/787Bprfj5QVBQ8LJ5+OsaixhrrI+SHD5vneHj/3OKtPOQ5r4JVAcwVkeUiMszq10xVd1ivfwHQLPSkZZg0CfWRi9zC2qVv7L/8AqxcaV4XFTk11D59gHnznOkSEoCffgJeeMH0W7IEmDKl/DK4G3hLO4AVD1/EWISge56ZmUDNmqZdOdpisVPIz4/8gCNrrFQKr4L1dFXtCuACAMNF5Az3QFVVmPAtQUSGicgyEVm2a9eu4IGdO6Neu6bYV1ATKD7MdtJJQHq6ef39907/BQuAG25wghYAbr4ZGDHCjPeHPwAjR5a/ZmUFq/1FjId7tdq1v2hyB+tnn5ll/Otf0V9OLM646NwZqFOnYtPGw47S9sorZjuL5HTBQ4eARo2AGTNiV67fGE+CVVWzrOdsADMA9ACwU0RaAID1nF3KtBNUtZuqdmvatGmJ4fUbJ2If6pmDNIcOmXujfuZqc3Uf7f/hh+CJW7d22gnz85020Q0bwl859wZdWrDGw0nl7mB95x1TtoqcreBu7nCvV82a5jmSdldVYN++8seLRVtuJJ+xzf48K3rOcyzcbx2aKOuzfPllU8Gw/fST+V7ce29sy1ZVtm41n83cuZ4VocqDVURqi0hd+zWAcwGsATATwI3WaDcC+Kgi86/XJAW5qG/e3DFjTHjed1/JEQ8fNs0Cbs2bO2F48CDQpo15vXp1yelL+4nrrrFOmADMmeN0VzRY588Hfv45smnK4w7WMWPMc4MGkQeM+2e5e70qsq4vvQTUrw9s21b2eJEE66FDwPXXm6aJcLh/sYQrWrX/aPySsWvPZR1o/ctfgHXrnHW1P6PExMovPx58/bV5fuMNz4rgRY21GYDFIrISwBIAs1T1MwBPAThHRDYBONvqjlj9FrVMjXX0aOCRR0zP5s3N86ZNzoi5ueYvVNwOHnSC9dAhZyP9+OOSC+raNXQB3LWXsWOB888vOU6kNa6zzwa6dIlsmtLYgecOA/cZEO4dQTjc83GHqH2gL5JgnTbNPP/4Y9njRfL+zZwJvP028Le/hTd+aecvh2IHYTSCdeZMIDnZBF64tmwJ3t5GjHBqquEcaLXD115nvwSrvT1XZCcZrSJU9QJVdYuqdrYeHVX1cav/HlXtq6rtVfVsVf21IvOv19SqsbpDtEED4P33gRNOcPrl5passf76q/OldYfs8uWhF3bLLcGnagFlX/Zlh1pOjgmcggJTOyvrS2B/abOzzc/kQYMqd3J/ecFa/EwGt7w84K23gtsUS6uxViRY7aAKdVBo927ndSRtrJHWAot/nqXZsMFpx4/kYNr69cCDD5Zsl7V3KuFeHnvgAHD88cDQoU6/F15wXocTrDNmmHOP7WBNSgpv2eF4+eWKNy8VFACLF1d82fb281sK1lirVw84ghTkD7oFePFF03P+fHPFkVtOTuhgdddYyztvdcIEYNQo8yWxxw3V3maHgv1levhhc6DkjjtMc0O9esEH0tzcbY5//zswaRLw7rtllysc7mB1B5k7WAMB4IsvnHKPGmUO8C1YEHo+7pqkXe5wQ/D994EdO4KnOXDAhNb8+UCrVqGXUx57RxfqvORQwglWVaBDB2D6dNNdvMb62Wfm6rxVq8z2cNppTo35tNOAf/wDWLgwdLNLuAfC7FB/663Qw8MJ1kGDgLS02NRY//lP82x/psWplr6ujz4K9O5tzsSpCHtnzmCNnobW9Vq7H3vZXPN/wQXm+vjili41P6VSUpx+xYM1nFA4cMAEbN26piYZqsZq/7xzfwEDARMmtlDtuEBwsM6fb55TU8svl9v8+eaGISJOTd5dk3SHjvsL+eabQN++5t4ChYXOT/TNm51xyquxFj87Y86c4FrkuHHmc7rqKmf++/aZ+dapY9b17LODl3PwoHkvS/vSuv36a/A6HjoEPP64mZ/9ebm/4PbOdvLk0u+pUHyHWzxYL7jAXJ3XubNZ1tdfA888Y4bZIXbWWSaciwsnEIHgbXrPHuDKK4OH5+SYzy+c67tjEaz2e1paM8m115rLu4t7/XVz2iNgdjz33x/epdK7dgH/+595bX9nPDxbw3fB2rGjebZPVUWTJqFHvO02U6M47jin3/79wKJF5vWcOSZ8y1NYaE5xAYBPPw1dY7WDtfjPYve4gQDwxBMlD1K5f0qtXWuey2sHDASAZ581NeVAwATTRRcFj2Nv8N98Exxa7p/cdpPD6tXAnXeacAaCDy6V1sZqb9zZ2c4G/tlnps35mWfMez19uvkc/v3v4LItX+4sK5QDB0x4HXss8NVXpY/nXp+33zYHMp97zrS/v/AC8H//B/z5z8Hr/+ab5vnGG809FUIpvrMoq4310UfNc926Zdeg7J1NaacJFuf+nF54wak9255+Ghg82Oz0y2NvTxUJorffBs49t/ThOTkl+/36K/DeeyY4V6ww73N+vjnAOHQo8O23ZrxPPzUHVocNKzkPt1WrzDGI0083v7jsnRNrrNFjH+M52ixqtxvddZf5KV3cTTcBLVqYU46KC7VRAMEb0vTpzhkCn3xSMlhr1TLB+sUXJWtY7trJF1+YewnccEPwOKFOP8rIMBu029tvO6H81VfAPfeYu06V9nPq8GETkD17mg3Ttnu3+YLl5jqnTO3fH/wFdR9gCVVj3b3buY3gkSPOzsGuLW/bZgKteC3L9swzwBVXhB4GmGD973/N6xUrnPsQ2R8AABImSURBVGVPnlyyTdVds+vf3zlgaZ8pMnGiUwOtWdPUlso7fcodakB4B6/273eOVrvZYWbXrCsSrHZt2M3eJrdsMc+BQOnHCuxgXb7c/Kq57LLgbX/fPvP92bfP3Kuhd2/nfb3+evOeNW7snF3iXq/cXPN67Fizo87JCa6wnHKK+WWQkWEebnZFovj7DZi26nvuMdt3585OrTY7u2Lt+9GmqtX2ccopp2goJ5+s2rWraiCgqp99pnrxxaq7d6u+/LLdsuM8Fi50JjzjjJLDiz/q1VP99NPyx7MfXbqoJiaWP16vXs7rpUtVV60yZfrPf0y/Ro1KTvPUU2Yld+403T16qGZlOcO7dVN94YXQy/vzn1WnTi27TLVrm+drrgnu37mzKdtLL6lee63Tv0YN1ccfNw/3+Bs2mPHt/m3bhv/+hXq0bh3cnZSket995vVrrwVvDJddVv78tmwxz336mOevvgoe5paZqTprVsl5LF9uPrMNG0oO69HDPN9xR8lhO3c6nzGgeuWVzrL27FF95BHVgwdVJ05UHTxY9fPPVYuKnM+1Xbuy1+300828xowx3QMHlhwnPb1kv4cfdsrx/PNO//r1ndfjxgVPk5rqTGOXa9Ik1TVrSm7jxR8zZqj+4x+hh3XsqLpunTPvd991ynz55cHjLlumOny40/3ss6r79oXMCc3NNbkwfbrqTz+pfvvt0UEAlqlWPJsqPGE8PEoL1tdfN2v21lvFBmRnq7ZpYwJ282bVBx5QLShwhn/zjeoJJ5g3ufgG+/TT5vn221Vzckp++KNHh94oiodMJI///lf1/PPN606dQo+zeLHqokXmdaNGqq+84gz7v/8zAVqZEAv1qF/fCXP7YYdwqABZtMhsxLfeWvo8f//70P3r1YusbH/8o/miTJ+u+vPPqiedVP409jLuvdd5H93Di4pU//Uvp7tfv8jK9Npr5jk1teSwxx4r2a9vX7NdnnOO6S7+Gdatq3rVVaoiqnfeWfpy7eU99FB4n6m7u1Yts85z5qg++mh469mihVnXceOcneeLL6q+8455XbNm6dPedptqnTplz3/NGtX+/cse5733VAcMCO535pmq//ufea9ff131wgvNDrJZs5LTP/SQ6htvKIM1hMJC1Z49TQVq5Eir5hope0/YsKF5/u47UxspLDTDi38gO3ea8HD3693bLNzdr3FjE+jhbKinnOK8Pu+80OPcdZfqq6+a18cc49Tc7KCrU8d8ScJZ3hVXmOp+OON+8EFwd8uWzuvjj1dt1cqp+XXpYp47dw49r2OOUd26VXXsWKdf166qf/pT6PcaMLXxK64I7leRmvCZZzqv33039Dg1akQ+3+Rk1YsuMq/373e+xPZ7UfzRr5/qxx9HtowmTVRnzgzu99RTzuu33jIhXN58Wrc2NUZAtWlT1bPPDh5evEb7+OPBvwQuu0z1iSec7sREE/qA6llnqZ52WujlJiWVXabi/ezvYvFHaZUO9+OEE4K7mzYtc3wGaymys833EjA798zMsOLU8dxzZuInnzShWdyvv5rHV1+Zmo5t7VrzUzE7W/XQIdPP/sDq1jUhaP80Asye9Kqryt8witcCn3wydM0pJUU1LU11716n39ChpuZx3HGlz3/0aGcd3ntPNSPDGeb+shZ/2Bto8S/Pueeq7trlfMHKevz4o7Nsu5/dfODuZ39pAbN+9g6lSRPVyZPNwx5v0iTn9WuvBTdZAOZn/y+/mB3ld9+pPvOM+cltfwH79TM/w8sr+1lnqU6YENzv8cdNjbOgwNl27CC6666S87j3XrNjUVXduNE0SXXtqvrvf5uat4hT805OdqZ74w3V/Hyn+7TTzDwuucR0r1vn1PAGDDAhfNddqoMGBS//mWfMdKtXm8qB/QuoVy9np+LesS9ZElyLfeIJ88spJUX1r38NLmNZj9mzVS+91Fm3/v2d7fycc1Tbtw8ev7T5vviiaUJx9ystzGvXVh02zLy+4gpTy27Txhl++eWqM2YwWMsSCJjaf40a5jO59dbg73C5E3/9dentM5Ho2NF6qy0HD5oN54svnH7jx5sv+9NPq+7YoTpvnmrz5ma6Y481tZ4HH3Rq0jt3Bn+h7Y0wMdH8nHEvd948023XZuvVM+OtWqV69dWmX/G2RFVn3oGA+RIPH25qqlddZWrBjRqZncuYMeZL6d6AZ80y83jlleBaof2wv5gnnhi8zBkzzE7ArW1bU9v98UcThPZnkp1tvswff2y6t28379ltt5nuzZvNz+rsbPOe/+c/pr29rM90507VadOc7i+/NG2kHTqYYfavjaFDTQjl55vx8vJM/4svDj3fn382e/pVq0q+F0VFpZdH1bzHquZzyM01n+8ttzjDv/nG7NBtBQWmrVHVhOu115oKgNvs2eYznzzZ+RVmCwTM8Px8856++aaZv93WXlRk3sO//tUs225Os9cj1DGIZ581Px8//th8KceMcZa3aZNzTMHejkaPNsv/8cfgbWrePNPmK2K+0F26mB2kqmnzf/99891asaLk8YUNG8znVFBgKjSBgHkcPmyazf7xj6NFYrCGYcsW00xl/0Lp1898d93NqzGVk6P6ww+RTxcIlPzS7d9vvqSqqkeOmJ97+flmvNzc4HaP7783G6g9jz17zMG8/HwnEA4cCB2qqubLuXFj6GEFBabh3y0nx3zRcnJKjj9+vPlS3XKLCT9VE3Z2OcpSVFTyy++VQMDU/EKF87594W1UX3xhQn/tWhPc1cWRI2YbC8eGDWYH8P77pjZb3s7DLS8vePxPPnHC0xbu/PLyzPTltQcWG17ZYBUzj+qpW7duuiyCf8jcvh149VVz5tDOneac8d69zVk/550HtGsX/gU6RORfIrJcnX83iXz631Kw2vLzgY8+MpdlT51q7poGmAt9unQxp9adcoq52q95c3ONgV/uT0FE5WOwRuE/3VesMOdTf/edOUc6IyP4nO/ERFObbd7cXIV47LHm31yaNjX92rQx9wmuFcE/whBR/KpssEbxdjbVV9euwXcBLCw0Fxdt2GCaDH7+Gdi40Tx/+GHoWw8AJnBr1za3FK1f31yMomrCt1EjcxFYkybmuXFjU0OuXRuoUcM8GjUyty5ITjYXAdWowX//IKqOGKwhJCWZZoC0tNDDCwrMlYe7d5v7ae/caa6k27TJXE134IC5+m/7dudG5hW5expg7kHifiQlmXbgVq2c8E1ONrXqw4dNWKuaS9Pr1jW16NRUc0VjQYEZLynJPNuP2rVNoNevb6atVcuU236omnVq0cJZvnv6SB/cWZDfMVgrIDnZ1E6PPRbo1Cm8aVSdQM7LM8F7+LC5UdORI+axe7cZp6DAXOacn2/GcT8KCkyNOivLhLU9bWGhCce8PBNe+/ebZcTTv4bYRCoeygUFJviPHDHPdeqY+24kJJidTF6eeQ61A0lMNOPl55v31/5F0aCBec7PN/NNTHR+ReTlmTum2e97SorZSSUmmufkZDPPhASzXlX92t5JpaSY96FGDeeGXe6dY/HHkSPOztr+TOxn+2F3Fxaa56Qk52HvXO3yBAJmmQkJzntSVOTcE8Xudh+v2LvXVA7cN5gDzHzs+dnPNWqYstoHl907Z/ftV+2HiJnGHm5XOFTNuiclmXKJmPmmpJjPv169sm9JHC4GaxWxP+iWLat2uYGACWQ7aOyNvbAweMM/csRs6ImJzp8n2A8R82Wx718SCDjTVvUjKckJz8OHzY6qVi1TppwcU+s+csQMCzV9IGC+RKmpzk269u41861RwwlOe4flXl5SkvNXaIWFptsOMfsLXdpr+m1hsPpcQkLwQbVQZzc0blx15fmtsndSZYVvpK/twFY1O0M76JOTnSac0h41aphpjhwx3e4yFu+2m28KC82joMBZvr2zsmvPgYCz03Y3Ndnz2bPHqRHWr+/8CnO/T8Vr5HYN275Zlft4u7us7lq0vXO0y5SXF/zfj8nJpqnM/gWTn2/65eaa99H+T8aKYrASVQE7IHiedPVQ2WDlx0xEFGUMViKiKGOwEhFFGYOViCjKGKxERFHGYCUiijIGKxFRlDFYiYiijMFKRBRlDFYioihjsBIRRRmDlYgoyhisRERRxmAlIooyBisRUZQxWImIoozBSkQUZXEXrCJyvohsEJHNIlLJ+3gTEVW9uApWEUkEMA7ABQBOAnCNiJzkbamIiCITV8EKoAeAzaq6RVWPAHgPwCUel4mIKCLxFqwtAWx3dWda/YiIqo1q9y+tIjIMwDCrM19E1nhZnhhrAmC314WIIa5f9eXndQOA31dm4ngL1iwArV3drax+R6nqBAATAEBElqlqt6orXtXi+lVvfl4/P68bYNavMtPHW1PAUgDtRaStiNQAcDWAmR6XiYgoInFVY1XVQhG5DcAcAIkAJqrqWo+LRUQUkbgKVgBQ1dkAZoc5+oRYliUOcP2qNz+vn5/XDajk+omqRqsgRESE+GtjJSKq9qptsPrh0lcRmSgi2e5TxkSkkYjME5FN1nNDq7+IyIvW+q4Ska7elbx8ItJaRBaIyPcislZE7rT6+2X9UkVkiYistNbvEat/WxH51lqPqdZBWIhIitW92Rrexsvyh0NEEkXkOxH5xOr2zboBgIhsFZHVIpJhnwUQre2zWgarjy59fRPA+cX63Q9gvqq2BzDf6gbMura3HsMAjK+iMlZUIYB7VPUkAKcCGG59Rn5Zv3wAfVS1M4B0AOeLyKkAxgB4TlV/B2AvgCHW+EMA7LX6P2eNF+/uBLDO1e2ndbOdparprlPHorN9qmq1ewDoCWCOq3skgJFel6uC69IGwBpX9wYALazXLQBssF6/AuCaUONVhweAjwCc48f1A1ALwAoAf4A5aT7J6n90O4U506Wn9TrJGk+8LnsZ69TKCpY+AD4BIH5ZN9c6bgXQpFi/qGyf1bLGCn9f+tpMVXdYr38B0Mx6XW3X2fpp2AXAt/DR+lk/lTMAZAOYB+AHADmqWmiN4l6Ho+tnDc8F0LhqSxyR5wHcCyBgdTeGf9bNpgDmishy64pOIErbZ9ydbkUOVVURqdanbYhIHQAfABihqvtE5Oiw6r5+qloEIF1EGgCYAaCDx0WKChH5E4BsVV0uImd6XZ4YOl1Vs0TkGADzRGS9e2Blts/qWmMt99LXamyniLQAAOs52+pf7dZZRJJhQnWKqn5o9fbN+tlUNQfAApifxw1ExK6wuNfh6PpZw+sD2FPFRQ1XLwAXi8hWmDvM9QHwAvyxbkepapb1nA2zY+yBKG2f1TVY/Xzp60wAN1qvb4Rpm7T732AdnTwVQK7rJ0vcEVM1fR3AOlV91jXIL+vX1KqpQkRqwrQfr4MJ2Cus0Yqvn73eVwD4Qq3GunijqiNVtZWqtoH5bn2hqgPhg3WziUhtEalrvwZwLoA1iNb26XUDciUani8EsBGmXesBr8tTwXV4F8AOAAUwbTZDYNqm5gPYBOBzAI2scQXmTIgfAKwG0M3r8pezbqfDtGGtApBhPS700fp1AvCdtX5rADxo9W8HYAmAzQDeB5Bi9U+1ujdbw9t5vQ5hrueZAD7x27pZ67LSeqy1MyRa2yevvCIiirLq2hRARBS3GKxERFHGYCUiijIGKxFRlDFYiYiijMFKZBGRM+07ORFVBoOViCjKGKxU7YjIdda9UDNE5BXrZih5IvKcdW/U+SLS1Bo3XUS+se6hOcN1f83ficjn1v1UV4jI8dbs64jIdBFZLyJTxH1zA6IwMVipWhGREwEMANBLVdMBFAEYCKA2gGWq2hHAQgAPWZNMBnCfqnaCuWLG7j8FwDg191M9DeYKOMDchWsEzH1+28FcN08UEd7diqqbvgBOAbDUqkzWhLlRRgDAVGuctwF8KCL1ATRQ1YVW/0kA3reuEW+pqjMAQFUPA4A1vyWqmml1Z8DcL3dx7FeL/ITBStWNAJikqiODeor8vdh4Fb1WO9/1ugj8jlAFsCmAqpv5AK6w7qFp/0fRcTDbsn3npWsBLFbVXAB7RaS31f96AAtVdT+ATBHpb80jRURqVelakK9xb0zViqp+LyKjYe78ngBzZ7DhAA4A6GENy4ZphwXMrd9etoJzC4DBVv/rAbwiIo9a87iyCleDfI53tyJfEJE8Va3jdTmIADYFEBFFHWusRERRxhorEVGUMViJiKKMwUpEFGUMViKiKGOwEhFFGYOViCjK/h8OpMa1Bz3CpQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2303eb7-6cd9-40b4-dbd8-d75fe28330c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_60 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_55 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_55 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_56 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_56 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_57 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_57 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_58 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_58 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_59 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_59 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_60 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_60 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_61 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_61 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_62 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_62 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_63 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_63 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_64 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_64 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_70 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_65 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_65 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,489\n",
            "Trainable params: 5,137\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dbe5d45-e1b5-4b10-9b0b-e6173f0dce5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 4s 11ms/step - loss: 3640.9424 - val_loss: 3596.1763\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 3318.9282 - val_loss: 2955.6389\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 2893.6436 - val_loss: 2644.5027\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 2339.0000 - val_loss: 1943.2218\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1719.0771 - val_loss: 924.9457\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1161.3173 - val_loss: 1099.2028\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 707.6641 - val_loss: 534.9633\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 384.1786 - val_loss: 333.9709\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 189.4499 - val_loss: 78.0118\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 95.1928 - val_loss: 68.9604\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 56.2858 - val_loss: 47.8743\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 43.1389 - val_loss: 53.5060\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 39.4683 - val_loss: 45.7331\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.7919 - val_loss: 59.5152\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 37.3301 - val_loss: 60.8928\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.0604 - val_loss: 39.8630\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 36.4833 - val_loss: 57.0208\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.1150 - val_loss: 48.7112\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 35.7553 - val_loss: 47.2552\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.6598 - val_loss: 63.8773\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 35.4853 - val_loss: 42.3718\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 35.1155 - val_loss: 45.6940\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 34.8824 - val_loss: 42.4828\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 34.6382 - val_loss: 53.5529\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.5056 - val_loss: 44.9615\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 34.1718 - val_loss: 54.3607\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 33.9449 - val_loss: 56.8593\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 33.9263 - val_loss: 80.2461\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5066 - val_loss: 41.6286\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.3505 - val_loss: 44.4096\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 33.3048 - val_loss: 49.3609\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8546 - val_loss: 45.1366\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8161 - val_loss: 41.1342\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 32.6666 - val_loss: 44.8273\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4548 - val_loss: 38.1318\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2919 - val_loss: 51.5561\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2466 - val_loss: 48.5212\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 32.1969 - val_loss: 48.0391\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0312 - val_loss: 41.5617\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 31.7857 - val_loss: 39.7870\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6001 - val_loss: 43.6332\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5739 - val_loss: 54.9313\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 31.4871 - val_loss: 44.0709\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 31.2623 - val_loss: 40.4131\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 31.1866 - val_loss: 37.8095\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 31.2226 - val_loss: 38.5555\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0817 - val_loss: 43.6757\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.9657 - val_loss: 60.1384\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0581 - val_loss: 42.3592\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.8306 - val_loss: 38.2617\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.7590 - val_loss: 38.5015\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7217 - val_loss: 38.5089\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.5132 - val_loss: 37.6499\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.4034 - val_loss: 42.4321\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.3181 - val_loss: 39.3980\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.3314 - val_loss: 42.1158\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.3167 - val_loss: 43.7324\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.2530 - val_loss: 45.1441\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.2969 - val_loss: 48.4237\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.1306 - val_loss: 37.8327\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.0005 - val_loss: 40.0784\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.9444 - val_loss: 36.0067\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.8231 - val_loss: 40.4869\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.8200 - val_loss: 39.7145\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6556 - val_loss: 45.1824\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6916 - val_loss: 52.1464\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.5934 - val_loss: 36.6964\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6804 - val_loss: 44.4050\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6752 - val_loss: 42.0077\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.5835 - val_loss: 44.4832\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 29.4707 - val_loss: 40.1667\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.3900 - val_loss: 36.7337\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.2002 - val_loss: 49.2900\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.2842 - val_loss: 42.0731\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.2385 - val_loss: 45.8122\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.2616 - val_loss: 37.6327\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.1786 - val_loss: 45.2717\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.1089 - val_loss: 37.7263\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.1526 - val_loss: 41.4044\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.0044 - val_loss: 40.8131\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9508 - val_loss: 45.4613\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.8039 - val_loss: 40.2430\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.8323 - val_loss: 39.5182\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9360 - val_loss: 39.4082\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.7821 - val_loss: 42.2867\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7443 - val_loss: 41.6837\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.7108 - val_loss: 36.9769\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6395 - val_loss: 38.4163\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6612 - val_loss: 45.3078\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7581 - val_loss: 41.4604\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5922 - val_loss: 53.7658\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6284 - val_loss: 42.7726\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6966 - val_loss: 38.7124\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4402 - val_loss: 39.1477\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.4972 - val_loss: 35.2963\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4774 - val_loss: 39.2572\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3881 - val_loss: 59.4167\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.3358 - val_loss: 38.5467\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.3517 - val_loss: 38.0098\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3155 - val_loss: 41.6839\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2690 - val_loss: 36.8427\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.2482 - val_loss: 42.8176\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.2905 - val_loss: 36.3704\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.2325 - val_loss: 37.7593\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1354 - val_loss: 38.0238\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2092 - val_loss: 36.2815\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0862 - val_loss: 57.5906\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1707 - val_loss: 48.8731\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1405 - val_loss: 41.6494\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0921 - val_loss: 44.2935\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.1201 - val_loss: 38.8173\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 28.0268 - val_loss: 41.3779\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9578 - val_loss: 41.7844\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.0478 - val_loss: 37.3068\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.9948 - val_loss: 36.6281\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0310 - val_loss: 46.6815\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9545 - val_loss: 39.5301\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9302 - val_loss: 39.1845\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8560 - val_loss: 41.0510\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.8141 - val_loss: 35.9086\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8640 - val_loss: 40.6687\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0123 - val_loss: 34.9173\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7829 - val_loss: 38.0248\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7761 - val_loss: 40.3733\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7681 - val_loss: 58.3995\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8391 - val_loss: 37.2450\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7528 - val_loss: 36.3143\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6432 - val_loss: 38.3696\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7076 - val_loss: 37.4688\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6506 - val_loss: 40.0959\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7300 - val_loss: 40.6923\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5577 - val_loss: 38.4756\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6330 - val_loss: 36.4595\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6201 - val_loss: 39.8874\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6075 - val_loss: 37.8565\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6407 - val_loss: 38.0207\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5687 - val_loss: 39.0889\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.5425 - val_loss: 42.5513\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5369 - val_loss: 39.6218\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6036 - val_loss: 38.1148\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.4861 - val_loss: 42.9916\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.5185 - val_loss: 37.9464\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3475 - val_loss: 37.9933\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5177 - val_loss: 39.4593\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.4663 - val_loss: 35.9477\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4403 - val_loss: 37.3119\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.3750 - val_loss: 36.6378\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4650 - val_loss: 38.2498\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.4134 - val_loss: 40.4029\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4028 - val_loss: 40.6825\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3900 - val_loss: 50.2950\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.3881 - val_loss: 44.4522\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2888 - val_loss: 35.4607\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.3514 - val_loss: 43.3538\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3307 - val_loss: 36.4496\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3901 - val_loss: 38.8715\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3164 - val_loss: 37.2517\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3212 - val_loss: 37.7598\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2907 - val_loss: 47.4229\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2144 - val_loss: 37.7064\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2017 - val_loss: 44.2534\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3058 - val_loss: 37.7993\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1984 - val_loss: 54.3702\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2263 - val_loss: 39.0918\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1674 - val_loss: 38.0759\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1669 - val_loss: 51.6675\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1723 - val_loss: 41.4805\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.0871 - val_loss: 36.3670\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1092 - val_loss: 37.4727\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1956 - val_loss: 42.8690\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0497 - val_loss: 37.6849\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0794 - val_loss: 34.7008\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0141 - val_loss: 48.2047\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1119 - val_loss: 40.3464\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0636 - val_loss: 36.6612\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 27.0060 - val_loss: 40.0723\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0273 - val_loss: 37.1074\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0557 - val_loss: 47.7679\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0505 - val_loss: 35.7549\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0706 - val_loss: 39.4336\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0391 - val_loss: 35.5761\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0150 - val_loss: 37.9461\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9937 - val_loss: 37.0138\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9971 - val_loss: 36.7854\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9649 - val_loss: 42.8491\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9785 - val_loss: 40.1664\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8436 - val_loss: 52.5304\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.9236 - val_loss: 37.5294\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.9212 - val_loss: 37.1873\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.0173 - val_loss: 43.3871\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8888 - val_loss: 40.1624\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8771 - val_loss: 37.0391\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8275 - val_loss: 36.2701\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9689 - val_loss: 44.4615\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9145 - val_loss: 39.3222\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8778 - val_loss: 51.2183\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8518 - val_loss: 44.9933\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.9548 - val_loss: 36.6962\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.9226 - val_loss: 36.4874\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8757 - val_loss: 36.2521\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.8855 - val_loss: 42.4445\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8364 - val_loss: 40.0918\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8402 - val_loss: 34.8479\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8356 - val_loss: 34.7602\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.8316 - val_loss: 37.0307\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8700 - val_loss: 35.6584\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.7950 - val_loss: 38.1167\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7902 - val_loss: 36.6774\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7427 - val_loss: 36.1136\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7878 - val_loss: 37.1163\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7279 - val_loss: 51.7742\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.8269 - val_loss: 40.9686\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7864 - val_loss: 35.7372\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7220 - val_loss: 33.9201\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6928 - val_loss: 38.8733\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7226 - val_loss: 41.1686\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7431 - val_loss: 34.6753\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7152 - val_loss: 38.6460\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.7534 - val_loss: 37.0392\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6876 - val_loss: 36.5482\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6473 - val_loss: 36.6852\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6742 - val_loss: 39.6601\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6685 - val_loss: 40.1018\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6100 - val_loss: 36.5252\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6647 - val_loss: 41.4033\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5944 - val_loss: 34.7601\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6535 - val_loss: 35.1722\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6861 - val_loss: 40.6333\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5711 - val_loss: 39.4792\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5898 - val_loss: 44.3980\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6108 - val_loss: 37.5333\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6294 - val_loss: 37.1335\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.6359 - val_loss: 41.5446\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.5495 - val_loss: 35.3054\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5650 - val_loss: 38.7501\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5376 - val_loss: 37.8417\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.5874 - val_loss: 34.1044\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5389 - val_loss: 42.9929\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5099 - val_loss: 42.2792\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5991 - val_loss: 39.8215\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5494 - val_loss: 43.0424\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.6438 - val_loss: 34.7466\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5453 - val_loss: 34.0796\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5548 - val_loss: 36.3711\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4937 - val_loss: 34.9120\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5301 - val_loss: 34.7787\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5573 - val_loss: 49.3835\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4611 - val_loss: 37.7898\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4811 - val_loss: 37.5252\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4381 - val_loss: 44.5849\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.5720 - val_loss: 40.9108\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4106 - val_loss: 37.7963\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4825 - val_loss: 35.5966\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5227 - val_loss: 34.9471\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4258 - val_loss: 38.3895\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4796 - val_loss: 37.0859\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4753 - val_loss: 35.3469\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3988 - val_loss: 37.8760\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4182 - val_loss: 37.3212\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4685 - val_loss: 36.9558\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.5446 - val_loss: 37.0431\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3712 - val_loss: 40.5817\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3383 - val_loss: 38.5385\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3932 - val_loss: 35.8409\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4785 - val_loss: 35.6828\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3573 - val_loss: 36.3267\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3838 - val_loss: 37.5994\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3309 - val_loss: 38.5822\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4216 - val_loss: 37.5425\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4508 - val_loss: 49.1514\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3931 - val_loss: 36.1882\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4155 - val_loss: 34.7984\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3305 - val_loss: 37.1146\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.4108 - val_loss: 36.7200\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3796 - val_loss: 38.7700\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3368 - val_loss: 39.3013\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3318 - val_loss: 37.1629\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3502 - val_loss: 42.9973\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2693 - val_loss: 37.5861\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3141 - val_loss: 35.2716\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3355 - val_loss: 39.7909\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3355 - val_loss: 37.9858\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3032 - val_loss: 34.9133\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2397 - val_loss: 36.3514\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3600 - val_loss: 51.4025\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2588 - val_loss: 39.5373\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3198 - val_loss: 38.4377\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3061 - val_loss: 33.7126\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2109 - val_loss: 35.8265\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2405 - val_loss: 34.7507\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.3116 - val_loss: 38.1821\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2898 - val_loss: 34.0316\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2495 - val_loss: 39.8088\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2607 - val_loss: 37.9199\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2744 - val_loss: 37.4092\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1379 - val_loss: 36.3366\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2103 - val_loss: 37.1840\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2978 - val_loss: 45.3391\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2882 - val_loss: 39.7731\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 26.2434 - val_loss: 35.9262\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.1800 - val_loss: 38.6815\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2284 - val_loss: 35.3033\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2252 - val_loss: 37.9982\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2369 - val_loss: 36.6694\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1581 - val_loss: 40.1359\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1996 - val_loss: 35.5796\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.1959 - val_loss: 35.5872\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2541 - val_loss: 34.2573\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1248 - val_loss: 35.4585\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1893 - val_loss: 37.2187\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1599 - val_loss: 40.0975\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1635 - val_loss: 34.2795\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1321 - val_loss: 38.4001\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1993 - val_loss: 35.3501\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0511 - val_loss: 34.8651\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1962 - val_loss: 39.2170\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1140 - val_loss: 39.8303\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1883 - val_loss: 36.0787\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1217 - val_loss: 35.8792\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0697 - val_loss: 47.5899\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1058 - val_loss: 52.9524\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1621 - val_loss: 37.7137\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.2017 - val_loss: 35.4783\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1425 - val_loss: 41.3051\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1097 - val_loss: 36.0857\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1522 - val_loss: 36.5377\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1238 - val_loss: 36.6385\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1033 - val_loss: 36.8743\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1354 - val_loss: 36.7532\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1133 - val_loss: 38.9201\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0360 - val_loss: 35.3537\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1135 - val_loss: 35.6709\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1533 - val_loss: 37.5102\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1047 - val_loss: 36.5924\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1022 - val_loss: 35.7816\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0345 - val_loss: 38.0848\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0792 - val_loss: 39.3131\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0703 - val_loss: 36.6221\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0657 - val_loss: 37.1857\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0149 - val_loss: 36.7150\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0725 - val_loss: 50.7989\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.1427 - val_loss: 35.7126\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1748 - val_loss: 35.4680\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0061 - val_loss: 37.6884\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0523 - val_loss: 36.5906\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0364 - val_loss: 36.5846\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.1399 - val_loss: 37.3882\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0860 - val_loss: 37.3433\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9934 - val_loss: 39.3620\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0594 - val_loss: 37.6034\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9929 - val_loss: 35.7618\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0530 - val_loss: 38.8876\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0820 - val_loss: 41.6403\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9734 - val_loss: 37.0908\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0176 - val_loss: 36.4724\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0366 - val_loss: 36.9855\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0885 - val_loss: 34.8320\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9254 - val_loss: 34.7151\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0452 - val_loss: 35.4607\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9641 - val_loss: 39.5457\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0284 - val_loss: 43.6427\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9679 - val_loss: 36.4991\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0466 - val_loss: 36.3058\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9816 - val_loss: 34.5902\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9843 - val_loss: 35.7455\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9272 - val_loss: 38.1582\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0389 - val_loss: 36.5045\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9696 - val_loss: 38.7890\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9467 - val_loss: 37.1228\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9588 - val_loss: 37.2247\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9597 - val_loss: 35.1730\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9683 - val_loss: 34.1587\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 26.0275 - val_loss: 42.9172\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9383 - val_loss: 35.4532\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9226 - val_loss: 37.1561\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8995 - val_loss: 37.6642\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9364 - val_loss: 39.8089\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9363 - val_loss: 36.3212\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8917 - val_loss: 36.7053\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9346 - val_loss: 39.6814\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8885 - val_loss: 35.2691\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8925 - val_loss: 35.5447\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9373 - val_loss: 52.5639\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9995 - val_loss: 37.7416\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9629 - val_loss: 40.8871\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.9624 - val_loss: 39.0224\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9292 - val_loss: 43.5440\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8957 - val_loss: 34.6059\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9359 - val_loss: 38.4207\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9618 - val_loss: 40.5719\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9088 - val_loss: 35.1189\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8506 - val_loss: 35.6528\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8757 - val_loss: 36.5277\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8956 - val_loss: 44.6188\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8488 - val_loss: 37.7603\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8942 - val_loss: 36.3706\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8845 - val_loss: 46.8918\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9219 - val_loss: 35.4596\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9257 - val_loss: 34.8116\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8584 - val_loss: 34.9559\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7863 - val_loss: 36.7879\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9324 - val_loss: 38.5243\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.8946 - val_loss: 37.9030\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9325 - val_loss: 45.3900\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9044 - val_loss: 35.0273\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8670 - val_loss: 36.2537\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.8326 - val_loss: 38.5160\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8490 - val_loss: 35.9117\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8563 - val_loss: 38.5172\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8325 - val_loss: 42.3337\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8257 - val_loss: 36.4119\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.8067 - val_loss: 40.3861\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8543 - val_loss: 34.9401\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8311 - val_loss: 37.4143\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8199 - val_loss: 40.6809\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7727 - val_loss: 42.8429\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8939 - val_loss: 48.6793\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8655 - val_loss: 42.2152\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8596 - val_loss: 35.9934\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7943 - val_loss: 39.3967\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7928 - val_loss: 36.3520\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8396 - val_loss: 35.9096\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7779 - val_loss: 35.6419\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.8534 - val_loss: 39.9871\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7775 - val_loss: 43.5579\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7269 - val_loss: 35.2445\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8344 - val_loss: 37.8287\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7944 - val_loss: 37.7684\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7865 - val_loss: 37.4837\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9247 - val_loss: 35.0580\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8097 - val_loss: 39.2621\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8171 - val_loss: 36.0798\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8283 - val_loss: 34.7935\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7899 - val_loss: 45.3225\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.9479 - val_loss: 38.7527\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7567 - val_loss: 38.0349\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7572 - val_loss: 37.8139\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7683 - val_loss: 41.0596\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7790 - val_loss: 41.5611\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8059 - val_loss: 36.4518\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8534 - val_loss: 38.2707\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7846 - val_loss: 36.4757\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8023 - val_loss: 42.0944\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8151 - val_loss: 35.9112\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7820 - val_loss: 37.3602\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7732 - val_loss: 34.5141\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7756 - val_loss: 35.2556\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8548 - val_loss: 35.6817\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7854 - val_loss: 35.0783\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7775 - val_loss: 39.1261\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8159 - val_loss: 39.0339\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7130 - val_loss: 37.6470\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7003 - val_loss: 39.1096\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7434 - val_loss: 36.3505\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7638 - val_loss: 38.5192\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7916 - val_loss: 39.7899\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7160 - val_loss: 43.1432\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.8062 - val_loss: 35.5442\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7158 - val_loss: 34.4362\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7621 - val_loss: 37.5806\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7180 - val_loss: 44.7528\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7778 - val_loss: 37.4760\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8020 - val_loss: 37.6501\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7306 - val_loss: 38.9964\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7219 - val_loss: 35.2910\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7846 - val_loss: 36.2591\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7327 - val_loss: 35.1699\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.8118 - val_loss: 39.4064\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7499 - val_loss: 41.2816\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6970 - val_loss: 47.4554\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7909 - val_loss: 38.2761\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7482 - val_loss: 62.4671\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7816 - val_loss: 35.8223\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7055 - val_loss: 37.6603\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6622 - val_loss: 36.3966\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7682 - val_loss: 34.8092\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7571 - val_loss: 35.7899\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6895 - val_loss: 36.3468\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7139 - val_loss: 39.9017\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7654 - val_loss: 44.6310\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 25.7221 - val_loss: 35.4481\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7039 - val_loss: 38.4104\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7424 - val_loss: 36.2853\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7511 - val_loss: 40.1314\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7468 - val_loss: 42.9226\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7664 - val_loss: 64.4568\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6419 - val_loss: 50.1313\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6152 - val_loss: 39.7687\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7053 - val_loss: 38.3212\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7288 - val_loss: 42.9570\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6943 - val_loss: 36.5344\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7560 - val_loss: 37.4408\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7210 - val_loss: 34.8371\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.6573 - val_loss: 36.8030\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.7199 - val_loss: 43.0555\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.5948 - val_loss: 35.8982\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.7281 - val_loss: 43.3995\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6499 - val_loss: 36.2896\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6322 - val_loss: 37.0168\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 25.6826 - val_loss: 36.7256\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0954b80-49ba-404b-ab8c-23fa5df1a68a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  1.366151725981926 \n",
            "MAE:  4.441088491146755 \n",
            "SD:  5.904169536079997\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "e67ccbbc-252c-4ca8-c8f1-fbd11a243e6a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d974ce17-6c29-41b1-ce3c-d7092afbb5a5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  0.8453961684278154 \n",
            "Ensemble_std:  5.866553340539298\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "\bBP_hv3_3(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}