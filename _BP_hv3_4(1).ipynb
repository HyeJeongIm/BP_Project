{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/_BP_hv3_4(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiiiBla2-j1S"
      },
      "source": [
        "# batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCoux5AOZnK",
        "outputId": "05ec15c5-4d35-4fbf-df2c-7869c9eda218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version :  3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]\n",
            "TensorFlow version :  2.3.0\n",
            "Keras version :  2.4.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러오기\n",
        "# train_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/train_shuffled_raw_v1.mat')\n",
        "# test_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/test_not_shuffled_raw_v1.mat')\n",
        "\n",
        "train_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/new/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/new/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "b570a293-9e53-473a-f3ed-a9b19951a83d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "8f731ca9-0203-46e7-87ab-30015e694029"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "8abc45d0-bd5c-49cd-8d12-1cde358bdca2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "outputs": [],
      "source": [
        "#parameter\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN",
        "outputId": "34b994bb-410d-4287-c82f-01d7d5b43600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 7,809\n",
            "Trainable params: 7,553\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8SHBwBOZnO"
      },
      "outputs": [],
      "source": [
        "# model = model1()\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGT6-7NcOZnO",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 12000.9102 - val_loss: 11624.1270\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 10894.3906 - val_loss: 10368.0205\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 9376.7197 - val_loss: 8048.0435\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 7541.8916 - val_loss: 5947.2031\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 5558.8369 - val_loss: 6252.6172\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 3661.9639 - val_loss: 3170.1213\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 2125.8474 - val_loss: 1358.4507\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 966.8816 - val_loss: 729.4528\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 407.8870 - val_loss: 520.0372\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 182.4191 - val_loss: 284.8596\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 111.5823 - val_loss: 118.5316\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 92.8817 - val_loss: 109.7314\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 88.0296 - val_loss: 99.0081\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 86.1069 - val_loss: 106.8083\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 84.6207 - val_loss: 110.1316\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 83.4669 - val_loss: 132.4166\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 82.5541 - val_loss: 104.6429\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 81.6739 - val_loss: 109.9565\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 80.9390 - val_loss: 113.9597\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 80.0329 - val_loss: 104.2261\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 79.7365 - val_loss: 120.3391\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 78.7346 - val_loss: 104.4044\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 78.2440 - val_loss: 102.4614\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 77.7765 - val_loss: 118.2514\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 77.1482 - val_loss: 103.9031\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 76.7645 - val_loss: 138.8671\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 76.5640 - val_loss: 99.3462\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 76.1900 - val_loss: 105.7185\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 75.5027 - val_loss: 98.1821\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 75.2377 - val_loss: 132.7877\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 75.0699 - val_loss: 115.4234\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 74.2843 - val_loss: 112.3081\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 74.4862 - val_loss: 97.0677\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 73.9104 - val_loss: 106.9055\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 73.5928 - val_loss: 113.4747\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 73.4767 - val_loss: 113.1083\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 73.2054 - val_loss: 116.5948\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 73.1453 - val_loss: 155.3905\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 72.6091 - val_loss: 115.5174\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 72.4410 - val_loss: 103.0840\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 72.0975 - val_loss: 103.5007\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 72.3134 - val_loss: 103.8608\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 71.7585 - val_loss: 105.6971\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 71.8622 - val_loss: 128.8929\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 71.2836 - val_loss: 117.1854\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 71.3772 - val_loss: 97.3704\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 71.1801 - val_loss: 116.4682\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.9101 - val_loss: 100.0207\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 70.7389 - val_loss: 108.7052\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 70.8021 - val_loss: 106.9433\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.7125 - val_loss: 152.2675\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 70.6420 - val_loss: 105.6619\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.0830 - val_loss: 113.9375\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 69.9572 - val_loss: 116.9608\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 69.5850 - val_loss: 113.0149\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 69.6891 - val_loss: 99.3700\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 69.6914 - val_loss: 103.6510\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 69.4804 - val_loss: 104.3250\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 69.2841 - val_loss: 118.8707\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 69.2773 - val_loss: 106.2344\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 69.1354 - val_loss: 165.8432\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.7645 - val_loss: 95.7479\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 68.7505 - val_loss: 97.8898\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.5904 - val_loss: 105.2711\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 68.3643 - val_loss: 103.4730\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 68.4101 - val_loss: 96.9965\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.2861 - val_loss: 110.4673\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.1053 - val_loss: 102.7086\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 4s 26ms/step - loss: 68.0244 - val_loss: 92.4803\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 4s 26ms/step - loss: 68.0063 - val_loss: 115.3805\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 67.7077 - val_loss: 120.6050\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 67.8145 - val_loss: 92.6784\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 67.9073 - val_loss: 99.1211\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.6321 - val_loss: 92.5563\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 67.6209 - val_loss: 101.3238\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.6072 - val_loss: 98.4743\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 67.2952 - val_loss: 104.1783\n",
            "Epoch 78/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 67.0498 - val_loss: 97.3172\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 67.2626 - val_loss: 99.3115\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.2388 - val_loss: 117.5398\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 67.0493 - val_loss: 106.6342\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 66.9315 - val_loss: 116.3897\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 5s 32ms/step - loss: 66.7527 - val_loss: 97.6127\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 3s 18ms/step - loss: 66.8639 - val_loss: 108.0669\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.7037 - val_loss: 98.3937\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 66.8163 - val_loss: 94.2601\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.4623 - val_loss: 99.8440\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 66.5887 - val_loss: 104.4832\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.4343 - val_loss: 111.7815\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 66.3794 - val_loss: 91.7943\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.4867 - val_loss: 100.5612\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 66.2872 - val_loss: 101.6703\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 66.0369 - val_loss: 100.8068\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 65.9912 - val_loss: 99.9431\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 66.0562 - val_loss: 108.1595\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 65.8293 - val_loss: 97.5287\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 65.9174 - val_loss: 105.8437\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.7162 - val_loss: 98.3932\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 65.6720 - val_loss: 98.0107\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 65.5480 - val_loss: 106.7497\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 65.6593 - val_loss: 125.7428\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.6627 - val_loss: 103.1998\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 65.3610 - val_loss: 103.1421\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.1855 - val_loss: 120.0502\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 65.3937 - val_loss: 92.5885\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 65.2172 - val_loss: 122.2171\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 65.2927 - val_loss: 106.4630\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.3590 - val_loss: 95.4278\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 65.1305 - val_loss: 98.1658\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 65.1852 - val_loss: 101.7882\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.0279 - val_loss: 95.0297\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 65.0208 - val_loss: 95.9359\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.8878 - val_loss: 113.6265\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 65.1481 - val_loss: 92.0845\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.7710 - val_loss: 99.6767\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 64.8586 - val_loss: 119.9198\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.7718 - val_loss: 135.8435\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 64.8543 - val_loss: 138.2928\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 3s 18ms/step - loss: 64.5504 - val_loss: 114.8433\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 64.4930 - val_loss: 91.1489\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 64.6229 - val_loss: 112.0573\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 64.5054 - val_loss: 97.7205\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.3705 - val_loss: 118.3512\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 64.4164 - val_loss: 167.8852\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.4650 - val_loss: 104.7508\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 64.3834 - val_loss: 99.1944\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.3389 - val_loss: 94.6515\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 64.2588 - val_loss: 91.8787\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 63.9531 - val_loss: 99.8785\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 64.1691 - val_loss: 104.6240\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 64.0211 - val_loss: 117.4797\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.8991 - val_loss: 90.9550\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.8251 - val_loss: 134.3248\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.0360 - val_loss: 100.2380\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 64.0404 - val_loss: 93.0006\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.8517 - val_loss: 107.7828\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.8560 - val_loss: 98.2510\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.9382 - val_loss: 104.1514\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.6397 - val_loss: 98.0841\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.8592 - val_loss: 98.4000\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.0301 - val_loss: 107.7703\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.6436 - val_loss: 134.8712\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.7462 - val_loss: 97.7324\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.3996 - val_loss: 103.7707\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.4679 - val_loss: 98.1485\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.5769 - val_loss: 109.4364\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 63.7137 - val_loss: 113.7129\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.5174 - val_loss: 113.6427\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.2743 - val_loss: 112.8012\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.6226 - val_loss: 105.5014\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.1848 - val_loss: 105.0152\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.2581 - val_loss: 109.3377\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.1758 - val_loss: 98.9344\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 63.2169 - val_loss: 104.4157\n",
            "Epoch 155/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 13ms/step - loss: 63.1771 - val_loss: 139.2584\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.2857 - val_loss: 96.2887\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.9561 - val_loss: 97.0259\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.2013 - val_loss: 103.4742\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.1938 - val_loss: 96.3056\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 63.0661 - val_loss: 97.5276\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 62.9620 - val_loss: 108.3971\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 62.9626 - val_loss: 99.4430\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.0392 - val_loss: 116.3431\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.9312 - val_loss: 95.5148\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.8936 - val_loss: 100.2146\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.7686 - val_loss: 107.7589\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.6766 - val_loss: 95.4988\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.7270 - val_loss: 108.5833\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.8909 - val_loss: 103.9284\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 62.8654 - val_loss: 101.8919\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 62.5489 - val_loss: 102.1353\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.6647 - val_loss: 97.6098\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.5881 - val_loss: 121.9695\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.5437 - val_loss: 92.1240\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.5033 - val_loss: 112.2269\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.5589 - val_loss: 111.2935\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.6221 - val_loss: 113.6932\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 62.5992 - val_loss: 93.9771\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.4381 - val_loss: 91.7812\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.4009 - val_loss: 137.7738\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.4903 - val_loss: 120.2270\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.5003 - val_loss: 95.1751\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.1414 - val_loss: 101.6963\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.2434 - val_loss: 96.0396\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 62.1600 - val_loss: 142.1924\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.3097 - val_loss: 112.1122\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 62.2758 - val_loss: 99.9865\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 62.2098 - val_loss: 101.0032\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.2952 - val_loss: 108.3527\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 62.0729 - val_loss: 96.7297\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 61.9691 - val_loss: 149.8922\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 62.0184 - val_loss: 111.0860\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 62.1196 - val_loss: 91.5899\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.9704 - val_loss: 102.0445\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.1298 - val_loss: 94.5975\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.9628 - val_loss: 100.1071\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.7673 - val_loss: 108.9889\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.9997 - val_loss: 114.6103\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.9670 - val_loss: 97.1445\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 62.0536 - val_loss: 93.0779\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.6931 - val_loss: 101.7227\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.9873 - val_loss: 101.8125\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.8314 - val_loss: 108.8464\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 61.6728 - val_loss: 128.1084\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.7809 - val_loss: 105.1565\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.7670 - val_loss: 93.4512\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.6315 - val_loss: 91.4887\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 61.5454 - val_loss: 105.7117\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 61.8062 - val_loss: 111.2065\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.6213 - val_loss: 105.0789\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4156 - val_loss: 97.9969\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.6879 - val_loss: 119.1928\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.5890 - val_loss: 103.9998\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 61.4785 - val_loss: 118.6349\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 61.4856 - val_loss: 105.7094\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.4601 - val_loss: 106.3787\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.5012 - val_loss: 102.6681\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.5870 - val_loss: 95.1820\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 5s 33ms/step - loss: 61.5190 - val_loss: 105.6001\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 61.1522 - val_loss: 140.8014\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.3353 - val_loss: 106.8349\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.4604 - val_loss: 95.4455\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.3157 - val_loss: 121.5384\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.1251 - val_loss: 99.1797\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.1699 - val_loss: 92.9773\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.2949 - val_loss: 91.2326\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.3715 - val_loss: 96.9718\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.3828 - val_loss: 102.4188\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.2476 - val_loss: 116.1642\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.1628 - val_loss: 97.4699\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.2160 - val_loss: 101.6835\n",
            "Epoch 232/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 13ms/step - loss: 61.1878 - val_loss: 100.0319\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.9392 - val_loss: 100.4928\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.9285 - val_loss: 100.5883\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 5s 30ms/step - loss: 61.0182 - val_loss: 91.4781\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 3s 19ms/step - loss: 61.0044 - val_loss: 104.2664\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 61.0194 - val_loss: 99.2067\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.9560 - val_loss: 113.8857\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0226 - val_loss: 96.4053\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 61.0127 - val_loss: 113.1723\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 60.9519 - val_loss: 117.0536\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 61.1335 - val_loss: 100.8611\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0912 - val_loss: 103.1131\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.7113 - val_loss: 93.8080\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.8608 - val_loss: 105.9463\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 60.8168 - val_loss: 93.9409\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 60.7344 - val_loss: 100.5394\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.7781 - val_loss: 102.9466\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.8015 - val_loss: 106.7873\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.7453 - val_loss: 100.5518\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0225 - val_loss: 94.2077\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 60.6985 - val_loss: 158.7139\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.7706 - val_loss: 108.2655\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 60.7096 - val_loss: 114.7354\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 5s 32ms/step - loss: 60.6008 - val_loss: 108.7441\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.6629 - val_loss: 94.9613\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.6148 - val_loss: 103.2475\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.5345 - val_loss: 97.9371\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 60.6725 - val_loss: 107.0625\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4648 - val_loss: 105.1556\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.7982 - val_loss: 97.5076\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 60.7222 - val_loss: 101.8807\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.5628 - val_loss: 99.5046\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.2740 - val_loss: 94.8143\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 60.4644 - val_loss: 129.3602\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.5761 - val_loss: 108.7298\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.5141 - val_loss: 98.1127\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.5692 - val_loss: 118.7581\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.3065 - val_loss: 99.4778\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.2664 - val_loss: 129.8795\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4818 - val_loss: 97.3014\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 60.3983 - val_loss: 109.7667\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.2637 - val_loss: 108.5409\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.1700 - val_loss: 111.0627\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 60.4511 - val_loss: 98.2360\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.3472 - val_loss: 109.6639\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 60.2589 - val_loss: 98.0097\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.1418 - val_loss: 98.6304\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 60.2804 - val_loss: 107.8418\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1220 - val_loss: 104.6937\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.0639 - val_loss: 91.8646\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1252 - val_loss: 100.3754\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 60.1773 - val_loss: 116.5513\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.1173 - val_loss: 113.6694\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.1282 - val_loss: 104.4809\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1192 - val_loss: 101.5212\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.1702 - val_loss: 99.6744\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.1326 - val_loss: 97.4185\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9336 - val_loss: 107.6712\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 60.1627 - val_loss: 102.4885\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.0174 - val_loss: 97.4169\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 60.1947 - val_loss: 120.2624\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0819 - val_loss: 99.8517\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.8860 - val_loss: 98.2656\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8546 - val_loss: 99.6263\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.8518 - val_loss: 94.3613\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0482 - val_loss: 109.8041\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.9611 - val_loss: 93.1522\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.0890 - val_loss: 96.3297\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.8443 - val_loss: 132.6947\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8961 - val_loss: 103.3242\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.0687 - val_loss: 95.7690\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.7986 - val_loss: 109.7741\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.7216 - val_loss: 112.6177\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.8183 - val_loss: 110.3129\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9906 - val_loss: 95.5496\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.6737 - val_loss: 96.2505\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.7644 - val_loss: 111.9270\n",
            "Epoch 309/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 12ms/step - loss: 59.7387 - val_loss: 116.6829\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8364 - val_loss: 93.1298\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.6780 - val_loss: 118.8923\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.7253 - val_loss: 111.2917\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.5460 - val_loss: 98.2766\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 60.0890 - val_loss: 107.9115\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.7004 - val_loss: 103.7641\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.8156 - val_loss: 134.6186\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.5471 - val_loss: 119.8555\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.6451 - val_loss: 121.7570\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.7066 - val_loss: 98.8748\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.7625 - val_loss: 102.7683\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.7297 - val_loss: 118.9717\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.6427 - val_loss: 104.9092\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.6688 - val_loss: 96.6279\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8395 - val_loss: 98.3217\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.6458 - val_loss: 95.6381\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.6914 - val_loss: 96.5905\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.5894 - val_loss: 91.7059\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.4933 - val_loss: 94.0141\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.6700 - val_loss: 108.9200\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.4507 - val_loss: 150.9825\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.4506 - val_loss: 111.3476\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.5273 - val_loss: 109.8118\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.5955 - val_loss: 100.2362\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.6229 - val_loss: 112.7562\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3414 - val_loss: 98.5753\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.5112 - val_loss: 99.1220\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.3973 - val_loss: 98.4287\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.4822 - val_loss: 115.2584\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.3013 - val_loss: 101.4438\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.3671 - val_loss: 98.3621\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.3205 - val_loss: 121.3784\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.3412 - val_loss: 99.0487\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.3786 - val_loss: 103.9373\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3915 - val_loss: 96.1184\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.3116 - val_loss: 109.0772\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1912 - val_loss: 92.7144\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.3069 - val_loss: 96.4972\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.2440 - val_loss: 103.8636\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.4226 - val_loss: 110.1688\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.3152 - val_loss: 107.1097\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1405 - val_loss: 116.7940\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.1436 - val_loss: 103.1494\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2310 - val_loss: 93.7340\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.2802 - val_loss: 95.1262\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2355 - val_loss: 103.2094\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.0614 - val_loss: 97.3201\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0245 - val_loss: 108.3860\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3425 - val_loss: 96.5224\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.0278 - val_loss: 104.0883\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.1229 - val_loss: 100.6618\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.9289 - val_loss: 97.7917\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1674 - val_loss: 94.3638\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.1711 - val_loss: 99.3740\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.0304 - val_loss: 101.9506\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.1899 - val_loss: 104.0257\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0202 - val_loss: 92.9105\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.0765 - val_loss: 118.8521\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0610 - val_loss: 95.8918\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.9407 - val_loss: 121.1695\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.1645 - val_loss: 130.6624\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1051 - val_loss: 97.2967\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.9808 - val_loss: 102.9968\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.0918 - val_loss: 112.8934\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.1249 - val_loss: 96.6200\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.0654 - val_loss: 96.5384\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.9286 - val_loss: 108.1930\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8914 - val_loss: 119.9867\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.0919 - val_loss: 100.2203\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.9700 - val_loss: 102.1073\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1228 - val_loss: 108.1441\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.7912 - val_loss: 103.1888\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7753 - val_loss: 112.3483\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.8070 - val_loss: 98.4484\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.8796 - val_loss: 105.0017\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.0034 - val_loss: 103.7871\n",
            "Epoch 386/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7788 - val_loss: 104.1271\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.9000 - val_loss: 109.0943\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8754 - val_loss: 98.8144\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.8462 - val_loss: 94.4361\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7836 - val_loss: 99.3101\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.8069 - val_loss: 103.1671\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.8684 - val_loss: 105.7145\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6839 - val_loss: 94.7652\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.8726 - val_loss: 100.3923\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.7864 - val_loss: 101.1910\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.7379 - val_loss: 92.6113\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8525 - val_loss: 102.6248\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.5419 - val_loss: 95.6053\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8675 - val_loss: 106.7586\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.7844 - val_loss: 91.7178\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6880 - val_loss: 97.7392\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.7412 - val_loss: 105.5069\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.7883 - val_loss: 100.7423\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5733 - val_loss: 110.1349\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.7417 - val_loss: 97.5813\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7034 - val_loss: 122.7403\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.6131 - val_loss: 98.5735\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.7284 - val_loss: 100.9299\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.7585 - val_loss: 105.1120\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6951 - val_loss: 135.7527\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.6432 - val_loss: 101.3606\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6505 - val_loss: 98.6982\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.5089 - val_loss: 105.8832\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6044 - val_loss: 96.7457\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.7639 - val_loss: 106.5921\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.4672 - val_loss: 93.4974\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6788 - val_loss: 110.9346\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.4709 - val_loss: 105.4315\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5146 - val_loss: 102.1587\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.6628 - val_loss: 111.1122\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6105 - val_loss: 100.8460\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.3481 - val_loss: 114.6858\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4220 - val_loss: 104.8572\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.5991 - val_loss: 102.5719\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.5066 - val_loss: 94.5595\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7168 - val_loss: 97.1295\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.5409 - val_loss: 106.6426\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3121 - val_loss: 120.6082\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.5655 - val_loss: 99.4761\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3860 - val_loss: 94.2637\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.3455 - val_loss: 98.0015\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3908 - val_loss: 95.3869\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.2647 - val_loss: 102.1353\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.5390 - val_loss: 108.0727\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.3464 - val_loss: 108.8330\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.3416 - val_loss: 92.3418\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.2456 - val_loss: 121.1474\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.4741 - val_loss: 99.8734\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3081 - val_loss: 90.0540\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.2344 - val_loss: 94.9196\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2609 - val_loss: 103.1015\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.2080 - val_loss: 96.3010\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.3026 - val_loss: 100.9402\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 58.5702 - val_loss: 105.8267\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.3878 - val_loss: 105.5091\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.4117 - val_loss: 106.7020\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.2435 - val_loss: 102.7405\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.4279 - val_loss: 119.5732\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.3643 - val_loss: 92.7040\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.3608 - val_loss: 91.7272\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.5487 - val_loss: 117.2381\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.2201 - val_loss: 99.8770\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2562 - val_loss: 96.1635\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.1547 - val_loss: 102.5187\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.3200 - val_loss: 99.2146\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 58.1736 - val_loss: 100.5413\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.1690 - val_loss: 118.4877\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.1884 - val_loss: 95.8877\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0793 - val_loss: 102.0126\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.3484 - val_loss: 95.2135\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1996 - val_loss: 132.1942\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.2784 - val_loss: 107.7443\n",
            "Epoch 463/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 3ms/step - loss: 58.2179 - val_loss: 95.1478\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.3069 - val_loss: 97.0059\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3291 - val_loss: 93.1422\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 57.9231 - val_loss: 106.4572\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.2394 - val_loss: 99.3332\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1610 - val_loss: 96.8926\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.3840 - val_loss: 101.5085\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0554 - val_loss: 99.4118\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.1630 - val_loss: 98.9088\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.1759 - val_loss: 96.8002\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 5s 30ms/step - loss: 58.1529 - val_loss: 101.8802\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 3s 19ms/step - loss: 57.9809 - val_loss: 111.4778\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.1043 - val_loss: 94.7857\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.2584 - val_loss: 101.4402\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2029 - val_loss: 107.0002\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.1196 - val_loss: 104.9534\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 58.2938 - val_loss: 113.5943\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9044 - val_loss: 109.5886\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.9476 - val_loss: 102.0366\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 58.0660 - val_loss: 99.7713\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.0230 - val_loss: 98.0591\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.1962 - val_loss: 106.5272\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3832 - val_loss: 105.8548\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 57.8685 - val_loss: 95.1524\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 57.9313 - val_loss: 102.5649\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 57.8296 - val_loss: 97.9804\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.1924 - val_loss: 100.4968\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 3s 21ms/step - loss: 58.0537 - val_loss: 109.0642\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2991 - val_loss: 96.0360\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 57.9185 - val_loss: 100.1754\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0039 - val_loss: 100.1287\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.1435 - val_loss: 99.6914\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8731 - val_loss: 97.2347\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 6s 37ms/step - loss: 57.8361 - val_loss: 95.9048\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9052 - val_loss: 100.5524\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 57.9245 - val_loss: 103.8925\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6999 - val_loss: 101.2654\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 57.7967 - val_loss: 101.3944\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  0.8179448189675153 \n",
            "MAE:  7.551773536762022 \n",
            "SD:  10.036203179756088\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su2Sj5jZCSwH",
        "outputId": "4a85f233-7730-4d19-a30a-66ad769ace4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 7,809\n",
            "Trainable params: 7,553\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPRh6v-mCSwH",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 3s 21ms/step - loss: 11888.9053 - val_loss: 11789.3369\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 10753.1201 - val_loss: 9844.3701\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 9111.1611 - val_loss: 8208.1318\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 7000.3188 - val_loss: 7375.1465\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 4807.1309 - val_loss: 2908.3450\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 2919.4094 - val_loss: 2280.4607\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 1559.3119 - val_loss: 1369.2844\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 738.7524 - val_loss: 965.2203\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 331.7408 - val_loss: 296.2718\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 167.4054 - val_loss: 120.9767\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 112.6087 - val_loss: 146.6222\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 97.8970 - val_loss: 152.2844\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 91.7789 - val_loss: 155.6135\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 88.9301 - val_loss: 108.5043\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 87.0371 - val_loss: 107.9099\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 85.6937 - val_loss: 125.3221\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 84.6272 - val_loss: 104.1364\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 83.4783 - val_loss: 114.3774\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 83.0176 - val_loss: 123.8577\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 81.5682 - val_loss: 103.5219\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 80.8954 - val_loss: 160.2640\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 80.4707 - val_loss: 104.2108\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 79.4179 - val_loss: 105.5957\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 79.3495 - val_loss: 110.1625\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 78.4672 - val_loss: 118.6438\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 78.6198 - val_loss: 92.9035\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 3s 18ms/step - loss: 77.8432 - val_loss: 103.7363\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 5s 31ms/step - loss: 77.2991 - val_loss: 98.3484\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 76.9370 - val_loss: 143.6181\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 76.4322 - val_loss: 104.9602\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 76.0456 - val_loss: 113.5933\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 75.8476 - val_loss: 118.5244\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 75.2761 - val_loss: 97.6090\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 75.0160 - val_loss: 109.1766\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 74.8400 - val_loss: 108.8840\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 74.4975 - val_loss: 107.9274\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 74.0006 - val_loss: 119.0735\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 73.7886 - val_loss: 97.7501\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 73.4013 - val_loss: 101.6697\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 73.0572 - val_loss: 151.7742\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.7967 - val_loss: 97.3085\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.8888 - val_loss: 176.2200\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 72.8685 - val_loss: 93.8053\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 72.1122 - val_loss: 112.2709\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 71.9068 - val_loss: 123.5118\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 72.0591 - val_loss: 92.8579\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 71.3604 - val_loss: 148.5688\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 71.3449 - val_loss: 118.5643\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 71.3054 - val_loss: 90.3432\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 71.0854 - val_loss: 157.5723\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.6750 - val_loss: 107.0987\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.5068 - val_loss: 110.2577\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.7600 - val_loss: 99.2766\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.7611 - val_loss: 115.6034\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.1532 - val_loss: 118.6933\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 69.8722 - val_loss: 94.0322\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.1391 - val_loss: 109.1095\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 69.7888 - val_loss: 95.5236\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 69.4954 - val_loss: 106.3707\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.3469 - val_loss: 101.7174\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.1126 - val_loss: 90.9022\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.7863 - val_loss: 102.3782\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.7455 - val_loss: 99.3456\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.6516 - val_loss: 106.0102\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.7931 - val_loss: 92.5016\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.4118 - val_loss: 115.4427\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.2027 - val_loss: 91.8131\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.2233 - val_loss: 103.0388\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.3576 - val_loss: 99.3919\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 67.8139 - val_loss: 107.0638\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 67.9334 - val_loss: 117.2953\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 67.6014 - val_loss: 167.2779\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.4802 - val_loss: 90.9889\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.4158 - val_loss: 104.6189\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.6034 - val_loss: 110.4589\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.3386 - val_loss: 149.8283\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.0120 - val_loss: 133.6121\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.1175 - val_loss: 96.1735\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.8407 - val_loss: 106.5033\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.9025 - val_loss: 106.5300\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.4096 - val_loss: 123.9581\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.6191 - val_loss: 207.0156\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.4459 - val_loss: 143.5135\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.3494 - val_loss: 102.8227\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.4224 - val_loss: 154.1530\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.5155 - val_loss: 111.4682\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.1182 - val_loss: 102.4943\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.0679 - val_loss: 97.4839\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 65.7671 - val_loss: 111.4612\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 65.5778 - val_loss: 92.7212\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 65.7283 - val_loss: 105.1608\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.5513 - val_loss: 108.0472\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.3109 - val_loss: 101.2911\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.7998 - val_loss: 130.3896\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.6053 - val_loss: 133.3575\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.5042 - val_loss: 101.9354\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.1659 - val_loss: 106.8869\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9763 - val_loss: 97.1251\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.0831 - val_loss: 108.4164\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.7613 - val_loss: 110.1977\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.7727 - val_loss: 103.0982\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.7205 - val_loss: 119.5448\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.9146 - val_loss: 115.9345\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.9418 - val_loss: 95.0389\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.6888 - val_loss: 93.4036\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.8886 - val_loss: 123.4804\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.5211 - val_loss: 97.6793\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.5590 - val_loss: 104.1911\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.4949 - val_loss: 108.1258\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.2176 - val_loss: 101.0906\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.4526 - val_loss: 106.7458\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.2515 - val_loss: 108.6909\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.2458 - val_loss: 105.5265\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.4525 - val_loss: 125.5911\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.1880 - val_loss: 142.8683\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.1731 - val_loss: 173.8633\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.8140 - val_loss: 90.6579\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.9818 - val_loss: 137.4746\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.8839 - val_loss: 95.1180\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.7449 - val_loss: 97.4769\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.6142 - val_loss: 97.9474\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.6649 - val_loss: 100.4173\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.6459 - val_loss: 107.7981\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.6157 - val_loss: 102.5188\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.3070 - val_loss: 94.0888\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.6369 - val_loss: 110.4151\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.7272 - val_loss: 97.7458\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.1187 - val_loss: 93.4569\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.2562 - val_loss: 111.8229\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.2790 - val_loss: 102.8334\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.1515 - val_loss: 98.7356\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.4513 - val_loss: 190.6283\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.2905 - val_loss: 111.9784\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.1996 - val_loss: 108.7343\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.2221 - val_loss: 104.5514\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.9387 - val_loss: 113.5381\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.1907 - val_loss: 90.5717\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.7941 - val_loss: 93.9517\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.6387 - val_loss: 105.0062\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.0503 - val_loss: 114.1943\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.8245 - val_loss: 98.5092\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.6502 - val_loss: 99.9591\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.6811 - val_loss: 102.8652\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.8527 - val_loss: 117.5658\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.6124 - val_loss: 103.5479\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.7128 - val_loss: 143.9536\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.7065 - val_loss: 95.2160\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.5797 - val_loss: 100.9215\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.4847 - val_loss: 98.5003\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.4815 - val_loss: 96.0606\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.3576 - val_loss: 101.5364\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.3524 - val_loss: 106.6718\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.1751 - val_loss: 103.4466\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.2862 - val_loss: 106.2385\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.3765 - val_loss: 99.5179\n",
            "Epoch 156/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 62.1875 - val_loss: 100.1698\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.2255 - val_loss: 128.8777\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.0780 - val_loss: 94.7656\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.9944 - val_loss: 110.8736\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.2888 - val_loss: 100.6618\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.0028 - val_loss: 98.0409\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.1359 - val_loss: 93.9288\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.0443 - val_loss: 101.6166\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.0014 - val_loss: 109.4657\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.7912 - val_loss: 96.5156\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.9258 - val_loss: 98.0430\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.6805 - val_loss: 89.0363\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.7456 - val_loss: 97.3943\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.5857 - val_loss: 95.8074\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.5947 - val_loss: 106.7798\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.8597 - val_loss: 99.3319\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.5046 - val_loss: 96.9501\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.7626 - val_loss: 96.8288\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.7358 - val_loss: 99.9255\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.6616 - val_loss: 98.5296\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.6374 - val_loss: 106.5285\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4975 - val_loss: 101.5425\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4667 - val_loss: 94.1511\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4803 - val_loss: 93.2127\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4560 - val_loss: 98.0167\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.5669 - val_loss: 109.1617\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4123 - val_loss: 105.9512\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.1685 - val_loss: 106.8304\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4739 - val_loss: 98.5048\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.1600 - val_loss: 115.2795\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.3397 - val_loss: 100.7240\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.3820 - val_loss: 108.5762\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.3119 - val_loss: 93.5159\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0556 - val_loss: 90.7735\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.1175 - val_loss: 109.7789\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0350 - val_loss: 90.4433\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.1851 - val_loss: 103.8147\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.9361 - val_loss: 93.5539\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.1833 - val_loss: 101.3541\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.1039 - val_loss: 99.3063\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0048 - val_loss: 105.5039\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.8683 - val_loss: 114.0908\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.7160 - val_loss: 112.3023\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.8602 - val_loss: 89.9332\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.9712 - val_loss: 99.8090\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.9082 - val_loss: 87.1213\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.7684 - val_loss: 101.1056\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.9851 - val_loss: 99.2362\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.9301 - val_loss: 91.7971\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.9762 - val_loss: 88.7368\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.7484 - val_loss: 89.1161\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.7111 - val_loss: 91.8610\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.8525 - val_loss: 111.7777\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.8295 - val_loss: 100.2134\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.8228 - val_loss: 109.5998\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.5754 - val_loss: 122.6041\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.5427 - val_loss: 163.0622\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4151 - val_loss: 104.0190\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.6094 - val_loss: 98.2075\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.5754 - val_loss: 127.4411\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.7081 - val_loss: 88.2860\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.8005 - val_loss: 102.3354\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4504 - val_loss: 108.8968\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4843 - val_loss: 99.6301\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.3989 - val_loss: 96.8882\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.6280 - val_loss: 145.7097\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.3797 - val_loss: 95.9382\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4901 - val_loss: 100.7519\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.2709 - val_loss: 90.1625\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.5202 - val_loss: 89.9605\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.3673 - val_loss: 112.8665\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.6176 - val_loss: 120.6589\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.3825 - val_loss: 100.7227\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.2152 - val_loss: 96.5488\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.3679 - val_loss: 102.1653\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1887 - val_loss: 100.7095\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.3293 - val_loss: 111.7097\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.3255 - val_loss: 107.4032\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.3259 - val_loss: 100.9337\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1878 - val_loss: 97.6989\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.2884 - val_loss: 103.7059\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.3177 - val_loss: 103.0255\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0649 - val_loss: 106.3880\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0600 - val_loss: 94.5812\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.2125 - val_loss: 112.2607\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1299 - val_loss: 101.0424\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1697 - val_loss: 113.3643\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1635 - val_loss: 105.6821\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8343 - val_loss: 99.0631\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0330 - val_loss: 98.5021\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9418 - val_loss: 97.1942\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9996 - val_loss: 123.1393\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.0978 - val_loss: 97.4849\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.8920 - val_loss: 105.5263\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.9190 - val_loss: 104.5872\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0339 - val_loss: 123.6669\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9234 - val_loss: 96.3698\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1778 - val_loss: 110.7748\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8053 - val_loss: 104.6022\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8178 - val_loss: 89.7830\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.7600 - val_loss: 97.1104\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9319 - val_loss: 98.8131\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9970 - val_loss: 102.7080\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8123 - val_loss: 95.8626\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9386 - val_loss: 99.9226\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5738 - val_loss: 92.8595\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8631 - val_loss: 100.1428\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.7240 - val_loss: 96.7593\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.6779 - val_loss: 113.9903\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.6882 - val_loss: 88.7113\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.8135 - val_loss: 95.2850\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.5982 - val_loss: 113.3647\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5323 - val_loss: 95.5355\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.7413 - val_loss: 97.0830\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5084 - val_loss: 91.6080\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.7030 - val_loss: 96.8477\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.4531 - val_loss: 128.8727\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.6155 - val_loss: 101.1160\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.6624 - val_loss: 98.5879\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5594 - val_loss: 102.5843\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5535 - val_loss: 118.1651\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.4249 - val_loss: 105.7218\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5610 - val_loss: 98.0270\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3984 - val_loss: 110.3465\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3806 - val_loss: 95.3193\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3366 - val_loss: 100.5250\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.4241 - val_loss: 96.1121\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5798 - val_loss: 91.3360\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3287 - val_loss: 102.4357\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2311 - val_loss: 113.6483\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3131 - val_loss: 115.8897\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 59.3826 - val_loss: 94.8194\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.4462 - val_loss: 90.9789\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3981 - val_loss: 109.8974\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5497 - val_loss: 99.8994\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2327 - val_loss: 96.5239\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3334 - val_loss: 90.9337\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2134 - val_loss: 95.3730\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3472 - val_loss: 104.7768\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3424 - val_loss: 93.3623\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2899 - val_loss: 104.7902\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3886 - val_loss: 95.4472\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1482 - val_loss: 97.6401\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1779 - val_loss: 102.1913\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1584 - val_loss: 102.4942\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2602 - val_loss: 100.2672\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9978 - val_loss: 107.0782\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1425 - val_loss: 90.2704\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0811 - val_loss: 96.0723\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1653 - val_loss: 95.9222\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0324 - val_loss: 87.9929\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2119 - val_loss: 100.2834\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0764 - val_loss: 122.7699\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8937 - val_loss: 96.2587\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1398 - val_loss: 90.2502\n",
            "Epoch 311/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0663 - val_loss: 88.6380\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9798 - val_loss: 93.6754\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9873 - val_loss: 99.2697\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0682 - val_loss: 107.0062\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0652 - val_loss: 94.4094\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7497 - val_loss: 99.6013\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9267 - val_loss: 90.7216\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1644 - val_loss: 113.0949\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.9961 - val_loss: 97.5992\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.8686 - val_loss: 100.4315\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9920 - val_loss: 113.5279\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8005 - val_loss: 101.7941\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8751 - val_loss: 117.5970\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0150 - val_loss: 98.3687\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8908 - val_loss: 111.6078\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6758 - val_loss: 94.4994\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7868 - val_loss: 100.4321\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9092 - val_loss: 90.4154\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8214 - val_loss: 97.6623\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8633 - val_loss: 102.9995\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9923 - val_loss: 92.1570\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7461 - val_loss: 96.1912\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8627 - val_loss: 145.7230\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5713 - val_loss: 91.3014\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6434 - val_loss: 90.2834\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6713 - val_loss: 90.3512\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8122 - val_loss: 101.5439\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8162 - val_loss: 133.8664\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6639 - val_loss: 97.6203\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6696 - val_loss: 87.1279\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5530 - val_loss: 96.3315\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6276 - val_loss: 112.7730\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7393 - val_loss: 90.1324\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6618 - val_loss: 91.6650\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7380 - val_loss: 92.6895\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5585 - val_loss: 97.0399\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6391 - val_loss: 92.7076\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.6454 - val_loss: 100.2661\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5973 - val_loss: 88.4117\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6956 - val_loss: 120.6866\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.3622 - val_loss: 104.7215\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4601 - val_loss: 92.1540\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6265 - val_loss: 88.7877\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5942 - val_loss: 110.4980\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6363 - val_loss: 91.3858\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6307 - val_loss: 98.6456\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4906 - val_loss: 92.7241\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5457 - val_loss: 100.9460\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4563 - val_loss: 114.4068\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4683 - val_loss: 97.2273\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5034 - val_loss: 94.8722\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4910 - val_loss: 101.1133\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5401 - val_loss: 115.6906\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4299 - val_loss: 107.0091\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5662 - val_loss: 99.4566\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2467 - val_loss: 104.7016\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3454 - val_loss: 98.0867\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2388 - val_loss: 105.0831\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5369 - val_loss: 96.2348\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3029 - val_loss: 91.4721\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1200 - val_loss: 97.9391\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2577 - val_loss: 93.4031\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4571 - val_loss: 102.0559\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3192 - val_loss: 94.2801\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3515 - val_loss: 94.1506\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4958 - val_loss: 102.8933\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1368 - val_loss: 97.1392\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2970 - val_loss: 92.4390\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5117 - val_loss: 92.7796\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2768 - val_loss: 98.1006\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1447 - val_loss: 100.6854\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3886 - val_loss: 129.8826\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4382 - val_loss: 104.9695\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1445 - val_loss: 98.0543\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1631 - val_loss: 112.1325\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0875 - val_loss: 110.6771\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0927 - val_loss: 96.4172\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9884 - val_loss: 89.0772\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2703 - val_loss: 96.6037\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2876 - val_loss: 95.3726\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0520 - val_loss: 102.1599\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1592 - val_loss: 96.3519\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1380 - val_loss: 89.8815\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4069 - val_loss: 99.3501\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1936 - val_loss: 103.4011\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0532 - val_loss: 94.0873\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1532 - val_loss: 96.7682\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1422 - val_loss: 94.8736\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1545 - val_loss: 97.1816\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1560 - val_loss: 91.5781\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9546 - val_loss: 93.7139\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1283 - val_loss: 89.7394\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1319 - val_loss: 136.5062\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0759 - val_loss: 95.8876\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1190 - val_loss: 120.9132\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0793 - val_loss: 96.4895\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0072 - val_loss: 125.3225\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1153 - val_loss: 104.9764\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0497 - val_loss: 103.8042\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0172 - val_loss: 102.0366\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0833 - val_loss: 94.3557\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0059 - val_loss: 97.1844\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 57.8301 - val_loss: 95.9914\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0127 - val_loss: 103.0935\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6821 - val_loss: 92.4305\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 57.9800 - val_loss: 93.4869\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0904 - val_loss: 105.3754\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8290 - val_loss: 93.9792\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7756 - val_loss: 90.6165\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8755 - val_loss: 91.7792\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6428 - val_loss: 93.2855\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9891 - val_loss: 98.3336\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8265 - val_loss: 104.9076\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9963 - val_loss: 95.3051\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8362 - val_loss: 100.0336\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9375 - val_loss: 96.9463\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6711 - val_loss: 98.5668\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7199 - val_loss: 98.1419\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8695 - val_loss: 94.4807\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6975 - val_loss: 92.5861\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8410 - val_loss: 91.1130\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9434 - val_loss: 95.9124\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8555 - val_loss: 100.8985\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0270 - val_loss: 97.6125\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8077 - val_loss: 94.1646\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7770 - val_loss: 90.3346\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6090 - val_loss: 88.6892\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9118 - val_loss: 97.9604\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7166 - val_loss: 99.1958\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7621 - val_loss: 102.8360\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6975 - val_loss: 96.6388\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7040 - val_loss: 96.9148\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6071 - val_loss: 92.4570\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6642 - val_loss: 107.4638\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6457 - val_loss: 90.2489\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7491 - val_loss: 93.2818\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7749 - val_loss: 102.8363\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6229 - val_loss: 98.8877\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 57.6969 - val_loss: 97.3841\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 57.6084 - val_loss: 98.6642\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8651 - val_loss: 93.3320\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 57.5256 - val_loss: 87.1146\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 57.6587 - val_loss: 91.4398\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6714 - val_loss: 92.9596\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7677 - val_loss: 94.5710\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5731 - val_loss: 103.6100\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5751 - val_loss: 107.0025\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6501 - val_loss: 129.0804\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5894 - val_loss: 93.0956\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5451 - val_loss: 109.8727\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6814 - val_loss: 99.8785\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6698 - val_loss: 94.9367\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5687 - val_loss: 105.9377\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4916 - val_loss: 94.2273\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5006 - val_loss: 94.3766\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5791 - val_loss: 90.2314\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5359 - val_loss: 90.7758\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5924 - val_loss: 103.0649\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5027 - val_loss: 94.0527\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6281 - val_loss: 99.8786\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6411 - val_loss: 99.6534\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4634 - val_loss: 92.8266\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7135 - val_loss: 96.0701\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6976 - val_loss: 89.9869\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4646 - val_loss: 93.8229\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5848 - val_loss: 112.3874\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.2409 - val_loss: 96.3353\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5068 - val_loss: 96.7506\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4644 - val_loss: 93.5938\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.3161 - val_loss: 91.4365\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4905 - val_loss: 93.9845\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.3719 - val_loss: 116.6054\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5065 - val_loss: 89.1497\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4748 - val_loss: 98.5055\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.3761 - val_loss: 90.6934\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4234 - val_loss: 96.8791\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6329 - val_loss: 93.0909\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.3935 - val_loss: 93.1761\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.3200 - val_loss: 99.6958\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5107 - val_loss: 91.2739\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4099 - val_loss: 111.0437\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6042 - val_loss: 103.9131\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.5460 - val_loss: 100.1246\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4584 - val_loss: 107.0562\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4831 - val_loss: 97.5833\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4496 - val_loss: 93.1928\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4881 - val_loss: 93.8604\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4969 - val_loss: 96.0148\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.3679 - val_loss: 92.4622\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.4543 - val_loss: 103.2719\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  2.2022495201934826 \n",
            "MAE:  7.664580199049766 \n",
            "SD:  9.920786903511084\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "outputId": "4a85f233-7730-4d19-a30a-66ad769ace4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 7,809\n",
            "Trainable params: 7,553\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXAOqd2CTsa",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 11921.5967 - val_loss: 11914.0596\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 10798.2383 - val_loss: 10225.0400\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 9266.3203 - val_loss: 7013.2324\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 7340.2319 - val_loss: 5055.4087\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 5235.4189 - val_loss: 4009.9504\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 3266.5889 - val_loss: 3378.9292\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 1687.8630 - val_loss: 2366.2112\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 761.4007 - val_loss: 809.6404\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 314.3580 - val_loss: 202.4307\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 151.4426 - val_loss: 153.5461\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 106.3360 - val_loss: 152.9812\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 96.5422 - val_loss: 151.3991\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 92.5691 - val_loss: 165.7629\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 90.8598 - val_loss: 278.5432\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 89.0285 - val_loss: 150.1904\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 87.8552 - val_loss: 127.5834\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 86.0350 - val_loss: 145.0936\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 84.8004 - val_loss: 129.6825\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 83.7426 - val_loss: 118.6364\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 83.1750 - val_loss: 121.1248\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 82.1992 - val_loss: 126.5292\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 81.4291 - val_loss: 108.2605\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 80.6576 - val_loss: 129.7637\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 80.0106 - val_loss: 102.8794\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 79.4360 - val_loss: 115.9786\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 79.2535 - val_loss: 100.7479\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 78.3752 - val_loss: 97.4636\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 77.9073 - val_loss: 114.6523\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 77.5103 - val_loss: 100.3918\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 76.8708 - val_loss: 96.5573\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 76.7983 - val_loss: 126.3265\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 76.3708 - val_loss: 118.4578\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 75.8161 - val_loss: 133.0645\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 75.4481 - val_loss: 132.1086\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 75.5127 - val_loss: 115.2021\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 75.1853 - val_loss: 96.7264\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 74.8101 - val_loss: 105.0983\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 74.1438 - val_loss: 103.3354\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 74.1721 - val_loss: 106.3660\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 73.8437 - val_loss: 99.0117\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 73.4348 - val_loss: 117.1678\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 73.1898 - val_loss: 100.2508\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 72.8234 - val_loss: 98.6491\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 72.7850 - val_loss: 139.0448\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 72.2649 - val_loss: 113.8604\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 72.0264 - val_loss: 127.0814\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 72.2028 - val_loss: 118.0038\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 71.6368 - val_loss: 99.1524\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 71.7612 - val_loss: 138.4305\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 71.3058 - val_loss: 98.8762\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 71.2870 - val_loss: 120.7819\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.9838 - val_loss: 97.8417\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.8032 - val_loss: 137.8808\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.5834 - val_loss: 102.9543\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.3226 - val_loss: 132.9884\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.0394 - val_loss: 110.4940\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.9671 - val_loss: 99.3446\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 70.0609 - val_loss: 102.9312\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 69.5623 - val_loss: 120.3999\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 69.6021 - val_loss: 103.1683\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 69.2666 - val_loss: 92.9870\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 69.1488 - val_loss: 113.3620\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 69.1083 - val_loss: 113.1388\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 69.0832 - val_loss: 95.4139\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.6274 - val_loss: 160.2223\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 69.0622 - val_loss: 131.5749\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.8649 - val_loss: 101.8840\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.5686 - val_loss: 120.7956\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.5239 - val_loss: 95.2240\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.4640 - val_loss: 97.9953\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 68.2262 - val_loss: 100.2524\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.9464 - val_loss: 104.1072\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.8231 - val_loss: 106.7628\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.9046 - val_loss: 93.4979\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.8633 - val_loss: 122.5289\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.7840 - val_loss: 109.8909\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.5803 - val_loss: 129.6497\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.8479 - val_loss: 94.6544\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.4401 - val_loss: 101.1477\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.3505 - val_loss: 120.0321\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.9178 - val_loss: 93.9108\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.1652 - val_loss: 125.7929\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.9660 - val_loss: 104.7236\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 67.0791 - val_loss: 94.1339\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.9478 - val_loss: 100.2448\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.6492 - val_loss: 100.0839\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.7996 - val_loss: 98.8350\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.7415 - val_loss: 122.5734\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.3493 - val_loss: 95.9251\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.2788 - val_loss: 99.6701\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.3270 - val_loss: 124.2866\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.3753 - val_loss: 101.1183\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.9981 - val_loss: 113.9119\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.0207 - val_loss: 133.8500\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.0913 - val_loss: 108.2976\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 66.3005 - val_loss: 89.3175\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.9395 - val_loss: 101.5875\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.8003 - val_loss: 95.6822\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.8682 - val_loss: 121.0709\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.6176 - val_loss: 100.9835\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.7414 - val_loss: 94.2002\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.5229 - val_loss: 103.0991\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.4961 - val_loss: 102.8797\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.6852 - val_loss: 110.1125\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.3676 - val_loss: 94.2832\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 65.2380 - val_loss: 122.7503\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.0962 - val_loss: 105.1258\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.1234 - val_loss: 94.9415\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.1805 - val_loss: 127.2573\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.9820 - val_loss: 115.3047\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 65.2508 - val_loss: 100.2129\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.9780 - val_loss: 99.4079\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.6933 - val_loss: 100.5033\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.7685 - val_loss: 101.4161\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.7697 - val_loss: 97.1858\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.7397 - val_loss: 106.3359\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.4821 - val_loss: 128.1255\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.4223 - val_loss: 162.8602\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.5540 - val_loss: 104.0539\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.4217 - val_loss: 99.0046\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.3704 - val_loss: 120.7238\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.7009 - val_loss: 108.8292\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.2906 - val_loss: 105.0236\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.2296 - val_loss: 96.6850\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.4541 - val_loss: 93.7468\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.9931 - val_loss: 100.1238\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.0335 - val_loss: 146.6149\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.1051 - val_loss: 90.7388\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.9584 - val_loss: 98.7583\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 64.1529 - val_loss: 142.0777\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.9345 - val_loss: 96.2740\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.8098 - val_loss: 95.4505\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.8794 - val_loss: 97.6739\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.7224 - val_loss: 96.7875\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.7534 - val_loss: 127.6554\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.7132 - val_loss: 100.0119\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.5314 - val_loss: 105.1467\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.6007 - val_loss: 92.7593\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.6200 - val_loss: 171.9409\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.3903 - val_loss: 101.6742\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.5184 - val_loss: 96.4542\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.6420 - val_loss: 104.1202\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.3025 - val_loss: 145.7802\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.2841 - val_loss: 101.3897\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.2406 - val_loss: 151.3342\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.2086 - val_loss: 136.8282\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.1415 - val_loss: 107.2600\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.9822 - val_loss: 121.3578\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.1352 - val_loss: 96.1067\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.8660 - val_loss: 94.2841\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.1288 - val_loss: 108.2650\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.9019 - val_loss: 123.4672\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 63.0236 - val_loss: 107.3591\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.9461 - val_loss: 110.9414\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.9393 - val_loss: 130.9069\n",
            "Epoch 156/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 62.8478 - val_loss: 113.3117\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.8161 - val_loss: 97.5249\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.7164 - val_loss: 102.6506\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.6218 - val_loss: 141.4676\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.8898 - val_loss: 104.0713\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.6775 - val_loss: 96.2428\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.6893 - val_loss: 114.8573\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.6658 - val_loss: 103.8433\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.5151 - val_loss: 97.1329\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.5072 - val_loss: 125.9352\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.3700 - val_loss: 101.4130\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.2647 - val_loss: 112.6382\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.5486 - val_loss: 101.3727\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.1661 - val_loss: 93.3607\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.4306 - val_loss: 97.9685\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.1879 - val_loss: 103.9246\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 62.6585 - val_loss: 111.9797\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.1915 - val_loss: 103.3177\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.2404 - val_loss: 99.1194\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.0987 - val_loss: 106.4904\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.2251 - val_loss: 98.2234\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.3811 - val_loss: 104.3834\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.9513 - val_loss: 94.0351\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.1706 - val_loss: 106.4816\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 62.0610 - val_loss: 104.0854\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.8859 - val_loss: 124.1922\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.9715 - val_loss: 95.7934\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.9292 - val_loss: 100.4742\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.8169 - val_loss: 125.4372\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.9538 - val_loss: 101.8177\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.8389 - val_loss: 113.7054\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.7958 - val_loss: 90.7372\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.8395 - val_loss: 96.6031\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.6173 - val_loss: 97.5450\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.5944 - val_loss: 96.7591\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.7630 - val_loss: 99.8661\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4281 - val_loss: 144.2478\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.6689 - val_loss: 105.6866\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4265 - val_loss: 103.3255\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4849 - val_loss: 105.2492\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.5518 - val_loss: 173.3358\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.6067 - val_loss: 104.5815\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4699 - val_loss: 126.7120\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.6956 - val_loss: 106.9970\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.3007 - val_loss: 99.7414\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4888 - val_loss: 130.8646\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.5782 - val_loss: 138.7913\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.5112 - val_loss: 105.0331\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.2825 - val_loss: 112.0659\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.2810 - val_loss: 126.2860\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.4397 - val_loss: 96.9879\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.2282 - val_loss: 102.3254\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.2349 - val_loss: 100.2708\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.2765 - val_loss: 101.0657\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.1210 - val_loss: 92.2108\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.2886 - val_loss: 98.8239\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0468 - val_loss: 95.6813\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0148 - val_loss: 97.4439\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.1704 - val_loss: 132.3248\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.1847 - val_loss: 106.1594\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.8375 - val_loss: 91.8729\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.1474 - val_loss: 121.1176\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0844 - val_loss: 128.6552\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0369 - val_loss: 94.1854\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0443 - val_loss: 117.9566\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.9094 - val_loss: 102.1806\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.9861 - val_loss: 97.6294\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0971 - val_loss: 95.6335\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.8525 - val_loss: 126.6039\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.9929 - val_loss: 117.3216\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 60.9330 - val_loss: 102.3380\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 61.0245 - val_loss: 150.7923\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.9687 - val_loss: 100.5131\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 60.7007 - val_loss: 98.9665\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.6612 - val_loss: 118.3254\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.8449 - val_loss: 102.1202\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.7443 - val_loss: 91.3887\n",
            "Epoch 233/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 60.7260 - val_loss: 132.8065\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.8963 - val_loss: 98.4798\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.7146 - val_loss: 109.1016\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 60.6127 - val_loss: 106.7931\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.5703 - val_loss: 98.7973\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4743 - val_loss: 124.5264\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.7043 - val_loss: 96.7856\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.6379 - val_loss: 132.3440\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.5279 - val_loss: 98.5235\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4409 - val_loss: 100.1012\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.5730 - val_loss: 99.8872\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.6619 - val_loss: 103.5733\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 60.5086 - val_loss: 102.1434\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4119 - val_loss: 98.3755\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.5967 - val_loss: 109.4394\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.5141 - val_loss: 105.6338\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4003 - val_loss: 96.9754\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4646 - val_loss: 100.2887\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 60.4365 - val_loss: 99.3063\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 60.2515 - val_loss: 101.9093\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.5313 - val_loss: 94.3397\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.2420 - val_loss: 94.8614\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4321 - val_loss: 101.8044\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.2539 - val_loss: 95.8689\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.4088 - val_loss: 97.3660\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.2394 - val_loss: 107.6480\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 60.1881 - val_loss: 103.0466\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 60.4283 - val_loss: 107.1861\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.2737 - val_loss: 106.2412\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 60.3913 - val_loss: 101.4661\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.2694 - val_loss: 94.4230\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1667 - val_loss: 105.0592\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.3381 - val_loss: 110.6022\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.2593 - val_loss: 126.1283\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1048 - val_loss: 102.3549\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 60.0679 - val_loss: 95.3587\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1922 - val_loss: 94.2121\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1577 - val_loss: 96.6061\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0674 - val_loss: 101.5969\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9861 - val_loss: 98.2344\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0199 - val_loss: 92.4753\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0030 - val_loss: 115.3898\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1263 - val_loss: 97.1234\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9588 - val_loss: 99.6844\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9282 - val_loss: 102.6503\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0634 - val_loss: 102.8024\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9335 - val_loss: 113.5256\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.7919 - val_loss: 100.0757\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1143 - val_loss: 98.8831\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.1688 - val_loss: 129.1835\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0352 - val_loss: 99.4563\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9541 - val_loss: 127.2204\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9792 - val_loss: 102.2756\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9569 - val_loss: 106.1640\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0110 - val_loss: 100.2071\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9899 - val_loss: 105.5611\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.9191 - val_loss: 116.8859\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8005 - val_loss: 97.7715\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0080 - val_loss: 97.6907\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 60.0275 - val_loss: 105.0214\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8512 - val_loss: 104.2982\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8110 - val_loss: 99.3921\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8087 - val_loss: 121.2888\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5713 - val_loss: 155.2348\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5801 - val_loss: 93.5450\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.6838 - val_loss: 120.0167\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.7127 - val_loss: 108.1346\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.8153 - val_loss: 96.2751\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.6024 - val_loss: 129.1914\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5209 - val_loss: 100.8500\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.7296 - val_loss: 105.9505\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.7056 - val_loss: 115.8889\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5503 - val_loss: 107.3289\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.6162 - val_loss: 125.0264\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.6829 - val_loss: 93.1432\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3869 - val_loss: 113.8162\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.6191 - val_loss: 103.9120\n",
            "Epoch 310/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 59.6602 - val_loss: 105.1903\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.5654 - val_loss: 105.4706\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5034 - val_loss: 100.4348\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.4507 - val_loss: 115.2472\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.6594 - val_loss: 100.5680\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.4368 - val_loss: 94.3356\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.3565 - val_loss: 111.1707\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3569 - val_loss: 94.7209\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.6672 - val_loss: 99.8301\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.4065 - val_loss: 115.8080\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.6272 - val_loss: 98.3260\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1886 - val_loss: 94.6588\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.5357 - val_loss: 120.5316\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2764 - val_loss: 99.9895\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3916 - val_loss: 113.9498\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 59.25 - 1s 3ms/step - loss: 59.2437 - val_loss: 120.1949\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3927 - val_loss: 125.6284\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.6024 - val_loss: 110.0965\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3407 - val_loss: 106.4253\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.3042 - val_loss: 93.9447\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.6629 - val_loss: 173.8948\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.6254 - val_loss: 125.7046\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.1064 - val_loss: 98.6399\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3370 - val_loss: 102.9213\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3952 - val_loss: 99.9625\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3489 - val_loss: 103.1206\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3543 - val_loss: 106.2944\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.4440 - val_loss: 100.6776\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0927 - val_loss: 110.2282\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3025 - val_loss: 103.3111\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2087 - val_loss: 109.3186\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2188 - val_loss: 105.2033\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.4774 - val_loss: 95.8518\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.1079 - val_loss: 99.6910\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0628 - val_loss: 99.2582\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3114 - val_loss: 91.8064\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0502 - val_loss: 103.7657\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1728 - val_loss: 114.8873\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2721 - val_loss: 101.4901\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9588 - val_loss: 105.4628\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.2217 - val_loss: 106.2577\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0733 - val_loss: 95.1804\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0080 - val_loss: 107.9047\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0245 - val_loss: 96.2187\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0172 - val_loss: 99.0629\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.3361 - val_loss: 108.1345\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9749 - val_loss: 116.2043\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0188 - val_loss: 106.6138\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9701 - val_loss: 97.3634\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 59.1149 - val_loss: 112.1678\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9674 - val_loss: 124.2744\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1414 - val_loss: 94.8644\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9189 - val_loss: 96.8975\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.1070 - val_loss: 100.6574\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9659 - val_loss: 100.7358\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0575 - val_loss: 102.3571\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8403 - val_loss: 98.8360\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9931 - val_loss: 101.5455\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9844 - val_loss: 95.3340\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9410 - val_loss: 98.3867\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.9877 - val_loss: 119.8420\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9097 - val_loss: 99.9004\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7248 - val_loss: 116.1989\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9894 - val_loss: 101.4932\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7942 - val_loss: 99.4968\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8541 - val_loss: 95.2818\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7726 - val_loss: 98.6661\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9198 - val_loss: 98.6207\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 59.0548 - val_loss: 96.9920\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6469 - val_loss: 122.3688\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7754 - val_loss: 96.8864\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.9316 - val_loss: 99.5828\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.8639 - val_loss: 95.1833\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6845 - val_loss: 103.6393\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7442 - val_loss: 97.4316\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7100 - val_loss: 104.5268\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.7137 - val_loss: 125.5651\n",
            "Epoch 387/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7911 - val_loss: 102.4703\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6763 - val_loss: 117.9995\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6626 - val_loss: 104.7178\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7052 - val_loss: 100.6233\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6291 - val_loss: 95.3662\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6899 - val_loss: 94.9072\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7022 - val_loss: 103.3497\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5147 - val_loss: 110.2233\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7017 - val_loss: 153.3262\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7015 - val_loss: 101.0761\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7824 - val_loss: 95.3149\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5672 - val_loss: 96.1222\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.7016 - val_loss: 89.0533\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4438 - val_loss: 103.1637\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6690 - val_loss: 97.9701\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6107 - val_loss: 147.6792\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6178 - val_loss: 100.3373\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6264 - val_loss: 98.6788\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4467 - val_loss: 99.3721\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.5232 - val_loss: 96.1989\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4740 - val_loss: 93.5589\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4965 - val_loss: 100.0965\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4794 - val_loss: 106.4171\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5424 - val_loss: 96.9128\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.6372 - val_loss: 104.2709\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5798 - val_loss: 106.9000\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.6679 - val_loss: 115.0189\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.6487 - val_loss: 100.2992\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4557 - val_loss: 99.5327\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3845 - val_loss: 93.4029\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4631 - val_loss: 101.7009\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4660 - val_loss: 116.5357\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.5413 - val_loss: 95.2076\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3653 - val_loss: 91.4354\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4770 - val_loss: 93.4516\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.2024 - val_loss: 100.2169\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2896 - val_loss: 98.0794\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2951 - val_loss: 126.2438\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.2453 - val_loss: 124.3551\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.6617 - val_loss: 99.5868\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3046 - val_loss: 144.3625\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.4760 - val_loss: 122.8251\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.2887 - val_loss: 98.8602\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3506 - val_loss: 109.5762\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2313 - val_loss: 95.0057\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.2729 - val_loss: 135.3164\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.2903 - val_loss: 91.4110\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2807 - val_loss: 116.5273\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1742 - val_loss: 102.7770\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.3790 - val_loss: 127.8635\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.1704 - val_loss: 92.4164\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.0424 - val_loss: 99.8343\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1291 - val_loss: 156.2388\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.1161 - val_loss: 94.4785\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.1958 - val_loss: 97.9393\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1854 - val_loss: 97.1498\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1532 - val_loss: 118.2240\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1561 - val_loss: 99.0183\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 58.3068 - val_loss: 102.6516\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2360 - val_loss: 102.5647\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1990 - val_loss: 100.0635\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2571 - val_loss: 95.9681\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1966 - val_loss: 93.1428\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1836 - val_loss: 103.3130\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1139 - val_loss: 116.3091\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.2859 - val_loss: 96.7709\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 57.9859 - val_loss: 101.7435\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1563 - val_loss: 109.9255\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1380 - val_loss: 142.3248\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0293 - val_loss: 107.1675\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0904 - val_loss: 97.2777\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9906 - val_loss: 101.6288\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 58.1220 - val_loss: 106.7884\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8600 - val_loss: 107.6219\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1232 - val_loss: 100.0268\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0542 - val_loss: 117.1124\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1135 - val_loss: 104.4483\n",
            "Epoch 464/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1035 - val_loss: 95.2374\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9749 - val_loss: 93.4273\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0247 - val_loss: 106.7766\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8009 - val_loss: 96.1944\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9230 - val_loss: 99.1846\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9943 - val_loss: 95.4306\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0198 - val_loss: 103.7376\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0383 - val_loss: 101.0753\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 57.9336 - val_loss: 97.9323\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0366 - val_loss: 127.2807\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 57.9279 - val_loss: 108.0823\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 57.7444 - val_loss: 98.8587\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.1265 - val_loss: 100.2289\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8822 - val_loss: 127.5600\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8881 - val_loss: 97.5518\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 57.9760 - val_loss: 105.1403\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.9749 - val_loss: 104.4368\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0577 - val_loss: 105.8042\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7420 - val_loss: 99.8211\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8314 - val_loss: 100.3454\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0338 - val_loss: 112.1845\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8641 - val_loss: 96.7852\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 57.9488 - val_loss: 94.5493\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8240 - val_loss: 108.7469\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6826 - val_loss: 148.3198\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7345 - val_loss: 106.0803\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8049 - val_loss: 124.0027\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7851 - val_loss: 100.8166\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8399 - val_loss: 94.0746\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8627 - val_loss: 116.7858\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.8460 - val_loss: 92.3240\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 58.0243 - val_loss: 115.1783\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.6403 - val_loss: 97.2508\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7666 - val_loss: 99.8795\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7942 - val_loss: 104.1744\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 57.8008 - val_loss: 111.5064\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 57.7252 - val_loss: 124.4333\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696v_fuFCTsa",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  5.226464691869462 \n",
            "MAE:  8.599506852715681 \n",
            "SD:  9.8548149264056\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdZF2osWCUQS",
        "outputId": "584bf1c6-0f85-43e6-ab6e-df44e03616fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  2.7488863436768205 \n",
            "Ensemble_std:  9.937268336557592\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 7,809\n",
            "Trainable params: 7,553\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3531.4404 - val_loss: 3460.0598\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 3013.0972 - val_loss: 3071.7036\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2333.8103 - val_loss: 2337.7554\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 1560.9373 - val_loss: 1066.5610\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 854.9070 - val_loss: 687.5303\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 360.8759 - val_loss: 194.8224\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 116.9680 - val_loss: 178.4025\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 48.1225 - val_loss: 58.3140\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4232 - val_loss: 57.4189\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.0886 - val_loss: 64.8494\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 32.3794 - val_loss: 56.1920\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.8344 - val_loss: 51.1143\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 31.3811 - val_loss: 39.0783\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 30.9827 - val_loss: 42.4023\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 30.6201 - val_loss: 51.9482\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.2736 - val_loss: 42.6997\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 30.0279 - val_loss: 42.8691\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 29.9015 - val_loss: 45.7677\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 29.5851 - val_loss: 34.8913\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 29.4023 - val_loss: 41.2035\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 29.2505 - val_loss: 38.5924\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 29.1163 - val_loss: 47.7678\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 29.0269 - val_loss: 57.5390\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 28.8480 - val_loss: 49.4497\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.5976 - val_loss: 39.7599\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 28.5068 - val_loss: 47.7804\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.4734 - val_loss: 37.3057\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 28.3125 - val_loss: 46.2229\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.1118 - val_loss: 37.8062\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.9392 - val_loss: 40.0999\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.8778 - val_loss: 38.9542\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.8398 - val_loss: 55.4798\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.7080 - val_loss: 36.5941\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.6582 - val_loss: 36.0161\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.4215 - val_loss: 41.7122\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.4233 - val_loss: 36.9728\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.3714 - val_loss: 36.4649\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.2278 - val_loss: 35.9820\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.0919 - val_loss: 46.7494\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.1606 - val_loss: 37.6667\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.0318 - val_loss: 40.9807\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.9468 - val_loss: 54.3983\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.9314 - val_loss: 38.6836\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.7062 - val_loss: 35.6394\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.7509 - val_loss: 37.4105\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.6138 - val_loss: 56.6562\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.6137 - val_loss: 36.9617\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.5976 - val_loss: 38.2047\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.4528 - val_loss: 37.9156\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.3699 - val_loss: 38.1006\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.3163 - val_loss: 37.4646\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.2824 - val_loss: 36.9070\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.3012 - val_loss: 39.2150\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.1149 - val_loss: 38.6254\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.1849 - val_loss: 34.8712\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.1532 - val_loss: 40.3533\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.1268 - val_loss: 37.0791\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.9815 - val_loss: 38.7959\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.9556 - val_loss: 38.4358\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.9078 - val_loss: 49.4846\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.8805 - val_loss: 35.7083\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.8340 - val_loss: 66.9004\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.8149 - val_loss: 40.7681\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.8478 - val_loss: 38.8708\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.8399 - val_loss: 37.3255\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.6961 - val_loss: 44.3711\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.5849 - val_loss: 40.4075\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.5401 - val_loss: 38.2858\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.5263 - val_loss: 38.2441\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.4979 - val_loss: 41.4935\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4766 - val_loss: 38.0429\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5217 - val_loss: 39.3385\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4140 - val_loss: 49.4441\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3602 - val_loss: 36.5642\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.2745 - val_loss: 42.3267\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.3164 - val_loss: 38.6195\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.1758 - val_loss: 39.0955\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.2856 - val_loss: 42.6364\n",
            "Epoch 79/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 25.2072 - val_loss: 47.3638\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.2148 - val_loss: 46.7577\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.0875 - val_loss: 45.2552\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.2158 - val_loss: 39.3338\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.1607 - val_loss: 39.8156\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.0114 - val_loss: 39.3698\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.0465 - val_loss: 39.3900\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.0530 - val_loss: 37.0755\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.9643 - val_loss: 40.1636\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.9054 - val_loss: 35.2164\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.8783 - val_loss: 40.8191\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.8394 - val_loss: 40.7516\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.9377 - val_loss: 37.6403\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8051 - val_loss: 36.9451\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.8215 - val_loss: 39.4894\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.8634 - val_loss: 40.1578\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.8325 - val_loss: 42.9659\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.7271 - val_loss: 39.1382\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.7699 - val_loss: 54.5244\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.7231 - val_loss: 39.3747\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.6624 - val_loss: 39.1218\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.6247 - val_loss: 55.1552\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.7040 - val_loss: 53.5213\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.6134 - val_loss: 36.1504\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.5570 - val_loss: 40.2829\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.6054 - val_loss: 36.8211\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.5570 - val_loss: 37.3988\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.5405 - val_loss: 35.2615\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.4307 - val_loss: 38.4389\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5358 - val_loss: 40.4790\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.5046 - val_loss: 47.3131\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.5346 - val_loss: 42.5234\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.4798 - val_loss: 37.9512\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.3454 - val_loss: 36.8081\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.3876 - val_loss: 51.9801\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.3939 - val_loss: 38.4079\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2670 - val_loss: 35.2286\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.3629 - val_loss: 35.8633\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.4107 - val_loss: 41.9988\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2880 - val_loss: 41.4700\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.3226 - val_loss: 36.7883\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2809 - val_loss: 37.7232\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2092 - val_loss: 41.4390\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.1740 - val_loss: 38.2125\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2543 - val_loss: 36.9719\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.1668 - val_loss: 42.4089\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2487 - val_loss: 42.5619\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.0652 - val_loss: 44.5737\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2143 - val_loss: 35.3968\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.1227 - val_loss: 32.6395\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.0534 - val_loss: 38.6378\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.1047 - val_loss: 35.3252\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.0596 - val_loss: 37.8187\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.0374 - val_loss: 41.5137\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.0066 - val_loss: 37.5176\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.0758 - val_loss: 43.8896\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.9943 - val_loss: 36.9867\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.9672 - val_loss: 37.9933\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.0060 - val_loss: 39.2265\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.9917 - val_loss: 39.0152\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.0371 - val_loss: 38.0928\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.8829 - val_loss: 33.5283\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.9174 - val_loss: 39.2040\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.8936 - val_loss: 40.6721\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.9176 - val_loss: 67.3491\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.8968 - val_loss: 36.6192\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.8614 - val_loss: 42.0413\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7906 - val_loss: 51.1120\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.8616 - val_loss: 35.7229\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.8925 - val_loss: 48.1856\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7936 - val_loss: 40.3674\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.7429 - val_loss: 35.0464\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.6695 - val_loss: 38.3698\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7067 - val_loss: 55.5130\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.6803 - val_loss: 43.1907\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.6880 - val_loss: 36.9251\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7120 - val_loss: 47.0513\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7403 - val_loss: 37.4339\n",
            "Epoch 157/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7023 - val_loss: 42.5050\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.6638 - val_loss: 40.0257\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.6627 - val_loss: 38.9038\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.6856 - val_loss: 37.5752\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.5344 - val_loss: 40.5031\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.6384 - val_loss: 36.4698\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.6806 - val_loss: 37.8909\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.5221 - val_loss: 35.5869\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.6242 - val_loss: 36.0256\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.5736 - val_loss: 50.9205\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.5067 - val_loss: 38.4088\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.5294 - val_loss: 35.1982\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5448 - val_loss: 42.0470\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.4566 - val_loss: 41.3135\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5477 - val_loss: 39.9859\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.5026 - val_loss: 37.0564\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.4569 - val_loss: 35.8900\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5014 - val_loss: 36.5518\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.4916 - val_loss: 37.5175\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.4372 - val_loss: 38.4917\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.4831 - val_loss: 32.3637\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5109 - val_loss: 38.9668\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.3654 - val_loss: 37.2765\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3812 - val_loss: 39.0749\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.2851 - val_loss: 35.2421\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2983 - val_loss: 35.6830\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 23.4547 - val_loss: 55.4338\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3366 - val_loss: 38.9328\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.2586 - val_loss: 34.2281\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3254 - val_loss: 39.1504\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2621 - val_loss: 35.0576\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.3231 - val_loss: 34.5938\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3322 - val_loss: 42.4072\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.2598 - val_loss: 38.0568\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2168 - val_loss: 37.4194\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.2059 - val_loss: 38.0699\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2554 - val_loss: 35.4444\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.2282 - val_loss: 42.8794\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2464 - val_loss: 55.8041\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2167 - val_loss: 39.4573\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.2470 - val_loss: 33.8303\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1197 - val_loss: 57.4542\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.1826 - val_loss: 38.6946\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2217 - val_loss: 37.1722\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.1375 - val_loss: 34.5005\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0979 - val_loss: 39.3569\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.1567 - val_loss: 36.2497\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1166 - val_loss: 43.8638\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.1097 - val_loss: 35.9665\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2044 - val_loss: 34.5433\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1169 - val_loss: 43.6198\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 23.1049 - val_loss: 34.0063\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1154 - val_loss: 35.7681\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.1514 - val_loss: 36.1309\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0599 - val_loss: 36.6695\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.0518 - val_loss: 41.4554\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0287 - val_loss: 39.6012\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9559 - val_loss: 40.0660\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0376 - val_loss: 34.6042\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9794 - val_loss: 61.2020\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0673 - val_loss: 36.9345\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9793 - val_loss: 37.1171\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.0326 - val_loss: 49.7535\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9297 - val_loss: 35.7866\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.0432 - val_loss: 40.1677\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9553 - val_loss: 52.8559\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9740 - val_loss: 35.6070\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9405 - val_loss: 38.0406\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8753 - val_loss: 39.4768\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.0286 - val_loss: 36.3952\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9344 - val_loss: 36.4823\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9195 - val_loss: 36.3940\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9361 - val_loss: 36.4827\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9061 - val_loss: 37.7172\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9108 - val_loss: 36.1917\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.0015 - val_loss: 34.3494\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9392 - val_loss: 41.0462\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7810 - val_loss: 38.7315\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8693 - val_loss: 38.7378\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9154 - val_loss: 38.0489\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8669 - val_loss: 34.1956\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9430 - val_loss: 40.5041\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8886 - val_loss: 36.5074\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 22.8554 - val_loss: 36.6505\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 5s 33ms/step - loss: 22.8803 - val_loss: 33.7858\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 5s 29ms/step - loss: 22.8372 - val_loss: 36.7192\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8624 - val_loss: 33.7553\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8445 - val_loss: 35.4042\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8597 - val_loss: 58.6995\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8006 - val_loss: 35.9067\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8788 - val_loss: 44.9107\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.8470 - val_loss: 49.2839\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7996 - val_loss: 35.9521\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 22.8209 - val_loss: 39.1352\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.7337 - val_loss: 40.8723\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7109 - val_loss: 39.8421\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8632 - val_loss: 43.6252\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7194 - val_loss: 42.8779\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7625 - val_loss: 50.9857\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7807 - val_loss: 56.0595\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.7810 - val_loss: 36.6112\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7318 - val_loss: 36.4069\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8164 - val_loss: 39.2668\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.7737 - val_loss: 34.7627\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7488 - val_loss: 36.2045\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7146 - val_loss: 38.2158\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7407 - val_loss: 42.1861\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7064 - val_loss: 45.0672\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6822 - val_loss: 43.5558\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.7337 - val_loss: 35.7088\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7985 - val_loss: 39.6936\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6070 - val_loss: 39.0951\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7694 - val_loss: 38.4115\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6216 - val_loss: 36.3616\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7052 - val_loss: 47.4963\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.6507 - val_loss: 34.1026\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5979 - val_loss: 38.1527\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6425 - val_loss: 35.6801\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6800 - val_loss: 34.2879\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.5953 - val_loss: 34.0714\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 8s 47ms/step - loss: 22.6604 - val_loss: 36.2543\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6815 - val_loss: 35.9799\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6937 - val_loss: 38.1068\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.6233 - val_loss: 36.8485\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6349 - val_loss: 39.0245\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6750 - val_loss: 40.5422\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 5s 33ms/step - loss: 22.7191 - val_loss: 39.4879\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 22.6296 - val_loss: 41.8699\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5370 - val_loss: 36.0023\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6513 - val_loss: 37.9412\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5459 - val_loss: 37.6053\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 3s 20ms/step - loss: 22.6358 - val_loss: 39.7904\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 5s 30ms/step - loss: 22.5900 - val_loss: 36.4234\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6210 - val_loss: 40.3872\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.6741 - val_loss: 37.0845\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5701 - val_loss: 39.2695\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5580 - val_loss: 35.6047\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 5s 32ms/step - loss: 22.5927 - val_loss: 39.9907\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 3s 18ms/step - loss: 22.6312 - val_loss: 41.0779\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.5954 - val_loss: 35.5866\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 5s 33ms/step - loss: 22.5840 - val_loss: 36.0400\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.6742 - val_loss: 33.9054\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 5s 30ms/step - loss: 22.5781 - val_loss: 34.5927\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5709 - val_loss: 34.7548\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.5544 - val_loss: 34.4786\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5434 - val_loss: 38.8144\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5572 - val_loss: 36.0740\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.5530 - val_loss: 38.8743\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5620 - val_loss: 51.1359\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 5s 33ms/step - loss: 22.5324 - val_loss: 34.8584\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 22.6352 - val_loss: 38.2496\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5553 - val_loss: 39.9197\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 22.5560 - val_loss: 37.3447\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.6219 - val_loss: 37.8539\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 5s 29ms/step - loss: 22.5267 - val_loss: 39.3805\n",
            "Epoch 312/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 3s 21ms/step - loss: 22.5276 - val_loss: 33.8225\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4898 - val_loss: 35.7961\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5298 - val_loss: 36.4305\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5761 - val_loss: 39.8009\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4658 - val_loss: 35.4860\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.4985 - val_loss: 37.5580\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4477 - val_loss: 35.0344\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4794 - val_loss: 60.4822\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5526 - val_loss: 35.5521\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4340 - val_loss: 37.7291\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5030 - val_loss: 36.0394\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5043 - val_loss: 37.2838\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 22.4685 - val_loss: 46.7931\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6197 - val_loss: 34.2118\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3675 - val_loss: 37.5774\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5614 - val_loss: 36.5275\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4982 - val_loss: 38.6671\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5063 - val_loss: 43.2956\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4792 - val_loss: 35.5836\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.4133 - val_loss: 36.5880\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 22.4875 - val_loss: 33.7336\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3990 - val_loss: 41.6613\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4044 - val_loss: 38.6604\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.3946 - val_loss: 36.2216\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.3694 - val_loss: 42.9987\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.4263 - val_loss: 36.5449\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.4097 - val_loss: 37.0582\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.4139 - val_loss: 38.3198\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.3744 - val_loss: 39.1302\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.4535 - val_loss: 34.4363\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.3277 - val_loss: 35.3865\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4563 - val_loss: 43.9166\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4051 - val_loss: 40.3582\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.3936 - val_loss: 36.9844\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4202 - val_loss: 34.5963\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3520 - val_loss: 45.3311\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3421 - val_loss: 35.7761\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3508 - val_loss: 33.5340\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3585 - val_loss: 38.6240\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3497 - val_loss: 39.7996\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4010 - val_loss: 40.8973\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3446 - val_loss: 46.7620\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4234 - val_loss: 37.0538\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.3537 - val_loss: 38.3508\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3963 - val_loss: 34.5365\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.3814 - val_loss: 38.9112\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.3369 - val_loss: 37.0012\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3699 - val_loss: 34.7885\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2748 - val_loss: 36.3708\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3387 - val_loss: 35.2945\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3292 - val_loss: 43.5658\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3252 - val_loss: 40.2283\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3792 - val_loss: 37.2228\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3440 - val_loss: 34.8212\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3081 - val_loss: 33.6422\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3004 - val_loss: 35.9895\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.2998 - val_loss: 34.3718\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3087 - val_loss: 44.1918\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2511 - val_loss: 37.9627\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.2928 - val_loss: 33.8976\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.3199 - val_loss: 42.8451\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2760 - val_loss: 36.0413\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3112 - val_loss: 40.1908\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3447 - val_loss: 35.8253\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2833 - val_loss: 35.2008\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2944 - val_loss: 33.8703\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2317 - val_loss: 37.1794\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2758 - val_loss: 35.4134\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1990 - val_loss: 38.7936\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2583 - val_loss: 36.1380\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2046 - val_loss: 40.3277\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2840 - val_loss: 34.9707\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2252 - val_loss: 35.8266\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2567 - val_loss: 36.4952\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2550 - val_loss: 34.9235\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2701 - val_loss: 37.0368\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3050 - val_loss: 41.2815\n",
            "Epoch 389/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2493 - val_loss: 36.2005\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2555 - val_loss: 35.9252\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2707 - val_loss: 36.4241\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2432 - val_loss: 37.7162\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1827 - val_loss: 37.1572\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2307 - val_loss: 32.8454\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2248 - val_loss: 33.5184\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1869 - val_loss: 37.6572\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3115 - val_loss: 34.2288\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2404 - val_loss: 41.8039\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1717 - val_loss: 34.6066\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1823 - val_loss: 33.2007\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2076 - val_loss: 37.6541\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.2561 - val_loss: 35.9951\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 22.1574 - val_loss: 38.4981\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 22.2322 - val_loss: 41.3206\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1851 - val_loss: 36.3895\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.1401 - val_loss: 33.7774\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1550 - val_loss: 37.2207\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.1383 - val_loss: 51.0771\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0875 - val_loss: 35.4245\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2193 - val_loss: 36.2040\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2300 - val_loss: 47.2462\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1872 - val_loss: 44.5728\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0964 - val_loss: 37.7133\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1597 - val_loss: 47.4883\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1734 - val_loss: 35.1297\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1293 - val_loss: 36.5977\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 22.1665 - val_loss: 40.1982\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1463 - val_loss: 37.1505\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1311 - val_loss: 43.0526\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2060 - val_loss: 36.2350\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 22.1701 - val_loss: 34.4721\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.1691 - val_loss: 38.7036\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1238 - val_loss: 37.9501\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1606 - val_loss: 35.2610\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1194 - val_loss: 50.2587\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1515 - val_loss: 35.7205\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1250 - val_loss: 35.5202\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1082 - val_loss: 39.8931\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0949 - val_loss: 37.8820\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1167 - val_loss: 50.7138\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.1808 - val_loss: 52.2780\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0927 - val_loss: 35.8164\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1811 - val_loss: 34.2504\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1299 - val_loss: 35.6719\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1047 - val_loss: 35.7223\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0638 - val_loss: 37.9666\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.1162 - val_loss: 41.8816\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1247 - val_loss: 33.9364\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1291 - val_loss: 34.8465\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.1405 - val_loss: 34.6059\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1365 - val_loss: 37.1427\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0815 - val_loss: 37.5033\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0757 - val_loss: 35.0738\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.0783 - val_loss: 36.6019\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1321 - val_loss: 39.0255\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1084 - val_loss: 39.2094\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0791 - val_loss: 35.9764\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1060 - val_loss: 33.9661\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0054 - val_loss: 37.4232\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0637 - val_loss: 47.0775\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0707 - val_loss: 39.3262\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.0912 - val_loss: 35.9501\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0919 - val_loss: 36.4255\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0918 - val_loss: 36.5271\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9824 - val_loss: 34.3342\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1061 - val_loss: 39.2723\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0482 - val_loss: 34.9599\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9894 - val_loss: 35.1350\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0077 - val_loss: 34.8728\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1038 - val_loss: 43.3085\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0307 - val_loss: 33.9288\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0492 - val_loss: 36.1981\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0130 - val_loss: 35.4079\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0439 - val_loss: 33.9576\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0561 - val_loss: 34.8309\n",
            "Epoch 466/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0467 - val_loss: 36.2317\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 21.9699 - val_loss: 36.7268\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0549 - val_loss: 44.5672\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.0364 - val_loss: 39.1618\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0617 - val_loss: 39.1422\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 21.9917 - val_loss: 47.1775\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.0152 - val_loss: 46.2528\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9485 - val_loss: 36.0046\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0118 - val_loss: 33.9519\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0175 - val_loss: 39.2286\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0355 - val_loss: 33.8732\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0379 - val_loss: 34.3837\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0011 - val_loss: 34.8520\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9661 - val_loss: 33.8709\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9413 - val_loss: 35.9093\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 21.9801 - val_loss: 40.8869\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 21.9304 - val_loss: 39.3582\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9499 - val_loss: 34.6669\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9317 - val_loss: 36.8089\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9900 - val_loss: 33.8235\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0079 - val_loss: 37.8142\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.9820 - val_loss: 36.6182\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9280 - val_loss: 34.4522\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8896 - val_loss: 34.5137\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0222 - val_loss: 35.4341\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9539 - val_loss: 37.8057\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9588 - val_loss: 40.4176\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9691 - val_loss: 35.7890\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9436 - val_loss: 32.6646\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9421 - val_loss: 37.4153\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.9177 - val_loss: 33.7934\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 21.9717 - val_loss: 36.1921\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8646 - val_loss: 34.6106\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9651 - val_loss: 34.0630\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9292 - val_loss: 34.5022\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  1.2683924838257687 \n",
            "MAE:  4.3535561863843 \n",
            "SD:  5.735278040343333\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cip38xZOZnV",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAFBCAYAAAAsfIegAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2GklEQVR4nO2deXgV1fnHv28WE2TVCIiALC2KSjAgIEpdKta1bnVBxQ1RWmur/LRVcK191NbivpRFoYKiiFYKFRQUKZS6ETBsguxIIhLCvoRAkvf3x5mTmXtzk9yb3Js7Gb+f55lnZs6cOXPOzJnvvPPOOWdEVUEIISR+pCQ7A4QQEjQorIQQEmcorIQQEmcorIQQEmcorIQQEmcorIQQEmcSJqwikikiX4nIYhFZLiKPOeGdRORLEVkjIu+IyGFOeIazvsbZ3jFReSOEkESSSIu1BMA5qnoygBwAF4hIXwBPAXhOVX8KYAeAwU78wQB2OOHPOfEIIaTBkTBhVcNeZzXdmRTAOQDec8LHA7jcWb7MWYezvb+ISKLyRwghiSKhPlYRSRWRPACFAD4GsBbATlUtdaLkA2jrLLcFsAkAnO27AGQlMn+EEJII0hKZuKqWAcgRkRYApgDoWtc0RWQIgCEA0Lhx41O6du2KDV/vwB40RXaPhBaHEPIjYeHChUWq2rK2+9eLEqnqThGZA+A0AC1EJM2xStsBKHCiFQBoDyBfRNIANAewLUJaYwCMAYBevXppbm4uBjWZjNnoj9xcGriEkLojIhvrsn8iWwW0dCxViEgjAL8AsALAHABXOdFuBjDVWZ7mrMPZ/qlGOUIMHbGEED+RSIu1DYDxIpIKI+CTVfUDEfkGwCQReRzA1wDGOvHHAnhDRNYA2A7g2lgOxkG6CCF+IWHCqqpLAPSIEL4OQJ8I4QcAXF2bYwkUSruVEOITAvG1R0RNQy5CfM6hQ4eQn5+PAwcOJDsrBEBmZibatWuH9PT0uKYbCGEFAFVarMT/5Ofno2nTpujYsSPYTDu5qCq2bduG/Px8dOrUKa5pB2KsAAENVtIwOHDgALKysiiqPkBEkJWVlZC3h4AIK2WVNBwoqv4hUdciEMIKgB+vCCG+IRDCKqJsbkVIA6dJkyZVbtuwYQO6detWj7mpG8EQ1mRngBBCPARCWAG6AgiJlg0bNqBr16645ZZbcNxxx2HgwIH45JNP0K9fP3Tp0gVfffUV5s6di5ycHOTk5KBHjx7Ys2cPAGDEiBHo3bs3unfvjkcffbTKYwwbNgyvvPJKxfqf/vQnPP3009i7dy/69++Pnj17Ijs7G1OnTq0yjao4cOAABg0ahOzsbPTo0QNz5swBACxfvhx9+vRBTk4OunfvjtWrV2Pfvn24+OKLcfLJJ6Nbt2545513Yj5ebQhEcyuBsrkVaXgMHQrk5cU3zZwc4Pnna4y2Zs0avPvuuxg3bhx69+6Nt956C/Pnz8e0adPw5JNPoqysDK+88gr69euHvXv3IjMzE7NmzcLq1avx1VdfQVVx6aWXYt68eTjzzDMrpT9gwAAMHToUd955JwBg8uTJmDlzJjIzMzFlyhQ0a9YMRUVF6Nu3Ly699NKYPiK98sorEBEsXboUK1euxHnnnYdVq1Zh1KhRuPvuuzFw4EAcPHgQZWVlmDFjBo455hhMnz4dALBr166oj1MXAmGxitDBSkgsdOrUCdnZ2UhJScFJJ52E/v37Q0SQnZ2NDRs2oF+/frjnnnvw4osvYufOnUhLS8OsWbMwa9Ys9OjRAz179sTKlSuxevXqiOn36NEDhYWF+P7777F48WIcccQRaN++PVQVDzzwALp3745zzz0XBQUF2LJlS0x5nz9/Pm644QYAQNeuXdGhQwesWrUKp512Gp588kk89dRT2LhxIxo1aoTs7Gx8/PHHuP/++/Hf//4XzZs3r/O5i4ZAWKwA27GSBkgUlmWiyMjIqFhOSUmpWE9JSUFpaSmGDRuGiy++GDNmzEC/fv0wc+ZMqCqGDx+OX//611Ed4+qrr8Z7772HH374AQMGDAAATJw4EVu3bsXChQuRnp6Ojh07xq0d6fXXX49TTz0V06dPx0UXXYTRo0fjnHPOwaJFizBjxgw89NBD6N+/Px555JG4HK86AiGsdAIQEl/Wrl2L7OxsZGdnY8GCBVi5ciXOP/98PPzwwxg4cCCaNGmCgoICpKeno1WrVhHTGDBgAG6//XYUFRVh7ty5AMyreKtWrZCeno45c+Zg48bYR+c744wzMHHiRJxzzjlYtWoVvvvuOxx//PFYt24dOnfujLvuugvfffcdlixZgq5du+LII4/EDTfcgBYtWuC1116r03mJlkAIK8CPV4TEk+effx5z5sypcBVceOGFyMjIwIoVK3DaaacBMM2j3nzzzSqF9aSTTsKePXvQtm1btGnTBgAwcOBAXHLJJcjOzkavXr3QtWvsY9//9re/xR133IHs7GykpaXh9ddfR0ZGBiZPnow33ngD6enpOProo/HAAw9gwYIF+OMf/4iUlBSkp6dj5MiRtT8pMSBRDnnqS+xA179r/gbePnA5tpU0TXaWCKmWFStW4IQTTkh2NoiHSNdERBaqaq/aphmMj1f0sBJCfEQwXAFCVwAhyWDbtm3o379/pfDZs2cjKyv2XyUtXboUN954Y0hYRkYGvvzyy1rnMRkEQlgFHDaQkGSQlZWFvDi2xc3Ozo5resmCrgBCCIkzgRBW4woghBB/EAhh5T+vCCF+IjDCSgghfiEQwgoIx2MlxGdUN75q0AmEsNIVQAjxE8FobsXRrUgDJFmjBm7YsAEXXHAB+vbti88++wy9e/fGoEGD8Oijj6KwsBATJ05EcXEx7r77bgDmv1Dz5s1D06ZNMWLECEyePBklJSW44oor8Nhjj9WYJ1XFfffdhw8//BAigoceeggDBgzA5s2bMWDAAOzevRulpaUYOXIkTj/9dAwePBi5ubkQEdx66634v//7v7qfmHomEMJqbFZarIRES6LHY/Xy/vvvIy8vD4sXL0ZRURF69+6NM888E2+99RbOP/98PPjggygrK8P+/fuRl5eHgoICLFu2DACwc+fOejgb8ScQwmoGuk52LgiJjSSOGlgxHiuAiOOxXnvttbjnnnswcOBA/OpXv0K7du1CxmMFgL1792L16tU1Cuv8+fNx3XXXITU1Fa1bt8ZZZ52FBQsWoHfv3rj11ltx6NAhXH755cjJyUHnzp2xbt06/P73v8fFF1+M8847L+HnIhEExsdKCImeaMZjfe2111BcXIx+/fph5cqVFeOx5uXlIS8vD2vWrMHgwYNrnYczzzwT8+bNQ9u2bXHLLbdgwoQJOOKII7B48WKcffbZGDVqFG677bY6lzUZBEJYOVYAIfHFjsd6//33o3fv3hXjsY4bNw579+4FABQUFKCwsLDGtM444wy88847KCsrw9atWzFv3jz06dMHGzduROvWrXH77bfjtttuw6JFi1BUVITy8nJceeWVePzxx7Fo0aJEFzUhBMQVQAiJJ/EYj9VyxRVX4PPPP8fJJ58MEcHf/vY3HH300Rg/fjxGjBiB9PR0NGnSBBMmTEBBQQEGDRqE8vJyAMBf/vKXhJc1EQRiPNb7ssbipZ03orjssGRniZBq4Xis/oPjsVaBiNLLSgjxDQFxBVBWCUkG8R6PNSgEQlgBfrwiJBnEezzWoBAMVwA40DVpODTk7xpBI1HXIiDCyopKGgaZmZnYtm0bxdUHqCq2bduGzMzMuKcdIFcAIf6nXbt2yM/Px9atW5OdFQLzoGvXrl3c002YsIpIewATALSG0b0xqvqCiPwJwO0AbM16QFVnOPsMBzAYQBmAu1R1ZnTH4uhWpGGQnp6OTp06JTsbJMEk0mItBXCvqi4SkaYAForIx86251T1aW9kETkRwLUATgJwDIBPROQ4VS2r6UCUVEKIn0iYj1VVN6vqImd5D4AVANpWs8tlACapaomqrgewBkCf6I9Xl9wSQkj8qJePVyLSEUAPAPbn4L8TkSUiMk5EjnDC2gLY5NktH9ULsZs+B7omhPiIhAuriDQB8E8AQ1V1N4CRAH4CIAfAZgDPxJjeEBHJFZFc+wFAqKmEEB+RUGEVkXQYUZ2oqu8DgKpuUdUyVS0H8Crc1/0CAO09u7dzwkJQ1TGq2ktVe7Vs2dIND0bLMUJIAEiYGomIABgLYIWqPusJb+OJdgWAZc7yNADXikiGiHQC0AXAV1Edi42tCCE+IpGtAvoBuBHAUhHJc8IeAHCdiOTANMHaAODXAKCqy0VkMoBvYFoU3BlNiwAA9AUQQnxFwoRVVecjckuoGdXs8wSAJ2I9lrVYVamxhJDkEwjHJF0BhBA/EQhhtXYx27ISQvxAIITVvv1TWAkhfiAgwkpFJYT4h0AIq4UWKyHEDwRCWEXcVgGEEJJsgiGsyc4AIYR4CISwWmixEkL8QCCEla4AQoifCIawJjsDhBDiIRDCaqHFSgjxA4EQVu9YAYQQkmyCIaxCRSWE+IdACKuFFishxA8EQlj58YoQ4icCIawWWqyEED8QCGFlO1ZCiJ8IhrAmOwOEEOIhEMJqocVKCPEDgRBWugIIIX4iGMKa7AwQQoiHQAirhRYrIcQPBEJY2aWVEOIngiGs9AUQQnxEIITVQouVEOIHAiGsdAUQQvxEMISVo1sRQnxEIITVNriixUoI8QOBEFbrCiCEED8QCGG10GIlhPiBQAirbW5FYSWE+IFgCCtdAYQQHxEIYbXQYiWE+IFACCtdAYQQPxEMYaUrgBDiIwIhrBZarIQQPxAIYaUrgBDiJxImrCLSXkTmiMg3IrJcRO52wo8UkY9FZLUzP8IJFxF5UUTWiMgSEekZ/bGoqIQQ/5BIi7UUwL2qeiKAvgDuFJETAQwDMFtVuwCY7awDwIUAujjTEAAjYz0gLVZCiB9ImLCq6mZVXeQs7wGwAkBbAJcBGO9EGw/gcmf5MgAT1PAFgBYi0iaaY9nhWCmshBA/UC8+VhHpCKAHgC8BtFbVzc6mHwC0dpbbAtjk2S3fCYsifSoqIcQ/JFxYRaQJgH8CGKqqu73bVFWB2NpKicgQEckVkdytW7eGbKPFSgjxAwkVVhFJhxHViar6vhO8xb7iO/NCJ7wAQHvP7u2csBBUdYyq9lLVXi1btjTHqdgW/zIQQkisJLJVgAAYC2CFqj7r2TQNwM3O8s0ApnrCb3JaB/QFsMvjMqjhWFRUQoh/SEtg2v0A3AhgqYjkOWEPAPgrgMkiMhjARgDXONtmALgIwBoA+wEMivWAtFgJIX4gYcKqqvPhvqWH0z9CfAVwZ22OJfxNKyHERwSi5xUOOwwALVZCiD8IhLDKYekAKKyEEH8QDGHNOCzZWSCEkAoCIax0BRBC/EQghJWuAEKInwiGsNIVQAjxEYEQVqQ7FmtpWZIzQgghARFWa7Fq8YEk54QQQgImrDhAYSWEJJ9ACGtFqwBarIQQHxAIYaUrgBDiJwIlrHQFEEL8QCCEla4AQoifCISw0mIlhPiJQAhrRTvWkoNJzgghhAREWO1wrFrOPq2EkOQTDGFNscpKYSWEJJ9ACKs1WamrhBA/EAhhrXAFlJUnNyOEEIKgCGtKxQ+wk5oPQggBAiKsqPh4ldxsEEIIEBBhlRRTDLYKIIT4gWAIa4UngMJKCEk+gRDWilYBtFgJIT4gEMJqP17RYCWE+IFACSvK+fWKEJJ8AiGsFlqshBA/EAhhlVT6WAkh/iEYwiocK4AQ4h8CIaxsFUAI8ROBEFa3SyshhCSfqIRVRBqLSIqzfJyIXCoi6YnNWgxYi5WDsBBCfEC0Fus8AJki0hbALAA3Ang9UZmKFbpYCSF+IlphFVXdD+BXAP6uqlcDOClx2YoNSXWKQWUlhPiAqIVVRE4DMBDAdCcsNTFZqj38eEUI8QPRCutQAMMBTFHV5SLSGcCchOUqRtillRDiJ6ISVlWdq6qXqupTzkesIlW9q7p9RGSciBSKyDJP2J9EpEBE8pzpIs+24SKyRkS+FZHzYymE2FJQWQkhPiDaVgFviUgzEWkMYBmAb0TkjzXs9jqACyKEP6eqOc40w0n/RADXwvhtLwDwdxGJ3tUgHI+VEOIfonUFnKiquwFcDuBDAJ1gWgZUiarOA7A9yvQvAzBJVUtUdT2ANQD6RLmv6wqgsBJCfEC0wprutFu9HMA0VT2E2v9g6ncissRxFRzhhLUFsMkTJ98JiwoOdE0I8RPRCutoABsANAYwT0Q6ANhdi+ONBPATADkANgN4JtYERGSIiOSKSO7WrVtNIH/NQgjxEdF+vHpRVduq6kVq2Ajg57EeTFW3qGqZqpYDeBXu634BgPaeqO2csEhpjFHVXqraq2XLlgDYQYAQ4i+i/XjVXESetZaiiDwDY73GhIi08axeAfMhDACmAbhWRDJEpBOALgC+ijrdFCorIcQ/pEUZbxyMCF7jrN8I4B8wPbEiIiJvAzgbwFEikg/gUQBni0gOjH92A4BfA4DTNnYygG8AlAK4U1XLoi6FsB0rIcQ/RCusP1HVKz3rj4lIXnU7qOp1EYLHVhP/CQBPRJmfEPhrFkKIn4j241WxiPzMrohIPwDFiclSLaDFSgjxEdFarL8BMEFEmjvrOwDcnJgsxQ67tBJC/ERUwqqqiwGcLCLNnPXdIjIUwJIE5i1q6AoghPiJmP4goKq7nR5YAHBPAvJTO+gKIIT4iLr8msU3/0OhK4AQ4ifqIqy+kTG6AgghfqJaH6uI7EFkARUAjRKSo9pAVwAhxEdUK6yq2rS+MlIX6AoghPiJYP3+mq4AQogPCISw0hVACPETgRBWjm5FCPETgRJWugIIIX4gEMJqocVKCPEDgRBWugIIIX4iUMJKVwAhxA8EQlgttFgJIX4gEMLKv7QSQvxEIITVohRWQogPCISwVny8oouVEOIDAiWshBDiBwIhrBYtpyuAEJJ8AiGsbMdKCPETgRJWKishxA8EQlgt1FVCiB8IhLC6rQKorISQ5BMIYU1xSkGLlRDiBwIlrBwqgBDiBwIlrGXlbNBKCEk+gRDW1FQzp8VKCPEDgRDWCleA0mIlhCSfQAlrWVly80EIIUBAhLXCFUCLlRDiAwIhrK4rILn5IIQQIGDCWlZGi5UQknwCIax0BRBC/EQghJUdBAghfiJhwioi40SkUESWecKOFJGPRWS1Mz/CCRcReVFE1ojIEhHpGcux6GMlhPiJRFqsrwO4ICxsGIDZqtoFwGxnHQAuBNDFmYYAGBnLgdyeV4EwwAkhDZyEKZGqzgOwPSz4MgDjneXxAC73hE9QwxcAWohIm2iP5fpY65BhQgiJE/Vt4rVW1c3O8g8AWjvLbQFs8sTLd8KiwvWx8uMVIST5JO3dWc2/qmO2MUVkiIjkikju1q1bAXAQFkKIv6hvYd1iX/GdeaETXgCgvSdeOyesEqo6RlV7qWqvli1bAmBzK0KIv6hvYZ0G4GZn+WYAUz3hNzmtA/oC2OVxGdQIB2EhhPiJtEQlLCJvAzgbwFEikg/gUQB/BTBZRAYD2AjgGif6DAAXAVgDYD+AQbEci+1YCSF+ImHCqqrXVbGpf4S4CuDO2h6rwsdKi5UQ4gMC0fDTHeiawkoIST6BEFb6WAkhfiJQwsrmVoQQPxAIYRUBBOW0WAkhviAQwgoAKRRWQohPCI6witIVQAjxBcERVlqshBCfEBhhTRUKKyHEHwRGWGmxEkL8QnCEVZQ9rwghviAwwpoq5ex5RQjxBYERVroCCCF+ITjCKooyDUxxCCENmMAoEVsFEEL8QmCENQVKYSWE+ILgCCstVkKITwiQsNLHSgjxB4FRIvpYCSF+ITDCSh8rIcQvBEdYpZyuAEKILwiMEtEVQAjxC4ER1hShK4AQ4g8CJax0BRBC/EBglIgWKyHELwRGWFOlHOVIgrCuXw+8/Xb9H7cufP65+QPjpk3JzgkhgSQwwmqaWyWhOKeeClx/PaBa/8euLSNHmvmnnyY3H4QElOAIa4qibM8+4Mkn6/fAW7eaeWlp/R63LtiHgNB1QkgiCIywGldACvDgg0BZWf1n4ODB+j9mbaGwEpJQAiOsKaJGWAFg5876z0BDFFZCSEIIlLCWIdWsbN9e/xloSMJq8bPFumUL8PXXyc4FIbUiUMJaYbFu21b/GWhIwtoQXAHZ2UDPnsnORWIpKwPuuANYuzbZOSFxJjDCWuFjBWix1kRDEFb7UbC2qAIlJfHJS6LIzQVGjQIGDkx2TkicCYywhlisFNbo8LOw1pUXXgAyM4HCwmTnpGbo8w4cwRHW5k1dH2skV8CyZcDhh5sG/dFy8GD0r2kNSVjtjZyM1hO/+Q3QoUP08WsrOm+8YeYNoRMEhTVwBEZYUzt1QHnHzmYlkrCuWAEUFwPffhsavnlz1Yn+9rfAT38K7NhRdRxr9TVEYfXmWRXYsKHuae/ZE1mwDx4Exo0DRo8Gvvsu+vQOHap7nrxs3w4MGgTs3h3fdAnxEBhhTUkVlB/VGjj6aOD77ytHsOLodRPMmAEccwzw0UehcffuNfFmznTXqyLewrpsGbBgAXDCCcC6dfFJMxwrrF7RevNNoFMnYO7cuqXbrBkwZEjlbY8/DgweHHuadT2v4dbgn/8MvP66a9EmE1qqgSU4wpriGEodOgAbN1aOEElYP//czBcsCI3buTOQleWuV/fKHE9hVTVfw/v0AVauBIYPr3uaVR0HCBXWL7808yVLap+uTW/cuMrbCgpql2ZdP0CVl4eu24fuEUfULd140JDeckhMJEVYRWSDiCwVkTwRyXXCjhSRj0VktTOPqeanpjr3UFXCajsNeIXVCkxK2GkI/yJd3c0dT2ENT+OLL+qeZiQiCWuq45+uS9fcAweq3hb+cIrWWjt40LRpPf302MS5quuyZYuZZ2ZGn1ai+DEL644dgW5mlkyL9eeqmqOqvZz1YQBmq2oXALOd9ahJSXGE9dhjjQ/vF78Azj7bjWAtVq+/tKZmR3Z7dYIRT2Hdvz+6OCLAyy/X/ji2XNu3u0KTlmbm9SWs0VqiJSXA2LHm7aI2ZQ4/zg8/xHb8urJpk/E7RyIowlpa6p7XaGnd2ny/CCh+cgVcBmC8szwewOWx7FzhCjj2WHPTfPJJqL8wksVqXxNranZUX8K6b1/NceyHub/8pe7He/xx45MGahbWRx81Za1OeKs7T+Gv5NXF9XLwoHtMm8dosA+PcAG1byPRHr+uHHsscMYZkbfZOtPQfa333AO0aVPzB8FZs4D8fNMELt4fJX1GsoRVAcwSkYUiYr90tFZV+4n+BwCtY0mwwmJt3jxyhEg+1rparFOmuDdHuLCuX28+4sRSgcIt1kj5sunV5WaMtK8Vrar8ySNGmHl1H/JiEdbi4qrjejn+eCPqgOuuiIUPPwz9OGmtx/oSVgBYvDhyuN87METLe++ZeVWWueX884FTTomtVUgDJVnC+jNV7QngQgB3isiZ3o2qqjDiWwkRGSIiuSKSu9XjC63wsTZrFvmIVljz812rpSZhtSJW1U34q1+5y+HCeuutwKuvuh/IoiFcWCNZwTZOuFDFQrjYjx1bs+Vtt9dWWMMFuzbCFovFavP78svAhReaZVXX+q0PYa3pLSYorgBbF6t7UNjzXlgYWofqUo99TFKEVVULnHkhgCkA+gDYIiJtAMCZR+wyo6pjVLWXqvZq2bJlRXh6unNdmzYN3cGKiHUF5OUBrVqZZXtR7U1fXg4sXerua0Usmpsw/Caxlcwq/kUXGQGrjnBh9Vp1338PXHqp+wGnLhZreF5vu82t7P/9L/Cvf1Xex37gq84qiUVYo7VYvdTGYvXiPb/VHf+444A//rFuxwJqtuC816G8vO4fKxcvTs64wLYuPv+86fEmAkydGhrHe+695yWaejB6dPQfuoqKTPvzoqLo4ieIehdWEWksIk3tMoDzACwDMA3AzU60mwFMjZxCZJo0cVyU4cI6ZYrx/YQ38h8wwK2EVgSfegro3t2NUxdhtUJSWmrE8MMPjYBV1/Mr3MfqrXQjRgD//repuEB8hRVwBfs//wGuuKLyditqtRXW8GPWxmKsaxdc781W3fFXrwaefrpuxwJiE9ZnngFOOw2YN692x1qyBMjJAZ54onb71wVroLz0EjB0qFn+859D43iF1Wux1iSsxcWmt96ZZ1YfzzJzpvlDxpVXRhc/QSTDYm0NYL6ILAbwFYDpqvoRgL8C+IWIrAZwrrMeNU2bOvU4XFgHDDAXO3yM1smT3S+Z9uLatpyWaFoFWA4eDK0wVrT37g1t6P/661WnEW6xlpS4At2okZlbcahKWDdsqL43mc1rODV1/bQWa21dAeEPjdoIa11f37098rxpjRhhrCIgvh+SYhHWhQvNPD8/+vSff948bMrKzMMASM5Qi9X57C3e6++tQzW1hLH3baROP+GsWOEaCKtW1Rw/gdS7sKrqOlU92ZlOUtUnnPBtqtpfVbuo6rmqGtNIKk2aGB06lNm08sZ164zQeVwHANyLYG+yql41o7mhP/jAiPr//mfWvcJqrdS0NOCbb6pOI1Ils8e2lbc6V4Cq6T2Vne2Gbd5sXo28/q9ohDW8zHV1BYQLcnExsGsXcNddwPLlxgVRE9E0R7NEsm4nTHCXvef1vvuMVbRvX3QtM6KlJmG118Tr+01LM8vVfWF/+23zILAdSHbvds/v4YeHxv3oI9dwWL/enJevvoqtHDURjbDW1hVQVXfy8vLKH6JPPBG4//7Ix69n/NTcqk40aWLm+1IiCKttKdC5c2i4tQ7sxQ3vKGCJRljtj/nsF2hraX72manQKSlA375VP3n37zc/JQzH5s2O0mT3j+T0f+stM/daZvfea16NvF/GIwlreL7eeSd03SusVVl14efp++9NDzK7X3jcv/7VvD526xbdq15t/LLefa0bxZvXNWvcsM8+i+8YArFYrFZYVYGbbzZ11p7nZ54Jdc+MGmVe+e012bHDffvyCmt+vvlw99prZv2TT8z8+uvNwyReRKoPqanGCs/MNPmoq8UaziOPmN6RVlzDH9yxPIQTQGCE1XoA9miTyoH2y3ynTqE7WWGdPt34hGKxWMOFzT4ht283r/s27RdfND847NAB6Nix6t5DVfnWrJjYhvwWVeCqq4Cf/9xYwbNmAXffbbbZ7poFBe6vub0fNaL5Gn3LLaE+SXsT33ij+TNtpGZk4efp+OPNmAdA5aEci4srV35Vcx4WLYqcp0jCum9f5Bs7/PrYj5KHHw60b+/mNTfXjbN1a83C+uKLwJ13Rj5euHUVrbCWl4e+4dgH5HffAb/7HfCHP4R+UNy+3bxh2PO3c6dbr7wfCa3Y2jJad9Latcb9UV09KCpyH4reMg4c6L5dfPedqXtVCesLLxirfNas6n2sa9caazOS0VGVsE6c6OYTqNxbcteupLYPDoywWot1b7FHHAsLzQWzJ93b/x9wxWHzZtNWsqqmH+GCMXt25dcpe2N88YUZPWnXrtBt3boBbduaymNvwuefB9q1A8aMqdo39p//mHl4zxZV4J//NNtPOsm0EbSWanq6mXubg3kraLS+yg8+AIYNMzeF15pfsAAYP75yfG+6qu4NVFxcecSxAwcqV/ziYuCss0xbx0iEC+v27ebC/+1vpnwPPOC+XoeLxqmnmvmnn5p9bF5XrHDjFBVVL6wTJpiH19//7ubnpZfM9b3qKuDII0MfYDV9pPG2gfYKq2XSJOCVV9x1a/WFn8sdO9xR27zibuPZuhX+MJw2LXT9o4/cNqYnn+w+FL3pvfWW23xtyBDgmmsi3zeHDrnGQkpKqMXqrYv795sPhStWAO++WzmdqlwBtu7Yh1d4K4CystBj3nsvcO65ldNZtsyc5ziTXEdEHKmwWL1GQnh/8Pbtq08kfEhBi70JN282onLZZVWnUZW1lZ1tmnkdOmQE1iuUL71UdQW66SZz04b7ZqvqeNChgxFv1VCLY+ZMY1V37x798ICDBpl5uKUPmNfR224zy0VFxqL2DuAyZYq7vHBh5eZW69ebcnup6Zc606cDf/qTmQDXLzxsmNl3xAjj7rnttqqtsa5dTb3IyzOtLFauNPusW2dE05Y5nL17zSu65c03jfUOGFeLFegffjAPy3Af4OGHmwdpZqbpynnuuebDKmDyaq+ntwKHW4xr15prG36eZs40ViFgRGv7dnM8KzbffGOEPdz6u/pqM67GsceaOn7hhSbvmza51uP+/a57wfvhtLTUfE8oLY3sz7TfGgBTt7334quvusv797vpetugb9wIfPxx6IOmrMwI4datrpjb+yZS86px44yFnZUFPPusCfv2W/MmZXn0UVOvrrmmaldgbVDVBjudcsopavnPf1QB1dmz1Sz06WM2dOxo1gHVb79VHT7cXY9leu011fPOq92+gOq//626dKnqscdWHadrV3f55JPd5bFjzfzOO2s+ziWXmHmjRrXPa/h0002qzZq564cfrpqaqrpnj+rgwTXvf/750R3no4+ii7drl+qBA6offFB52+OPm+veoUPlbdOmmW2nneaGdeum+stfRj7Ol1+a+EVF7nmtafr8c7PPTTdVHeecc8z86KPN/KijVHv3Nsu33+7G69s3+mvUqpXq2We763fcofrCC6FleeSRyvvNmaO6c2domKq7vGKFanGxCZs3z4RlZqouXBifujVpkupZZ5nlZ5+tuJ+1TRsT1quXG3fDBnf5mGPMfPJkE3/8eHdb69bu8rnnhpZn1CjVwkJTf8rKVLOyTPimTeoFQG5dtKnWO/ph8gprbq4pzb/+peakHTxoNrRsaTZcfLFqebnqli3uSW7RQvWUUyJf8MMOi75ytG3rLvfoUXn7u++aY1veeCNyOocOmXgHD5onhHdbaqrqkiU15+XBB6ve1q6dmTduHHl7Roa7nJVVtThfc42Z33BD9OcIUG3atPrtTz5p5t26VR9vzhzVE09017t0Cc1bQYERGu8+LVq45//nP4/+nKmGiuSsWdXnzd7o1cX5yU+iO1+xPBxvvdWt64Ap/0MPuesjR6r+/veV93v5ZdUzzggN277dXb7hBtX0dNUXX4ztWkc7jRun2r69WR4+XPXee1WPOy5y3ClT3OXmzc189Ghzvp9+2qxv3Kj64YduvLZtQx8cl15q5j17GkPHhnfvrvrWWxTWcGH99ltTmjff1FCsiCxfbtbLy1Xvvlv1/fdVd+92n5Z26tTJTcxOP/uZsVjDK/q0aWZ+wgnmwr7/vnvc0lI3XjhFRaHpHH+86rXXhsZ5773KFUs1VMQB15L8wx9MZXvrrdDtd9yhetJJqqefrvrqqyasKmH9wx/c8q5fr/rxx5Hjvf12zTdMeEUGXAsskjXpnX7zm+jTByJbruGTfYNRVf3FL0K3ffGFu2xvWDt5H4LXXKP6zTfVH+foo0OvvZ1++tOqH+K1mawY2emxx4x4RorbqpXqZZdF3nbCCfHLE6A6daoRqeriNG3q3mOXX+6G9+8fXVnDp/XrzcO1WTNzf3/+ubutXTvVxYsj72fru3caO1ZVVSmsDt9/b0rz4othImZP2PbtGpEFC0xFePNNM3/lFRN+xRVmv717XWvz4EFX2JYscQVy+PDIaS9erLp2beRtO3aYfdu0MZZqWVno9j17VK+8UnXmTDeeqrHGN21yy7Vxo1u28nLVTz4JrSh79rjbrBWclqa6bp0puzfuf/9bOR8LFoS6U6wQPfqoeSBYy/GWWyJX3tdfV73wQrNs3QZ2PdI0YoTZBzAPm3ARf/llc0PcdZcb5n0LCZ/69zfl/fRTt0wrV6reeKMbp6zMFZj16yO/rfz976olJea1uGdP49qZMcO8EnstZiDUZWNfWceMqXy+vVPPnu5y+NvK4MFGNAsK3LChQyufZ1Xzmh6e9oABVR+3uumXvzTC5H078E733Vc5bM8eV1g/+yzysS+5xJxHb1ikfNs6deWVVT8YvNOkSeYceM8TEOr6CZ/s9fFOM2YohVVd3WjdWvW660J1Qf/xD/Pq5X0Vj4a9e1W/+65y+LRpRuT27TPrRUXGQqkNzzxjLKCaGD3aCLmX554LFQvLDz9UrmgWr4/Ksn27al6e6vXXG+GIxI4dqqtWmafXsGFuecvLXZGcOFH1qadcn4ydPvvM3GwffOD6/G67zcwvuEB1+vTQ+KrGrwcYMVE1N+G2beaG8WL3KS+PfNNYn2okSkrMG8gTT5j1/fuN71Y1VLQB1XvuqTode342bqxsGU2aZPyShx2munmzmey2b75xX31tHfCegy+/rHytvHHy80OPZeuCPbfnnGNEY+rUUP+jnfr2NQ8Yuz53rjnmWWcZP+/Qoe4x//znyvu3bh2an3ffNXVEVXX+fFO28nLVf/7TbB8+3NTjOXNM3lVdAZ461U2nVSvz4JwyxX1buOYaE99rkVu3lp06dQq9xyPVB+/DNCXFzKtwcVBYPQwcqHrkke51+9ES6YZUdV9RhwyJ37EuusikOWWKGzZ9uvFfbdkSGveHH4z1u3y5seStNb16tepvf2usP0thYc0Pw7VrjRWoam7OhQuNuAPm45/1s1dFVekXFRkhyM01IrN6dfXpeHnkETP973+VH7hlZeZjyrvvRt537lwjxJZTTlG9//7QOCtXuhW8sND1LVojoKTEWN1eSkqMyP7yl+YBfckl7rl++GHjRqgOr3vJilu3bmbbyy+bj2bVsWBB5TchVfOgf+MNszxqlHmgbdvmbt+3zzzwV64062Vl5twdOmTWH3zQPKxvuMF19VneecdYqhdfbHzqzz1nrsekSeahs3KlOd/l5eb7BWDmTz6p+vDDFFYvCxca902XLqH36I+O2283r06R2LXLrZjx4KWXTDVaujR+aZLYqMrN5SWSsEXL5s1GjK14ffJJ1S6uhsikSaqdO4e8sdVVWMWk0TDp1auX5np7zsD0SrzqKtM34K67zPgrxx6bnPz9KFA1vcxqaiNMSANCRBaq+9uomAlMzyvL6aebMT1uucX0qOvQwYQ995xpp56M4SoDjQhFlZAwAmexelm71vw1Yvx4t2NM06am81FOjhn3IyfH6ILtRk0IIXW1WAMtrF4KCszYEXPnml5xX3/tdiUWMQLbpo3p8ZiVZXpxdu1qRLdJE3/8LZkQUj9QWKMU1nAOHTLdxVeuNF2pFy0y3aO//TZyN/yOHc1wrs2bm6lVK+CYY8xAUs2bG3Fu0wbo0sUIcWqq6WKd5GEhCSG1oK7C+qO97dPTgd69zRTO7t1mmM5Vq8yYGrt3G8Hdvt0MWvX992ZQqZrGDElLM+LbuLEZ3+Hoo82YE82aAUcdZSzjrCzj983KMuNKtGwJHHaYK8pHHWX2zcwEMjLcuR10Ji2t7r+CIoTElx+tsFZHs2ZAz55mqo6SEjNgkB36ccMGM+ravn1GLLdvNx/Md+ww4rd5sxHM7duNaG/bVvtxle3vvtPTjdg2b27W09JMWKtWRoBTUkxYWpor0C1amIGC0tONiNspIyN03U6AEfK0NNcaT001Vvphh5k0MzPNtuJiEzcz02wvL3ctetX4DiBEiF+hsNaBjAygdWszAaGjkUXLwYNGcPbvN/OiIvf3WaWlRpTLy42IHzjgzrduNUK3f78J27HDiNfevSb+rl3uaHT795u0SkvNqHTFxUbgSktNnIMHnd/aVDESYV2xogqY41qxzspyxTYlxYh1+DxSmHduHyJ79pgyZGSYNwR7LBuvWTNz3ho1MudH1axnZpqHXXq6OWeNGrmj5GVmumNQ20nEvHnY8PJyk759qOzfb/Zr2tTNf0mJOcci7ptIebmJs3+/yXN6unvtAPPwS083+zRu7D4c09JMWrt2mXD7AAPMw3LPHrNf+DkMn+y1z8hwR+tLSTHnxL4F2etS3eQ9zv795hw0amTyVVbmDrubkWEm7x9zVN08BA0Ka5KxImMr15FHJi8vqkZcrdh6RXfvXnNT7NtnbpjSUjd+cbG5gfbsMTdZcbE7HGpKijtkZmqqm74drjQlxaRTVmbCyspCl6ua2+Vdu4z136KFuZkPHnQHq7fCV17u/lGmtNSc8/JyI3IHDpj8lpQYQajql1ciJv82ryQU78OzOlJSTFz75nLokLkeVtRtWnZKTTVCb/+ZmJlprlWTJuaaej8q23Ttvt5jeeepqWa/lBTzxtiokduX9fDD43N9KaykAvtqb8X+x4YVXlXXMvRazYAR5C1bXLeK/fffvn3GgmzUyH3IWPFPS3Ot3+Ji9xjFxSa8uNhsS001D7CsLDO3DyfvG4e1mrOyzDG9P20QMVawfaDYB1D45HUZHThgHk4pKWZbRob7MKpusunYqaTElL9xY/dhlZbmGgwlJe5Ph62IWat9926zbq1vu80+xOyxrBDaX903aRI6nrk3Xe88PKy01OTx0CHz8fnAAff67tpl8rx8ed3qEoWVEAcR9682QOQmdikppvVHOPZ/lYB5MHkHwycNj0g/+Y0FfkoghJA4Q2ElhJA4Q2ElhJA4Q2ElhJA4Q2ElhJA4Q2ElhJA4Q2ElhJA4Q2ElhJA4Q2ElhJA4Q2ElhJA4Q2ElhJA4Q2ElhJA4Q2ElhJA4Q2ElhJA44zthFZELRORbEVkjIsOSnR9CCIkVXwmriKQCeAXAhQBOBHCdiJyY3FwRQkhs+EpYAfQBsEZV16nqQQCTAFyW5DwRQkhM+E1Y2wLY5FnPd8IIIaTB0OB+zSIiQwAMcVZLRGRZMvOTYI4CUJTsTCQQlq/hEuSyAUAt/rns4jdhLQDQ3rPezgmrQFXHABgDACKSq6q96i979QvL17AJcvmCXDbAlK8u+/vNFbAAQBcR6SQihwG4FsC0JOeJEEJiwlcWq6qWisjvAMwEkApgnKrW8Ue0hBBSv/hKWAFAVWcAmBFl9DGJzIsPYPkaNkEuX5DLBtSxfKKq8coIIYQQ+M/HSgghDZ4GK6xB6PoqIuNEpNDbZExEjhSRj0VktTM/wgkXEXnRKe8SEemZvJzXjIi0F5E5IvKNiCwXkbud8KCUL1NEvhKRxU75HnPCO4nIl0453nE+wkJEMpz1Nc72jkktQBSISKqIfC0iHzjrgSkbAIjIBhFZKiJ5thVAvOpngxTWAHV9fR3ABWFhwwDMVtUuAGY764ApaxdnGgJgZD3lsbaUArhXVU8E0BfAnc41Ckr5SgCco6onA8gBcIGI9AXwFIDnVPWnAHYAGOzEHwxghxP+nBPP79wNYIVnPUhls/xcVXM8TcfiUz9VtcFNAE4DMNOzPhzA8GTnq5Zl6QhgmWf9WwBtnOU2AL51lkcDuC5SvIYwAZgK4BdBLB+AwwEsAnAqTKP5NCe8op7CtHQ5zVlOc+JJsvNeTZnaOcJyDoAPAEhQyuYp4wYAR4WFxaV+NkiLFcHu+tpaVTc7yz8AaO0sN9gyO6+GPQB8iQCVz3lVzgNQCOBjAGsB7FTVUieKtwwV5XO27wKQVa8Zjo3nAdwHoNxZz0JwymZRALNEZKHToxOIU/30XXMr4qKqKiINutmGiDQB8E8AQ1V1t4hUbGvo5VPVMgA5ItICwBQAXZObo/ggIr8EUKiqC0Xk7CRnJ5H8TFULRKQVgI9FZKV3Y13qZ0O1WGvs+tqA2SIibQDAmRc64Q2uzCKSDiOqE1X1fSc4MOWzqOpOAHNgXo9biIg1WLxlqCifs705gG31m9Oo6QfgUhHZADPC3DkAXkAwylaBqhY480KYB2MfxKl+NlRhDXLX12kAbnaWb4bxTdrwm5yvk30B7PK8svgOMabpWAArVPVZz6aglK+lY6lCRBrB+I9XwAjsVU608PLZcl8F4FN1nHV+Q1WHq2o7Ve0Ic299qqoDEYCyWUSksYg0tcsAzgOwDPGqn8l2INfB8XwRgFUwfq0Hk52fWpbhbQCbARyC8dkMhvFNzQawGsAnAI504gpMS4i1AJYC6JXs/NdQtp/B+LCWAMhzposCVL7uAL52yrcMwCNOeGcAXwFYA+BdABlOeKazvsbZ3jnZZYiynGcD+CBoZXPKstiZllsNiVf9ZM8rQgiJMw3VFUAIIb6FwkoIIXGGwkoIIXGGwkoIIXGGwkoIIXGGwkqIg4icbUdyIqQuUFgJISTOUFhJg0NEbnDGQs0TkdHOYCh7ReQ5Z2zU2SLS0ombIyJfOGNoTvGMr/lTEfnEGU91kYj8xEm+iYi8JyIrRWSieAc3ICRKKKykQSEiJwAYAKCfquYAKAMwEEBjALmqehKAuQAedXaZAOB+Ve0O02PGhk8E8Iqa8VRPh+kBB5hRuIbCjPPbGabfPCExwdGtSEOjP4BTACxwjMlGMANllAN4x4nzJoD3RaQ5gBaqOtcJHw/gXaePeFtVnQIAqnoAAJz0vlLVfGc9D2a83PkJLxUJFBRW0tAQAONVdXhIoMjDYfFq21e7xLNcBt4jpBbQFUAaGrMBXOWMoWn/UdQBpi7bkZeuBzBfVXcB2CEiZzjhNwKYq6p7AOSLyOVOGhkicnh9FoIEGz6NSYNCVb8RkYdgRn5PgRkZ7E4A+wD0cbYVwvhhATP02yhHONcBGOSE3whgtIj82Unj6nosBgk4HN2KBAIR2auqTZKdD0IAugIIISTu0GIlhJA4Q4uVEELiDIWVEELiDIWVEELiDIWVEELiDIWVEELiDIWVEELizP8D5RXwhawWvO4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 7,809\n",
            "Trainable params: 7,553\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 3452.9180 - val_loss: 3207.7839\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 2867.9287 - val_loss: 2274.9009\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 2113.7842 - val_loss: 1101.2292\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 1294.6301 - val_loss: 773.8401\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 637.9351 - val_loss: 490.2017\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 248.9526 - val_loss: 172.3377\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.9908 - val_loss: 96.2756\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 42.8723 - val_loss: 59.5034\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 34.2282 - val_loss: 44.6659\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.2956 - val_loss: 73.3060\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.7264 - val_loss: 38.7003\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 31.1566 - val_loss: 47.2750\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.6732 - val_loss: 57.6593\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 30.3612 - val_loss: 42.5987\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.0468 - val_loss: 39.2052\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 29.7848 - val_loss: 45.8656\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.4137 - val_loss: 48.6974\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.2852 - val_loss: 39.9563\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 29.0389 - val_loss: 39.9418\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.8122 - val_loss: 44.6746\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 28.6472 - val_loss: 42.7198\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.4872 - val_loss: 44.6096\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.3654 - val_loss: 38.3872\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.2092 - val_loss: 36.2905\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.1569 - val_loss: 36.8322\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 28.1279 - val_loss: 43.0688\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.8371 - val_loss: 58.7189\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.8360 - val_loss: 37.2969\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.5505 - val_loss: 37.1234\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.5064 - val_loss: 39.1816\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.3816 - val_loss: 49.0337\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 27.2965 - val_loss: 40.0535\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.3094 - val_loss: 37.7878\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.1497 - val_loss: 41.7188\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.0797 - val_loss: 39.9823\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 27.0574 - val_loss: 39.8330\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 26.9847 - val_loss: 38.1438\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.9066 - val_loss: 39.1832\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.9347 - val_loss: 38.6996\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.7546 - val_loss: 35.0655\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.6997 - val_loss: 38.0308\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.7009 - val_loss: 46.9081\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.5441 - val_loss: 40.7934\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.5514 - val_loss: 38.0771\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.4941 - val_loss: 47.8433\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 26.4096 - val_loss: 39.9632\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 26.4635 - val_loss: 42.7357\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.2844 - val_loss: 39.7626\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.3504 - val_loss: 35.3125\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.2587 - val_loss: 36.7073\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.1499 - val_loss: 33.9320\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.2026 - val_loss: 41.7209\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.0518 - val_loss: 47.3002\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 25.9518 - val_loss: 42.2191\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.9940 - val_loss: 37.1316\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.8908 - val_loss: 38.3099\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.8674 - val_loss: 37.2287\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.8051 - val_loss: 43.9214\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.7861 - val_loss: 40.1074\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.8697 - val_loss: 36.8744\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.6451 - val_loss: 36.6135\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.6748 - val_loss: 35.7898\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 25.5558 - val_loss: 37.6245\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.5720 - val_loss: 37.0239\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.6398 - val_loss: 41.4416\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.5164 - val_loss: 37.1469\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.5782 - val_loss: 42.7594\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.3885 - val_loss: 39.1627\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3386 - val_loss: 44.1886\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.2719 - val_loss: 37.9168\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.3443 - val_loss: 38.4392\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.3536 - val_loss: 36.3946\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 25.3650 - val_loss: 39.9919\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.2125 - val_loss: 38.1051\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.1431 - val_loss: 53.8868\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.2330 - val_loss: 35.7198\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.2139 - val_loss: 44.1812\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1254 - val_loss: 45.8051\n",
            "Epoch 79/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 13ms/step - loss: 25.1133 - val_loss: 39.1602\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.1801 - val_loss: 34.9922\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.9906 - val_loss: 39.7508\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.0552 - val_loss: 43.2557\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 24.9509 - val_loss: 37.1919\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 24.9452 - val_loss: 52.9904\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.9106 - val_loss: 36.0062\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.8578 - val_loss: 39.4002\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.9205 - val_loss: 35.6622\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.8411 - val_loss: 41.4370\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8842 - val_loss: 39.5006\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.8108 - val_loss: 37.7528\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.8187 - val_loss: 35.9637\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.8547 - val_loss: 37.4776\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.6810 - val_loss: 34.7867\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.6485 - val_loss: 39.3728\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 24.6547 - val_loss: 35.6434\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 24.7249 - val_loss: 52.3368\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 24.6390 - val_loss: 43.1912\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.5346 - val_loss: 41.0329\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.5367 - val_loss: 37.2754\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.5099 - val_loss: 37.4260\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5659 - val_loss: 38.9300\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.4787 - val_loss: 44.9712\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5236 - val_loss: 39.4119\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.4803 - val_loss: 38.5095\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.4995 - val_loss: 45.6895\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.4869 - val_loss: 46.0850\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.4145 - val_loss: 36.7085\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 24.3923 - val_loss: 37.0946\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 24.4090 - val_loss: 38.5784\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3710 - val_loss: 34.6087\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.3234 - val_loss: 39.5068\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.3370 - val_loss: 35.6972\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.3286 - val_loss: 37.0371\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2269 - val_loss: 35.2852\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.2441 - val_loss: 35.4531\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.2560 - val_loss: 50.5683\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2169 - val_loss: 38.8181\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.2324 - val_loss: 41.5173\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2050 - val_loss: 39.2087\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.2402 - val_loss: 35.9961\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1582 - val_loss: 36.4569\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 24.2469 - val_loss: 35.7886\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 24.1371 - val_loss: 37.9733\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.0380 - val_loss: 40.9885\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.0720 - val_loss: 38.6312\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.1353 - val_loss: 43.4101\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.0214 - val_loss: 54.5451\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.0244 - val_loss: 39.6536\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.9653 - val_loss: 56.6609\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.9544 - val_loss: 41.8034\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.9580 - val_loss: 36.6731\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.9970 - val_loss: 36.4442\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.0325 - val_loss: 39.0714\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.9642 - val_loss: 36.7938\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.0190 - val_loss: 39.2974\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.9654 - val_loss: 37.9898\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.9291 - val_loss: 45.9253\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 23.8890 - val_loss: 38.5995\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 23.9230 - val_loss: 40.1208\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.8253 - val_loss: 35.8586\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.8285 - val_loss: 58.4737\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.8253 - val_loss: 43.6030\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.8607 - val_loss: 36.3478\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7926 - val_loss: 35.5634\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.8037 - val_loss: 35.4831\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7620 - val_loss: 35.2208\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.7789 - val_loss: 40.9420\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.8337 - val_loss: 37.5257\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.6862 - val_loss: 40.1340\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.6807 - val_loss: 36.1020\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7501 - val_loss: 35.1889\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.6599 - val_loss: 42.3166\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7339 - val_loss: 42.6966\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.6985 - val_loss: 42.4181\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.6137 - val_loss: 44.5406\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 23.5962 - val_loss: 44.8222\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 157/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 23.6098 - val_loss: 36.7765\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.6892 - val_loss: 37.4655\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.6239 - val_loss: 35.5355\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.5885 - val_loss: 47.5742\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5613 - val_loss: 35.6568\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.5488 - val_loss: 41.4859\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5593 - val_loss: 41.6340\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.5770 - val_loss: 36.9742\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5765 - val_loss: 42.3827\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5317 - val_loss: 38.1937\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.4021 - val_loss: 40.9550\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5150 - val_loss: 42.9121\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.5183 - val_loss: 36.8004\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.4759 - val_loss: 41.0906\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.4557 - val_loss: 38.4488\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.4561 - val_loss: 40.1939\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3759 - val_loss: 39.7808\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.4540 - val_loss: 41.0008\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.4410 - val_loss: 44.7834\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 23.4531 - val_loss: 43.0175\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 23.4004 - val_loss: 35.5026\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.4116 - val_loss: 45.8947\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3943 - val_loss: 41.1313\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.3482 - val_loss: 38.6970\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3318 - val_loss: 36.1539\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.4149 - val_loss: 59.9189\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3370 - val_loss: 47.2461\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3476 - val_loss: 34.6146\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.2705 - val_loss: 40.9613\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2354 - val_loss: 37.8671\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.3200 - val_loss: 37.3520\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2970 - val_loss: 38.6649\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.2432 - val_loss: 36.2896\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2005 - val_loss: 37.7275\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2040 - val_loss: 35.5511\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.1554 - val_loss: 37.6098\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2600 - val_loss: 35.8199\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.2114 - val_loss: 35.2677\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2601 - val_loss: 40.6043\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.1723 - val_loss: 43.9151\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 23.1499 - val_loss: 39.3342\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 23.2083 - val_loss: 42.0755\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2967 - val_loss: 39.1968\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0797 - val_loss: 39.0155\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.0910 - val_loss: 37.5289\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1476 - val_loss: 36.1196\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.1241 - val_loss: 36.7358\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1047 - val_loss: 35.7852\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1063 - val_loss: 37.4801\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.0361 - val_loss: 42.2978\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1282 - val_loss: 47.5522\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.0564 - val_loss: 36.4210\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0038 - val_loss: 35.0115\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.0847 - val_loss: 35.6503\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1273 - val_loss: 39.5786\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0586 - val_loss: 53.0492\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.0511 - val_loss: 34.7341\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0468 - val_loss: 36.3735\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9451 - val_loss: 34.4094\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9735 - val_loss: 39.8278\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.0549 - val_loss: 36.8790\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 23.0127 - val_loss: 36.7287\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.9323 - val_loss: 38.3041\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0353 - val_loss: 36.5379\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9620 - val_loss: 33.9822\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9368 - val_loss: 34.9052\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8854 - val_loss: 40.2452\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9142 - val_loss: 34.7714\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9293 - val_loss: 36.2779\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9535 - val_loss: 38.3973\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8583 - val_loss: 36.8502\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8199 - val_loss: 38.2573\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.8686 - val_loss: 45.7359\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9362 - val_loss: 34.5834\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9523 - val_loss: 39.8599\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9209 - val_loss: 37.7082\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8656 - val_loss: 37.2817\n",
            "Epoch 234/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8741 - val_loss: 33.5818\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7842 - val_loss: 37.0440\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.7903 - val_loss: 41.7471\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8489 - val_loss: 44.9378\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8424 - val_loss: 37.3866\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9430 - val_loss: 35.9114\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7788 - val_loss: 36.5052\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8238 - val_loss: 36.9091\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8282 - val_loss: 38.5808\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7994 - val_loss: 35.3897\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7765 - val_loss: 39.5441\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7994 - val_loss: 39.5079\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.8195 - val_loss: 35.2743\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.7224 - val_loss: 46.2204\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.7839 - val_loss: 43.7462\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.8276 - val_loss: 37.7978\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6959 - val_loss: 35.9275\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.7599 - val_loss: 34.1892\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6866 - val_loss: 35.2618\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7197 - val_loss: 36.8586\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5848 - val_loss: 43.4355\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6623 - val_loss: 49.9767\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6934 - val_loss: 38.4576\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6539 - val_loss: 39.0898\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7451 - val_loss: 36.1459\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6895 - val_loss: 37.0746\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6806 - val_loss: 36.7980\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7057 - val_loss: 37.6616\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.6670 - val_loss: 37.1082\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6081 - val_loss: 37.3296\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7257 - val_loss: 36.5206\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6248 - val_loss: 35.0952\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6997 - val_loss: 36.8495\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5888 - val_loss: 39.4848\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6514 - val_loss: 38.7594\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6183 - val_loss: 35.5572\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5217 - val_loss: 43.3676\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6136 - val_loss: 38.6011\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.5621 - val_loss: 35.5146\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5575 - val_loss: 36.3600\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.5855 - val_loss: 34.7457\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5434 - val_loss: 37.6953\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.5768 - val_loss: 36.0448\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5670 - val_loss: 34.8674\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5726 - val_loss: 36.9375\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5590 - val_loss: 39.7910\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6283 - val_loss: 38.1598\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5074 - val_loss: 41.2607\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5332 - val_loss: 38.0501\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5188 - val_loss: 35.1162\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5154 - val_loss: 36.7250\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4825 - val_loss: 36.2278\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5030 - val_loss: 36.2218\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4754 - val_loss: 38.9112\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4902 - val_loss: 36.6809\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4855 - val_loss: 37.4115\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4333 - val_loss: 35.0457\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5122 - val_loss: 38.9294\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4934 - val_loss: 38.6681\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.4616 - val_loss: 39.3226\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5338 - val_loss: 40.0340\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.4681 - val_loss: 41.1796\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3944 - val_loss: 39.1617\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.4291 - val_loss: 34.8769\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3969 - val_loss: 45.4866\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3660 - val_loss: 35.2502\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.4416 - val_loss: 36.8245\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3814 - val_loss: 36.2906\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4043 - val_loss: 39.2287\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4167 - val_loss: 39.5320\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.4341 - val_loss: 36.5320\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3732 - val_loss: 37.1152\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3837 - val_loss: 37.6831\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4200 - val_loss: 38.2411\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4402 - val_loss: 40.8752\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3760 - val_loss: 39.2134\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.3681 - val_loss: 37.3522\n",
            "Epoch 311/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3481 - val_loss: 45.5280\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3499 - val_loss: 36.2365\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3712 - val_loss: 37.9027\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.4343 - val_loss: 36.6858\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3381 - val_loss: 37.9236\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.3191 - val_loss: 36.3790\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3472 - val_loss: 36.6052\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3476 - val_loss: 41.6314\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3432 - val_loss: 39.5720\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2606 - val_loss: 41.8411\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3302 - val_loss: 35.2538\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2941 - val_loss: 36.7121\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2862 - val_loss: 35.3587\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3630 - val_loss: 34.9723\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.3330 - val_loss: 40.5729\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2554 - val_loss: 36.5751\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2729 - val_loss: 42.5695\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2993 - val_loss: 40.9183\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2698 - val_loss: 35.1353\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.2563 - val_loss: 42.5528\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2315 - val_loss: 39.0692\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2793 - val_loss: 47.5507\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2542 - val_loss: 47.8581\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2994 - val_loss: 40.2273\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2024 - val_loss: 39.2704\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2815 - val_loss: 36.2407\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2640 - val_loss: 38.4580\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.1813 - val_loss: 44.8147\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.2201 - val_loss: 40.9254\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2628 - val_loss: 36.9236\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3089 - val_loss: 40.9063\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3017 - val_loss: 36.7304\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2025 - val_loss: 37.2714\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1472 - val_loss: 34.5203\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2699 - val_loss: 53.4559\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1747 - val_loss: 41.2121\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1466 - val_loss: 38.2262\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1706 - val_loss: 35.4708\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2356 - val_loss: 38.4213\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1637 - val_loss: 38.8463\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1507 - val_loss: 51.4469\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1470 - val_loss: 34.0292\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1399 - val_loss: 37.5295\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2322 - val_loss: 41.4225\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1972 - val_loss: 39.4912\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2028 - val_loss: 35.9942\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1261 - val_loss: 41.4279\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2084 - val_loss: 37.4445\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0128 - val_loss: 37.1019\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1826 - val_loss: 34.9466\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1803 - val_loss: 38.1001\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.1339 - val_loss: 38.0141\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.0769 - val_loss: 41.9139\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0421 - val_loss: 39.0754\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1776 - val_loss: 36.4777\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 22.0544 - val_loss: 36.9862\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1434 - val_loss: 40.4728\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1240 - val_loss: 40.6328\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1342 - val_loss: 36.5054\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1184 - val_loss: 40.4917\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0402 - val_loss: 35.4402\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0938 - val_loss: 39.8135\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0442 - val_loss: 37.3869\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1177 - val_loss: 39.0180\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1407 - val_loss: 38.6221\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1150 - val_loss: 37.8074\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1177 - val_loss: 42.0628\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0632 - val_loss: 40.4808\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0136 - val_loss: 39.9730\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0873 - val_loss: 34.7565\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9885 - val_loss: 35.2439\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0695 - val_loss: 36.5722\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0716 - val_loss: 43.2437\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.0131 - val_loss: 43.1565\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0723 - val_loss: 35.6437\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0305 - val_loss: 42.6641\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1183 - val_loss: 35.9789\n",
            "Epoch 388/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0523 - val_loss: 36.4742\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0526 - val_loss: 39.9137\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.0122 - val_loss: 38.5492\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0127 - val_loss: 40.6159\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9515 - val_loss: 37.7077\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9679 - val_loss: 35.7484\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0040 - val_loss: 37.2253\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0601 - val_loss: 36.9309\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9665 - val_loss: 36.7347\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0222 - val_loss: 38.5300\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9902 - val_loss: 35.4759\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9403 - val_loss: 35.7401\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9406 - val_loss: 35.0220\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9291 - val_loss: 38.6421\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9777 - val_loss: 39.9816\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9640 - val_loss: 35.0028\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9829 - val_loss: 37.2217\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9370 - val_loss: 36.7458\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9275 - val_loss: 34.7655\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9203 - val_loss: 37.6274\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9154 - val_loss: 36.2692\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9789 - val_loss: 36.6572\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9598 - val_loss: 34.4201\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9001 - val_loss: 36.6919\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9751 - val_loss: 35.3815\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9746 - val_loss: 36.4374\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0135 - val_loss: 37.8969\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9465 - val_loss: 36.8004\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8442 - val_loss: 41.4188\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8748 - val_loss: 37.6849\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 21.9961 - val_loss: 46.1082\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9104 - val_loss: 50.6587\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8646 - val_loss: 35.1325\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9304 - val_loss: 34.0214\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 21.8878 - val_loss: 36.7167\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8910 - val_loss: 38.3681\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9321 - val_loss: 38.2600\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9852 - val_loss: 37.2553\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7911 - val_loss: 35.3160\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8599 - val_loss: 34.4737\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9201 - val_loss: 42.9099\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8742 - val_loss: 34.7246\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9281 - val_loss: 40.2061\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8691 - val_loss: 39.4052\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 21.8885 - val_loss: 35.9472\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8329 - val_loss: 41.8796\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8715 - val_loss: 43.2701\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8879 - val_loss: 39.0305\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9046 - val_loss: 34.0961\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8803 - val_loss: 37.6648\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7869 - val_loss: 38.2822\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8214 - val_loss: 36.5296\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8494 - val_loss: 33.9217\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8784 - val_loss: 34.1598\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.7936 - val_loss: 34.6919\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8817 - val_loss: 37.2086\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8971 - val_loss: 36.0581\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8688 - val_loss: 35.1491\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8551 - val_loss: 38.5272\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8210 - val_loss: 38.9808\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8394 - val_loss: 39.4627\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9528 - val_loss: 42.1246\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0431 - val_loss: 38.3612\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.7670 - val_loss: 38.7437\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7498 - val_loss: 33.9382\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8658 - val_loss: 38.9598\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8585 - val_loss: 37.6291\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8019 - val_loss: 37.8169\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8683 - val_loss: 36.5602\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8203 - val_loss: 35.7084\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.8096 - val_loss: 40.1749\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7838 - val_loss: 35.8673\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7525 - val_loss: 36.0075\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8413 - val_loss: 36.2366\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7258 - val_loss: 43.4376\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.8667 - val_loss: 34.2363\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7660 - val_loss: 40.1083\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.8162 - val_loss: 37.8342\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7921 - val_loss: 37.5309\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8127 - val_loss: 37.8143\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.7484 - val_loss: 34.8092\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7990 - val_loss: 37.4568\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8324 - val_loss: 35.6935\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7324 - val_loss: 34.5359\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8692 - val_loss: 41.2118\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9254 - val_loss: 36.8386\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7646 - val_loss: 34.3354\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8067 - val_loss: 40.1770\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7590 - val_loss: 45.6421\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.7344 - val_loss: 37.3134\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7746 - val_loss: 40.3391\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.7348 - val_loss: 40.2994\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 21.7154 - val_loss: 33.2477\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7586 - val_loss: 39.0150\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 21.7659 - val_loss: 35.5716\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7503 - val_loss: 38.9571\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 21.7504 - val_loss: 39.6270\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7063 - val_loss: 37.0681\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7260 - val_loss: 34.5923\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7414 - val_loss: 37.3128\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7253 - val_loss: 34.1137\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.6764 - val_loss: 34.0258\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7583 - val_loss: 36.1357\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7795 - val_loss: 36.6731\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.6845 - val_loss: 41.5957\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.6754 - val_loss: 41.2646\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7239 - val_loss: 35.5352\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8024 - val_loss: 33.9520\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.6507 - val_loss: 57.0782\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.6486 - val_loss: 38.5702\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.6994 - val_loss: 49.5895\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.6742 - val_loss: 33.6582\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.6745 - val_loss: 38.7174\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -1.8598798268070376 \n",
            "MAE:  4.695472886949465 \n",
            "SD:  5.937861801646746\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_25 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 7,809\n",
            "Trainable params: 7,553\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3502.3987 - val_loss: 3445.6626\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 2931.7859 - val_loss: 2811.6956\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 2222.9656 - val_loss: 1737.2628\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 1429.9371 - val_loss: 1254.0847\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 739.5057 - val_loss: 411.8135\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 318.0805 - val_loss: 233.1318\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 121.9746 - val_loss: 58.1859\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 54.1591 - val_loss: 63.5701\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 37.0458 - val_loss: 67.8211\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 33.0402 - val_loss: 43.5420\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.8827 - val_loss: 40.2690\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 31.2747 - val_loss: 38.3497\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.9317 - val_loss: 74.3194\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.6429 - val_loss: 49.1142\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 30.0977 - val_loss: 35.9085\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.8442 - val_loss: 38.9422\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 29.6806 - val_loss: 62.3209\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.3063 - val_loss: 37.6668\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 29.0982 - val_loss: 35.5925\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.9727 - val_loss: 39.9683\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 28.7191 - val_loss: 52.5385\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.6370 - val_loss: 36.7517\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.5029 - val_loss: 37.9101\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 28.3090 - val_loss: 41.2940\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.2687 - val_loss: 35.1910\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 28.2347 - val_loss: 36.8337\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.9742 - val_loss: 42.6830\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.8629 - val_loss: 39.9894\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.8399 - val_loss: 40.6767\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.7074 - val_loss: 47.3719\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.6641 - val_loss: 36.5722\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.5312 - val_loss: 36.1513\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.3469 - val_loss: 39.1638\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.3880 - val_loss: 38.1438\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 27.2109 - val_loss: 40.8319\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.1473 - val_loss: 34.1066\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.9348 - val_loss: 35.8546\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.9273 - val_loss: 39.4945\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.7731 - val_loss: 35.6914\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.7500 - val_loss: 35.9309\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.6373 - val_loss: 38.0705\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.6391 - val_loss: 40.7852\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.4717 - val_loss: 60.7843\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.5532 - val_loss: 39.1317\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.3779 - val_loss: 35.5745\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.3153 - val_loss: 38.7783\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.2423 - val_loss: 34.6539\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.1868 - val_loss: 42.7129\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 26.1608 - val_loss: 34.9895\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.0769 - val_loss: 39.3168\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.9916 - val_loss: 38.4091\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.0182 - val_loss: 36.7669\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.9331 - val_loss: 38.5571\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.9118 - val_loss: 33.0961\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.8463 - val_loss: 43.7633\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.8374 - val_loss: 50.0998\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.7387 - val_loss: 34.7620\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.7171 - val_loss: 37.8289\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5993 - val_loss: 39.0903\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.5669 - val_loss: 35.1205\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.5905 - val_loss: 38.5961\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.5698 - val_loss: 48.0284\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4224 - val_loss: 41.6857\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.3868 - val_loss: 37.0190\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.3327 - val_loss: 52.2374\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3465 - val_loss: 37.1954\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.2833 - val_loss: 35.8922\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.2834 - val_loss: 35.8322\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.3308 - val_loss: 43.1160\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.2412 - val_loss: 34.9450\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.1962 - val_loss: 38.7962\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.2299 - val_loss: 40.0836\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.1369 - val_loss: 40.7777\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.0452 - val_loss: 38.6352\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 25.0591 - val_loss: 40.6125\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 25.0089 - val_loss: 35.4611\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.9831 - val_loss: 35.1587\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 25.0561 - val_loss: 57.2880\n",
            "Epoch 79/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 4ms/step - loss: 24.9796 - val_loss: 39.5816\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.9533 - val_loss: 39.4239\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.9371 - val_loss: 37.3623\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.9326 - val_loss: 41.0978\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.8570 - val_loss: 35.6852\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.7954 - val_loss: 34.3499\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.7963 - val_loss: 39.3361\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 24.7821 - val_loss: 41.5907\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 24.8390 - val_loss: 39.1250\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.7341 - val_loss: 35.5370\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.6747 - val_loss: 41.7977\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6786 - val_loss: 43.6528\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.6220 - val_loss: 37.6792\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.6195 - val_loss: 34.3528\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.6008 - val_loss: 37.6559\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.5214 - val_loss: 38.8622\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.6526 - val_loss: 33.7988\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.4926 - val_loss: 36.2840\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.4754 - val_loss: 44.7030\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 24.5142 - val_loss: 44.1573\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.4426 - val_loss: 37.0921\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.4231 - val_loss: 46.1529\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.3741 - val_loss: 40.8336\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.3372 - val_loss: 39.0638\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.3538 - val_loss: 40.0361\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.3666 - val_loss: 38.6917\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.3923 - val_loss: 52.4054\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2483 - val_loss: 35.7963\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.2976 - val_loss: 46.0613\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2583 - val_loss: 38.0427\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.2733 - val_loss: 40.3210\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.3103 - val_loss: 41.5400\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2046 - val_loss: 35.2737\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.3046 - val_loss: 44.2969\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.2483 - val_loss: 36.4315\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.0069 - val_loss: 38.9436\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.1202 - val_loss: 56.2692\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.0764 - val_loss: 41.7207\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.0925 - val_loss: 37.8080\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.1292 - val_loss: 38.1069\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.0454 - val_loss: 40.9487\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.0471 - val_loss: 50.4290\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.9812 - val_loss: 36.1030\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.0181 - val_loss: 40.0130\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.0397 - val_loss: 42.2706\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.0659 - val_loss: 36.9187\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.0139 - val_loss: 34.8104\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.9590 - val_loss: 35.2740\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 24.0147 - val_loss: 43.2421\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.9616 - val_loss: 36.8385\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.9670 - val_loss: 36.7250\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.8475 - val_loss: 34.7492\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.9170 - val_loss: 39.7299\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.8969 - val_loss: 34.9375\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.9547 - val_loss: 38.4607\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.8234 - val_loss: 35.8090\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.8266 - val_loss: 34.2917\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.8107 - val_loss: 36.3778\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 24.0016 - val_loss: 35.2881\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7488 - val_loss: 36.8609\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.7315 - val_loss: 43.0316\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7701 - val_loss: 46.8726\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.8147 - val_loss: 36.5697\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.7167 - val_loss: 38.0857\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.7630 - val_loss: 48.2410\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.7387 - val_loss: 49.4013\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.6729 - val_loss: 32.9022\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.6727 - val_loss: 34.7931\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.6934 - val_loss: 32.4452\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.6790 - val_loss: 42.6495\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.6234 - val_loss: 36.9325\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.6595 - val_loss: 40.7272\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.6353 - val_loss: 38.9067\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.5325 - val_loss: 38.5541\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.6240 - val_loss: 39.4986\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.5908 - val_loss: 38.6053\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5385 - val_loss: 35.7523\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.6454 - val_loss: 53.6565\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.5979 - val_loss: 40.3361\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5493 - val_loss: 41.1927\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.4890 - val_loss: 34.6143\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.4653 - val_loss: 47.7149\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.5931 - val_loss: 35.2312\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5098 - val_loss: 36.9736\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.4661 - val_loss: 36.4641\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.4929 - val_loss: 37.8621\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.5130 - val_loss: 38.1706\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.5388 - val_loss: 33.9548\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.4479 - val_loss: 40.5009\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 23.4381 - val_loss: 34.6514\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3890 - val_loss: 44.1380\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.4338 - val_loss: 43.8564\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3906 - val_loss: 40.3755\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.3737 - val_loss: 40.7925\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.4468 - val_loss: 47.5296\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3906 - val_loss: 35.0138\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.3254 - val_loss: 36.8123\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3659 - val_loss: 35.4484\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.3670 - val_loss: 35.9360\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 23.3534 - val_loss: 38.1284\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.2590 - val_loss: 34.1867\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2325 - val_loss: 42.2653\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.2354 - val_loss: 40.6616\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3025 - val_loss: 36.1759\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.3051 - val_loss: 36.6485\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.2434 - val_loss: 35.1169\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1904 - val_loss: 37.7050\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.2641 - val_loss: 41.6813\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2122 - val_loss: 37.0971\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.2578 - val_loss: 39.4193\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1933 - val_loss: 33.5695\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2137 - val_loss: 44.5657\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.2040 - val_loss: 37.5177\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1602 - val_loss: 33.5192\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.2188 - val_loss: 39.4932\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.2329 - val_loss: 36.4303\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.1888 - val_loss: 41.6679\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0946 - val_loss: 41.8415\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1557 - val_loss: 38.8621\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.1146 - val_loss: 34.5730\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 23.1126 - val_loss: 35.3013\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.1035 - val_loss: 42.5935\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1094 - val_loss: 34.6723\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.1279 - val_loss: 36.4135\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.1389 - val_loss: 35.7976\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.0726 - val_loss: 35.4458\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0292 - val_loss: 37.8215\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0495 - val_loss: 38.1952\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 23.0588 - val_loss: 37.0341\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0799 - val_loss: 36.5123\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9825 - val_loss: 44.1860\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0124 - val_loss: 34.6732\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9623 - val_loss: 36.6061\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 23.0274 - val_loss: 48.4418\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9509 - val_loss: 36.4826\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.9892 - val_loss: 39.7206\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9442 - val_loss: 38.8053\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9510 - val_loss: 36.1053\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9823 - val_loss: 48.0748\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9366 - val_loss: 35.5406\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9109 - val_loss: 38.4535\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9250 - val_loss: 39.4867\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9611 - val_loss: 37.3334\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.9503 - val_loss: 36.2122\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8786 - val_loss: 40.4441\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8858 - val_loss: 36.3647\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8829 - val_loss: 38.7269\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8689 - val_loss: 34.2883\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8791 - val_loss: 41.1624\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8086 - val_loss: 36.8504\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8402 - val_loss: 35.9287\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.9327 - val_loss: 46.5457\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8099 - val_loss: 44.7233\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7562 - val_loss: 38.5010\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8571 - val_loss: 33.9977\n",
            "Epoch 234/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8164 - val_loss: 50.1493\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8844 - val_loss: 34.9338\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7339 - val_loss: 34.9866\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.8544 - val_loss: 35.2590\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7397 - val_loss: 35.3061\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7960 - val_loss: 36.4173\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.8117 - val_loss: 49.1056\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7721 - val_loss: 35.3008\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7303 - val_loss: 35.6804\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.8123 - val_loss: 42.6844\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6885 - val_loss: 38.4078\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7684 - val_loss: 39.5444\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.6917 - val_loss: 33.6392\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6517 - val_loss: 37.6428\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7502 - val_loss: 36.8062\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.7634 - val_loss: 52.3212\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6425 - val_loss: 35.8330\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6554 - val_loss: 39.8175\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6612 - val_loss: 36.2569\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6941 - val_loss: 34.4721\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6413 - val_loss: 37.9602\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6259 - val_loss: 41.0632\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6547 - val_loss: 37.4922\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6782 - val_loss: 39.4713\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6953 - val_loss: 39.4144\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5543 - val_loss: 34.2577\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6311 - val_loss: 38.1340\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6223 - val_loss: 42.4287\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6658 - val_loss: 43.4600\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6235 - val_loss: 39.1992\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.7040 - val_loss: 43.1545\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6120 - val_loss: 35.7715\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5752 - val_loss: 37.6167\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4734 - val_loss: 41.9770\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5764 - val_loss: 37.7890\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.6616 - val_loss: 33.9358\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.6144 - val_loss: 38.7584\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4749 - val_loss: 37.8038\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5518 - val_loss: 35.5358\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.6074 - val_loss: 36.5007\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5684 - val_loss: 38.4659\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5099 - val_loss: 36.7333\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5834 - val_loss: 36.6101\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5546 - val_loss: 36.6605\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5145 - val_loss: 39.1161\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5304 - val_loss: 34.4553\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.5655 - val_loss: 39.9264\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5469 - val_loss: 39.0478\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4845 - val_loss: 36.3654\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4516 - val_loss: 38.0380\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5447 - val_loss: 36.2274\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4808 - val_loss: 32.8314\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4480 - val_loss: 40.3925\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.5218 - val_loss: 36.9957\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5106 - val_loss: 33.8235\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4153 - val_loss: 37.2149\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4815 - val_loss: 39.1910\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4721 - val_loss: 42.8583\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4784 - val_loss: 35.1378\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.5116 - val_loss: 36.9264\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.4297 - val_loss: 36.7978\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4201 - val_loss: 36.2070\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.4076 - val_loss: 34.0293\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4503 - val_loss: 43.6231\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3897 - val_loss: 38.4014\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4016 - val_loss: 36.3805\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3516 - val_loss: 36.6325\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3802 - val_loss: 42.2953\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4002 - val_loss: 39.5587\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3902 - val_loss: 39.8806\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3723 - val_loss: 37.5469\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3906 - val_loss: 33.9557\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4495 - val_loss: 43.8839\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.4067 - val_loss: 39.4005\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3146 - val_loss: 35.2993\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2837 - val_loss: 36.0526\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3847 - val_loss: 42.8757\n",
            "Epoch 311/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3922 - val_loss: 38.6988\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3622 - val_loss: 37.1985\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3555 - val_loss: 37.7127\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3684 - val_loss: 46.8555\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3127 - val_loss: 37.6201\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3533 - val_loss: 34.8449\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2748 - val_loss: 35.1126\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3163 - val_loss: 33.5297\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2530 - val_loss: 43.8644\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3074 - val_loss: 34.1946\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3161 - val_loss: 43.4443\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3025 - val_loss: 34.2124\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2501 - val_loss: 50.6834\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.2947 - val_loss: 37.8111\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3057 - val_loss: 38.0176\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3453 - val_loss: 34.9300\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.3365 - val_loss: 35.4974\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.3055 - val_loss: 37.4697\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2695 - val_loss: 35.2556\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2351 - val_loss: 36.2123\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.2533 - val_loss: 35.1350\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2121 - val_loss: 41.3075\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2770 - val_loss: 35.7712\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2156 - val_loss: 42.4330\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2354 - val_loss: 39.9062\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1106 - val_loss: 37.6315\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2216 - val_loss: 37.1813\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.2250 - val_loss: 36.7403\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1826 - val_loss: 46.8709\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2154 - val_loss: 39.8006\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2048 - val_loss: 37.1133\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.1904 - val_loss: 40.0396\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2131 - val_loss: 37.6431\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.1503 - val_loss: 39.3052\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2615 - val_loss: 35.8270\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2220 - val_loss: 40.6059\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.2477 - val_loss: 33.9296\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1650 - val_loss: 36.2797\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1412 - val_loss: 42.6431\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.2053 - val_loss: 38.1982\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1540 - val_loss: 34.0895\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1373 - val_loss: 40.0230\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1928 - val_loss: 36.7755\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1113 - val_loss: 44.6694\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1821 - val_loss: 35.9826\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1637 - val_loss: 35.8196\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1708 - val_loss: 35.9446\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1599 - val_loss: 36.2959\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1278 - val_loss: 39.8756\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1145 - val_loss: 34.7403\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.2305 - val_loss: 40.4087\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0930 - val_loss: 35.1840\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.2010 - val_loss: 36.8394\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1536 - val_loss: 47.6812\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1473 - val_loss: 38.0541\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1443 - val_loss: 35.1964\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1215 - val_loss: 45.2115\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1079 - val_loss: 38.5331\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0211 - val_loss: 34.8417\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0506 - val_loss: 35.7528\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1173 - val_loss: 36.8776\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1271 - val_loss: 35.2332\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0450 - val_loss: 36.0954\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0204 - val_loss: 39.7337\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.1100 - val_loss: 38.2955\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0698 - val_loss: 38.5679\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1159 - val_loss: 39.4677\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1282 - val_loss: 46.0687\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0016 - val_loss: 36.1433\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0970 - val_loss: 39.0481\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0606 - val_loss: 35.6289\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0456 - val_loss: 37.1164\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9986 - val_loss: 40.8954\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 22.0430 - val_loss: 39.1499\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.1129 - val_loss: 35.3588\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.1085 - val_loss: 37.5133\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0921 - val_loss: 37.3937\n",
            "Epoch 388/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0192 - val_loss: 37.1062\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0280 - val_loss: 35.8655\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9601 - val_loss: 35.0477\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0268 - val_loss: 33.7476\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0091 - val_loss: 47.6627\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9979 - val_loss: 34.7093\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 22.0341 - val_loss: 40.6514\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 22.0068 - val_loss: 41.8955\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 21.9684 - val_loss: 36.9133\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0129 - val_loss: 39.5584\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9727 - val_loss: 35.8782\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0656 - val_loss: 37.8642\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0734 - val_loss: 35.2275\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0137 - val_loss: 37.8895\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9623 - val_loss: 38.5019\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 22.0336 - val_loss: 44.4299\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9879 - val_loss: 38.2921\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9879 - val_loss: 36.4964\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 22.0055 - val_loss: 36.6501\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9513 - val_loss: 35.5981\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9683 - val_loss: 40.3318\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9153 - val_loss: 38.1431\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9997 - val_loss: 41.2784\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9429 - val_loss: 38.2633\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9670 - val_loss: 35.1928\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9558 - val_loss: 35.0482\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9190 - val_loss: 42.3185\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8374 - val_loss: 37.3685\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9205 - val_loss: 37.0611\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9876 - val_loss: 33.7134\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8936 - val_loss: 36.0097\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9770 - val_loss: 38.0068\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.9091 - val_loss: 43.6866\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9515 - val_loss: 37.9770\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9299 - val_loss: 38.2534\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9489 - val_loss: 40.1847\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.9485 - val_loss: 36.5364\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8698 - val_loss: 35.6240\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9248 - val_loss: 38.0436\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8763 - val_loss: 35.1128\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9185 - val_loss: 34.5003\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8031 - val_loss: 37.8838\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8719 - val_loss: 39.1026\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8365 - val_loss: 34.4848\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.9540 - val_loss: 37.2433\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8668 - val_loss: 35.3128\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8881 - val_loss: 34.5731\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8733 - val_loss: 38.2702\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8844 - val_loss: 41.5572\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8499 - val_loss: 39.3594\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8085 - val_loss: 44.3045\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7933 - val_loss: 39.3102\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8334 - val_loss: 38.0162\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8203 - val_loss: 36.6477\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8528 - val_loss: 43.1567\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.8854 - val_loss: 36.0597\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8618 - val_loss: 35.1138\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.9161 - val_loss: 35.1126\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8203 - val_loss: 35.4236\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8151 - val_loss: 36.9539\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.9145 - val_loss: 37.8740\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8016 - val_loss: 37.6185\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 21.8248 - val_loss: 35.7160\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8355 - val_loss: 37.7611\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8941 - val_loss: 45.3512\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8403 - val_loss: 34.8613\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7987 - val_loss: 33.8371\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8602 - val_loss: 38.2851\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7749 - val_loss: 34.7252\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8085 - val_loss: 39.4130\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8031 - val_loss: 34.9498\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7840 - val_loss: 34.7793\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7931 - val_loss: 37.2042\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7938 - val_loss: 36.1200\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8931 - val_loss: 36.2255\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8233 - val_loss: 34.5252\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7838 - val_loss: 39.3693\n",
            "Epoch 465/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8391 - val_loss: 35.6692\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7937 - val_loss: 35.8039\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7439 - val_loss: 37.3371\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8544 - val_loss: 34.9590\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7950 - val_loss: 37.3692\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7889 - val_loss: 34.9932\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7463 - val_loss: 32.8589\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7188 - val_loss: 40.1641\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8037 - val_loss: 38.0450\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8260 - val_loss: 42.0189\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7590 - val_loss: 35.2314\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8173 - val_loss: 40.8039\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8177 - val_loss: 36.0481\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7559 - val_loss: 41.2910\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8397 - val_loss: 39.6153\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7741 - val_loss: 35.0033\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7437 - val_loss: 39.8262\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7388 - val_loss: 35.3865\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7926 - val_loss: 37.4465\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.6885 - val_loss: 34.8046\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8113 - val_loss: 41.6167\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.7613 - val_loss: 34.7278\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7668 - val_loss: 37.6061\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.6785 - val_loss: 35.4427\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7827 - val_loss: 35.5126\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.8290 - val_loss: 37.2901\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.8100 - val_loss: 35.5883\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7301 - val_loss: 39.4753\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.6897 - val_loss: 39.5706\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7351 - val_loss: 41.4313\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7151 - val_loss: 38.5254\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.6809 - val_loss: 33.7404\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.7309 - val_loss: 36.0263\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.6670 - val_loss: 35.8128\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 21.6655 - val_loss: 37.5787\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7154 - val_loss: 33.8204\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -0.29249394584290733 \n",
            "MAE:  4.290874718599266 \n",
            "SD:  5.808172036625007\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "outputId": "4c631050-dc97-4897-8703-48db9ff4ff0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  -0.2946604296080587 \n",
            "Ensemble_std:  5.827103959538362\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "_BP_hv3_4(1).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}