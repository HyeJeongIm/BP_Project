{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/%08BP_hv3_7(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2YTF6cMiY1Hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# batch_size"
      ],
      "metadata": {
        "id": "XiiiBla2-j1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsCoux5AOZnK",
        "outputId": "d9753c81-ebaa-4da5-aab3-8202bfb93927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "TensorFlow version :  2.8.2\n",
            "Keras version :  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlHICkovd809",
        "outputId": "0875decb-2f84-4c0f-cda3-897bd01c6871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러z오기\n",
        "train_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ],
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "a791c4e8-b447-4d75-8e07-338e63052b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "0fb75498-c2c4-42c5-bd32-af11d12e0bd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba47ea99-ec74-4eb6-9dd8-103cf4797c58\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba47ea99-ec74-4eb6-9dd8-103cf4797c58')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ba47ea99-ec74-4eb6-9dd8-103cf4797c58 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ba47ea99-ec74-4eb6-9dd8-103cf4797c58');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "f2f9d4c5-7f90-4dd3-c003-2e51757dd0e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-abec7859-13b9-4ffa-b488-b52a07020337\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-abec7859-13b9-4ffa-b488-b52a07020337')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-abec7859-13b9-4ffa-b488-b52a07020337 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-abec7859-13b9-4ffa-b488-b52a07020337');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ],
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8SHBwBOZnO",
        "outputId": "70eba728-c4fd-483c-efe6-899d49e714be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 8)                32        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,313\n",
            "Trainable params: 2,105\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dGT6-7NcOZnO",
        "outputId": "03f3ef0d-d895-4d47-b86c-68941c845025",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 11s 13ms/step - loss: 12171.9316 - val_loss: 12110.3535\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 11738.1094 - val_loss: 11580.7822\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 11215.2549 - val_loss: 10851.6074\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 10566.3711 - val_loss: 10099.8604\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 9769.7295 - val_loss: 9033.6621\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 8864.4912 - val_loss: 8489.6748\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 7845.1343 - val_loss: 7922.9741\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 6773.1187 - val_loss: 6616.5176\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 5710.7979 - val_loss: 5949.9043\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 4702.5781 - val_loss: 4063.9053\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3721.4771 - val_loss: 4720.0718\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2872.3357 - val_loss: 2128.0642\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2143.7109 - val_loss: 2177.9883\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1557.9700 - val_loss: 1022.7633\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1094.5941 - val_loss: 909.0702\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 736.2842 - val_loss: 782.6614\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 490.6808 - val_loss: 227.7066\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 331.2368 - val_loss: 448.5895\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 242.6064 - val_loss: 588.4177\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 191.6989 - val_loss: 303.3816\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 163.4203 - val_loss: 221.9758\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 149.6186 - val_loss: 175.9900\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 139.6414 - val_loss: 171.2851\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 133.1984 - val_loss: 168.1162\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 131.7997 - val_loss: 232.3192\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 127.7177 - val_loss: 141.2510\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 124.8679 - val_loss: 133.1238\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 126.9940 - val_loss: 258.6538\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 122.7792 - val_loss: 131.9512\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 121.1030 - val_loss: 128.6329\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 118.9072 - val_loss: 154.9909\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 117.7404 - val_loss: 139.4295\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 117.1034 - val_loss: 211.2817\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 116.4696 - val_loss: 158.4575\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 115.2157 - val_loss: 144.4991\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.9548 - val_loss: 135.1795\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 112.7141 - val_loss: 129.2365\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.5732 - val_loss: 134.7226\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.1513 - val_loss: 160.1333\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.2988 - val_loss: 171.2554\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.5581 - val_loss: 202.3395\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.0996 - val_loss: 128.3480\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.9488 - val_loss: 144.2653\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.0347 - val_loss: 186.2651\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.6862 - val_loss: 148.7330\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.7215 - val_loss: 116.0487\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.5941 - val_loss: 114.7795\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.6520 - val_loss: 117.8894\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.1608 - val_loss: 121.1205\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.8724 - val_loss: 125.9860\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.7477 - val_loss: 123.9662\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.2584 - val_loss: 117.3461\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.3528 - val_loss: 125.3439\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.0799 - val_loss: 114.9057\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.6031 - val_loss: 106.4055\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.0718 - val_loss: 156.7074\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.1382 - val_loss: 136.1829\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.0037 - val_loss: 233.1746\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.8523 - val_loss: 156.0228\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.7577 - val_loss: 109.7578\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.9365 - val_loss: 124.9422\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.4648 - val_loss: 111.3053\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.2898 - val_loss: 112.5612\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.6281 - val_loss: 136.5895\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 96.8327 - val_loss: 131.9998\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 96.5142 - val_loss: 123.6981\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.0445 - val_loss: 118.1942\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.0655 - val_loss: 124.0378\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.9217 - val_loss: 124.3041\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.2668 - val_loss: 113.0952\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.9429 - val_loss: 122.0760\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.7357 - val_loss: 111.1011\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.8217 - val_loss: 109.8092\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.2731 - val_loss: 109.4986\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.1177 - val_loss: 110.2230\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.4918 - val_loss: 106.5363\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.6829 - val_loss: 115.0587\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.6653 - val_loss: 118.2643\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.3481 - val_loss: 107.4238\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.3137 - val_loss: 113.3049\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.6823 - val_loss: 117.8261\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.9095 - val_loss: 119.3809\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.1001 - val_loss: 100.6197\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.1581 - val_loss: 105.8545\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.5364 - val_loss: 108.6784\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.0915 - val_loss: 108.8294\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.9035 - val_loss: 111.2631\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.1103 - val_loss: 119.1542\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.8221 - val_loss: 109.6627\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.8763 - val_loss: 121.5773\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.1339 - val_loss: 118.8675\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.9848 - val_loss: 150.7945\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.0995 - val_loss: 109.5120\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.5702 - val_loss: 107.0990\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.2041 - val_loss: 115.8736\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.2671 - val_loss: 174.4753\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.2807 - val_loss: 116.2837\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.9653 - val_loss: 130.2201\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.9868 - val_loss: 108.3617\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.0236 - val_loss: 101.8255\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.3366 - val_loss: 103.7239\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.2586 - val_loss: 106.5716\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.9810 - val_loss: 106.5492\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.8176 - val_loss: 103.5137\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.7519 - val_loss: 139.6321\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.9191 - val_loss: 103.2244\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.8049 - val_loss: 111.0940\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.9723 - val_loss: 122.6129\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.2877 - val_loss: 104.9718\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.1685 - val_loss: 128.6977\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.0386 - val_loss: 108.6346\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.8786 - val_loss: 137.0670\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.0188 - val_loss: 99.6608\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.5995 - val_loss: 110.8779\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.5588 - val_loss: 105.9656\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.5951 - val_loss: 102.9513\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.3717 - val_loss: 147.6280\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.2174 - val_loss: 121.3213\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.5520 - val_loss: 107.7543\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.3378 - val_loss: 140.4210\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.3501 - val_loss: 116.4235\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.4073 - val_loss: 109.3315\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.0333 - val_loss: 97.7417\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.7218 - val_loss: 128.6822\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.9512 - val_loss: 131.3808\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.5643 - val_loss: 100.9071\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.3813 - val_loss: 94.8414\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.4742 - val_loss: 107.0833\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.1691 - val_loss: 119.5143\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.0348 - val_loss: 110.7396\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.4089 - val_loss: 109.1079\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.9235 - val_loss: 98.2921\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.0107 - val_loss: 105.8780\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.8687 - val_loss: 108.5286\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.9637 - val_loss: 97.6465\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.7266 - val_loss: 99.8789\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.6589 - val_loss: 97.6522\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.6153 - val_loss: 192.5505\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.6928 - val_loss: 98.8377\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.5216 - val_loss: 94.6301\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.3854 - val_loss: 119.8882\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.3788 - val_loss: 107.2395\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.1786 - val_loss: 109.5508\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.0647 - val_loss: 142.9190\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.1809 - val_loss: 113.7746\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.0911 - val_loss: 105.2808\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.9281 - val_loss: 96.2115\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.7022 - val_loss: 123.2571\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.9258 - val_loss: 112.2141\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.8699 - val_loss: 98.4262\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.7657 - val_loss: 145.8460\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.0313 - val_loss: 95.7651\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.5749 - val_loss: 97.9224\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.4509 - val_loss: 102.9332\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.5957 - val_loss: 103.2928\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.4659 - val_loss: 95.1159\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.6815 - val_loss: 108.1433\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.2496 - val_loss: 100.8667\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.4128 - val_loss: 99.9017\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.0238 - val_loss: 96.4120\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.5155 - val_loss: 107.8186\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.4292 - val_loss: 101.4630\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.1717 - val_loss: 93.8294\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.1622 - val_loss: 97.9632\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.3798 - val_loss: 94.3024\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.2499 - val_loss: 96.3462\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.4060 - val_loss: 93.7949\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.7483 - val_loss: 100.2702\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.7602 - val_loss: 95.3420\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.9089 - val_loss: 93.5931\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.8966 - val_loss: 136.4021\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.7382 - val_loss: 132.0533\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.6693 - val_loss: 116.0711\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.8543 - val_loss: 99.7605\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.6362 - val_loss: 102.5597\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.3874 - val_loss: 119.8106\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.2908 - val_loss: 96.7419\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.3694 - val_loss: 105.2933\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.1552 - val_loss: 98.9230\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.2622 - val_loss: 185.8399\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.2191 - val_loss: 94.8915\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.4316 - val_loss: 105.7082\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.2633 - val_loss: 92.4494\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.3149 - val_loss: 96.6997\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.2364 - val_loss: 103.7330\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.9885 - val_loss: 91.1790\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.0259 - val_loss: 96.8944\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 81.9446 - val_loss: 102.5553\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 81.8491 - val_loss: 100.7135\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.8444 - val_loss: 92.6558\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.2189 - val_loss: 103.5459\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.9616 - val_loss: 114.0027\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.7431 - val_loss: 96.3329\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.4995 - val_loss: 108.4488\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.4409 - val_loss: 93.0710\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.7531 - val_loss: 96.6585\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.4078 - val_loss: 91.8820\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.3595 - val_loss: 99.5430\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.7062 - val_loss: 156.7723\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.5621 - val_loss: 97.3759\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.2132 - val_loss: 121.7965\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.4846 - val_loss: 99.6380\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.4684 - val_loss: 106.0268\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.3296 - val_loss: 95.1297\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0823 - val_loss: 96.7469\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.3493 - val_loss: 148.3352\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.1287 - val_loss: 131.6138\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0564 - val_loss: 97.5329\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0904 - val_loss: 102.9294\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0737 - val_loss: 107.4538\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.2011 - val_loss: 98.1530\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.1883 - val_loss: 94.3952\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.9921 - val_loss: 95.5099\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.1586 - val_loss: 93.8241\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0024 - val_loss: 127.8785\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.9658 - val_loss: 134.7136\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.8112 - val_loss: 92.9388\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.9908 - val_loss: 135.0404\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.7102 - val_loss: 106.4675\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.6950 - val_loss: 98.5088\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.6289 - val_loss: 103.9768\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0334 - val_loss: 99.6152\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.7018 - val_loss: 90.9874\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.8456 - val_loss: 104.6464\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.7091 - val_loss: 107.8031\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.7053 - val_loss: 99.1490\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.6890 - val_loss: 93.5276\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.6151 - val_loss: 90.6481\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.5942 - val_loss: 95.3582\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.7257 - val_loss: 103.1006\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.7535 - val_loss: 95.5494\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.4572 - val_loss: 94.4907\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.4139 - val_loss: 132.8346\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.5449 - val_loss: 96.1526\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2026 - val_loss: 90.0589\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2462 - val_loss: 93.8230\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.5682 - val_loss: 90.6220\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1969 - val_loss: 98.2921\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.3616 - val_loss: 97.2330\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.4434 - val_loss: 92.5525\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1244 - val_loss: 93.8540\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.5857 - val_loss: 95.9192\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.4357 - val_loss: 98.8245\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1520 - val_loss: 97.5751\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1293 - val_loss: 104.1452\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2588 - val_loss: 93.0124\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2335 - val_loss: 97.7838\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1703 - val_loss: 98.7341\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.4345 - val_loss: 113.8868\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8362 - val_loss: 93.6964\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8990 - val_loss: 91.0973\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.9795 - val_loss: 92.2152\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8292 - val_loss: 89.7558\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8824 - val_loss: 91.5604\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8600 - val_loss: 93.2467\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7924 - val_loss: 115.6272\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8263 - val_loss: 93.2860\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.7710 - val_loss: 103.4479\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1199 - val_loss: 104.8874\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7135 - val_loss: 101.8318\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.9301 - val_loss: 134.0410\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.0294 - val_loss: 91.4186\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.6800 - val_loss: 100.9826\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8181 - val_loss: 168.1261\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5332 - val_loss: 91.8921\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8558 - val_loss: 96.7754\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.0571 - val_loss: 146.8426\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7453 - val_loss: 96.5822\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8346 - val_loss: 94.5068\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7322 - val_loss: 102.0997\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8571 - val_loss: 146.5676\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8899 - val_loss: 96.8253\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7567 - val_loss: 103.4623\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7078 - val_loss: 101.0873\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7933 - val_loss: 104.9618\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7156 - val_loss: 93.7309\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5702 - val_loss: 93.1699\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8234 - val_loss: 102.3527\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.4586 - val_loss: 111.3851\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5962 - val_loss: 99.5581\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.4832 - val_loss: 102.0005\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5659 - val_loss: 90.3159\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.6379 - val_loss: 144.0446\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.6842 - val_loss: 96.6465\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5267 - val_loss: 102.6810\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.4957 - val_loss: 93.4626\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3786 - val_loss: 95.9153\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2147 - val_loss: 103.9171\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3218 - val_loss: 114.5654\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.4590 - val_loss: 135.3805\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.4545 - val_loss: 96.1314\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3708 - val_loss: 94.9112\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5755 - val_loss: 89.8837\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5737 - val_loss: 101.3585\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3298 - val_loss: 91.2853\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1958 - val_loss: 93.6058\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3083 - val_loss: 96.2118\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2767 - val_loss: 106.9490\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2110 - val_loss: 99.9963\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2652 - val_loss: 103.3037\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5662 - val_loss: 114.6199\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3237 - val_loss: 93.2488\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1664 - val_loss: 92.4439\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0191 - val_loss: 103.2639\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2797 - val_loss: 96.5594\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1777 - val_loss: 99.6754\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0255 - val_loss: 92.7524\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5595 - val_loss: 91.8568\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1166 - val_loss: 99.1550\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.4785 - val_loss: 138.4272\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2464 - val_loss: 98.1274\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.0557 - val_loss: 136.4758\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.9676 - val_loss: 114.3950\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1394 - val_loss: 94.1560\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0958 - val_loss: 90.4773\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1680 - val_loss: 95.9088\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0253 - val_loss: 98.2870\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3087 - val_loss: 121.4296\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1103 - val_loss: 98.9479\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0126 - val_loss: 93.6241\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9628 - val_loss: 107.2264\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2863 - val_loss: 116.3640\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0916 - val_loss: 94.1413\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8727 - val_loss: 102.8933\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9172 - val_loss: 93.9684\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0717 - val_loss: 109.8220\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9273 - val_loss: 151.0013\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0256 - val_loss: 98.2536\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9340 - val_loss: 93.3868\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8067 - val_loss: 104.8394\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8569 - val_loss: 90.0449\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0137 - val_loss: 96.7298\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8408 - val_loss: 91.3669\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8809 - val_loss: 108.1448\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7666 - val_loss: 157.5566\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7553 - val_loss: 96.5380\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1167 - val_loss: 112.7637\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7459 - val_loss: 99.1668\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9687 - val_loss: 89.9518\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.6912 - val_loss: 106.2943\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.6173 - val_loss: 91.5959\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7144 - val_loss: 92.3601\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7225 - val_loss: 92.4530\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.6275 - val_loss: 91.4863\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7625 - val_loss: 93.2374\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7286 - val_loss: 100.3522\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8186 - val_loss: 89.6500\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7406 - val_loss: 90.9442\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8743 - val_loss: 91.7941\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7761 - val_loss: 110.7647\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8776 - val_loss: 97.3951\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7503 - val_loss: 90.9790\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0004 - val_loss: 91.3590\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.6175 - val_loss: 96.1633\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3849 - val_loss: 107.7596\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.4634 - val_loss: 94.9040\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7501 - val_loss: 97.2448\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5234 - val_loss: 99.1177\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5106 - val_loss: 103.0695\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4896 - val_loss: 100.7306\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5677 - val_loss: 96.8254\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7969 - val_loss: 101.0025\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7181 - val_loss: 105.0882\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5069 - val_loss: 92.7994\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.6078 - val_loss: 104.3540\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.5510 - val_loss: 114.4224\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7267 - val_loss: 125.4406\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8004 - val_loss: 98.9507\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5613 - val_loss: 90.5809\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7715 - val_loss: 93.5045\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.6853 - val_loss: 92.1302\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4592 - val_loss: 90.6350\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.4102 - val_loss: 91.2934\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0852 - val_loss: 89.5048\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.3676 - val_loss: 113.8941\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4383 - val_loss: 90.1078\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3294 - val_loss: 92.7754\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4384 - val_loss: 118.8130\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4583 - val_loss: 98.0735\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5719 - val_loss: 95.8309\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4481 - val_loss: 101.5632\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5028 - val_loss: 108.5914\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5147 - val_loss: 91.0113\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5739 - val_loss: 95.7122\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3572 - val_loss: 91.1096\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3125 - val_loss: 116.2824\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3815 - val_loss: 101.2493\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.2673 - val_loss: 109.7174\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4899 - val_loss: 93.3914\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1696 - val_loss: 108.0393\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.3179 - val_loss: 100.5117\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1846 - val_loss: 115.5429\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.2658 - val_loss: 139.5923\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4713 - val_loss: 89.5307\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3954 - val_loss: 96.7452\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3971 - val_loss: 90.5551\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.2537 - val_loss: 94.8463\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3857 - val_loss: 91.9902\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3073 - val_loss: 112.6010\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3301 - val_loss: 93.0433\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3025 - val_loss: 92.0781\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.2904 - val_loss: 105.7079\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.2501 - val_loss: 92.0270\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3541 - val_loss: 103.6627\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7706 - val_loss: 91.1532\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9047 - val_loss: 88.6892\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.1525 - val_loss: 93.8789\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0068 - val_loss: 91.4729\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.0756 - val_loss: 90.4775\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1155 - val_loss: 97.0480\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0237 - val_loss: 114.2727\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0763 - val_loss: 121.6474\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0762 - val_loss: 90.5412\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1322 - val_loss: 94.2329\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3364 - val_loss: 93.4774\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1241 - val_loss: 107.2546\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.9760 - val_loss: 90.7899\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4063 - val_loss: 101.4045\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.4164 - val_loss: 120.6815\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3174 - val_loss: 91.8947\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1345 - val_loss: 103.0988\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1284 - val_loss: 96.0543\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8261 - val_loss: 88.3162\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.0136 - val_loss: 99.6162\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0367 - val_loss: 132.3149\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 78.1025 - val_loss: 109.5226\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 78.0266 - val_loss: 95.7706\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.3401 - val_loss: 95.8987\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1369 - val_loss: 95.8976\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1927 - val_loss: 96.7201\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9517 - val_loss: 91.4652\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0475 - val_loss: 92.7406\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.2101 - val_loss: 89.1658\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1320 - val_loss: 94.8803\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.8547 - val_loss: 90.8940\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8379 - val_loss: 88.0941\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9279 - val_loss: 96.6833\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.9725 - val_loss: 93.3911\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.8897 - val_loss: 90.1600\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.0064 - val_loss: 102.4432\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9427 - val_loss: 93.2135\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.7643 - val_loss: 93.5112\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9440 - val_loss: 89.7728\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.7086 - val_loss: 93.8011\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9755 - val_loss: 89.0138\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.0740 - val_loss: 158.4055\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8755 - val_loss: 92.4039\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.8455 - val_loss: 91.9957\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9944 - val_loss: 95.4199\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.9925 - val_loss: 93.2721\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.9032 - val_loss: 92.5585\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.0316 - val_loss: 97.8419\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8608 - val_loss: 104.4963\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1193 - val_loss: 99.0069\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0478 - val_loss: 94.6353\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.7003 - val_loss: 97.2224\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.8169 - val_loss: 89.4172\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0319 - val_loss: 97.3598\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6882 - val_loss: 90.0059\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.7824 - val_loss: 99.4398\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8428 - val_loss: 128.2215\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.7630 - val_loss: 119.8754\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6610 - val_loss: 90.3583\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.7621 - val_loss: 95.6895\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.7878 - val_loss: 93.4828\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.8384 - val_loss: 92.4463\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.6436 - val_loss: 92.8010\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.1983 - val_loss: 90.6938\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.7928 - val_loss: 109.2158\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.9524 - val_loss: 87.4913\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6690 - val_loss: 113.3657\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6534 - val_loss: 90.0921\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6362 - val_loss: 90.7856\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8355 - val_loss: 104.1317\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.5902 - val_loss: 99.8900\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8439 - val_loss: 95.4104\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.8455 - val_loss: 92.2938\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.6164 - val_loss: 92.9684\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6944 - val_loss: 91.2990\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0108 - val_loss: 89.6826\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9151 - val_loss: 97.7323\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8117 - val_loss: 92.1643\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6837 - val_loss: 106.1827\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9654 - val_loss: 90.2169\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.7699 - val_loss: 91.6114\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.7404 - val_loss: 96.8534\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.7649 - val_loss: 100.8893\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8027 - val_loss: 96.1163\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.6127 - val_loss: 95.6387\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.5504 - val_loss: 88.7304\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.4452 - val_loss: 101.4436\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.5359 - val_loss: 112.3647\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.6042 - val_loss: 93.1034\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.6871 - val_loss: 92.2273\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.5504 - val_loss: 96.5632\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.5574 - val_loss: 91.0883\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6940 - val_loss: 89.5297\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4381 - val_loss: 143.8401\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9339 - val_loss: 89.3346\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.7426 - val_loss: 97.4430\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "7fe4da02-b076-4f8b-d6c3-a2f3f318321f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -1.3190035211246445 \n",
            "MAE:  7.304794437589654 \n",
            "SD:  9.782803698435687\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "db0201c2-cd61-4290-e21f-bfcf13a93855",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAFBCAYAAAAsfIegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZgU1fX3v2d29mEHASMoKOqwKCiK4oJRhMQtKioaRdRETaLRGHFJNHmzGVwSE6NBRSViokaM/gQDggTcIlsAQdlEQIYd2YZtZnrO+8epa1VXV+9V3dPd5/M8/XTXre1WddW3Tp177rnEzFAURVH8oyjbFVAURck3VFgVRVF8RoVVURTFZ1RYFUVRfEaFVVEUxWdUWBVFUXwmMGElogoimktEi4loGRH9wirvTkQfE9FqInqZiMqs8nJrerU1/4ig6qYoihIkQVqshwCczcx9AfQDMIyIBgF4CMBjzHwUgJ0AxljLjwGw0yp/zFpOURQl5whMWFmosSZLrQ8DOBvAP63yFwBcZP2+0JqGNX8oEVFQ9VMURQmKQH2sRFRMRIsAbAXwDoDPAexi5nprkQ0Auli/uwD4EgCs+bsBtA2yfoqiKEFQEuTGmTkEoB8RVQJ4HcAx6W6TiG4CcBMANGvW7MRjjomxyQUL8AmOR3PsQ/f2NcDhh6e7e0VRCoAFCxZsZ+b2qa4fqLAamHkXEc0CcAqASiIqsazSrgCqrcWqAXQDsIGISgC0ArDDY1vjAYwHgAEDBvD8+fOj75gIR+F1DKK5ePGyD4AnnvDzsBRFyVOIaF066wcZFdDeslRBRE0AfBPAZwBmAbjUWuxaAG9Yv9+0pmHNf5d9yBBThAY0UDHQ0JDuphRFURIiSIu1M4AXiKgYIuCvMPNbRPQpgH8Q0a8A/A/As9byzwL4GxGtBvAVgCv8qIQKq6IomSYwYWXmJQD6e5SvAXCSR/lBAJf5XQ8Co4GKVFgVRckYGfGxZpMiNIBVWJVGQl1dHTZs2ICDBw9muyoKgIqKCnTt2hWlpaW+brcghFVdAUpjYcOGDWjRogWOOOIIaJh2dmFm7NixAxs2bED37t193Xbe5wooQgMaoMKqNA4OHjyItm3bqqg2AogIbdu2DeTtIe+FVX2sSmNDRbXxENR/kffCqj5WRVEyTUEIq7oCFKXx07x586jz1q5di+OPPz6DtUmPwhBWtVgVRckgeS+sBFaLVVEcrF27Fscccwyuu+469OrVC6NGjcKMGTMwePBg9OzZE3PnzsXs2bPRr18/9OvXD/3798fevXsBAOPGjcPAgQPRp08fPPDAA1H3MXbsWDzh6EL+4IMP4uGHH0ZNTQ2GDh2KE044AVVVVXjjjTeibiMaBw8exOjRo1FVVYX+/ftj1qxZAIBly5bhpJNOQr9+/dCnTx+sWrUK+/btw4gRI9C3b18cf/zxePnll5PeXyoUSLiVWqxKI+T224FFi/zdZr9+wB/+EHex1atX49VXX8WECRMwcOBAvPTSS3j//ffx5ptv4je/+Q1CoRCeeOIJDB48GDU1NaioqMD06dOxatUqzJ07F8yMCy64AHPmzMGQIUMitj9y5EjcfvvtuPXWWwEAr7zyCqZNm4aKigq8/vrraNmyJbZv345BgwbhggsuSKoR6YknngAR4ZNPPsHy5ctx7rnnYuXKlXjqqadw2223YdSoUaitrUUoFMLUqVNx2GGHYcqUKQCA3bt3J7yfdMh7i1UbrxQlku7du6OqqgpFRUU47rjjMHToUBARqqqqsHbtWgwePBh33HEHHn/8cezatQslJSWYPn06pk+fjv79++OEE07A8uXLsWrVKs/t9+/fH1u3bsXGjRuxePFitG7dGt26dQMz495770WfPn1wzjnnoLq6Glu2bEmq7u+//z6uvvpqAMAxxxyDb3zjG1i5ciVOOeUU/OY3v8FDDz2EdevWoUmTJqiqqsI777yDu+++G++99x5atWqV9rlLhMKwWFGqwqo0PhKwLIOivLz8699FRUVfTxcVFaG+vh5jx47FiBEjMHXqVAwePBjTpk0DM+Oee+7B9773vYT2cdlll+Gf//wnNm/ejJEjRwIAJk2ahG3btmHBggUoLS3FEUcc4Vsc6VVXXYWTTz4ZU6ZMwfDhw/HXv/4VZ599NhYuXIipU6fi/vvvx9ChQ/Hzn//cl/3FokCEVX2sipIMn3/+OaqqqlBVVYV58+Zh+fLlOO+88/Czn/0Mo0aNQvPmzVFdXY3S0lJ06NDBcxsjR47EjTfeiO3bt2P27NkA5FW8Q4cOKC0txaxZs7BuXfLZ+U4//XRMmjQJZ599NlauXIn169fj6KOPxpo1a9CjRw/86Ec/wvr167FkyRIcc8wxaNOmDa6++mpUVlbimWeeSeu8JEreC6s0XhUB6WcgVJSC4Q9/+ANmzZr1tavg/PPPR3l5OT777DOccsopACQ86sUXX4wqrMcddxz27t2LLl26oHPnzgCAUaNG4dvf/jaqqqowYMAAxExUH4VbbrkFN998M6qqqlBSUoLnn38e5eXleOWVV/C3v/0NpaWl6NSpE+69917MmzcPd911F4qKilBaWoonn3wy9ZOSBORDytOskUii67MxE/XNKzHn9PuBqVMzVzlF8eCzzz5D7969s10NxYHXf0JEC5h5QKrbLIjGqwZo45WiKJkj710B6mNVlODYsWMHhg4dGlE+c+ZMtG2b/Fign3zyCa655pqwsvLycnz88ccp1zEb5L2wfu1jVWFVFN9p27YtFvkYi1tVVeXr9rJFQbgCmEiFVVGUjFEQwqoWq6IomaQwhJVVWBVFyRx5L6zqY1UUJdPkvbCKK0B9rIqSaWLlV813CkJYWS1WRVEySN6HW6nFqjRWspU1cO3atRg2bBgGDRqEDz/8EAMHDsTo0aPxwAMPYOvWrZg0aRIOHDiA2267DYCMCzVnzhy0aNEC48aNwyuvvIJDhw7h4osvxi9+8Yu4dWJm/PSnP8Xbb78NIsL999+PkSNHYtOmTRg5ciT27NmD+vp6PPnkkzj11FMxZswYzJ8/H0SE66+/Hj/+8Y/9ODUZpUCEVS1WRXESdD5WJ5MnT8aiRYuwePFibN++HQMHDsSQIUPw0ksv4bzzzsN9992HUCiE/fv3Y9GiRaiursbSpUsBALt27crE6fCdvBdWAmtUgNIoyWLWwK/zsQLwzMd6xRVX4I477sCoUaNwySWXoGvXrmH5WAGgpqYGq1atiius77//Pq688koUFxejY8eOOOOMMzBv3jwMHDgQ119/Perq6nDRRRehX79+6NGjB9asWYMf/vCHGDFiBM4999zAz0UQFIiPVV0BiuIkkXyszzzzDA4cOIDBgwdj+fLlX+djXbRoERYtWoTVq1djzJgxKddhyJAhmDNnDrp06YLrrrsOEydOROvWrbF48WKceeaZeOqpp3DDDTekfazZoCCEVX2sipIcJh/r3XffjYEDB36dj3XChAmoqakBAFRXV2Pr1q1xt3X66afj5ZdfRigUwrZt2zBnzhycdNJJWLduHTp27Igbb7wRN9xwAxYuXIjt27ejoaEB3/nOd/CrX/0KCxcuDPpQAyHvXQFf+1hzOD2iomQaP/KxGi6++GJ89NFH6Nu3L4gIv//979GpUye88MILGDduHEpLS9G8eXNMnDgR1dXVGD16NBosQ+i3v/1t4McaBHmfj/W7eAHvNR2GL444C1i2LHOVUxQPNB9r40PzsaaA+FihrgBFUTJG4bgCnMJ64ACwcSNw5JHZq5ii5AF+52PNFwpDWN3hVldeCbzxBlBbC5SWZq9yipLj+J2PNV/Ie1eAJGFxRQW8/bZ8q3tAyQK53K6RbwT1X+S9sIrFquFWSuOgoqICO3bsUHFtBDAzduzYgYqKCt+3XRCugKgdBPTiVjJM165dsWHDBmzbti3bVVEgD7quXbv6vt3AhJWIugGYCKAjAAYwnpn/SEQPArgRgLmy7mXmqdY69wAYAyAE4EfMPC3desTMFaDCqmSY0tJSdO/ePdvVUAImSIu1HsCdzLyQiFoAWEBE71jzHmPmh50LE9GxAK4AcByAwwDMIKJezBxKpxIxXQEqrIqiBEBgPlZm3sTMC63fewF8BqBLjFUuBPAPZj7EzF8AWA3gpHTrIUlYVFgVRckcGWm8IqIjAPQHYAYH/wERLSGiCUTU2irrAuBLx2obEFuIEyKmj1UbtBRFCYDAhZWImgN4DcDtzLwHwJMAjgTQD8AmAI8kub2biGg+Ec1PpAFAXQGKomSaQIWViEohojqJmScDADNvYeYQMzcAeBr26341gG6O1btaZWEw83hmHsDMA9q3bx+3Dtp4pShKpglMWImIADwL4DNmftRR3tmx2MUAllq/3wRwBRGVE1F3AD0BzE27HupjVRQlwwQZFTAYwDUAPiEi0+ftXgBXElE/SAjWWgDfAwBmXkZErwD4FBJRcGu6EQGA5WNlqI9VUZSMEZiwMvP7AMhj1tQY6/wawK/9rEfMfKxqsSqKEgCF2aXVCKoKq6IoAZD3wkpghBrUx6ooSubIe2EtRgiMInDIIaxkeSjUx6ooSgAUhLACUTRULVZFUQKgYIQ11OBoR1Mfq6IoAVKYwmpQYVUUJQAKR1idIVfqY1UUJUAKSFiLIy1UtVgVRQmAwhJWY6Gqj1VRlAApTGE1pCOs+/YBNTVp1ExRlHwl78e8ihDWKVOAujqZmY6PtXlz+VarV1EUF4Vlsc6eDXzrW/ZMFUVFUQIg74W1BPUAgHqUAFu2hM9UYVUUJQDyXljDLNaQKwuhCquiKAFQWMLq9qlqHKuiKAGQ38J65JHBRQUoiqJEIb+jAhYvRvHfQ8CN6gpQFCVz5LewNmuGYmtwbRVWRVEyRX67AgAUF8u3+lgVRckUhSWsarEqipIBCktY6+vDZ6qwKooSAIUlrGqxKoqSAfJeWEus5rl6lERarOpjVRQlAPJeWNUVoChKpiksYTVZrQwqrIqiBEBhCatarIqiZIDCFlb1sSqKEgCFJazqClAUJQMUlrCqK0BRlAxQWMLqFce6YQOwenXmK6YoSt6S30lYEMcV0NAAdOsmv9V6VRTFJwrLYlVXgKIoGSDvhTVmzysVVkVRAiDvhVUtVkVRMk1hC6vGsSqKEgCFJay1teEz1WJVFCUAVFgVRVF8JjBhJaJuRDSLiD4lomVEdJtV3oaI3iGiVdZ3a6uciOhxIlpNREuI6AQ/6pGSsK5a5ceuFUUpUIK0WOsB3MnMxwIYBOBWIjoWwFgAM5m5J4CZ1jQAnA+gp/W5CcCTflQibhyrm6lTgV69gJdf9mP3iqIUIIEJKzNvYuaF1u+9AD4D0AXAhQBesBZ7AcBF1u8LAUxk4b8AKomoc7r1SNpiXbQo/FtRFCVJMuJjJaIjAPQH8DGAjsy8yZq1GUBH63cXAF86VttglaVF0sJqur2aFRVFUZIkcGElouYAXgNwOzPvcc5jZgaQVAsSEd1ERPOJaP62bdviLh+4sGoDmKIoLgIVViIqhYjqJGaebBVvMa/41vdWq7waQDfH6l2tsjCYeTwzD2DmAe3bt49bh7CeVyqsiqJkgCCjAgjAswA+Y+ZHHbPeBHCt9ftaAG84yr9rRQcMArDb4TJImZgWq1fjlelEkKiwaicDRVFcBJndajCAawB8QkSmJeheAL8D8AoRjQGwDsDl1rypAIYDWA1gP4DRflSiyHp0JO0KKEnw1IRCiS+rKEpBEJgiMPP7ACjK7KEeyzOAW4OoS3ExIxRKcAQBY4GqxaooSorkfc8rQDQyaYu1KMFTk0vCOns2cOBAtmuhKHlPYQurUxSNyCbbeJUrwrpyJXDmmcAPfpDtmihK3lMQwlpSkkBUgBHUfBXWnTvle+nS7NZDUQqAghDWsjKgDqWRMwpJWBVFyRgFIaylpYRalEXOUGFVFCUACkJYy8qAWqqInOEUxWSE1SnIKqyKorgoGGGtoyQt1o8/BsaN896gU0xVWIVt24D167NdC0VpFBREZHtpKVBL5ZEzvITVCOXTT8v3XXdFrpfLwhpUF9wOHYLdvqLkEAVksSbZeBULdQUoihKDghDWqBarl4/VPeCgF7lssVK0znCKovhFQQhrWRlQiwRdAW6L1Us4c9li1Vd1RQmcghHWlF0BXhZsLlusiqIETkEIa2kpko9jNeSbsKorQFECpyCENWrPq1hxrAZ3RixAXQGKosSkIIQ1KYvVLZT5ZrEqihI4BSGsKeUKMHhZrCqsiqLEoCCE1Xcfay67AhRFCZyCENayMqC2qEnkjFR9rEFZrPfeq41LipIHFIyw1hXlgMX629/6ty032milKBmjIIQ1LVdAvvhYc6WeipIHFISwlpUBdeyRb8YpotG6tOZLVIBarIqSMQpCWEtLgdqQR35Vp2gaYa2pCV8mG3GsQYhgrjwAFCUPKAhhLSsDamsJEXLlZbHu2RO+TDYs1lzZZr6wbZs9Jpii+EDB5GMFZKTWEniIqfO3W1iz4WMNhRIfGiZR1BUQHc0lq/hMwVisAFC7cQdw/PH2DLcrIBSKdAVkI441V7apKIonBSWsdU1bAW3a2DPcwuoWVSB7FqvfqLAqSsYoCGE1roDaWgBFjkN2uwJ2745cORs+1iCEVV9zFSVjFISwfu0KqEV4zya3sLr9q0B2ogJyZZuKonhSEMLatKl879+PcIvV7QrwEtZ8sVhVWBUlYxSEsDZrJt/79iF5Yc2GxaquAEXJaQpKWGtqkLwrIJ7FGoQIqsWqKDlNQsJKRM2IqMj63YuILiDyGkSqcdK8uXxHWKxuYT14MHLlbEQF5EpvLkUJioYG4OWXgzEyMkCiFuscABVE1AXAdADXAHg+qEr5TZjFGk1Ya2ut1i0X2YhjzWWLVQXcX2prgdtvB3bsyHZNMstzzwFXXAH85S/ZrklKJCqsxMz7AVwC4C/MfBmA44Krlr9EtVidorl/vy2spQ5j3MtiHTrU/p1rwhq08MXb/v794o7529+CrUe+8M9/An/8I/CTn2S7Jpll82b53rQpu/VIkYSFlYhOATAKwBSrzOc+l8ER1njl9LEuX27/rqmxhbWJIym2l8W6bZv9O1esy0xZkvHqbm6UBx8MvCp5gbn+vK5DpdGSqLDeDuAeAK8z8zIi6gFgVnDV8hdjsUa4At591/69bx9w6JD8rqiwy70sVie5ZrEGPUJBPAFXV0FymPNVqCNL5OhxJySszDybmS9g5oesRqztzPyjWOsQ0QQi2kpESx1lDxJRNREtsj7DHfPuIaLVRLSCiM5L+Yg8MHGsEa4AJ/v22RarU1jjWQpBWBK57ArQ6INgyFGBSZscfRAnGhXwEhG1JKJmAJYC+JSI7oqz2vMAhnmUP8bM/azPVGv7xwK4AuK3HQbgL0Tkm6uhqEje7iMsVifGFVBUBJQ4kn7Fs1hzJdyqsbgCClUgUiVHhaXQSdQVcCwz7wFwEYC3AXSHRAZEhZnnAPgqwe1fCOAfzHyImb8AsBrASQmumxDNm3v4WJ0Yi7WsLPxizoTFOmsW8MEH9nSQIVxBC1u8uqtQxObNN4FHH40s1wdSTpGosJZacasXAXiTmeuAyLzRCfIDIlpiuQpaW2VdAHzpWGaDVeYbzZrFsViNsJaXh4tDJizWs88GTjvN3226aSxRAQYVCm8uvBC48057utB9rDlKosL6VwBrATQDMIeIvgHAo5tSXJ4EcCSAfgA2AXgk2Q0Q0U1ENJ+I5m9zts7HoUULYO9exHYFHDqUvMWqroBwErVYnfWZOzdnA8EDp1CFNcePN9HGq8eZuQszD2dhHYCzkt0ZM29h5hAzNwB4GvbrfjWAbo5Fu1plXtsYz8wDmHlA+/btE953ZaWVFTARV0AyFmsQjVeJugKeew6YMiX+cs5tZtsV4J4/bx5w8snA//t/wdVJUTJMoo1XrYjoUWMpEtEjEOs1KYios2PyYkhDGAC8CeAKIionou4AegKYm+z2Y9GqFbBrF7wtVqLowtqYLdbrrwe+9a3Elm0sUQHm2IzAb9ki3/PmBVenRCESH+cjjwDTp2e7NoL6pHOSRMe8mgARwcut6WsAPAfpieUJEf0dwJkA2hHRBgAPADiTiPpB/LNrAXwPAKzY2FcAfAqgHsCtzOyrYlVWAsuWwVtYjQPW+FgPHLDnZcNizYYrgEh6lM2YEex+3McWliy3EfDII8CcOfK7MYlajr8aFxqJCuuRzPwdx/QviGhRrBWY+UqP4mdjLP9rAL9OsD5JU1kZw2ItKUk9KiBXfKyJuAJmzvRvP9FwH1t5uXybzhmNGWZg8mTgggvCuz0Hvc9CJtPHv3Il0KtX2ptJtPHqABF93WxNRIMBHIixfKPD+FgbvA65vFxmmsYrp5i6LVa3cGTTx5rKNs2F+umn4V16/d5PovPDxs3JAsncuG+8AVx6KfDb3wZXHzeF2niVDaZNA44+Gpg0Ke1NJWqxfh/ARCJqZU3vBHBt2nvPIJWVck/X1FegpXtmRYX4+vbsEWF1pg90C6fb4soVi9UtIMcd513u937cRDu2bAlrMg8xE4Wyfn0wdYlFoQprJo97qdXks3Bh2ptKNCpgMTP3BdAHQB9m7g/g7LT3nkFaWY+E3XVNI2eapCtbtkQKq9tibYzCunt3fEFrLFEB0c5fLgirIZOvp+oKyHYNUiKpEQSYeY/VAwsA7gigPoFRWSnfu0raRc50Cmt5efjNH89iDdoVsGVLeLIYLyorgfHjE9tmY4kKcE9nS1iTeYhl02rM5r6XLgXWrMne/nOQdIZmyal3kzZt5HtHaafImWeeKd/GFeAk2xbr4MHh+V+j8eabsec3lg4C7vnmWLPVeJWMxZquuH30UfwoEzeNwWKrqgKOPDLbtcgp0hHWRvCPJ04nS083wyWsjzwimcoNTmF1N2QB/lusXjeOcx+ffx6+XCgUf7gYLxrLCAKNzWJ1n5dEhCwVsfvkE+DUU4G7705+XaDwfKzZOF4fH2IxhZWI9hLRHo/PXgCH+VaLDGCEdVPI1VurRw+gdWt72imsLVsm1ni1dq1keE9FZL0sXi8RNMude26kVR1tnWTmJ8PChcB//pPaftwdBLJtsbrPf1AW4tat8v3YY8AXX8Rf3tRD0zDmJDGFlZlbMHNLj08LZk40oqBR0Lq16NHmg63DZ5SVRRfWVq0irUMvof3hD8MDy5PBS4y9xNbUI5q/NZMJpk88ETgrSo9mP32sBw8Cq1YlX79kSCZ8zm1F7d8P/OY3yT9QJ06Mv4w5L2bbhWaxZgMfz3FBDH8NyDnr1AnYtM8VbFVWJo0/5qSagHUgtsU6bpwIb329LAcAX36JpPF6rfcS1ng3bzZcAX64JKJZrD/5iTQq9upl5XsMgD17gA8/DC9LRCTNQ+rBB4H77kt+/C6v0YDdmPPitvCVnKBghBUAOncGNu12hVuVlUnPq3ZWtIAzsUvLltEbr1q3lvVCIaCLleFww4bkK5UpYQ3iFdfEczq3naqP1X18jzgSn8USouXLk28QMlxySWSuhWSsz1275DtZ/3Aibg+3sCo5RUEJa58+wIeLmmBn26PsQvPqb7q6dnMk2WraNLrFWlwsn/p6yUkI+CesXiIZrwdYOhZrstas6S1lXtOd66fqCkhmHcOXXwK9e6c+gunHHye+LyDSajTXRkkCXjHnA6dQLdZrrgH+9a/k1mkMUREpUFDCetNNwP79hCl/cPjtjLCaC9gprKWl0S1WI6yhkG2xpDJUb6oWq3uaWVIIlpXZlpSTWIKXrFXU2UpStnFj5LaDENZoVuT27fKdim8b8L5pk3EFmLonIqxOcsli9VPYXnwRuPhi/7bXiCkoYe3bV7QwrIu8EVYjCF272vNKSmJbrMYVYG6UHTuSr5SXsF53XaRVEy86oaFBnhx1dcC6dZHbjHWDJNv4YvzQJgtYMsIazccai3iv+qne/PFC3eJhzltxksOzJWOxZrvxqrFkHcsxCkpYS0slumrFCgDf+IYUmpurb1/5dgprIhZrfb198fklrABQXR253Ecf2dNuMWxosC1Irwxe0QSPOXmryDyM9u+3t+HcXiwSsVjd5ySomztZYXWLWyFYrH6d+yAaT+vrs39+olBQwgpI8poVK2AP2GaE9NVX5VW6TRs7sYCXxer0qwVlsQKRVlB9vQSYG9w3p1Mk4vlt3WkR0xVWv10BJubTEO0cpWvFpWqxmvWSsVid5yUVH2u2fI1+xRcn28CYyP9QWgp885up1ScWPrwdFJywHnecdIKpvP4S1NexHcPati0wfLj8XrdO+ug7LVZzE8WyWL/6KvkbINoF57YU3ALvvjnjDScTTXhra5N3BZh9peIKiNZBwElNTfh0UBarV12TiWNNpmHJud1ULNZsWWZ+nftkhTVRC3fWrOTrEo1M9bzKR04+Wb53746R/a1VK6BDB9tiXb5cRPbJJyUnJ2AL6969tsjV10tsZDJEu+Dcwulezn1zOi9Er5sh2nAzdXXJ37RmfS+LNdEsW+4GIK/tO+uYyLaSJdnGq2jdmZNtgEvFYk33VXrvXuB3v0v+v86WxZqppEFe+LDPnOo95QeDBtm/V64Un2tUjMU6f75M33KLPa+4WCyrt98OX+fpp2Wjl0QdtSacaBec+4J23/AHXHnG41mszgvVbbEme7OZ5f12BTQ0iH84UR9rEHkaEhFW90MhkXokYrE66+O3xTp1KnDPPcD559vtCYmQbYtVw61yg86dJeoDSKC3pLFYzYB3ToqLgZ07I8vvugv4znciy6MR7aaMJ6xuq8d548VyBbiFdckSscSTwW2xOi/+dFwBZruJWqzptpgn6wpwC2u0+nqRiLA66+MW7XSF1YTgJWuBZtvHmkkXiHZpTY+rrpK3/f/9L86CxmL1SppRXBz+R7hb4hMd3iGaReC+oE85JfZ853ZiWawNDeHzv/nN5IeeNjd7Oj7WL74Qi9/rgZCsxZqpcKtEe415kYgrwLl9v10BxkWVrAWa7PLRXGGp+vETOe5GmKimIIWVSMaDmzw5jrvLWKxr10bOKy4OFzd3vsqrr07saR+tVTOeH849PxlhTfcVOpYrIFEfa309cPbZ3haru/7xLNZUSVbi9KgAACAASURBVFdY07VYb75Z8u16LeO3K2D3bvlO1nJMxmL98EOxWKZMkWlmGVstlf2a6yTacTv/O3Ns6aKNV+lz5ZXyf8yeHWMhY7Gai8NJSUn4RefssWXwenonagHEu6Dd853TdXXSvdZL8DZtAv7yl8TqEA2nK+Ddd8N9Ksn4WOfN87ZY3UIVlI/Vi0SENV1XgHkoPvVUeBKYWMKaLYs1GWE1o/yaY3rqKQnDmTMndWGNdtzOc5VKmKMXProdClZYzzxTxhCcOjXGQiUl8sd+8QUwZEj4PHfsYkVF5PrmYm5okM+f/yy9lrZskZvzxBOj7zveBe22WI31CEgUQ7duwEMP2WXmAt22TVqH08EprEOHhltdyTZeeY2Im6zF6mcDRzJRAX43XmXCYnULaygk14k7xM2QjBAbP64ZB8nkYvj889R9rNGuJ2e9ko3EibdPjWNNnSZNJLHR88/H6OLvHDv+nHPC57mF1Sv5tLmYr7pKlv/hD2V682a5GGKNBpmssDpT6xnXxVtv2WV++qHcrgAn6UQFRHMFNDaL1b1/v3ysQQprNIt18mRg7FiJGPAiGYvVNOaa2HBnzLffrgDn9lLNbuZGLVZ/+OUv5Vx+85sS2x+Bs6viBReEz3MLa3m59Dz4/e/tMnMxv/xy+LKHDtkJRADxNbpJVli9hux2XnBBWHXukK9E9hNLWBuDKyAWfoZbOc/T/v3Af//rLaymLN0HYzSL1fyHzsQ9W7fa1kYqFqs7sVE6wpqIxeq3sPpwrxS0sPbuDfzf/8mbs2fDuPP1vm/f8OQmXhbr8cfbAxMCcjF7XRgHD4YLa+vW3svEIpbwmgvcecH5abGam92r0SCdJCzJNl6Z8mRuhL17gVtvTS15tl8WK3P49He/K1EfmzdH7itoi9VEszj/l44dgcOskZeSsVjd152pc1FR8g/BeK4Adyy2H/jlz0aBCysgI4xcdRXwzDMe/8+3vy3fJgn24Yfb87wsVsAeTQCQi9kr05RbWM0QsoBt8SZrsTox5ndQwmouwETTE27ZYh9PIhar364A57l6+GFpvHviicTWdRKt15hX541LLw0fMjpWHLJpQXW+AfjdeOVlsdbVhcc3e5FILzGDcQWY/8/UOQhXQJAWqw8ugYIXVkDi+WtqgBkzXDN69JDX+OnTI1eK5mNt3twu27MnMqEIIDdQNIu1d2/59kNYnRdfolZdIsvFEjSvUU87dQIuv1ymU3EFpNNBYMUKcaj/4x/h20rl5nGLnLF63fV9+23gtdeAO++MrKvB+f8aa9Lps3bf5LW16b2ielmsZWViLQPRhTsZy37vXvn2slj9dgVE87Hu2SMj4abSsUGF1V/OOkv08Iorwt/GAIgg9O8fuZI7VZyxWJ3W5+7d3qEgsVwB7dtHxsh6EZTFGu+iMmkGvRrrzHwnpp5vvum9/aAbr0wvEHfm+lREym2hGiswmmg6z1Esi9Ucn1PE3Df5jBnpRXNE87Eaop0PdwPlc8+Ji8xLKN3/nzNm2bl8Iv9bMo1XzmP65S/lre+FF+LvA5BwwV/+MrF9JoEKK+Ttffp0+w3OPHhj4rZYTQNUkyZykZaVycXs1Sp24ICEPRmcwtqmjYh0vFewWFl9vIQ1USGJd1GZi88MRxNtvsHth43lY03VYo11bM4GlGQx250zBzjtNPs/CYVknrECozW2xRJWrwdnLGEFonc9rq4Of1C7OXTI3l80YU3UYr3jDtmW103iFlante38D5PJ7pVs45X5jxK1WIcOBR54ILF9JoEKq8UppwATJkgu6RtuSECHnDfq2LHAiBHh8485Rp6c11wTue7Bg8CiRfa0yf8K2MIa78J4773o88yFtmOHHZ/ol8Vqbp5khdU0kmTax+r08yWLWffaa4EPPrB9pqGQnFd3RwF3fZ0j/sbL9QB4C6tzvWj/Ydeu4YNgunHGeSYjrMuXh1us8RL9RBPWurrkhTVVV0A6qCsgGK65RoaJf+UVOztgVIqL7e6ov/1tpJ/vvvuirzt/PjBtmj3dsaP9u7JSXrX8SH5x8KDEKS5f7r+wtmzpPd+9H2cniQMHUnMFpNOl1ennSxa3uJn/pL4+3BJPRVi9/l+nFeh1k6fqY3XWNZYrYPny8GV79/YWe8D7weB+MDot1mTTJibTeGV+z5xpN0ome64aGlRYg+TOO6Xxf/RoSSsYgbHUiotFHKP9CZdfLrGJXjz7rHzPmCGvcM5chsXFIqwHDqTXWNGtm2zr2mvlBlm2zHsZN/X1wN/+BvTqJTfVz34WfiOY440mrO46O2/UE0+MLawrV4oQRxOqRYskfMNZV699em0/FYvVLaym1TsUii2sxspL1MdqcGZL87rJE3k4TpgQGYmSqMXauzdw7rnh5ePHh9fJGBBe9TdRDcZCdT4onQ9Hr44lXvVxfrvxslhvv90uS/beOXRIhTVISkpE93btAv74R48FjKDU18tFFssSMlm1Dddfb//+/vfFv9O2rfhlnbRrJwHaDz8cuc0bbgifHjJEWt286ul0MXg1onXoEFkWCslTZdUqcer/6lfhuQVSdQUAwGefxfax3nwz8OtfR7dY+/cHbrwx+iu4F0ZIUrFY3Sn7THhZLGH96U/ltQcIb+BMVVjdOSC8wvcM+/cDY8ZE9hJ01jVaVIRZZu7c6NuPZbEy28JaWysPlXfftffp/E8TiTRIxccab+y1+vro1rLzbUqFNRjOOUdCsJ55xiNJy/e/L9/RLDY3I0bYwd9PPWWXu9MALl5sJ7Ho1El+//Snkdtz5yR46ing73+PXK5Fi8iYWkPz5tLg4YxgMIRCttCbEAlnw0g8Yf3Vr8Kn3f243Ret0wIFJHmMW1jdFo7TMoqHWddtsSZi0Th9hIAtrLFcAePGeec+dQurV/92p7AaQXEe+44dwBFHeKexBGxXgrOPdn19+LQRJLflav7rWA8gd0Paa6/Z007Bcgunu/EqWl4CJ6lEBcT7T7/1rUgjxnDwoAprJvjd76RNYMQI4PXXHTPuu08uHK/eUl689ZZk++nYMTz3wHHHhS/Xp48dWdCpU/TtmR4xhmivuC1ahFuszmD+AQNkO9GEtWlT+W1uAC8/W7QHy4IF4dPuqIB4F+3OnZEitHFjeHSFsb5juQLq64GlS21hMoJhlv35z2PXA5BuzL16Je8KMKQjrJddJg9Nr27DZjReN2abTn//5ZcDo0bZ00aE3D5eI6zRHpiA/ZYGAH/9q4TQGJz1dHccScVidbsCdu6UNyhzHr0sVqd1u3s38OWX4dt0tmu4yRVhJaIJRLSViJY6ytoQ0TtEtMr6bm2VExE9TkSriWgJEZ0QVL0S5aijJPSxQwdp1Hr7bes/JbKFJx3M8NtedO7sXf7aa5Lv0Ek0YW3ePLzxxHkxe8XcGqZPtzs1mJvNuW48ixUQ98Svfy2/kxXWXbsiLdF//1tcJoavvpJEM85Wejd33glUVUmDjLPe5oZMxF/53nviEnG/ljuFtVmz6AK/bp00Hjr3b/DqDuzO+3vzzd7+SK8hXIBwYV23TgTYaRU0bRrdYjViGOt/dfpYDWa4IqewukfWmDUrfH+JWKxukbv7bgmLMomFvHyszvPy4IPhPSXjkSvCCuB5AMNcZWMBzGTmngBmWtMAcD6AntbnJgBJjhUSDFVVwH/+I8bd8OHiIvAtl4lTKNyY3lsXXyw+RUBCui65JNzqBSJdA+ZVp0WL6A0VplHFy+oePdr+/fnn8u1srY4XFQCIIN1/v+RNcPvsvATN6Yv0sljdfPUV0L27NLI56+Tk3/+Wb2PdJRvf6MQt9B99JGObAfImYvbv9t9NmyY+Ja8GOS+L1TwEnHhZrNF8ncaqLyoSl0GXLuHrtWsXXVgNsYwGL8ExsbWxLNb//Ce8c0M0i3XaNOAXv5DfbovVXIPu3l1AbFeA1768wvqcwtqY41iZeQ4Ad3T8hQBMl4gXAFzkKJ/Iwn8BVBJRFLMtsxx+uLg/b75Z/K0nnxwegpoysbphmhvi2mulVZZZQroAW4TKyuRGdN885sZIRFi9LFYnZqwvr+QgsSwbw+zZkQlvvW5O58Nh5874vlN3pwuv5Y2lZ/zD6Qirl3AvWCDWqjnPN90UPWv61q2JCasXXsLqLHMKq/GlRnv6O6+JaOchWo86wNti9aqTVw4Jp58+msU6bJhYmkCksBqDwriB4jVeGZz5Ggx79oh7zmkwHDiQ0z2vOjKz8aRvBmACOLsAcDpENlhljYImTaSBfvhwMeJOPTU81anvXHGFqPmFF0bOMxd28+bA0UdHzjeWpB/CajCugVWrbL9aIsLqhddF63RZpCKsXsJnhNWIjREgpxCZsLdUadtWHnTr1okFe/753st98EHqwurlCti3T7Z3xhnhF6KxzqNtu6wsvsUaKxQqluDEcgW4iedjrauLFDlzTRi/aaIW6+rVkWV33inJ2Z2dbHLIFRATZmYASb9YE9FNRDSfiOZvc3YLDZimTWUon08/lXC/q68GfvQjectJqAusYcUK7zG0nBQVSWOWF8a6iza8tgmhiiWsRsicjVuxMOf5tttE8JNZ142XCDqFtbY2vui4M5O7t/nll7b4GgvHy2J1h8O5iRf50a6dCKuXUDhD2a67LvI1P9Fxmrxu8v375RzMmSPbNkTN2G7hFNZoFqupl/P6MlZkNMFpaAgXZC+LFbAbEOP5WHftinwtN29PGzbId6IWq9dQzKYRyznkUo4L6xbzim99m9RP1QCc0epdrbIImHk8Mw9g5gHtY3XjC4iOHaVn1oknSiz2WWfJ/ff88wn6X3v1it1wFY+WLcVCco9bZV6VjBXavLlk5/LCafVGw+nL3bFDuuc6faHl5XITOm/sRHC+Epq4Xrcwbt0aO6DfPQZZfb0IfqtW4tMdMyZyHS9hjeXnBsLPnwmzc2KE1W1Bz54tDnonK1aET3tmVk+Qffu845Kd0QJe569pU1vUoj10jbCa4VXatLHPQzRXQE1NuMUabdstW0od4lmsO3dGugKMO+qLL+T/NsdaWekdFWAwFqsxCABb+Jcutcucwmrib9Mg08L6JoBrrd/XAnjDUf5dKzpgEIDdDpdBo+PIIyXM9Msv7Sx/o0dLw6VfOXdjcvjhkY1Yjz0m3+ZhU14uT4DXXgMGDgxf1lyURlj79gXuuit8me7dw6fvvjvyxm3Z0rZUTFB8PN5+2/5tLEZzQffqJd8m1R8gN6IzTKZ168hxy+vqxGe2Z490oHDHeTZp4i2sRjyi4Rx5190jCYgurK1aRTYq1tbK//b002K5xcr1EI99+7zTUVY7bJHTTouc37atXdd4426Zc0NkP1Cj9fN2Cqvz7cNNy5bilzbjrjnTWz7+uL2cU1jNtWGuvXnz5Nr/3e9kW82aJeYK6NfPLjPXwpIldtl//mO/STZmi5WI/g7gIwBHE9EGIhoD4HcAvklEqwCcY00DwFQAawCsBvA0gFuCqpeftG4txtOcOeKyGTdOrqsLL/RvfLOEufVWubDMDVFTIyJ7ySUyqFt9PfD++zLP9N4xYV0nnCBBu4D47QBvy8QZo2putrvvlq6xp5+eeF0vuEB6YTVrJtPt2slNZgR0/Xrb4ioqsusESHIbd1/j2lpb4NeskRvk5pvt+SefbAuJU1C8BoAExA1zzjnhPeW8QnfatvVu7Kms9BaY884T4a+okPqaB0my7N/vLazO8zJihHS+cMaatm3r3fjjhVNYzX/xk594i9fevfZ2nXkv3LRsKdfNCy/IGFuvvirl774rbibDzp22uG3aJF2/d++OjO8uL5f6TJgg+0/Ux2pwWqx//nO40KZJkFEBVzJzZ2YuZeauzPwsM+9g5qHM3JOZz2Hmr6xlmZlvZeYjmbmKmecHVa8gOP100awpU4DvfU/aEw47TIyGGTP8HW4qLubicwqjuTnMjWwsm+7dpWHlz3+WgPTLLrPDZ044QS7maFaKEdYTThAh9PK5HnWUfJ94YvhgdUccIQJpIhhuuUWeUk4XiXklZQ4XqZ49I/dz6JDd+msadszoD4Dc7MlEBezZA7zzjv2wAcJ/m15zvXtHjoXWrp08aIxom0TSgO13NfOcVlQsXn453GqPZrE63QPnnScukUcftcvatJEHGHO4sHbuLANdfutbdpmXsALe6QlrauxXda/8E4aWLcUAMNb/li0SCeB+23FarGvX2smOjj8+fDmnS+BHP4q80YjktdKrESRaG4ZPaM8rHxk+XDrLfPyxdHhZskSuiQ4d5Jo1OQgC5cc/ll5FTovN0K6d+ESdYnnqqSJwnTuL66B3b0ke8/TTciNEe112m+THH2/HIBpM7zLm8PhIY+VdeKG8gpnRa4kkdjcWTstx9Ghb4N0+57POsn8bCxGILawnnyz5CEySCHc6R4Px8Z51lsSpOjnjDLGyzcPA6cc1bhpzLhINYG/SJDzmOJqwAiLW48fbPl5n9EbbtiJGv/99+HkoKpJXcWdvQFNHt7B6MWCAXHNt2sS+wJs3l56L06aJAbB6tfx2u0Wuuso7dM2ZZAUIjwx4//1IH6txh8yZE7ktd84Nn1FhDYABA+TtpLpaHqTHHitie8MNcn/cf39iCX5SoqJCBM6rTzSRZIB3io4XJ59sv6affrqI1vbtkpPg3Xeli6R7ZFki6cTgxISDtW4tFqrBNKCY13yndT15svhLTRjRkCHh23RarBMmeHdT7N/ftgorK6XxZcMGscaNG8Q4x5306CFDkhsrtFs3qXenTuE+7UmT5El59NGRr75mMEnT9bi21m4NN8JqRLJbNxGVX/5SLL5588S/7H5IONcB5A3joYcilwHkfN14Y2QD5THH2CI/dqzd+cOJM165Y0f5/OlPiSew6dRJXDzRcApfixZ2B49E6dcvfGQAZ6Pn+vV2xIDBuJCclrjB3e7gN8ycs58TTzyRc4WGBuY//IG5WTNmgPm005gfe4z5X/9i3rs327XzkVtvZb7xRjnIGTOYH32UeeNGOQEvvijlTzyR2LYWL7ZPjti9zJ99Zv9mDp8GmG++mXnDBpm3cSPzzp3M774bvsy//mXvw1n+3HORdQiFIuvgxrmNTz6Rsr17ma+4gvmDD5hvu03mzZol8wYPlunXXot+7GedZW9z4kTm7dvD9xPt89hjkdt6+23mTZuYX3/de50hQ2S5884L36dh4sTE9n3SSfbvzp0j559+ur3N1q29t3HGGZFlP/kJ8513yjU0ZYr3eqWlkWXPP8983HHey+/caf9+5JGI+QDmcxralBVB9OuTS8JqqK9nfuYZ5ubN7f+xZ0/mn/3M1oOc59NPmfv2FTFws2iR3CDJMnCg3DyhkJy0du2kfPdu+0SWl3uvu29f+I1z8KA9b9w4EV4jiLGIJ6yXXir1cxMKicCa477uOln+nXei72vAAFmmRw8R6fp6ez9TpjDffrstvhdeyPy978nvFSuib/Ott7xF5o47ZP5zz9llq1bZ6z32mF3+7W8z19XJOV2zJnw7xx5r/77lFvlevNh+oA4caG9z5kzvugwfHj69eXP49fLBB97r7doVWfZ//yfrdOok0xMmhP+Hzt9vv63Caj65KKyGvXuZV65kfvxx5vbt5Z9o2ZK5Tx/mUaOYx46Va0WxCIVEXJiZ585lXr9efjc0MPfuLWL1+efR16+rk5N8zjmp12HSJOaXXoosP/98b8GNxq5dzA8/7C3ChjfeYC4pYd6zxy4DmH/8Y3vaWMIrVsi5MecnGqGQWJ/V1baIVFYyr1sXvowbI0iDBkW+XpntPPEE8/z5zEuWiGgeOsS8fLksM3++LNOnj/e65nPaafJQdpa5WbzYnjduHPOrr8rHuR/zsKmtlfJHH5Wy2bOZr7zSfnNw7qO2VoXVfHJZWJ00NDAvXco8ciTz0KHMbdsyFxWJ2+C735W3oO9/375+lBTZtIl5/37/t+sU/Uyyd68IWSrs3i2ugUQIhZgnT/YW3WHD4j9UVq+WZc47L7z8ww/FdeQW0W3bxPr/+c8jt9XQIK98O3Z478tLkBsa5E3JTc+e4csuWsT87LPMkyenLazEzME6cQNkwIABPH9+TkVmJcyMGeLbf+MNO4IIkIbbzp1l8IGBA6WBva5O2ij69IkemqkogVBbK42R8fJOTJwouRS8ekv+6U/S2OeOsEgFIqmLV880N7W10qDmcdMQ0QJmHpByNVRYGzfMEit98KBExLz3noRxeeU6bttWGk4vu0wavYuLRYjjdTBSlLxh0yaJiEnzoldhzXNh9YJZooa++ELip1u1kofv+PESsePOcdGpk4RMHnWUhFf27y/RNx06SISRCXVkjp3NUFEKBRXWAhTWWIRCkkhp4ULpUl9fLx1Z1qyRh/mePeFvSUTiRigtFUH+9rftMRK7dJEu4McfL29MdXXSUebww+XtyZmTRVHyCRVWFdakYBaBXbJEBHfZMrF+y8slw9rKlSKsiWSgKymRPCVDhohVbES8dWt7WK1DhyRue9iw1FO4KkqmSVdY1eYoMIhE8NxjEro5eFAs37ZtpZdYaal0uX7xRXFf7d4tIrpqleT6MD0ki4u9kwM1ayauh7IyaYxr1UraMbp0kXlHHim9KLdule03aSLuipIS6Z25cKG0SXTubPudS0pi5/yIxYED0QfsVJR0UWFVPKmo8M4R0rev9/JffSXrmAx9H30kboe9e0VE33pLRHjfPhH1LVvEJbFkSXqpSXv0EFdGWZmI+sGD0rB39NEizAcPSh6OYcMkj0plpez71lsl1cBJJ0kKha5dgfnzpW4tW0qX944d5ZiI5AHQsqXUtaFB5hl/NLMIdazhotR/XVioK0DJKqGQiM6+fSKCbdqIG6KhQbqdb98uroSSEtt6Li6Wz5YtMv7YmjXiHza5Tw4dEhHbsUPEbv16/+tt0h9UV4sI79kjycPMoAPLlolvukMHqduHH0rdzjhDUgTU1ooY79wpScbWr5d8OMyyrVat5BjLy8WFsm6dnIMBA2ReXZ08tPbvl09Dg0wPHy7bLi2VdZcvl3PaqpXU9dAheQgNGiRvCsyy7ubNsp/du+VNoqIi/GFQXx/pU2eWHBgnnhiZHjhXYZZshiNHqo8129VQGjmHDtkjdBw4IDfP0UeL6BQViUW7ebNY6AcOiIgYy3bfPpm/b58IkMn8t3ix5DHp2lWs9MMOk6RggGy/slIiNjZuFEHavFlcGtu3S57nli1FANu0keiOVq3sjHzOEVSCpEULEWWi8Hwm5eV2KGirVvIQWbVKfhcVycPQ5OjZuFEeABddJOe5vl6Ox4QI7t4t0ShHHSXnaf9+OXcdOkj2yJ07xQ1kXEhEcu727ZMH7De+Id/19TI/FJL1mOUtomlTqW91tYQiduwoDw1zPmtqxP+/YIEsv3+/7LtTJ3lQd+ggbyF798p/ZA8+ocKa7WooSloYy3DXLhGf8nIRJfPZt8+2xE1SKiPeFRW2u+LgQUn2VVYmglNSIoL/1VciTGb065ISycpnrOqiInlAHDggYrtqldTl0CEpC4Ukqdj+/WIpl5dLnWprRczq60VoTaRI9+6y7saNkihtxQoZMadVK1m3c2dZd+NGKTMDRhjr2QxG0LGjPNy6dBGL+KuvvAdsSISiosRHtR4+HJg6VRuvFCWnMa/bzph2I5hunKPFeJFo/uYRIxJbzi9i+Zid80IhEVZjzTc0hGctNGJvUrEasTeDE7RsKZEthw7JeocdJq6ko4+WMmNJf/WVWOSbNonl2qSJzDv5ZHkopesPV2FVFCVwYgmVc15xcfgYl+5UsOXlsYfVAiT/sRN3fnEgfECIINBE14qiKD6jwqooiuIzKqyKoig+o8KqKIriMyqsiqIoPqPCqiiK4jMqrIqiKD6jwqooiuIzKqyKoig+o8KqKIriMyqsiqIoPqPCqiiK4jMqrIqiKD6jwqooiuIzKqyKoig+o8KqKIriMyqsiqIoPqPCqiiK4jMqrIqiKD6TlTGviGgtgL0AQgDqmXkAEbUB8DKAIwCsBXA5M+/MRv0URVHSIZsW61nM3M8xxOxYADOZuSeAmda0oihKztGYXAEXAnjB+v0CgIuyWBdFUZSUyZawMoDpRLSAiG6yyjoy8ybr92YAHbNTNUVRlPTIio8VwGnMXE1EHQC8Q0TLnTOZmYmIvVa0hPgmADj88MODr6miKEqSZMViZeZq63srgNcBnARgCxF1BgDre2uUdccz8wBmHtC+fftMVVlRFCVhMi6sRNSMiFqY3wDOBbAUwJsArrUWuxbAG5mum6Ioih9kwxXQEcDrRGT2/xIz/5uI5gF4hYjGAFgH4PIs1E1RFCVtMi6szLwGQF+P8h0Ahma6PoqiKH7TmMKtFEVR8gIVVkVRFJ9RYVUURfEZFVZFURSfUWFVFEXxGRVWRVEUn1FhVRRF8RkVVkVRFJ9RYVUURfEZFVZFURSfUWFVFEXxGRVWRVEUn1FhVRRF8RkVVkVRFJ9RYVUURfEZFVZFURSfUWFVFEXxGRVWRVEUn1FhVRRF8RkVVkVRFJ9RYVUURfEZFVZFURSfUWFVFEXxGRVWRVEUn1FhVRRF8RkVVkVRFJ9RYVUURfEZFVZFURSfUWFVFEXxGRVWRVEUn1FhVRRF8RkVVkVRFJ9RYVUURfEZFVZFURSfUWFVFEXxGRVWRVEUn2l0wkpEw4hoBRGtJqKx2a6PoihKsjQqYSWiYgBPADgfwLEAriSiY7NbK0VRlORoVMIK4CQAq5l5DTPXAvgHgAuzXCdFUZSkaGzC2gXAl47pDVaZoihKzlCS7QokCxHdBOAma/IQES3NZn0Cph2A7dmuRIDo8eUu+XxsAHB0Ois3NmGtBtDNMd3VKvsaZh4PYDwAENF8Zh6QueplFj2+3Cafjy+fjw2Q40tn/cbmCpgHoCcRdSeiMgBXAHgzy3VSFEVJikZlsTJzPRH9AMA0AMUAJjDzsixXhTzHBQAABU9JREFUS1EUJSkalbACADNPBTA1wcXHB1mXRoAeX26Tz8eXz8cGpHl8xMx+VURRFEVB4/OxKoqi5Dw5K6z50PWViCYQ0VZnyBgRtSGid4holfXd2ionInrcOt4lRHRC9moeHyLqRkSziOhTIlpGRLdZ5flyfBVENJeIFlvH9wurvDsRfWwdx8tWIyyIqNyaXm3NPyKb9U8EIiomov8R0VvWdN4cGwAQ0Voi+oSIFpkoAL+uz5wU1jzq+vo8gGGusrEAZjJzTwAzrWlAjrWn9bkJwJMZqmOq1AO4k5mPBTAIwK3Wf5Qvx3cIwNnM3BdAPwDDiGgQgIcAPMbMRwHYCWCMtfwYADut8ses5Ro7twH4zDGdT8dmOIuZ+zlCx/y5Ppk55z4ATgEwzTF9D4B7sl2vFI/lCABLHdMrAHS2fncGsML6/VcAV3otlwsfAG8A+GY+Hh+ApgAWAjgZEjRfYpV/fZ1CIl1OsX6XWMtRtuse45i6WsJyNoC3AFC+HJvjGNcCaOcq8+X6zEmLFfnd9bUjM2+yfm8G0NH6nbPHbL0a9gfwMfLo+KxX5UUAtgJ4B8DnAHYxc721iPMYvj4+a/5uAG0zW+Ok+AOAnwJosKbbIn+OzcAAphPRAqtHJ+DT9dnowq0UG2ZmIsrpsA0iag7gNQC3M/MeIvp6Xq4fHzOHAPQjokoArwM4JstV8gUi+haArcy8gIjOzHZ9AuQ0Zq4mog4A3iGi5c6Z6VyfuWqxxu36msNsIaLOAGB9b7XKc+6YiagUIqqTmHmyVZw3x2dg5l0AZkFejyuJyBgszmP4+vis+a0A7MhwVRNlMIALiGgtJMPc2QD+iPw4tq9h5mrreyvkwXgSfLo+c1VY87nr65sArrV+XwvxTZry71qtk4MA7Ha8sjQ6SEzTZwF8xsyPOmbly/G1tyxVEFETiP/4M4jAXmot5j4+c9yXAniXLWddY4OZ72Hmrsx8BOTeepeZRyEPjs1ARM2IqIX5DeBcAEvh1/WZbQdyGo7n4QBWQvxa92W7Pikew98BbAJQB/HZjIH4pmYCWAVgBoA21rIEiYT4HMAnAAZku/5xju00iA9rCYBF1md4Hh1fHwD/s45vKYCfW+U9AMwFsBrAqwDKrfIKa3q1Nb9Hto8hweM8E8Bb+XZs1rEstj7LjIb4dX1qzytFURSfyVVXgKIoSqNFhVVRFMVnVFgVRVF8RoVVURTFZ1RYFUVRfEaFVVEsiOhMk8lJUdJBhVVRFMVnVFiVnIOIrrZyoS4ior9ayVBqiOgxKzfqTCJqby3bj4j+a+XQfN2RX/MoIpph5VNdSERHWptvTkT/JKLlRDSJnMkNFCVBVFiVnIKIegMYCWAwM/cDEAIwCkAzAPOZ+TgAswE8YK0yEcDdzNwH0mPGlE8C8ARLPtVTIT3gAMnCdTskz28PSL95RUkKzW6l5BpDAZwIYJ5lTDaBJMpoAPCytcyLACYTUSsAlcw82yp/AcCrVh/xLsz8OgAw80EAsLY3l5k3WNOLIPly3w/+sJR8QoVVyTUIwAvMfE9YIdHPXMul2lf7kON3CHqPKCmgrgAl15gJ4FIrh6YZo+gbkGvZZF66CsD7zLwbwE4iOt0qvwbAbGbeC2ADEV1kbaOciJpm9CiUvEafxkpOwcyfEtH9kMzvRZDMYLcC2AfgJGveVogfFpDUb09ZwrkGwGir/BoAfyWiX1rbuCyDh6HkOZrdSskLiKiGmZtnux6KAqgrQFEUxXfUYlUURfEZtVgVRVF8RoVVURTFZ1RYFUVRfEaFVVEUxWdUWBVFUXxGhVVRFMVn/j8NgY5Wsf5obQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "ac3446d0-984e-4731-8fa4-888c470c728e",
        "id": "su2Sj5jZCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_14 (Dense)            (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_24 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_24 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_25 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_25 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,313\n",
            "Trainable params: 2,105\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "outputId": "a3749025-0728-48e3-9f14-88ba5758df21",
        "id": "kPRh6v-mCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 5s 13ms/step - loss: 12337.2490 - val_loss: 12407.8574\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 11958.1045 - val_loss: 11374.6299\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 11451.0762 - val_loss: 11000.5674\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 10753.5225 - val_loss: 10783.1201\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 9906.1650 - val_loss: 10332.3896\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 8969.0625 - val_loss: 8302.6377\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 7983.6680 - val_loss: 6623.0693\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 6978.9912 - val_loss: 5517.5747\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 5987.1973 - val_loss: 4674.7085\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 5033.0527 - val_loss: 6296.1855\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 4157.9316 - val_loss: 4414.3452\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3352.9226 - val_loss: 3197.3018\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2649.8809 - val_loss: 404.7409\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2031.0692 - val_loss: 1694.1714\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1526.2950 - val_loss: 1290.9215\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1117.4379 - val_loss: 517.7384\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 811.4794 - val_loss: 220.0716\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 595.0707 - val_loss: 586.7744\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 421.1198 - val_loss: 192.4827\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 298.6406 - val_loss: 195.7527\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 230.9391 - val_loss: 148.1762\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 188.1646 - val_loss: 207.7088\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 162.8952 - val_loss: 139.6530\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 151.5974 - val_loss: 215.6877\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 148.2371 - val_loss: 193.1377\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 141.6660 - val_loss: 163.6384\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 138.4565 - val_loss: 153.0069\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 136.2548 - val_loss: 179.7449\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 134.4489 - val_loss: 142.5617\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 132.7061 - val_loss: 151.6812\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 131.1193 - val_loss: 163.8987\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 129.7467 - val_loss: 133.0694\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 128.9742 - val_loss: 129.3162\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 127.7754 - val_loss: 153.7422\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 126.6318 - val_loss: 128.6575\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 126.1497 - val_loss: 134.0290\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 124.5231 - val_loss: 126.7372\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 124.3701 - val_loss: 135.9401\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 123.1642 - val_loss: 128.0674\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 121.9462 - val_loss: 150.1125\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 121.5722 - val_loss: 124.2124\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 121.8383 - val_loss: 137.7966\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 120.6803 - val_loss: 130.5886\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 118.8264 - val_loss: 123.5137\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 117.7602 - val_loss: 189.2522\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 116.6771 - val_loss: 182.6697\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 115.4157 - val_loss: 124.0406\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.6617 - val_loss: 122.1460\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.8972 - val_loss: 124.6000\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.4284 - val_loss: 124.1930\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.6673 - val_loss: 122.9326\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.3307 - val_loss: 118.7775\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.9532 - val_loss: 125.3802\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.9200 - val_loss: 132.4946\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.2654 - val_loss: 135.9771\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.6539 - val_loss: 119.9553\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.9269 - val_loss: 121.5443\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.6516 - val_loss: 127.2321\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.7554 - val_loss: 111.2520\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.2919 - val_loss: 119.1649\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.5750 - val_loss: 151.8527\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.0274 - val_loss: 115.3170\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.3480 - val_loss: 113.2540\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.5938 - val_loss: 112.1823\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.7139 - val_loss: 116.0885\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.1916 - val_loss: 107.7423\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.6676 - val_loss: 123.4623\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.9530 - val_loss: 112.1250\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.4926 - val_loss: 134.2787\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.5810 - val_loss: 187.7474\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.7297 - val_loss: 107.7083\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.0327 - val_loss: 108.2328\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.8944 - val_loss: 115.0865\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.9995 - val_loss: 115.2700\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.3162 - val_loss: 112.8781\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.2681 - val_loss: 134.9559\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.3657 - val_loss: 120.7754\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.7284 - val_loss: 177.4250\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.2402 - val_loss: 113.5693\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.8395 - val_loss: 121.6578\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.2486 - val_loss: 105.4481\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.0203 - val_loss: 111.6059\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.7960 - val_loss: 107.7234\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.2104 - val_loss: 104.1286\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.6673 - val_loss: 128.9681\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.7010 - val_loss: 118.6438\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.5701 - val_loss: 164.9857\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.9566 - val_loss: 103.7816\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.9326 - val_loss: 104.5489\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.9055 - val_loss: 119.1089\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.3186 - val_loss: 131.4804\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.7854 - val_loss: 105.5692\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.3025 - val_loss: 103.3429\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.9077 - val_loss: 103.2812\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.3727 - val_loss: 122.2986\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.7281 - val_loss: 113.2676\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.1580 - val_loss: 102.1751\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.9324 - val_loss: 110.0999\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.8684 - val_loss: 111.8432\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.8556 - val_loss: 101.5214\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.5814 - val_loss: 113.8501\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.1056 - val_loss: 107.9015\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.8849 - val_loss: 169.3945\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.7469 - val_loss: 112.9925\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.8066 - val_loss: 206.2298\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.8544 - val_loss: 234.3612\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.3715 - val_loss: 112.3351\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.5085 - val_loss: 113.2780\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.6216 - val_loss: 98.6060\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.7637 - val_loss: 137.2529\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.8740 - val_loss: 116.2893\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.1251 - val_loss: 139.7760\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.7892 - val_loss: 156.3232\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.9246 - val_loss: 99.5108\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.5636 - val_loss: 102.9303\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.4567 - val_loss: 103.0856\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.1760 - val_loss: 114.6563\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 87.7407 - val_loss: 106.1140\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.6157 - val_loss: 99.7391\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.3116 - val_loss: 141.4193\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.1903 - val_loss: 117.8397\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.4848 - val_loss: 99.5056\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.7987 - val_loss: 132.4015\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.1085 - val_loss: 120.7284\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.4542 - val_loss: 148.4618\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.8618 - val_loss: 108.3628\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.8335 - val_loss: 105.6297\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.5724 - val_loss: 98.3159\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.2886 - val_loss: 189.6385\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.3282 - val_loss: 107.4086\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.9607 - val_loss: 114.2294\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.8499 - val_loss: 94.4527\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.7851 - val_loss: 121.8840\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.5927 - val_loss: 155.9623\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.2566 - val_loss: 98.1502\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.2799 - val_loss: 105.6738\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.3485 - val_loss: 96.7171\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.4100 - val_loss: 102.5660\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.0278 - val_loss: 96.0662\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.1502 - val_loss: 99.0832\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.0607 - val_loss: 127.1992\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.8393 - val_loss: 147.3544\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.7747 - val_loss: 105.4850\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.6052 - val_loss: 196.3806\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.4030 - val_loss: 121.9153\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.8183 - val_loss: 100.1493\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.1205 - val_loss: 126.2968\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.6401 - val_loss: 100.6875\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.2990 - val_loss: 108.2895\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.0508 - val_loss: 117.1693\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.1425 - val_loss: 106.1796\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.7177 - val_loss: 99.2740\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.7671 - val_loss: 93.8956\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.5942 - val_loss: 95.6111\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.5353 - val_loss: 105.5845\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.6111 - val_loss: 92.7726\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.2034 - val_loss: 101.6090\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.0549 - val_loss: 112.4488\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.6060 - val_loss: 117.3489\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.9326 - val_loss: 103.8984\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.6353 - val_loss: 114.5204\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.0767 - val_loss: 104.0173\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.9581 - val_loss: 98.6051\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.3579 - val_loss: 94.8477\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.1932 - val_loss: 121.4995\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.9386 - val_loss: 109.6553\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.5219 - val_loss: 99.4155\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.3163 - val_loss: 130.7735\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.5744 - val_loss: 106.2535\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.7529 - val_loss: 96.0216\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.0196 - val_loss: 111.9168\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.1743 - val_loss: 103.8613\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.8449 - val_loss: 111.1455\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 82.3937 - val_loss: 101.7301\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 82.4124 - val_loss: 175.7645\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.2134 - val_loss: 191.9839\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.0522 - val_loss: 105.2241\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.9949 - val_loss: 112.0923\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.0361 - val_loss: 108.0573\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.0946 - val_loss: 110.3723\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.7075 - val_loss: 100.5419\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.9653 - val_loss: 120.4308\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.8764 - val_loss: 91.6067\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.6777 - val_loss: 99.0326\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.6740 - val_loss: 115.3220\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.8637 - val_loss: 135.3681\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.6954 - val_loss: 94.7512\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.3718 - val_loss: 100.1969\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.6438 - val_loss: 91.6747\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.6428 - val_loss: 94.6893\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.5707 - val_loss: 92.8726\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.5398 - val_loss: 102.4943\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.4697 - val_loss: 123.6181\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.5918 - val_loss: 98.3544\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.3834 - val_loss: 111.9679\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.2808 - val_loss: 94.4527\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.4789 - val_loss: 97.5859\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.2314 - val_loss: 91.4570\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.3568 - val_loss: 105.6901\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.0456 - val_loss: 97.5627\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.9846 - val_loss: 111.9896\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.1306 - val_loss: 116.3255\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.9597 - val_loss: 96.5677\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.1421 - val_loss: 126.1222\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.1495 - val_loss: 92.9560\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.1855 - val_loss: 110.4394\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.1273 - val_loss: 96.5500\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.5111 - val_loss: 97.2348\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0510 - val_loss: 143.0851\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.9049 - val_loss: 96.5949\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.6683 - val_loss: 102.0186\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.9781 - val_loss: 110.6609\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.9006 - val_loss: 98.4995\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.5186 - val_loss: 102.6893\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.3621 - val_loss: 97.6736\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.9828 - val_loss: 98.0083\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.9464 - val_loss: 102.9250\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.7987 - val_loss: 96.8595\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.7046 - val_loss: 116.1152\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.8994 - val_loss: 137.8965\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.5835 - val_loss: 91.8224\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.4590 - val_loss: 109.0145\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.3643 - val_loss: 145.3157\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.5002 - val_loss: 97.7711\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2487 - val_loss: 90.3510\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2064 - val_loss: 132.0784\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.4306 - val_loss: 98.2841\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1301 - val_loss: 99.2729\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.3517 - val_loss: 104.8268\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.2033 - val_loss: 121.4087\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2865 - val_loss: 122.4803\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2200 - val_loss: 102.1908\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.4194 - val_loss: 98.4156\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.5634 - val_loss: 107.8980\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1252 - val_loss: 108.3758\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1746 - val_loss: 98.2456\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.1000 - val_loss: 121.9295\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.0109 - val_loss: 102.1944\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2140 - val_loss: 116.8829\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8203 - val_loss: 94.2970\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8650 - val_loss: 120.2056\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.0041 - val_loss: 96.3907\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2249 - val_loss: 118.4017\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.1093 - val_loss: 111.9367\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.4323 - val_loss: 119.3780\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8614 - val_loss: 98.5723\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.6902 - val_loss: 104.4298\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.5964 - val_loss: 97.0707\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5391 - val_loss: 96.8092\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5918 - val_loss: 108.9420\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.5775 - val_loss: 106.7404\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.4993 - val_loss: 106.1367\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5779 - val_loss: 96.5716\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3836 - val_loss: 96.0428\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.4582 - val_loss: 118.6413\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7522 - val_loss: 134.6885\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7726 - val_loss: 103.8846\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1639 - val_loss: 118.5926\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5068 - val_loss: 92.8388\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.4011 - val_loss: 102.3351\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2931 - val_loss: 93.8980\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3003 - val_loss: 101.1640\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3121 - val_loss: 104.5464\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5456 - val_loss: 99.8762\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2696 - val_loss: 89.5336\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1102 - val_loss: 97.8244\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.4218 - val_loss: 99.9376\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1009 - val_loss: 96.1273\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.2335 - val_loss: 116.4356\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2496 - val_loss: 90.8196\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3065 - val_loss: 118.1391\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2846 - val_loss: 146.3216\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.9402 - val_loss: 97.3927\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.4735 - val_loss: 92.0196\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.0337 - val_loss: 90.6870\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.0188 - val_loss: 97.3443\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.4093 - val_loss: 150.5469\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1097 - val_loss: 117.9864\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0251 - val_loss: 92.8506\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0467 - val_loss: 92.2665\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9184 - val_loss: 97.0849\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1683 - val_loss: 110.9054\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1735 - val_loss: 110.3037\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1224 - val_loss: 108.1045\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7327 - val_loss: 108.0923\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9948 - val_loss: 102.5959\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9173 - val_loss: 108.9202\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7488 - val_loss: 104.2789\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7236 - val_loss: 93.8485\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.6234 - val_loss: 97.9275\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5767 - val_loss: 91.8796\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8208 - val_loss: 240.5539\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7586 - val_loss: 91.2365\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.6276 - val_loss: 134.4865\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7283 - val_loss: 102.0915\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.7301 - val_loss: 92.9653\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7074 - val_loss: 92.7813\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 78.5657 - val_loss: 95.2170\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 78.4934 - val_loss: 95.8623\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4400 - val_loss: 93.7796\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3825 - val_loss: 101.9704\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4879 - val_loss: 91.7166\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4962 - val_loss: 95.0340\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.1483 - val_loss: 92.9848\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4279 - val_loss: 104.2421\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4456 - val_loss: 91.3912\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.9354 - val_loss: 93.2925\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3834 - val_loss: 93.0000\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.6364 - val_loss: 102.3784\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1921 - val_loss: 92.7471\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1087 - val_loss: 92.0750\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1626 - val_loss: 94.0846\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0503 - val_loss: 166.5551\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3344 - val_loss: 122.3836\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9862 - val_loss: 92.7919\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0893 - val_loss: 111.2282\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1291 - val_loss: 91.9256\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1042 - val_loss: 97.7206\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.0683 - val_loss: 115.1348\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0756 - val_loss: 208.3201\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.2698 - val_loss: 90.7710\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3364 - val_loss: 101.3707\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9559 - val_loss: 90.1521\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1624 - val_loss: 102.1195\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1208 - val_loss: 91.3101\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8875 - val_loss: 97.4914\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8669 - val_loss: 96.9722\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9939 - val_loss: 100.5915\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9745 - val_loss: 93.4300\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8044 - val_loss: 115.7483\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8702 - val_loss: 127.6126\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0677 - val_loss: 96.0380\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6580 - val_loss: 116.3809\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8889 - val_loss: 104.5598\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9825 - val_loss: 123.0101\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.7858 - val_loss: 98.6422\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8296 - val_loss: 109.1916\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8181 - val_loss: 102.8225\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.7488 - val_loss: 116.3426\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9255 - val_loss: 107.7449\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8450 - val_loss: 103.2394\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6159 - val_loss: 95.1224\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.5343 - val_loss: 98.4163\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.5911 - val_loss: 111.9162\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6441 - val_loss: 110.1440\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6210 - val_loss: 259.6387\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6932 - val_loss: 122.9492\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.5198 - val_loss: 107.2310\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.5090 - val_loss: 97.8682\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.5738 - val_loss: 98.4163\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4582 - val_loss: 95.7938\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4787 - val_loss: 92.2485\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4206 - val_loss: 98.3867\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.5302 - val_loss: 88.9610\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4452 - val_loss: 93.7525\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.5701 - val_loss: 92.3153\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2777 - val_loss: 97.3694\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4622 - val_loss: 93.4588\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4755 - val_loss: 94.9788\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3141 - val_loss: 126.8464\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4111 - val_loss: 97.8192\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3593 - val_loss: 94.1471\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3562 - val_loss: 94.8822\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3334 - val_loss: 96.1675\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4755 - val_loss: 98.1637\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.5371 - val_loss: 108.7604\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2682 - val_loss: 115.6499\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4940 - val_loss: 96.0128\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2390 - val_loss: 95.5584\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4516 - val_loss: 96.1080\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3481 - val_loss: 96.8423\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4894 - val_loss: 143.7052\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.1924 - val_loss: 93.1117\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2444 - val_loss: 93.2899\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2253 - val_loss: 92.2129\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2484 - val_loss: 103.5933\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2255 - val_loss: 92.2763\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9990 - val_loss: 104.0386\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.1454 - val_loss: 93.2654\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3901 - val_loss: 99.3761\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.1549 - val_loss: 96.6251\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3206 - val_loss: 92.9762\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.0129 - val_loss: 93.5772\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2448 - val_loss: 107.0388\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2134 - val_loss: 108.8278\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2034 - val_loss: 92.2013\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.0772 - val_loss: 95.4482\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.1281 - val_loss: 111.8469\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3774 - val_loss: 94.1082\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.1245 - val_loss: 98.3918\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2104 - val_loss: 107.9759\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.0556 - val_loss: 130.7195\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.0829 - val_loss: 120.1239\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.1622 - val_loss: 91.4576\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2769 - val_loss: 95.3257\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.1603 - val_loss: 91.2277\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.0335 - val_loss: 90.9684\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3101 - val_loss: 95.6885\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9319 - val_loss: 93.9310\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9884 - val_loss: 158.5289\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.0328 - val_loss: 107.5774\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2365 - val_loss: 93.6888\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.0158 - val_loss: 174.8186\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.0645 - val_loss: 95.9365\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.0411 - val_loss: 92.3939\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.8584 - val_loss: 93.2107\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9889 - val_loss: 97.9806\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9273 - val_loss: 104.3097\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9655 - val_loss: 101.8628\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9646 - val_loss: 95.5100\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.0488 - val_loss: 103.8516\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.8557 - val_loss: 95.2607\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9168 - val_loss: 107.1729\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9425 - val_loss: 124.7758\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.0084 - val_loss: 91.7666\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9140 - val_loss: 92.4968\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.8873 - val_loss: 92.2196\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.8314 - val_loss: 94.2550\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.7667 - val_loss: 108.2333\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.7623 - val_loss: 95.2174\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.7566 - val_loss: 95.7472\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9224 - val_loss: 90.1648\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.8006 - val_loss: 179.7567\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3782 - val_loss: 128.3949\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.8968 - val_loss: 128.1922\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9396 - val_loss: 93.7233\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9046 - val_loss: 93.7488\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.8427 - val_loss: 140.6491\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.7355 - val_loss: 94.5126\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.7551 - val_loss: 105.5465\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.7999 - val_loss: 91.0499\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 76.9327 - val_loss: 100.5316\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 76.6559 - val_loss: 91.0722\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.9047 - val_loss: 120.8875\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.6939 - val_loss: 97.6380\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.7571 - val_loss: 101.1535\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.6829 - val_loss: 96.4444\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.8142 - val_loss: 137.0999\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.7767 - val_loss: 93.7245\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.8857 - val_loss: 100.5518\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 76.5473 - val_loss: 110.7633\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.7597 - val_loss: 91.9823\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.5646 - val_loss: 93.5055\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.7484 - val_loss: 104.6504\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.7502 - val_loss: 90.0867\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.5991 - val_loss: 97.1064\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.6369 - val_loss: 96.4668\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 76.4516 - val_loss: 93.8649\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.6992 - val_loss: 94.9082\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.5420 - val_loss: 93.4616\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.6443 - val_loss: 103.8642\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.7192 - val_loss: 97.2870\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.6314 - val_loss: 89.9291\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.5927 - val_loss: 100.1230\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.5877 - val_loss: 93.4486\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.6446 - val_loss: 91.8689\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.3430 - val_loss: 96.6141\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.6502 - val_loss: 111.4377\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.5965 - val_loss: 110.0932\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.8553 - val_loss: 88.1491\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.5363 - val_loss: 104.1872\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.4861 - val_loss: 91.5570\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.3933 - val_loss: 97.1858\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.4888 - val_loss: 97.5873\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.6159 - val_loss: 91.3699\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.6766 - val_loss: 95.3990\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.3625 - val_loss: 93.6133\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.6692 - val_loss: 93.7786\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.6934 - val_loss: 97.5464\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.5876 - val_loss: 96.9510\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.4946 - val_loss: 93.9340\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.5110 - val_loss: 96.2310\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.2282 - val_loss: 100.1959\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.3087 - val_loss: 102.5955\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.2211 - val_loss: 90.8732\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.2779 - val_loss: 91.9541\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.1448 - val_loss: 92.1523\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 76.2327 - val_loss: 100.0548\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.3862 - val_loss: 97.0745\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 76.3039 - val_loss: 119.3171\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 76.3003 - val_loss: 97.6345\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.2335 - val_loss: 90.7899\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 76.1326 - val_loss: 92.8292\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.2805 - val_loss: 111.1441\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 76.1911 - val_loss: 122.3146\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.2756 - val_loss: 110.8762\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.3423 - val_loss: 116.2766\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.1264 - val_loss: 90.8829\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 76.0421 - val_loss: 91.5007\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.2852 - val_loss: 129.1653\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.2119 - val_loss: 90.4241\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.2049 - val_loss: 88.8548\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.1933 - val_loss: 110.1843\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.0240 - val_loss: 91.2187\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.0928 - val_loss: 91.8260\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.0619 - val_loss: 105.3399\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.0461 - val_loss: 125.8905\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.2408 - val_loss: 139.3903\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 76.3444 - val_loss: 111.6420\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.0451 - val_loss: 87.7699\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95778aa-9762-4a9f-c764-d0af999aa079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.1504600345950664 \n",
            "MAE:  6.940838377497252 \n",
            "SD:  9.367351472156525\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "98f4a938-a3a4-4a95-8873-0d54f3cbc795"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c76c2b42-f834-4753-b00b-0c18aa6d4242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_28 (Dense)            (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_26 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_26 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_27 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_27 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_28 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_28 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_29 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_29 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_30 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_31 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_32 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_33 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_33 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_34 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_35 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_36 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_37 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_38 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_38 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,313\n",
            "Trainable params: 2,105\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dcXAOqd2CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517160cc-113e-40c2-8f7f-b15a9d9e8780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 5s 12ms/step - loss: 12423.3916 - val_loss: 12318.6943\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 12059.1221 - val_loss: 11900.1270\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 11572.3457 - val_loss: 11269.3555\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 10923.2910 - val_loss: 10261.7080\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 10086.8008 - val_loss: 10031.5752\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 9099.5010 - val_loss: 7730.4854\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 8070.5933 - val_loss: 6731.5205\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 7030.0410 - val_loss: 7708.7847\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 6009.9326 - val_loss: 6510.6211\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 5046.4951 - val_loss: 3672.5188\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 4147.9199 - val_loss: 3077.2571\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 3326.7644 - val_loss: 3466.8750\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2609.5999 - val_loss: 2227.8464\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1994.0077 - val_loss: 911.1060\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1488.5833 - val_loss: 692.5880\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1090.1455 - val_loss: 1236.1785\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 775.6420 - val_loss: 752.7619\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 543.8810 - val_loss: 369.6521\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 384.3145 - val_loss: 464.1165\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 279.0434 - val_loss: 231.5119\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 211.6958 - val_loss: 277.4400\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 170.3634 - val_loss: 690.2411\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 150.8486 - val_loss: 145.1421\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 136.9102 - val_loss: 143.3056\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 131.3620 - val_loss: 216.5204\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 131.5277 - val_loss: 158.9410\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 129.8460 - val_loss: 203.0746\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 129.1516 - val_loss: 139.6825\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 128.0321 - val_loss: 166.2428\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 128.0611 - val_loss: 156.0919\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 128.3894 - val_loss: 748.8353\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 127.4018 - val_loss: 123.0829\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 124.2121 - val_loss: 142.6433\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 124.1457 - val_loss: 157.1774\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 123.7007 - val_loss: 127.8099\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 121.3986 - val_loss: 120.9703\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 120.5609 - val_loss: 121.0584\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 119.6734 - val_loss: 266.6704\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 119.0084 - val_loss: 126.4121\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 121.5436 - val_loss: 141.0885\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 118.6276 - val_loss: 119.1216\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 117.7377 - val_loss: 137.0737\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 116.6728 - val_loss: 115.4732\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 117.8341 - val_loss: 155.1154\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 117.4796 - val_loss: 117.2628\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 114.2332 - val_loss: 113.9751\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.5187 - val_loss: 114.1642\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.9179 - val_loss: 133.2175\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.0358 - val_loss: 121.6912\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.4685 - val_loss: 148.1868\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.9160 - val_loss: 120.7829\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.3596 - val_loss: 112.8792\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.2341 - val_loss: 116.8153\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.8188 - val_loss: 134.5894\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.7057 - val_loss: 171.1794\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.9615 - val_loss: 115.7014\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.4607 - val_loss: 122.4056\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.4902 - val_loss: 999.4108\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.3691 - val_loss: 119.1646\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.5530 - val_loss: 211.8770\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.2217 - val_loss: 176.2364\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.8593 - val_loss: 224.7751\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.6012 - val_loss: 108.7453\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.1585 - val_loss: 107.6092\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.1914 - val_loss: 124.3930\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.9958 - val_loss: 107.7123\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.4021 - val_loss: 125.4320\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.2772 - val_loss: 130.1120\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.5535 - val_loss: 114.9539\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.7527 - val_loss: 132.2034\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.6146 - val_loss: 122.5507\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.7453 - val_loss: 107.3970\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.5510 - val_loss: 126.1352\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.0625 - val_loss: 131.5818\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.2506 - val_loss: 125.1186\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.8606 - val_loss: 129.3678\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.7815 - val_loss: 143.3924\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.3070 - val_loss: 122.2565\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.5758 - val_loss: 193.5396\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.2684 - val_loss: 112.2831\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.9249 - val_loss: 108.9752\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.3523 - val_loss: 115.9182\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.1138 - val_loss: 109.6981\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.6260 - val_loss: 128.5382\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.8148 - val_loss: 109.3688\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.0802 - val_loss: 102.9180\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.0333 - val_loss: 109.8945\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.9137 - val_loss: 131.6997\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.5126 - val_loss: 110.3783\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.9776 - val_loss: 118.8652\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.5559 - val_loss: 114.1746\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.5187 - val_loss: 109.7505\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.9532 - val_loss: 104.8410\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.8737 - val_loss: 108.7127\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.9481 - val_loss: 126.3798\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.5750 - val_loss: 110.6317\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.3130 - val_loss: 110.2401\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.9142 - val_loss: 111.4579\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.0286 - val_loss: 136.1776\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.3311 - val_loss: 122.4950\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.8184 - val_loss: 227.6351\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.5938 - val_loss: 114.5445\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.4268 - val_loss: 143.9464\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.6130 - val_loss: 112.6877\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.1006 - val_loss: 133.8689\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.8596 - val_loss: 114.1310\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.9594 - val_loss: 126.3910\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.9689 - val_loss: 118.9282\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.4576 - val_loss: 99.8777\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.4452 - val_loss: 128.3685\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.9437 - val_loss: 106.8754\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.9957 - val_loss: 104.0102\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.1026 - val_loss: 103.8242\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.8274 - val_loss: 115.3731\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.5023 - val_loss: 112.7964\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.2356 - val_loss: 126.8145\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.5067 - val_loss: 105.2247\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.0093 - val_loss: 110.3205\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.7910 - val_loss: 108.6577\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.3194 - val_loss: 106.7474\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.0010 - val_loss: 104.9544\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.7906 - val_loss: 118.2782\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.5672 - val_loss: 130.2432\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.4415 - val_loss: 103.9631\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.7865 - val_loss: 111.4985\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.6151 - val_loss: 115.1741\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.4305 - val_loss: 107.2555\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.1928 - val_loss: 122.1810\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.8594 - val_loss: 121.3379\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.4020 - val_loss: 100.5591\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.0445 - val_loss: 120.4174\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.9696 - val_loss: 121.2427\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.3933 - val_loss: 103.6111\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.4687 - val_loss: 100.6036\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.3561 - val_loss: 157.4231\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.4465 - val_loss: 120.0473\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.5793 - val_loss: 96.4990\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.1881 - val_loss: 103.1433\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.4066 - val_loss: 126.4346\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.0217 - val_loss: 126.4974\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.1207 - val_loss: 105.0545\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.4709 - val_loss: 144.4467\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.1077 - val_loss: 102.2150\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.6518 - val_loss: 118.8658\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.5626 - val_loss: 104.5395\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.4340 - val_loss: 105.6883\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.2833 - val_loss: 102.0013\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.2237 - val_loss: 106.8198\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.4533 - val_loss: 109.7813\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.3613 - val_loss: 111.8032\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.1410 - val_loss: 102.9314\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.1728 - val_loss: 102.2285\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.5188 - val_loss: 98.2222\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.5905 - val_loss: 117.5185\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.2230 - val_loss: 133.4245\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.8350 - val_loss: 130.3435\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.8661 - val_loss: 103.4046\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.5701 - val_loss: 97.6460\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.5339 - val_loss: 100.8241\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.9433 - val_loss: 147.4933\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 88.5746 - val_loss: 176.5314\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 88.4148 - val_loss: 96.0324\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.2608 - val_loss: 107.0841\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.0597 - val_loss: 102.7755\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.0973 - val_loss: 92.3558\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.9245 - val_loss: 95.3630\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.9930 - val_loss: 106.3159\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.8147 - val_loss: 103.1044\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.8162 - val_loss: 102.0580\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.5810 - val_loss: 100.1053\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.4774 - val_loss: 102.4692\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.5414 - val_loss: 122.7550\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.4079 - val_loss: 109.7169\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.5814 - val_loss: 126.5207\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.6964 - val_loss: 109.5649\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.7423 - val_loss: 121.4762\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.3254 - val_loss: 105.8487\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.2477 - val_loss: 97.4392\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.4722 - val_loss: 113.8562\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.1025 - val_loss: 98.2959\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.1282 - val_loss: 131.9162\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.9907 - val_loss: 105.3206\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.5209 - val_loss: 104.2324\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.5133 - val_loss: 109.9920\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.3520 - val_loss: 96.3984\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.5095 - val_loss: 104.4347\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.2402 - val_loss: 108.8590\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.1837 - val_loss: 104.5185\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.0467 - val_loss: 99.3859\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.1380 - val_loss: 97.2241\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.1461 - val_loss: 104.4868\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.2255 - val_loss: 112.8341\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.9617 - val_loss: 108.7176\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.8952 - val_loss: 94.6184\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.9798 - val_loss: 109.3271\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.8552 - val_loss: 108.8625\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.6670 - val_loss: 124.7648\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.7210 - val_loss: 197.5542\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.4509 - val_loss: 96.3889\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.5891 - val_loss: 97.1074\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.4371 - val_loss: 104.6911\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.4281 - val_loss: 147.5089\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.2980 - val_loss: 105.9113\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.2474 - val_loss: 97.9571\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.2143 - val_loss: 131.4954\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.9327 - val_loss: 148.8306\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.1082 - val_loss: 118.2811\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.2540 - val_loss: 93.3808\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.2430 - val_loss: 97.1756\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.7466 - val_loss: 94.7026\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.4322 - val_loss: 98.9764\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.7228 - val_loss: 108.9948\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.7008 - val_loss: 139.9593\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.6931 - val_loss: 114.5533\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.8220 - val_loss: 109.5588\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.3538 - val_loss: 102.0197\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.2471 - val_loss: 92.6106\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.3290 - val_loss: 94.6230\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.1325 - val_loss: 101.1403\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.0284 - val_loss: 97.0637\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.8921 - val_loss: 97.1955\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.0898 - val_loss: 101.9594\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.0177 - val_loss: 107.5890\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.6334 - val_loss: 131.7212\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.1425 - val_loss: 98.8255\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.8669 - val_loss: 121.6291\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.6018 - val_loss: 120.6358\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.4549 - val_loss: 103.7414\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.6095 - val_loss: 101.1709\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.7645 - val_loss: 207.4017\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.5730 - val_loss: 132.6721\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.2871 - val_loss: 129.4433\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.1938 - val_loss: 95.3740\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.2393 - val_loss: 99.0417\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.9890 - val_loss: 98.1933\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.8459 - val_loss: 113.4661\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.8109 - val_loss: 105.2766\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.8539 - val_loss: 96.8458\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.5355 - val_loss: 111.7011\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.8368 - val_loss: 111.5029\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.8252 - val_loss: 108.4365\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.4178 - val_loss: 122.5492\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.7651 - val_loss: 105.2882\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.6800 - val_loss: 101.8812\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.3446 - val_loss: 94.8662\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.2488 - val_loss: 98.6736\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.2520 - val_loss: 112.7748\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.1274 - val_loss: 99.4442\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.2744 - val_loss: 116.2943\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.6004 - val_loss: 106.7661\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.3029 - val_loss: 104.7445\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.2980 - val_loss: 105.3163\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.0448 - val_loss: 95.2729\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.9031 - val_loss: 171.4246\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.8493 - val_loss: 92.1129\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.0825 - val_loss: 98.1479\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.2267 - val_loss: 106.6839\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.1116 - val_loss: 102.7917\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.4175 - val_loss: 97.1299\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.8315 - val_loss: 102.4545\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.8377 - val_loss: 95.7434\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.8885 - val_loss: 121.6429\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.8826 - val_loss: 117.0755\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.7247 - val_loss: 102.1619\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.2222 - val_loss: 94.8233\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.4600 - val_loss: 94.6536\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.5582 - val_loss: 99.2353\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.4928 - val_loss: 101.0385\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.3586 - val_loss: 125.7340\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.7439 - val_loss: 94.9471\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.2621 - val_loss: 91.6178\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.5769 - val_loss: 99.1661\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.2443 - val_loss: 129.2023\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.1829 - val_loss: 89.8886\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.5296 - val_loss: 100.5950\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.1822 - val_loss: 93.5478\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.3506 - val_loss: 105.9807\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.2006 - val_loss: 91.1039\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.9539 - val_loss: 101.0582\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.9588 - val_loss: 94.2482\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0248 - val_loss: 97.3947\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0974 - val_loss: 106.5619\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.5315 - val_loss: 96.9487\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.1439 - val_loss: 98.0804\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.1459 - val_loss: 97.3557\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0812 - val_loss: 96.4228\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.8826 - val_loss: 102.3232\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0761 - val_loss: 101.9616\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.1401 - val_loss: 157.4405\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.8536 - val_loss: 91.0631\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 81.2445 - val_loss: 96.8327\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 81.1863 - val_loss: 99.7085\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0902 - val_loss: 124.4894\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0235 - val_loss: 96.4966\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.3806 - val_loss: 130.6549\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.5805 - val_loss: 123.8894\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.8298 - val_loss: 114.0457\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.8494 - val_loss: 116.5769\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.6654 - val_loss: 104.8939\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.7858 - val_loss: 92.2097\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.6443 - val_loss: 102.2210\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.6540 - val_loss: 118.4226\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.1819 - val_loss: 97.9226\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.3776 - val_loss: 107.9008\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.7047 - val_loss: 156.5370\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.9713 - val_loss: 98.7715\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.6639 - val_loss: 92.5522\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.7692 - val_loss: 152.0100\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.3201 - val_loss: 92.5135\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.9718 - val_loss: 108.1818\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.0331 - val_loss: 96.3016\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.8856 - val_loss: 95.6789\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.5527 - val_loss: 247.3763\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.7424 - val_loss: 126.5396\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.6421 - val_loss: 93.0652\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.8675 - val_loss: 103.2220\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.5903 - val_loss: 100.9403\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1437 - val_loss: 120.2018\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1971 - val_loss: 91.0047\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2355 - val_loss: 91.3622\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2820 - val_loss: 113.4491\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.6989 - val_loss: 91.7485\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.0680 - val_loss: 110.1766\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1106 - val_loss: 98.4445\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2216 - val_loss: 113.7151\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.0740 - val_loss: 101.7086\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.9364 - val_loss: 96.2870\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.0505 - val_loss: 104.3330\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.9315 - val_loss: 126.4577\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1491 - val_loss: 96.2502\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.6149 - val_loss: 96.1678\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.6497 - val_loss: 89.5251\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.9241 - val_loss: 90.3841\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7218 - val_loss: 131.4807\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7427 - val_loss: 89.8834\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.6973 - val_loss: 95.4552\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7172 - val_loss: 128.0169\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 80.0649 - val_loss: 90.9956\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.9183 - val_loss: 120.5088\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.1563 - val_loss: 92.8004\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.6886 - val_loss: 98.2248\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7912 - val_loss: 93.8464\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.9099 - val_loss: 102.0774\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.0084 - val_loss: 109.4337\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.3001 - val_loss: 95.9858\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.4971 - val_loss: 95.0691\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.8752 - val_loss: 135.5474\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.7545 - val_loss: 97.2515\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 79.6873 - val_loss: 98.2391\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 79.3755 - val_loss: 95.9176\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 79.6130 - val_loss: 90.4832\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 79.4009 - val_loss: 107.4442\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.6971 - val_loss: 91.6029\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 79.4045 - val_loss: 93.1613\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 79.1693 - val_loss: 92.4852\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 79.4852 - val_loss: 103.6997\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2675 - val_loss: 123.1451\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2332 - val_loss: 124.8231\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.5139 - val_loss: 106.1259\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2095 - val_loss: 90.4518\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1421 - val_loss: 101.5926\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1679 - val_loss: 99.7449\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3221 - val_loss: 95.8916\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3216 - val_loss: 106.4352\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2659 - val_loss: 109.9098\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0855 - val_loss: 92.1640\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9736 - val_loss: 96.4338\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2950 - val_loss: 94.5057\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1866 - val_loss: 89.1598\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8286 - val_loss: 91.4213\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9639 - val_loss: 105.7731\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.0930 - val_loss: 88.2575\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9766 - val_loss: 133.5345\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9999 - val_loss: 169.9288\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 79.2519 - val_loss: 97.0297\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 79.3925 - val_loss: 160.2877\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.9837 - val_loss: 97.6688\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.9390 - val_loss: 89.0869\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8967 - val_loss: 100.6514\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2478 - val_loss: 99.1739\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.1280 - val_loss: 95.7004\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7527 - val_loss: 126.0026\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8479 - val_loss: 91.8140\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.6253 - val_loss: 101.8953\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8307 - val_loss: 107.9103\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8535 - val_loss: 137.1088\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7479 - val_loss: 93.2592\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.6563 - val_loss: 106.6302\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.6827 - val_loss: 96.1040\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7214 - val_loss: 97.8088\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.8707 - val_loss: 106.9524\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.8494 - val_loss: 99.8658\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 79.0488 - val_loss: 96.4273\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.8931 - val_loss: 90.5363\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.9351 - val_loss: 92.0572\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.8863 - val_loss: 90.6851\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8666 - val_loss: 94.0170\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5947 - val_loss: 96.9000\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7792 - val_loss: 93.3218\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 79.0511 - val_loss: 95.6242\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.8917 - val_loss: 104.0624\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.4602 - val_loss: 155.3540\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.9230 - val_loss: 157.1946\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.7773 - val_loss: 92.4951\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5043 - val_loss: 108.0871\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.5481 - val_loss: 123.0073\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.3371 - val_loss: 101.4628\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.4911 - val_loss: 98.7970\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.3298 - val_loss: 118.2516\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.5555 - val_loss: 98.9288\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5929 - val_loss: 97.3802\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.6423 - val_loss: 91.8885\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.7674 - val_loss: 125.1034\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.5437 - val_loss: 93.7443\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 78.4525 - val_loss: 90.9818\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 78.5441 - val_loss: 94.7335\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.4479 - val_loss: 115.9227\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.5735 - val_loss: 110.2142\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.2626 - val_loss: 96.6289\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.2348 - val_loss: 88.8849\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.1902 - val_loss: 113.8800\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.3218 - val_loss: 89.2154\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.2877 - val_loss: 97.2920\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0893 - val_loss: 96.6211\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1873 - val_loss: 89.7314\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.2414 - val_loss: 113.0650\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.2393 - val_loss: 90.1076\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.9623 - val_loss: 93.0830\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.1378 - val_loss: 123.9755\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.0865 - val_loss: 101.2545\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.4029 - val_loss: 110.9186\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.1359 - val_loss: 98.5459\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.2878 - val_loss: 94.7827\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3863 - val_loss: 93.4143\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3149 - val_loss: 107.7553\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.2340 - val_loss: 115.1472\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9527 - val_loss: 92.6377\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.1336 - val_loss: 90.1052\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0270 - val_loss: 92.1351\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0891 - val_loss: 113.3143\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.2909 - val_loss: 95.1441\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.2066 - val_loss: 110.2410\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9253 - val_loss: 99.1412\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.2100 - val_loss: 114.2455\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3793 - val_loss: 89.5307\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3745 - val_loss: 93.5511\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.4584 - val_loss: 145.1494\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.3452 - val_loss: 87.6477\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9684 - val_loss: 98.3112\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9455 - val_loss: 98.8873\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9556 - val_loss: 108.6348\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.1025 - val_loss: 94.2550\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.0360 - val_loss: 92.9315\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.8361 - val_loss: 99.3140\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.1971 - val_loss: 98.9182\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.8059 - val_loss: 167.3060\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.7407 - val_loss: 89.5103\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9231 - val_loss: 90.6682\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 78.1806 - val_loss: 123.2092\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.7052 - val_loss: 99.8027\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 78.0340 - val_loss: 88.1926\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.9618 - val_loss: 90.6607\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.9744 - val_loss: 110.9709\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.8073 - val_loss: 95.0241\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.5843 - val_loss: 109.8972\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6353 - val_loss: 90.9967\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.8736 - val_loss: 128.1475\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.9933 - val_loss: 101.3544\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.5976 - val_loss: 92.7194\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.5432 - val_loss: 95.5021\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.8661 - val_loss: 103.2799\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.7377 - val_loss: 152.4828\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.7470 - val_loss: 105.0699\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.5028 - val_loss: 89.6151\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.6241 - val_loss: 96.3539\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.4572 - val_loss: 128.3517\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.7360 - val_loss: 86.2472\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 77.5273 - val_loss: 95.9992\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.4300 - val_loss: 107.5249\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.3087 - val_loss: 95.7364\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.6892 - val_loss: 93.2997\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.7183 - val_loss: 93.2363\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6812 - val_loss: 94.1182\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4433 - val_loss: 97.2370\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.5377 - val_loss: 91.9961\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6459 - val_loss: 98.4856\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4971 - val_loss: 104.7544\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.4262 - val_loss: 90.4214\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.4994 - val_loss: 94.5290\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.4066 - val_loss: 109.0191\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4401 - val_loss: 120.8595\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.6282 - val_loss: 106.1671\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3368 - val_loss: 106.5510\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6149 - val_loss: 103.3864\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3183 - val_loss: 99.0907\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.6825 - val_loss: 96.0952\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.6362 - val_loss: 89.7388\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.3284 - val_loss: 93.6635\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 77.3397 - val_loss: 119.2252\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4127 - val_loss: 87.1054\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696v_fuFCTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83366610-8236-42e0-dd27-cdf90c2c4c0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.14473456249173497 \n",
            "MAE:  6.918892441690865 \n",
            "SD:  9.331904692986614\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "0e642bad-746e-42a6-fa47-d5124e66b81e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "mdZF2osWCUQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcdd9ebf-be9b-49a5-aa94-1594cd28b01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -0.3412696413459477 \n",
            "Ensemble_std:  9.494019954526275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86617b3a-3451-4bbe-8c33-11ba267e5ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_42 (Dense)            (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_39 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_39 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_40 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_40 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_41 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_41 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_42 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_42 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_43 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_43 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_44 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_44 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_45 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_45 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_46 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_46 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_47 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_47 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_48 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_48 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_49 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_49 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_50 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_50 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_51 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_51 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,313\n",
            "Trainable params: 2,105\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8054090f-ebf1-4d38-ce0e-0f9a400dfdf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 5s 13ms/step - loss: 3692.8955 - val_loss: 3711.7959\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 3482.5208 - val_loss: 3477.2029\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 3239.0000 - val_loss: 2966.4050\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2943.6055 - val_loss: 2969.9124\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2607.6233 - val_loss: 2350.9182\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2244.7373 - val_loss: 2105.3533\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1863.1548 - val_loss: 1905.2878\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1466.3934 - val_loss: 1417.5853\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1090.5792 - val_loss: 701.8230\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 767.0771 - val_loss: 760.4785\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 517.3544 - val_loss: 546.7074\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 337.3577 - val_loss: 196.5977\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 218.1147 - val_loss: 232.1724\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 143.8751 - val_loss: 98.6920\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.1553 - val_loss: 132.5935\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 83.6131 - val_loss: 225.4095\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8235 - val_loss: 92.8478\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.6649 - val_loss: 152.0651\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 61.0446 - val_loss: 89.9983\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 57.3561 - val_loss: 85.3960\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.0272 - val_loss: 67.1079\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.9937 - val_loss: 63.9010\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 49.1929 - val_loss: 65.4300\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 48.0878 - val_loss: 81.5300\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 46.5904 - val_loss: 78.3588\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 44.7574 - val_loss: 60.0650\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 43.5782 - val_loss: 53.7578\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 43.1092 - val_loss: 90.4629\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.1755 - val_loss: 109.1268\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.6799 - val_loss: 45.8354\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.2420 - val_loss: 67.4465\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 40.3830 - val_loss: 126.1568\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.1215 - val_loss: 53.9104\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.8334 - val_loss: 69.3023\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 39.6833 - val_loss: 63.8503\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.3189 - val_loss: 75.2968\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.1780 - val_loss: 54.5501\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 38.3077 - val_loss: 68.3488\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.3055 - val_loss: 49.0080\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.0639 - val_loss: 53.4162\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.5408 - val_loss: 62.4582\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.3931 - val_loss: 112.3131\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.2906 - val_loss: 53.7808\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.0724 - val_loss: 47.0298\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.8353 - val_loss: 42.4471\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.5718 - val_loss: 131.6992\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.5799 - val_loss: 54.9457\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.6778 - val_loss: 73.6565\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.3013 - val_loss: 55.3588\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.0728 - val_loss: 49.4102\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.1440 - val_loss: 55.8271\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.9557 - val_loss: 43.2619\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.8676 - val_loss: 47.8413\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.0219 - val_loss: 38.6344\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.7769 - val_loss: 52.1113\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.5317 - val_loss: 57.6437\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.3031 - val_loss: 39.8822\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.4568 - val_loss: 89.9818\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.3149 - val_loss: 136.5749\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.1372 - val_loss: 46.2496\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.7324 - val_loss: 39.2124\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.0576 - val_loss: 52.0565\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.8733 - val_loss: 71.0117\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.7647 - val_loss: 43.8135\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.9150 - val_loss: 42.8049\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.7757 - val_loss: 53.5100\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.4314 - val_loss: 46.6277\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.5859 - val_loss: 57.4143\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.9200 - val_loss: 40.6698\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.2871 - val_loss: 51.7757\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.3149 - val_loss: 41.9763\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.2004 - val_loss: 56.5181\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.4035 - val_loss: 44.4538\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9455 - val_loss: 43.2822\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.7858 - val_loss: 36.9573\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.7823 - val_loss: 37.3166\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.8346 - val_loss: 40.0142\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.7638 - val_loss: 39.2013\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.6156 - val_loss: 45.3876\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.7906 - val_loss: 46.3576\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.4914 - val_loss: 39.8510\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.4912 - val_loss: 44.6488\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.2734 - val_loss: 40.8736\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.2933 - val_loss: 54.6223\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.4104 - val_loss: 38.4538\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.3652 - val_loss: 55.7552\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.5091 - val_loss: 60.3758\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0866 - val_loss: 36.7532\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0721 - val_loss: 37.0772\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.1806 - val_loss: 55.3397\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.1181 - val_loss: 38.1569\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.1687 - val_loss: 38.2536\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9605 - val_loss: 46.9188\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.1166 - val_loss: 39.7503\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.9379 - val_loss: 69.2936\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9016 - val_loss: 44.5138\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6036 - val_loss: 70.5731\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4824 - val_loss: 61.7489\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.7390 - val_loss: 36.7830\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3828 - val_loss: 35.2666\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4688 - val_loss: 53.9834\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2935 - val_loss: 46.5707\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3115 - val_loss: 36.5541\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.2579 - val_loss: 54.9333\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0886 - val_loss: 42.7998\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.2806 - val_loss: 43.0106\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2953 - val_loss: 36.3440\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9585 - val_loss: 36.0205\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9453 - val_loss: 43.4606\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0634 - val_loss: 43.8047\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9571 - val_loss: 36.5734\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1849 - val_loss: 41.6080\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8317 - val_loss: 41.5930\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7244 - val_loss: 44.9543\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7922 - val_loss: 60.0507\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8030 - val_loss: 40.0096\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7009 - val_loss: 39.1309\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7548 - val_loss: 50.5177\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5053 - val_loss: 39.8704\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4755 - val_loss: 38.4470\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5384 - val_loss: 36.8371\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5686 - val_loss: 37.0426\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3911 - val_loss: 37.7769\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4169 - val_loss: 37.0186\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4041 - val_loss: 51.7647\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2294 - val_loss: 40.2296\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7193 - val_loss: 42.7991\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3070 - val_loss: 36.2718\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3052 - val_loss: 34.6964\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1708 - val_loss: 67.9118\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1220 - val_loss: 38.4170\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2021 - val_loss: 39.1543\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0501 - val_loss: 35.8087\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0164 - val_loss: 39.8387\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9152 - val_loss: 34.9177\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9464 - val_loss: 36.8216\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9987 - val_loss: 49.0565\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8461 - val_loss: 37.6691\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9605 - val_loss: 34.6616\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7706 - val_loss: 41.2261\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.6872 - val_loss: 34.1468\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7006 - val_loss: 43.6523\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7647 - val_loss: 35.8341\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6588 - val_loss: 87.2048\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7372 - val_loss: 45.2358\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6857 - val_loss: 35.4455\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.6780 - val_loss: 35.3672\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8445 - val_loss: 38.9087\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7330 - val_loss: 43.8045\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9021 - val_loss: 36.4074\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5536 - val_loss: 38.0662\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5517 - val_loss: 39.2916\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7232 - val_loss: 35.8029\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6387 - val_loss: 40.4168\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5840 - val_loss: 45.2731\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 30.5229 - val_loss: 36.4144\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 30.3218 - val_loss: 35.4790\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3920 - val_loss: 37.1268\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4022 - val_loss: 37.0937\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2921 - val_loss: 35.7958\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3393 - val_loss: 34.9018\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3310 - val_loss: 36.2147\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3790 - val_loss: 44.2701\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4757 - val_loss: 40.1017\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6953 - val_loss: 40.0019\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3955 - val_loss: 49.5910\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3663 - val_loss: 38.6182\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1966 - val_loss: 36.6597\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2669 - val_loss: 35.8473\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3070 - val_loss: 37.7295\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2271 - val_loss: 37.1989\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2113 - val_loss: 36.6841\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.2730 - val_loss: 35.5477\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2505 - val_loss: 36.8586\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2416 - val_loss: 43.2320\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4177 - val_loss: 41.0546\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2259 - val_loss: 38.8056\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0864 - val_loss: 36.4682\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 30.2152 - val_loss: 38.9081\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 30.1811 - val_loss: 35.5592\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0553 - val_loss: 35.6568\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1030 - val_loss: 35.0168\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1986 - val_loss: 44.0568\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1445 - val_loss: 36.5483\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2736 - val_loss: 55.5849\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2452 - val_loss: 39.4262\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2212 - val_loss: 52.2135\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0888 - val_loss: 50.0855\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0918 - val_loss: 36.1763\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0034 - val_loss: 40.5676\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9370 - val_loss: 41.5359\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9102 - val_loss: 42.1904\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8609 - val_loss: 34.5707\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8268 - val_loss: 37.5983\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9650 - val_loss: 45.7233\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9162 - val_loss: 35.6851\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0388 - val_loss: 35.7592\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1208 - val_loss: 36.3685\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7737 - val_loss: 37.1819\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8252 - val_loss: 38.8026\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7651 - val_loss: 35.0546\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7026 - val_loss: 34.8297\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8351 - val_loss: 37.5330\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9267 - val_loss: 38.7690\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.7700 - val_loss: 35.8187\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7440 - val_loss: 36.3481\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7943 - val_loss: 34.8575\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7179 - val_loss: 46.3505\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7744 - val_loss: 41.4688\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7198 - val_loss: 34.7359\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8472 - val_loss: 36.6026\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7274 - val_loss: 43.0132\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7099 - val_loss: 49.9038\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7338 - val_loss: 34.6494\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.7196 - val_loss: 35.3769\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7963 - val_loss: 34.6577\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6974 - val_loss: 36.7784\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6920 - val_loss: 36.4012\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8044 - val_loss: 38.4115\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7736 - val_loss: 39.1776\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8800 - val_loss: 39.4565\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9212 - val_loss: 35.0581\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9134 - val_loss: 37.4905\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6149 - val_loss: 35.0225\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5972 - val_loss: 34.1902\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5555 - val_loss: 49.5133\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5475 - val_loss: 35.4425\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5478 - val_loss: 44.3487\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6206 - val_loss: 35.2655\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7592 - val_loss: 41.0304\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5677 - val_loss: 35.4391\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6465 - val_loss: 37.4465\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6739 - val_loss: 35.4015\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6903 - val_loss: 35.0710\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6868 - val_loss: 35.8617\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9458 - val_loss: 39.3339\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6062 - val_loss: 41.2589\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6688 - val_loss: 34.7766\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4916 - val_loss: 35.7616\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6667 - val_loss: 37.0800\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5910 - val_loss: 33.7562\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5108 - val_loss: 43.6102\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5259 - val_loss: 37.2114\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4757 - val_loss: 33.7234\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4624 - val_loss: 42.6721\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5346 - val_loss: 40.7094\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8917 - val_loss: 39.2362\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8395 - val_loss: 39.9877\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4587 - val_loss: 33.8168\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5376 - val_loss: 39.8213\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5943 - val_loss: 34.2696\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4195 - val_loss: 34.2091\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5850 - val_loss: 34.2682\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.4689 - val_loss: 42.4560\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4687 - val_loss: 35.0203\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4826 - val_loss: 35.9665\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5120 - val_loss: 36.4580\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4900 - val_loss: 40.0980\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4689 - val_loss: 36.4744\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5221 - val_loss: 37.3509\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5284 - val_loss: 42.8754\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3904 - val_loss: 34.7137\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7167 - val_loss: 35.6377\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4931 - val_loss: 36.7314\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3541 - val_loss: 34.3831\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3690 - val_loss: 33.6755\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4087 - val_loss: 45.6681\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8632 - val_loss: 41.0249\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5143 - val_loss: 41.8474\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4437 - val_loss: 35.3574\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3625 - val_loss: 37.7074\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4414 - val_loss: 36.7994\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4684 - val_loss: 35.0184\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4141 - val_loss: 35.4362\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3962 - val_loss: 34.1290\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4054 - val_loss: 34.4467\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4072 - val_loss: 36.0772\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3170 - val_loss: 36.4681\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2506 - val_loss: 36.3310\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4162 - val_loss: 32.9624\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4467 - val_loss: 56.2577\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4329 - val_loss: 34.4917\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4793 - val_loss: 35.1825\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4032 - val_loss: 35.5963\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.3480 - val_loss: 34.8049\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.3278 - val_loss: 34.9954\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4310 - val_loss: 36.3179\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4087 - val_loss: 34.2684\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3255 - val_loss: 35.0271\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4077 - val_loss: 35.5042\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4130 - val_loss: 35.2227\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3136 - val_loss: 37.8470\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3231 - val_loss: 33.8761\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2902 - val_loss: 35.3822\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3351 - val_loss: 35.8182\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2501 - val_loss: 37.2296\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2799 - val_loss: 37.6673\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3739 - val_loss: 36.3569\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2581 - val_loss: 33.6746\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2843 - val_loss: 34.4579\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3328 - val_loss: 38.1731\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3503 - val_loss: 39.2213\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2128 - val_loss: 35.6006\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2035 - val_loss: 36.7023\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3478 - val_loss: 39.5193\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1885 - val_loss: 33.3134\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2822 - val_loss: 37.5685\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2485 - val_loss: 34.7504\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3020 - val_loss: 36.7148\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2374 - val_loss: 36.2487\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2762 - val_loss: 38.1486\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2623 - val_loss: 45.9330\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2430 - val_loss: 33.8953\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2178 - val_loss: 46.8487\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1165 - val_loss: 33.7097\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3624 - val_loss: 35.3797\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2056 - val_loss: 43.2782\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2177 - val_loss: 37.7203\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2872 - val_loss: 36.6001\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2246 - val_loss: 40.0690\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5117 - val_loss: 36.8770\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2781 - val_loss: 33.6677\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1649 - val_loss: 34.3028\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0950 - val_loss: 38.0686\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1578 - val_loss: 38.4828\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0822 - val_loss: 35.6141\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1732 - val_loss: 40.3705\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0923 - val_loss: 33.5749\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1478 - val_loss: 35.1307\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1919 - val_loss: 35.5347\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1877 - val_loss: 36.7849\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4472 - val_loss: 36.6363\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3519 - val_loss: 42.9618\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3437 - val_loss: 33.5943\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1940 - val_loss: 40.0136\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3188 - val_loss: 35.4960\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2296 - val_loss: 37.2083\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3356 - val_loss: 43.9017\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6715 - val_loss: 34.4243\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4283 - val_loss: 35.6799\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4741 - val_loss: 37.4437\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4535 - val_loss: 37.7003\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2389 - val_loss: 36.1113\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1815 - val_loss: 34.3582\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1620 - val_loss: 38.6815\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1112 - val_loss: 36.9831\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1468 - val_loss: 38.5217\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0878 - val_loss: 33.4959\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2180 - val_loss: 34.5796\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1836 - val_loss: 33.9546\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1594 - val_loss: 42.1113\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2283 - val_loss: 38.3393\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4516 - val_loss: 37.0282\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2257 - val_loss: 34.2292\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2278 - val_loss: 34.6758\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1472 - val_loss: 43.2270\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2671 - val_loss: 35.8949\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1372 - val_loss: 34.7619\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1523 - val_loss: 35.6321\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1295 - val_loss: 35.5802\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2603 - val_loss: 42.8708\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1863 - val_loss: 34.8164\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2459 - val_loss: 36.8168\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1190 - val_loss: 33.7069\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0040 - val_loss: 37.5015\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1180 - val_loss: 38.7851\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2038 - val_loss: 35.7236\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0955 - val_loss: 40.2896\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2759 - val_loss: 38.7273\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1252 - val_loss: 38.3827\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0710 - val_loss: 35.0012\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9698 - val_loss: 32.9316\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9799 - val_loss: 37.4711\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0187 - val_loss: 33.2549\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0010 - val_loss: 42.8125\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0940 - val_loss: 39.2328\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2481 - val_loss: 48.5114\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2013 - val_loss: 36.1171\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0770 - val_loss: 37.8723\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1725 - val_loss: 36.2519\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1323 - val_loss: 37.6561\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9823 - val_loss: 34.5891\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9819 - val_loss: 38.9349\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0793 - val_loss: 33.9126\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1353 - val_loss: 34.5920\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1730 - val_loss: 34.2715\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9732 - val_loss: 36.3127\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0294 - val_loss: 34.3239\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2030 - val_loss: 39.6673\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1904 - val_loss: 97.6826\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1934 - val_loss: 35.8172\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0812 - val_loss: 39.9709\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9620 - val_loss: 45.8809\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1704 - val_loss: 35.2326\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0386 - val_loss: 51.2588\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1195 - val_loss: 38.7408\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1199 - val_loss: 41.8524\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0085 - val_loss: 34.5755\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9343 - val_loss: 36.6799\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0034 - val_loss: 33.3795\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1878 - val_loss: 34.4721\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0182 - val_loss: 35.9556\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0245 - val_loss: 35.4646\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1480 - val_loss: 33.4756\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1259 - val_loss: 48.0328\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.9594 - val_loss: 35.8881\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 29.0715 - val_loss: 35.7572\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9848 - val_loss: 38.9824\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2160 - val_loss: 33.7170\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1267 - val_loss: 47.3217\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0607 - val_loss: 34.4661\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9945 - val_loss: 34.8763\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2062 - val_loss: 56.3271\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3130 - val_loss: 36.2485\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1707 - val_loss: 38.2759\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0534 - val_loss: 39.9474\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9763 - val_loss: 36.5947\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3707 - val_loss: 38.7909\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9569 - val_loss: 33.6904\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0885 - val_loss: 50.8347\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0391 - val_loss: 35.2426\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1003 - val_loss: 35.4361\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0966 - val_loss: 34.3353\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1055 - val_loss: 48.6897\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0220 - val_loss: 35.5554\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0033 - val_loss: 34.6102\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9704 - val_loss: 37.0680\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9713 - val_loss: 38.5929\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9515 - val_loss: 36.9528\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2655 - val_loss: 34.5818\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9831 - val_loss: 35.1855\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9743 - val_loss: 43.0598\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0058 - val_loss: 36.9895\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1903 - val_loss: 38.7343\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0753 - val_loss: 36.9683\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9428 - val_loss: 39.9022\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1128 - val_loss: 34.9879\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0402 - val_loss: 35.4251\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9668 - val_loss: 39.5667\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.8681 - val_loss: 33.4840\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.8663 - val_loss: 35.3721\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.8778 - val_loss: 34.5001\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9648 - val_loss: 33.8702\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9318 - val_loss: 36.1898\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9394 - val_loss: 36.8514\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9408 - val_loss: 40.9823\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9227 - val_loss: 52.0538\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0052 - val_loss: 68.8436\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0393 - val_loss: 34.7897\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1839 - val_loss: 47.4848\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2106 - val_loss: 34.4009\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9703 - val_loss: 40.2433\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9565 - val_loss: 34.7305\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9633 - val_loss: 35.1846\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9124 - val_loss: 35.1222\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.8775 - val_loss: 37.5791\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9318 - val_loss: 34.1321\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9163 - val_loss: 33.7935\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9024 - val_loss: 34.0779\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.8554 - val_loss: 33.6017\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9444 - val_loss: 36.4976\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9314 - val_loss: 35.7105\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9378 - val_loss: 41.5380\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9347 - val_loss: 40.1532\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9436 - val_loss: 34.4425\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9605 - val_loss: 38.3338\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9225 - val_loss: 35.4893\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0344 - val_loss: 35.3636\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9018 - val_loss: 34.7022\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0138 - val_loss: 35.6400\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0415 - val_loss: 43.6321\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9133 - val_loss: 33.6868\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0121 - val_loss: 33.7801\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9865 - val_loss: 37.9449\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9105 - val_loss: 62.9095\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.8667 - val_loss: 34.1624\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0113 - val_loss: 33.9661\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9095 - val_loss: 39.1599\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0599 - val_loss: 43.4297\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.8760 - val_loss: 33.5428\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9232 - val_loss: 37.6560\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9740 - val_loss: 39.1950\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0870 - val_loss: 52.2856\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.8851 - val_loss: 33.5178\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9194 - val_loss: 37.7107\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9293 - val_loss: 41.7137\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0220 - val_loss: 34.2610\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.8854 - val_loss: 33.3793\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9090 - val_loss: 36.4510\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9021 - val_loss: 35.2154\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9567 - val_loss: 40.8699\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9171 - val_loss: 41.2699\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2243 - val_loss: 37.2663\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9667 - val_loss: 34.1307\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9385 - val_loss: 33.6854\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9966 - val_loss: 43.6786\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9817 - val_loss: 35.6972\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9223 - val_loss: 37.0908\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9865 - val_loss: 34.0276\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9083 - val_loss: 33.9906\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eeb676a-ed17-406b-ce37-7c98fcc09351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.7226736203821194 \n",
            "MAE:  4.2205320144473335 \n",
            "SD:  5.785186578829483\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cip38xZOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "1e2d8ad4-7841-4595-b514-5c0bf1363068"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e9a397-d61f-45d1-a643-b58d5f97d347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_56 (Dense)            (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_52 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_52 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_53 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_53 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_54 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_54 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_55 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_55 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_56 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_56 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_57 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_57 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_58 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_58 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_59 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_59 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_60 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_60 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_61 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_61 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_62 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_62 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_63 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_63 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_64 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_64 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,313\n",
            "Trainable params: 2,105\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590a4abd-a4b6-4031-9007-4e637c7d3096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 5s 13ms/step - loss: 3706.4312 - val_loss: 3714.2534\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3523.1245 - val_loss: 3390.7522\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 3299.8184 - val_loss: 3078.8040\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 3020.9216 - val_loss: 2700.9661\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2673.8909 - val_loss: 1880.5758\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2288.9402 - val_loss: 786.0988\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1911.1390 - val_loss: 1141.5149\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1552.9104 - val_loss: 446.9416\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1220.6587 - val_loss: 682.7697\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 935.8856 - val_loss: 831.4662\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 700.3822 - val_loss: 81.4282\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 508.6154 - val_loss: 120.0211\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 361.5222 - val_loss: 230.0334\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 260.8082 - val_loss: 151.8620\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 179.7753 - val_loss: 513.1213\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 126.6222 - val_loss: 317.7404\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.9333 - val_loss: 247.4899\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.3648 - val_loss: 146.2698\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 74.0814 - val_loss: 225.5257\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 68.6042 - val_loss: 86.3054\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.0446 - val_loss: 100.6657\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 62.6240 - val_loss: 181.4131\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 61.8961 - val_loss: 95.5005\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 60.2639 - val_loss: 138.3844\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 57.9858 - val_loss: 144.1661\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 56.7214 - val_loss: 105.8604\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 55.5126 - val_loss: 83.6996\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 54.6458 - val_loss: 66.6916\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 53.9809 - val_loss: 57.1967\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.7581 - val_loss: 58.6048\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.5879 - val_loss: 52.3901\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 50.7571 - val_loss: 69.6766\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 50.1522 - val_loss: 59.4298\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 49.6905 - val_loss: 59.2704\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 48.4797 - val_loss: 63.7181\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 47.2553 - val_loss: 52.5361\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 46.5520 - val_loss: 128.4442\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.6311 - val_loss: 54.6727\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.1318 - val_loss: 47.4427\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 44.3316 - val_loss: 44.5080\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 44.0388 - val_loss: 48.7098\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 43.1763 - val_loss: 87.2177\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.5256 - val_loss: 50.1528\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.9880 - val_loss: 46.9693\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 41.6827 - val_loss: 45.9406\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 41.2726 - val_loss: 78.5477\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.9852 - val_loss: 46.5027\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.4372 - val_loss: 51.9920\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 40.0460 - val_loss: 61.0552\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.6321 - val_loss: 43.9923\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.3234 - val_loss: 45.8215\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.7390 - val_loss: 44.9769\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.4444 - val_loss: 58.2482\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.2706 - val_loss: 57.5888\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.3867 - val_loss: 49.6020\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.0632 - val_loss: 43.5081\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.3429 - val_loss: 41.7138\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.5309 - val_loss: 75.7791\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.4544 - val_loss: 53.1053\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.8690 - val_loss: 39.1271\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.4611 - val_loss: 56.7112\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.4094 - val_loss: 47.8693\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.4381 - val_loss: 45.4420\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.8330 - val_loss: 39.0807\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.9194 - val_loss: 43.5358\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.6139 - val_loss: 45.9557\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.4652 - val_loss: 62.9346\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.6157 - val_loss: 38.0818\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.9038 - val_loss: 38.5287\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.7176 - val_loss: 45.2159\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.4576 - val_loss: 41.9917\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.8176 - val_loss: 66.2785\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.2435 - val_loss: 127.6741\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.5419 - val_loss: 48.6257\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.9846 - val_loss: 55.6487\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.0421 - val_loss: 47.1604\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.4434 - val_loss: 45.7912\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.5570 - val_loss: 38.8606\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.3271 - val_loss: 42.0678\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.2509 - val_loss: 74.7385\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.5169 - val_loss: 73.1762\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.0853 - val_loss: 57.0036\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.0083 - val_loss: 74.0816\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.7142 - val_loss: 41.4586\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.6461 - val_loss: 44.9073\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.8090 - val_loss: 40.4535\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5350 - val_loss: 40.8377\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.3561 - val_loss: 41.6469\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.2006 - val_loss: 52.0061\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.2685 - val_loss: 45.4481\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.2469 - val_loss: 106.9910\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.3293 - val_loss: 45.1984\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.7002 - val_loss: 48.1795\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.4084 - val_loss: 43.9298\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.3240 - val_loss: 51.1514\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0337 - val_loss: 50.5407\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0288 - val_loss: 38.2144\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8046 - val_loss: 47.2052\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8684 - val_loss: 39.6277\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.2340 - val_loss: 48.0534\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.7550 - val_loss: 39.8299\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.5412 - val_loss: 40.5858\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0118 - val_loss: 37.0259\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.7614 - val_loss: 60.5389\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0442 - val_loss: 42.1270\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6786 - val_loss: 37.8626\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6655 - val_loss: 37.2548\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5672 - val_loss: 38.1538\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5996 - val_loss: 37.6835\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0867 - val_loss: 42.3300\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.4159 - val_loss: 46.7413\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6264 - val_loss: 65.9309\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5026 - val_loss: 37.7593\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5166 - val_loss: 40.7458\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9558 - val_loss: 54.9663\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.1822 - val_loss: 52.1775\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.7798 - val_loss: 37.1739\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.4823 - val_loss: 40.4509\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9003 - val_loss: 43.5331\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.4643 - val_loss: 42.5429\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1907 - val_loss: 40.0135\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.4641 - val_loss: 45.3549\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9812 - val_loss: 41.0310\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9930 - val_loss: 44.5415\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8453 - val_loss: 38.2706\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7402 - val_loss: 39.2899\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6163 - val_loss: 50.3201\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6984 - val_loss: 36.5063\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6964 - val_loss: 51.5309\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8648 - val_loss: 36.5289\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1245 - val_loss: 41.6603\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9996 - val_loss: 68.4822\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7029 - val_loss: 36.9837\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4954 - val_loss: 36.6618\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4928 - val_loss: 36.3201\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6236 - val_loss: 85.0798\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9089 - val_loss: 52.2848\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9195 - val_loss: 39.9491\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6158 - val_loss: 36.0529\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7821 - val_loss: 35.7129\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5556 - val_loss: 43.0538\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4400 - val_loss: 37.4818\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2881 - val_loss: 35.9466\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4077 - val_loss: 39.8040\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2203 - val_loss: 35.6310\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3787 - val_loss: 46.9607\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2403 - val_loss: 48.7709\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2207 - val_loss: 43.8951\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4397 - val_loss: 36.7603\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2546 - val_loss: 36.1011\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3508 - val_loss: 43.1390\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7563 - val_loss: 36.4920\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.2630 - val_loss: 38.7011\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 31.6897 - val_loss: 43.5543\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4764 - val_loss: 113.3387\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4697 - val_loss: 41.4045\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0426 - val_loss: 44.0490\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0462 - val_loss: 35.0090\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1323 - val_loss: 44.9903\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2214 - val_loss: 34.6998\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0077 - val_loss: 45.6214\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9858 - val_loss: 37.5853\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9738 - val_loss: 36.6296\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3106 - val_loss: 37.5099\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2555 - val_loss: 103.7502\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4520 - val_loss: 56.7864\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1796 - val_loss: 44.6950\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3454 - val_loss: 102.0987\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.2066 - val_loss: 46.8028\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0697 - val_loss: 42.5782\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4418 - val_loss: 42.6203\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1793 - val_loss: 36.3286\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3008 - val_loss: 39.4013\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2529 - val_loss: 36.2360\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2815 - val_loss: 48.2971\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2035 - val_loss: 41.4049\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9954 - val_loss: 41.8071\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0608 - val_loss: 36.5848\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8880 - val_loss: 64.2581\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9677 - val_loss: 49.2876\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6454 - val_loss: 36.4261\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8541 - val_loss: 37.8372\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6941 - val_loss: 37.6484\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7546 - val_loss: 38.8975\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6955 - val_loss: 43.2201\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7338 - val_loss: 38.2882\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7031 - val_loss: 37.6309\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4867 - val_loss: 37.0845\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7354 - val_loss: 44.2427\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7114 - val_loss: 35.0402\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6464 - val_loss: 48.1335\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6023 - val_loss: 132.1158\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1832 - val_loss: 39.5591\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6489 - val_loss: 42.5380\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6861 - val_loss: 39.1434\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3873 - val_loss: 35.0615\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4795 - val_loss: 39.0775\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6291 - val_loss: 38.5039\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6598 - val_loss: 35.3602\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6150 - val_loss: 38.9215\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3975 - val_loss: 48.8840\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4761 - val_loss: 40.1920\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5544 - val_loss: 37.8711\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5388 - val_loss: 39.2822\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5400 - val_loss: 45.8567\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7750 - val_loss: 39.8167\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6416 - val_loss: 38.2200\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5296 - val_loss: 40.2724\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3830 - val_loss: 56.6558\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4197 - val_loss: 39.2971\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4194 - val_loss: 39.2565\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5097 - val_loss: 45.4753\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3066 - val_loss: 33.9259\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5195 - val_loss: 38.2808\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3003 - val_loss: 38.9027\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3496 - val_loss: 51.7251\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3515 - val_loss: 35.8880\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2193 - val_loss: 78.9572\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2910 - val_loss: 40.2897\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4964 - val_loss: 39.8339\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3008 - val_loss: 44.7063\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4179 - val_loss: 35.2301\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5579 - val_loss: 44.3587\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4939 - val_loss: 61.3257\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4439 - val_loss: 38.2369\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2629 - val_loss: 35.5696\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5579 - val_loss: 72.0737\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2190 - val_loss: 34.7299\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3444 - val_loss: 39.0169\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2215 - val_loss: 39.6947\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3382 - val_loss: 39.5601\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3046 - val_loss: 38.9585\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3161 - val_loss: 44.7300\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4989 - val_loss: 140.8271\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4809 - val_loss: 34.3755\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6224 - val_loss: 96.8534\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2693 - val_loss: 48.1346\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0845 - val_loss: 50.0668\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0197 - val_loss: 42.1067\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2024 - val_loss: 65.8001\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3484 - val_loss: 34.2752\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4878 - val_loss: 37.2551\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1478 - val_loss: 35.1312\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0220 - val_loss: 39.8884\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1987 - val_loss: 37.8858\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0421 - val_loss: 37.4775\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0557 - val_loss: 34.1294\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4121 - val_loss: 57.3191\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4588 - val_loss: 39.4400\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2710 - val_loss: 34.6602\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7499 - val_loss: 36.9836\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1018 - val_loss: 42.8923\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3488 - val_loss: 38.6585\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9659 - val_loss: 34.3449\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8539 - val_loss: 36.7551\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2087 - val_loss: 40.3904\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0505 - val_loss: 58.4302\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2934 - val_loss: 39.2329\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0613 - val_loss: 34.3099\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8797 - val_loss: 39.8096\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9072 - val_loss: 42.1749\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7962 - val_loss: 34.4310\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7412 - val_loss: 34.8729\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0069 - val_loss: 37.6289\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8041 - val_loss: 34.4917\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8328 - val_loss: 49.4019\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7605 - val_loss: 34.3888\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8034 - val_loss: 33.5471\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7842 - val_loss: 36.6176\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7820 - val_loss: 35.5720\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9245 - val_loss: 41.9426\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7530 - val_loss: 34.0631\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7952 - val_loss: 36.9437\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8097 - val_loss: 49.8567\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8627 - val_loss: 48.0017\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9216 - val_loss: 34.9503\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8064 - val_loss: 39.8570\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6220 - val_loss: 37.7022\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.7594 - val_loss: 37.3763\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8214 - val_loss: 36.5660\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7325 - val_loss: 38.1395\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6965 - val_loss: 34.8182\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7579 - val_loss: 36.8842\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8073 - val_loss: 37.8929\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7112 - val_loss: 35.3839\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 29.6122 - val_loss: 56.8240\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.7736 - val_loss: 33.5461\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6246 - val_loss: 43.0683\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6167 - val_loss: 57.0156\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6946 - val_loss: 35.0613\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7568 - val_loss: 36.5231\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6933 - val_loss: 35.7837\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5976 - val_loss: 37.9893\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9590 - val_loss: 43.8405\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8890 - val_loss: 36.2792\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8927 - val_loss: 35.9856\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6345 - val_loss: 40.9253\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7732 - val_loss: 37.2320\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6361 - val_loss: 59.0667\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1064 - val_loss: 51.1853\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6544 - val_loss: 35.8141\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5970 - val_loss: 38.0780\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5065 - val_loss: 37.9820\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4822 - val_loss: 33.9489\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5466 - val_loss: 35.7649\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5066 - val_loss: 34.7263\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8484 - val_loss: 34.1952\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5339 - val_loss: 36.9357\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5245 - val_loss: 96.0524\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0564 - val_loss: 38.3045\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5482 - val_loss: 37.5630\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6862 - val_loss: 42.3361\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8866 - val_loss: 37.9200\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7137 - val_loss: 52.7099\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5918 - val_loss: 68.7353\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5666 - val_loss: 36.0103\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7263 - val_loss: 42.0465\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8170 - val_loss: 51.0807\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4923 - val_loss: 35.8765\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5192 - val_loss: 35.5596\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5956 - val_loss: 40.0889\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5976 - val_loss: 35.2160\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4695 - val_loss: 37.1405\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4424 - val_loss: 43.7795\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7308 - val_loss: 71.4559\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2009 - val_loss: 36.6263\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7196 - val_loss: 34.6825\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5386 - val_loss: 35.1450\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9436 - val_loss: 39.0221\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5508 - val_loss: 49.4138\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0649 - val_loss: 54.4610\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5320 - val_loss: 35.5669\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7037 - val_loss: 48.0607\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5777 - val_loss: 36.2081\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7486 - val_loss: 41.4974\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6632 - val_loss: 37.6854\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6332 - val_loss: 44.3914\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5869 - val_loss: 53.0107\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4913 - val_loss: 34.2665\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3761 - val_loss: 34.8709\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3738 - val_loss: 36.3682\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4037 - val_loss: 49.6823\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6630 - val_loss: 52.9633\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4859 - val_loss: 42.2248\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5246 - val_loss: 57.1560\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5392 - val_loss: 35.2890\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2538 - val_loss: 40.3676\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5822 - val_loss: 45.6919\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5462 - val_loss: 112.6638\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5598 - val_loss: 40.2361\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3853 - val_loss: 38.2197\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0083 - val_loss: 37.2097\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4536 - val_loss: 39.2669\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7902 - val_loss: 38.0081\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2990 - val_loss: 37.0636\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3001 - val_loss: 41.1208\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3321 - val_loss: 34.8033\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4048 - val_loss: 38.5593\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0475 - val_loss: 62.2913\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0704 - val_loss: 35.3017\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7800 - val_loss: 39.0104\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5196 - val_loss: 39.1440\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5762 - val_loss: 34.8713\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2402 - val_loss: 38.7596\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3597 - val_loss: 41.4315\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3516 - val_loss: 46.9309\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5818 - val_loss: 35.9740\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3165 - val_loss: 35.4809\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5342 - val_loss: 35.7977\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4604 - val_loss: 37.2055\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9295 - val_loss: 48.9156\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7516 - val_loss: 63.0806\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0932 - val_loss: 44.4800\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5138 - val_loss: 51.3076\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4068 - val_loss: 38.5449\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.6560 - val_loss: 35.8521\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3540 - val_loss: 37.1282\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5834 - val_loss: 36.5282\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2971 - val_loss: 33.5579\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2726 - val_loss: 34.1897\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2459 - val_loss: 37.2113\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1898 - val_loss: 44.7283\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2067 - val_loss: 37.7006\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3096 - val_loss: 39.4346\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2950 - val_loss: 38.1152\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3003 - val_loss: 50.9082\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4089 - val_loss: 34.4095\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3079 - val_loss: 44.1218\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5055 - val_loss: 36.5791\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4206 - val_loss: 36.0758\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3101 - val_loss: 34.8525\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2541 - val_loss: 35.3313\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3886 - val_loss: 34.2737\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5071 - val_loss: 45.5945\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3550 - val_loss: 40.5054\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2047 - val_loss: 44.9546\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1925 - val_loss: 33.4785\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2159 - val_loss: 35.8342\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5344 - val_loss: 61.1669\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4164 - val_loss: 46.0338\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3147 - val_loss: 36.3013\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2191 - val_loss: 46.1324\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0986 - val_loss: 42.7666\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4371 - val_loss: 35.7559\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4536 - val_loss: 37.5687\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1834 - val_loss: 44.2683\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1017 - val_loss: 34.9011\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0942 - val_loss: 38.0523\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3477 - val_loss: 33.6954\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3880 - val_loss: 36.6176\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 29.0832 - val_loss: 35.9037\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.1516 - val_loss: 35.2149\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2894 - val_loss: 37.0742\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4886 - val_loss: 40.8268\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1934 - val_loss: 38.3932\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0184 - val_loss: 34.5508\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0910 - val_loss: 35.7842\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0323 - val_loss: 35.5117\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0553 - val_loss: 40.5656\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2919 - val_loss: 33.6743\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1456 - val_loss: 38.3558\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3995 - val_loss: 46.9278\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2514 - val_loss: 34.1474\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1655 - val_loss: 55.6103\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2535 - val_loss: 42.8446\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9844 - val_loss: 33.8191\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4937 - val_loss: 38.2462\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2560 - val_loss: 37.2426\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0353 - val_loss: 33.8742\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9557 - val_loss: 37.7977\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0729 - val_loss: 33.8937\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0484 - val_loss: 35.3039\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2231 - val_loss: 36.6083\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2899 - val_loss: 41.3083\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3839 - val_loss: 37.7077\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4242 - val_loss: 45.9414\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2105 - val_loss: 46.4162\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3276 - val_loss: 40.1772\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2715 - val_loss: 45.7306\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0895 - val_loss: 34.4944\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1348 - val_loss: 63.4310\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0830 - val_loss: 42.1518\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2001 - val_loss: 39.4915\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0701 - val_loss: 37.1998\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0456 - val_loss: 38.7850\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1764 - val_loss: 34.3283\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0436 - val_loss: 41.2751\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0613 - val_loss: 36.8017\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0748 - val_loss: 36.8703\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1825 - val_loss: 42.0397\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2318 - val_loss: 38.4874\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2514 - val_loss: 35.5927\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7432 - val_loss: 33.8891\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0711 - val_loss: 34.6836\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2160 - val_loss: 39.6223\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4710 - val_loss: 33.5642\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0690 - val_loss: 33.9859\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9934 - val_loss: 39.9364\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.1114 - val_loss: 36.1759\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1936 - val_loss: 56.2642\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1186 - val_loss: 48.2920\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9276 - val_loss: 34.5140\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1465 - val_loss: 33.1488\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0375 - val_loss: 44.9183\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9173 - val_loss: 33.4759\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0968 - val_loss: 37.1258\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0017 - val_loss: 37.7933\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9485 - val_loss: 35.6973\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1338 - val_loss: 56.7320\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1636 - val_loss: 36.8469\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9664 - val_loss: 37.1676\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9941 - val_loss: 33.4394\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1851 - val_loss: 46.8972\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9599 - val_loss: 38.8296\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9329 - val_loss: 42.9345\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.8981 - val_loss: 60.2967\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9419 - val_loss: 40.3682\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.8556 - val_loss: 35.7200\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1785 - val_loss: 34.7489\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0715 - val_loss: 34.0262\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5631 - val_loss: 36.1415\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1311 - val_loss: 33.9602\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2293 - val_loss: 51.1451\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1979 - val_loss: 34.0319\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0105 - val_loss: 54.9665\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2222 - val_loss: 36.0188\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9873 - val_loss: 34.7391\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5340 - val_loss: 57.3358\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0016 - val_loss: 36.9374\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0778 - val_loss: 38.3717\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1542 - val_loss: 38.6256\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0716 - val_loss: 39.0627\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9633 - val_loss: 36.5685\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9912 - val_loss: 33.8846\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9645 - val_loss: 39.6538\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9372 - val_loss: 65.6352\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1639 - val_loss: 41.1565\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3178 - val_loss: 33.1333\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.8986 - val_loss: 37.8524\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3759 - val_loss: 50.3125\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19598666-f93b-48e6-ef20-78b33cf57c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  2.1196398379151606 \n",
            "MAE:  5.356134758216825 \n",
            "SD:  6.769021008041629\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "9183d6c1-7cfb-4d0e-8145-4235c1576f52"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29873bd4-7591-485e-e01f-0769a8950829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_70 (Dense)            (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_65 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_65 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_66 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_66 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_67 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_67 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_68 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_68 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_69 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_69 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_70 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_70 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_71 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_71 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_72 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_72 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_78 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_73 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_73 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_79 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_74 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_74 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_75 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_75 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_76 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_76 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_77 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_77 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,313\n",
            "Trainable params: 2,105\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47679c6f-8776-4da7-cb75-4e2b70b0cd58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 5s 13ms/step - loss: 3805.2998 - val_loss: 3760.1113\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3670.7546 - val_loss: 3536.4302\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3471.1455 - val_loss: 3279.1057\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3254.6274 - val_loss: 3165.2588\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3001.6719 - val_loss: 2844.6750\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2708.1648 - val_loss: 2602.7913\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2410.8613 - val_loss: 2186.4524\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2109.6711 - val_loss: 1765.0306\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1817.2136 - val_loss: 1787.8010\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1536.9705 - val_loss: 1813.0558\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1274.9293 - val_loss: 527.3715\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1036.0605 - val_loss: 1049.4478\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 824.4811 - val_loss: 996.3208\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 644.0758 - val_loss: 391.1027\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 490.1299 - val_loss: 787.4292\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 367.3543 - val_loss: 500.4467\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 269.2400 - val_loss: 81.3401\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 195.2858 - val_loss: 205.7106\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 142.7404 - val_loss: 69.6684\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.3206 - val_loss: 56.3952\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 82.3308 - val_loss: 63.2760\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 67.6163 - val_loss: 50.4213\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 57.6148 - val_loss: 58.3556\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 52.8846 - val_loss: 102.0207\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 50.2096 - val_loss: 167.8058\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 49.2665 - val_loss: 75.0213\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 47.5389 - val_loss: 48.2313\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 47.6594 - val_loss: 97.6381\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 46.9687 - val_loss: 46.9005\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 47.0698 - val_loss: 54.2780\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 46.4407 - val_loss: 57.4484\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.0810 - val_loss: 46.8345\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 44.8684 - val_loss: 50.4053\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 44.7265 - val_loss: 51.7008\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 43.3243 - val_loss: 51.6705\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.3052 - val_loss: 44.2225\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.1519 - val_loss: 43.3263\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.4212 - val_loss: 53.2802\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.3724 - val_loss: 46.6671\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.5754 - val_loss: 62.2225\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.5467 - val_loss: 71.2271\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.5212 - val_loss: 44.9922\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.7524 - val_loss: 44.6537\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.4469 - val_loss: 43.9730\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.5896 - val_loss: 59.4286\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.2594 - val_loss: 44.7928\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.0704 - val_loss: 59.3889\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.9657 - val_loss: 47.5691\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.7913 - val_loss: 49.1540\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.5267 - val_loss: 49.0635\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.4919 - val_loss: 54.7539\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.1665 - val_loss: 44.3523\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.0258 - val_loss: 65.7408\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.8563 - val_loss: 76.2498\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.8293 - val_loss: 42.8546\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.6341 - val_loss: 47.4253\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.0420 - val_loss: 48.2874\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.9314 - val_loss: 43.2859\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.5933 - val_loss: 42.8487\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.3732 - val_loss: 40.9485\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.8782 - val_loss: 53.8100\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.5671 - val_loss: 60.5884\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.2786 - val_loss: 43.4253\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.2857 - val_loss: 46.3904\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.0062 - val_loss: 49.5655\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.7169 - val_loss: 41.4977\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.4989 - val_loss: 55.1378\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.1502 - val_loss: 47.4283\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.7026 - val_loss: 47.7286\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.4389 - val_loss: 39.6100\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.9136 - val_loss: 40.9229\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.2864 - val_loss: 48.5647\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.2268 - val_loss: 56.3060\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.4646 - val_loss: 45.3826\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.3669 - val_loss: 55.1406\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.0756 - val_loss: 38.1788\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.2689 - val_loss: 73.2142\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.0305 - val_loss: 40.1572\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.9037 - val_loss: 48.8123\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.9097 - val_loss: 48.0091\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.5986 - val_loss: 51.6352\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.0183 - val_loss: 44.8046\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.6589 - val_loss: 47.0349\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.9640 - val_loss: 44.2450\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.0941 - val_loss: 40.9056\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.7558 - val_loss: 39.0379\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.6829 - val_loss: 43.6110\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.9284 - val_loss: 59.6167\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.3780 - val_loss: 45.7869\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.4588 - val_loss: 51.0374\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.1898 - val_loss: 39.2773\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.0859 - val_loss: 38.5196\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.1639 - val_loss: 42.4263\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.1374 - val_loss: 38.2290\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.4883 - val_loss: 45.8688\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.6084 - val_loss: 56.0437\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9740 - val_loss: 43.4723\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9449 - val_loss: 41.7144\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9959 - val_loss: 38.2635\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.7943 - val_loss: 38.5133\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.6973 - val_loss: 59.1725\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9405 - val_loss: 42.2278\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.7614 - val_loss: 39.0687\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.6551 - val_loss: 38.3974\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.8956 - val_loss: 44.4565\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.6075 - val_loss: 38.9534\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.8626 - val_loss: 49.8127\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.7401 - val_loss: 42.6149\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.5286 - val_loss: 53.9946\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.3032 - val_loss: 39.6732\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.2514 - val_loss: 37.8262\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.3800 - val_loss: 36.8204\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.2916 - val_loss: 38.5652\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.1141 - val_loss: 39.1169\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9123 - val_loss: 45.2511\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9857 - val_loss: 38.4675\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9694 - val_loss: 42.2802\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8866 - val_loss: 37.8550\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9116 - val_loss: 35.4836\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0129 - val_loss: 42.0567\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.7727 - val_loss: 38.3064\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6509 - val_loss: 38.7902\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.7423 - val_loss: 36.7887\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5724 - val_loss: 45.7378\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5032 - val_loss: 40.0750\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0345 - val_loss: 37.3199\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5898 - val_loss: 38.2245\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5446 - val_loss: 38.1610\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5494 - val_loss: 37.5304\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6525 - val_loss: 44.9009\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5031 - val_loss: 45.1721\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3776 - val_loss: 39.4834\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6804 - val_loss: 38.6399\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5609 - val_loss: 37.8382\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3793 - val_loss: 39.5980\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.4336 - val_loss: 40.0830\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3067 - val_loss: 35.4663\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1289 - val_loss: 35.6379\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1230 - val_loss: 38.7356\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5822 - val_loss: 40.0921\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6940 - val_loss: 37.8635\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.4698 - val_loss: 51.2163\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1503 - val_loss: 41.8746\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0160 - val_loss: 44.3573\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1387 - val_loss: 41.8165\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1953 - val_loss: 40.5575\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.2504 - val_loss: 54.4226\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9432 - val_loss: 39.8045\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1817 - val_loss: 46.0160\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9256 - val_loss: 39.6163\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8685 - val_loss: 37.5198\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0105 - val_loss: 44.8682\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0533 - val_loss: 43.8360\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9977 - val_loss: 49.5608\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0623 - val_loss: 37.1049\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6004 - val_loss: 43.0284\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7503 - val_loss: 38.9141\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8989 - val_loss: 35.3723\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8267 - val_loss: 38.5760\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6244 - val_loss: 42.7367\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6504 - val_loss: 36.5669\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6993 - val_loss: 37.2152\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.5592 - val_loss: 37.6995\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 31.3992 - val_loss: 36.6432\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5012 - val_loss: 35.6769\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3083 - val_loss: 45.9762\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3476 - val_loss: 51.5632\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5092 - val_loss: 35.9193\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4235 - val_loss: 34.5776\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2321 - val_loss: 35.4556\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1811 - val_loss: 34.7052\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2828 - val_loss: 34.9163\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3856 - val_loss: 37.8071\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8215 - val_loss: 39.3021\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5308 - val_loss: 39.3745\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5168 - val_loss: 37.3420\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4212 - val_loss: 34.3519\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4262 - val_loss: 38.9789\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3068 - val_loss: 37.0610\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4598 - val_loss: 43.9322\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0911 - val_loss: 35.8942\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1855 - val_loss: 36.9553\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1217 - val_loss: 35.0680\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0657 - val_loss: 38.0292\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1633 - val_loss: 37.2806\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0851 - val_loss: 41.7425\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0168 - val_loss: 37.6080\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1765 - val_loss: 39.9289\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9403 - val_loss: 34.3421\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0072 - val_loss: 40.5966\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0308 - val_loss: 41.5954\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9278 - val_loss: 43.0413\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9537 - val_loss: 34.0388\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8231 - val_loss: 35.7954\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7688 - val_loss: 35.4600\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8196 - val_loss: 54.4912\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8828 - val_loss: 48.6229\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7975 - val_loss: 35.5095\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8500 - val_loss: 88.3625\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8686 - val_loss: 35.6642\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8606 - val_loss: 52.3932\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8770 - val_loss: 37.4694\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6876 - val_loss: 36.0253\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7489 - val_loss: 46.1020\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6499 - val_loss: 34.5279\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7094 - val_loss: 46.8986\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5906 - val_loss: 35.3292\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5856 - val_loss: 39.3759\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6798 - val_loss: 36.1957\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7826 - val_loss: 40.8534\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6102 - val_loss: 37.4368\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6436 - val_loss: 52.8612\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6435 - val_loss: 36.1531\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5498 - val_loss: 41.1619\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5242 - val_loss: 42.5485\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5270 - val_loss: 41.5684\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5409 - val_loss: 36.3485\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4795 - val_loss: 38.5749\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4745 - val_loss: 35.3426\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4087 - val_loss: 40.9159\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4474 - val_loss: 34.3572\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5371 - val_loss: 36.7640\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4854 - val_loss: 34.9417\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7293 - val_loss: 34.0362\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4387 - val_loss: 35.7210\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5902 - val_loss: 40.5170\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4070 - val_loss: 35.0798\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4819 - val_loss: 34.4669\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4518 - val_loss: 35.1767\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3177 - val_loss: 35.4671\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3003 - val_loss: 34.6550\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4201 - val_loss: 36.5504\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3886 - val_loss: 43.2376\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4307 - val_loss: 36.4744\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4537 - val_loss: 39.6676\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3464 - val_loss: 36.7527\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2567 - val_loss: 43.1206\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5481 - val_loss: 38.2896\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3559 - val_loss: 40.5560\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3080 - val_loss: 35.7734\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3988 - val_loss: 40.6011\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4827 - val_loss: 35.3377\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4163 - val_loss: 34.8269\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2776 - val_loss: 34.9274\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3068 - val_loss: 45.3054\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2088 - val_loss: 59.8820\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3657 - val_loss: 36.1412\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3024 - val_loss: 40.1269\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2313 - val_loss: 42.3299\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 30.3005 - val_loss: 34.6626\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2147 - val_loss: 33.6283\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3987 - val_loss: 38.9827\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2976 - val_loss: 38.0609\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1711 - val_loss: 35.6115\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1645 - val_loss: 39.2883\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1210 - val_loss: 37.7201\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2763 - val_loss: 55.5849\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2158 - val_loss: 36.6685\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9840 - val_loss: 34.8670\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2556 - val_loss: 122.2934\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0547 - val_loss: 34.4904\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1363 - val_loss: 35.7074\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1316 - val_loss: 64.1055\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0327 - val_loss: 33.3739\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9177 - val_loss: 39.7060\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1078 - val_loss: 54.6723\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1613 - val_loss: 46.3650\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1437 - val_loss: 40.3787\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0126 - val_loss: 48.4772\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1759 - val_loss: 34.4764\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0644 - val_loss: 34.2858\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0352 - val_loss: 33.4516\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1027 - val_loss: 34.6619\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0905 - val_loss: 34.8568\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2840 - val_loss: 34.1376\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0006 - val_loss: 35.7522\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0565 - val_loss: 62.2832\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9936 - val_loss: 43.4901\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0006 - val_loss: 35.1987\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0215 - val_loss: 40.0967\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0149 - val_loss: 36.6203\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9648 - val_loss: 35.2875\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9711 - val_loss: 34.2142\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9391 - val_loss: 34.0265\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9412 - val_loss: 37.9326\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1168 - val_loss: 33.6465\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9813 - val_loss: 38.0563\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0504 - val_loss: 33.7660\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9965 - val_loss: 38.6752\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8995 - val_loss: 33.5133\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9579 - val_loss: 32.8480\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9540 - val_loss: 45.6163\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0039 - val_loss: 57.1727\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 30.0750 - val_loss: 38.2087\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.2978 - val_loss: 40.0281\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.1950 - val_loss: 45.9176\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0699 - val_loss: 38.5239\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8855 - val_loss: 38.5978\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8717 - val_loss: 40.4027\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8848 - val_loss: 37.4790\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8244 - val_loss: 35.6572\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7911 - val_loss: 47.5124\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8570 - val_loss: 36.1446\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9910 - val_loss: 42.7885\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0083 - val_loss: 38.6943\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9013 - val_loss: 36.5062\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8961 - val_loss: 35.4215\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9399 - val_loss: 41.0107\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0141 - val_loss: 37.9239\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8744 - val_loss: 34.3878\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.0404 - val_loss: 33.8689\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9502 - val_loss: 39.7448\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9840 - val_loss: 37.8204\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9653 - val_loss: 34.5344\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8817 - val_loss: 34.9796\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8826 - val_loss: 47.9344\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8306 - val_loss: 36.0781\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8775 - val_loss: 35.2228\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7510 - val_loss: 35.7428\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7912 - val_loss: 35.5162\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7612 - val_loss: 33.7836\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8032 - val_loss: 34.8151\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7597 - val_loss: 45.3862\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8366 - val_loss: 35.4843\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8761 - val_loss: 45.0325\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7861 - val_loss: 33.3668\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7170 - val_loss: 33.7449\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7558 - val_loss: 32.8879\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6904 - val_loss: 36.2274\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7272 - val_loss: 36.2383\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6764 - val_loss: 38.7322\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.9109 - val_loss: 35.0099\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7387 - val_loss: 36.1573\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6478 - val_loss: 37.8100\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6920 - val_loss: 36.7910\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6629 - val_loss: 35.0378\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5935 - val_loss: 36.3670\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6932 - val_loss: 38.1478\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6142 - val_loss: 33.0215\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7291 - val_loss: 38.4162\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6004 - val_loss: 35.0994\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5586 - val_loss: 34.5649\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6611 - val_loss: 36.5813\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7135 - val_loss: 34.2119\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6651 - val_loss: 38.6552\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5699 - val_loss: 33.7242\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6327 - val_loss: 44.5034\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5720 - val_loss: 50.5276\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6451 - val_loss: 33.3293\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6168 - val_loss: 42.2048\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6753 - val_loss: 33.1812\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5546 - val_loss: 33.3934\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5996 - val_loss: 38.1451\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6283 - val_loss: 37.5447\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5827 - val_loss: 33.4535\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6612 - val_loss: 35.9832\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6559 - val_loss: 33.4804\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5226 - val_loss: 33.7802\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4760 - val_loss: 32.5524\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5407 - val_loss: 37.5332\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.7361 - val_loss: 36.7484\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4961 - val_loss: 39.8097\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4869 - val_loss: 34.3881\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5720 - val_loss: 41.4160\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7363 - val_loss: 39.1486\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.6583 - val_loss: 38.3451\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.6008 - val_loss: 37.2897\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6888 - val_loss: 41.2878\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5477 - val_loss: 44.8256\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6512 - val_loss: 44.5066\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5759 - val_loss: 40.2704\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.8055 - val_loss: 35.8762\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.7732 - val_loss: 38.1635\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6350 - val_loss: 33.8559\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6038 - val_loss: 36.2289\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4920 - val_loss: 33.4901\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5496 - val_loss: 40.5878\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.6062 - val_loss: 35.2146\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5172 - val_loss: 49.6303\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6144 - val_loss: 43.8324\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5028 - val_loss: 35.7427\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5400 - val_loss: 34.9231\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5571 - val_loss: 35.8675\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5450 - val_loss: 46.0582\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4627 - val_loss: 43.2538\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4558 - val_loss: 33.9737\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4193 - val_loss: 33.3949\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4201 - val_loss: 33.5274\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5106 - val_loss: 34.2833\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4868 - val_loss: 34.7438\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4753 - val_loss: 37.5712\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4453 - val_loss: 34.3251\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5469 - val_loss: 40.9704\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4246 - val_loss: 37.4857\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4226 - val_loss: 33.1300\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4736 - val_loss: 33.3384\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4775 - val_loss: 39.7270\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.6403 - val_loss: 35.8973\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4492 - val_loss: 33.9190\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5555 - val_loss: 36.5952\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5157 - val_loss: 38.1417\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5060 - val_loss: 32.5233\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5224 - val_loss: 39.8769\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4030 - val_loss: 37.7240\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4701 - val_loss: 45.6974\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4087 - val_loss: 46.9201\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4328 - val_loss: 33.1073\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4515 - val_loss: 36.8701\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.5318 - val_loss: 32.9801\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4591 - val_loss: 36.0817\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.4474 - val_loss: 34.7708\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.6407 - val_loss: 34.9467\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3048 - val_loss: 33.4740\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3610 - val_loss: 33.6153\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2606 - val_loss: 35.4347\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3877 - val_loss: 35.0417\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3664 - val_loss: 33.6026\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4070 - val_loss: 33.6333\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 29.4509 - val_loss: 40.5112\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.3471 - val_loss: 34.0304\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3565 - val_loss: 35.8595\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4109 - val_loss: 42.2825\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.5128 - val_loss: 36.3310\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4105 - val_loss: 40.4870\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4771 - val_loss: 34.4896\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4176 - val_loss: 35.6147\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3641 - val_loss: 35.1912\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3247 - val_loss: 32.3402\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4054 - val_loss: 33.0311\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4232 - val_loss: 40.1982\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2942 - val_loss: 33.0601\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2861 - val_loss: 33.8727\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3501 - val_loss: 34.5729\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2736 - val_loss: 47.9384\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3937 - val_loss: 42.8562\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3567 - val_loss: 35.5571\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3474 - val_loss: 34.3300\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4718 - val_loss: 34.8396\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2753 - val_loss: 36.3669\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3591 - val_loss: 33.3702\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3160 - val_loss: 35.2793\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2988 - val_loss: 35.7731\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3572 - val_loss: 34.1710\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1982 - val_loss: 48.7918\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1954 - val_loss: 34.5113\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3427 - val_loss: 38.4884\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2503 - val_loss: 33.1491\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2134 - val_loss: 38.6727\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3460 - val_loss: 32.8866\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2640 - val_loss: 44.5113\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3108 - val_loss: 37.3096\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2823 - val_loss: 34.7477\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2794 - val_loss: 33.5844\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3136 - val_loss: 33.5132\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3753 - val_loss: 34.3247\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3879 - val_loss: 33.2157\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.4959 - val_loss: 33.1582\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3456 - val_loss: 33.8107\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3646 - val_loss: 36.3349\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3210 - val_loss: 34.6253\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.3425 - val_loss: 37.7822\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3302 - val_loss: 35.3122\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1814 - val_loss: 40.7724\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3131 - val_loss: 42.9925\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2629 - val_loss: 34.9657\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3499 - val_loss: 34.6548\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.3127 - val_loss: 35.6023\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2736 - val_loss: 33.0062\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2051 - val_loss: 33.5807\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2613 - val_loss: 35.9598\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2275 - val_loss: 35.4459\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1957 - val_loss: 35.5604\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1783 - val_loss: 34.9240\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0825 - val_loss: 34.8113\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1693 - val_loss: 35.1335\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2438 - val_loss: 35.1882\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2111 - val_loss: 35.3808\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2081 - val_loss: 48.8772\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.2797 - val_loss: 36.3894\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2871 - val_loss: 36.6216\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1766 - val_loss: 37.0173\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2937 - val_loss: 38.9304\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2351 - val_loss: 34.4679\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1755 - val_loss: 38.5472\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1802 - val_loss: 37.1455\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2051 - val_loss: 33.7721\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0994 - val_loss: 33.7020\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.0993 - val_loss: 36.4149\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1805 - val_loss: 34.2043\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2159 - val_loss: 34.5136\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1204 - val_loss: 34.7128\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.0967 - val_loss: 35.2402\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1812 - val_loss: 55.1624\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.2368 - val_loss: 37.7709\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1698 - val_loss: 37.4947\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1877 - val_loss: 38.1170\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 29.1427 - val_loss: 35.6941\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 28.9829 - val_loss: 34.0244\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 28.9916 - val_loss: 35.8524\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 29.1376 - val_loss: 33.8903\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abec5ae8-270b-46fb-d96a-b5d00bb43502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.31741823830519916 \n",
            "MAE:  4.300148526873868 \n",
            "SD:  5.81287665782736\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "c51b6fb0-7ea2-43d3-e8e6-ac8bdf62f038"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f07cefb-5bee-4cc9-ab37-6a480d5ed4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  1.053243898867493 \n",
            "Ensemble_std:  6.1223614148994905\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "\bBP_hv3_7(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}