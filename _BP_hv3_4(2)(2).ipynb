{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/_BP_hv3_4(2)(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiiiBla2-j1S"
      },
      "source": [
        "# batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCoux5AOZnK",
        "outputId": "05ec15c5-4d35-4fbf-df2c-7869c9eda218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version :  3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]\n",
            "TensorFlow version :  2.3.0\n",
            "Keras version :  2.4.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러오기\n",
        "# train_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/train_shuffled_raw_v1.mat')\n",
        "# test_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/test_not_shuffled_raw_v1.mat')\n",
        "\n",
        "train_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/new/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/new/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "b570a293-9e53-473a-f3ed-a9b19951a83d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "8f731ca9-0203-46e7-87ab-30015e694029"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "8abc45d0-bd5c-49cd-8d12-1cde358bdca2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "outputs": [],
      "source": [
        "#parameter\n",
        "batch_size = 64\n",
        "epochs = 300\n",
        "lrate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN",
        "outputId": "85078f6d-be5b-48f8-f91c-aa3862a5239f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8SHBwBOZnO"
      },
      "outputs": [],
      "source": [
        "# model = model1()\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGT6-7NcOZnO",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 6646.5903 - val_loss: 253.5771\n",
            "Epoch 2/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 245.8407 - val_loss: 152.2613\n",
            "Epoch 3/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 123.3876 - val_loss: 352.7445\n",
            "Epoch 4/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 113.7495 - val_loss: 123.4600\n",
            "Epoch 5/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 108.7804 - val_loss: 118.8711\n",
            "Epoch 6/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 102.9399 - val_loss: 115.4290\n",
            "Epoch 7/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 99.1633 - val_loss: 120.3344\n",
            "Epoch 8/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 96.4086 - val_loss: 129.7985\n",
            "Epoch 9/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 95.7813 - val_loss: 144.6194\n",
            "Epoch 10/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 94.0837 - val_loss: 143.7026\n",
            "Epoch 11/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 92.8695 - val_loss: 204.1030\n",
            "Epoch 12/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 91.2490 - val_loss: 95.6998\n",
            "Epoch 13/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 89.7126 - val_loss: 104.2893\n",
            "Epoch 14/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 90.0689 - val_loss: 101.6354\n",
            "Epoch 15/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 90.3039 - val_loss: 131.1857\n",
            "Epoch 16/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 87.1582 - val_loss: 100.0078\n",
            "Epoch 17/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 86.2483 - val_loss: 149.0148\n",
            "Epoch 18/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 85.3578 - val_loss: 103.0412\n",
            "Epoch 19/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 85.0316 - val_loss: 91.9435\n",
            "Epoch 20/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 84.0005 - val_loss: 97.9854\n",
            "Epoch 21/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 84.6492 - val_loss: 104.3752\n",
            "Epoch 22/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 84.5019 - val_loss: 109.7391\n",
            "Epoch 23/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 83.6938 - val_loss: 94.5641\n",
            "Epoch 24/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 83.0694 - val_loss: 104.8820\n",
            "Epoch 25/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 83.1318 - val_loss: 116.3655\n",
            "Epoch 26/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 82.4806 - val_loss: 110.4559\n",
            "Epoch 27/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 81.0613 - val_loss: 113.2266\n",
            "Epoch 28/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 81.4189 - val_loss: 133.6381\n",
            "Epoch 29/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 81.3843 - val_loss: 97.5042\n",
            "Epoch 30/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 80.6533 - val_loss: 122.4871\n",
            "Epoch 31/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 79.9947 - val_loss: 119.7866\n",
            "Epoch 32/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 79.8397 - val_loss: 85.2106\n",
            "Epoch 33/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 79.9819 - val_loss: 100.4386\n",
            "Epoch 34/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 79.9611 - val_loss: 102.6348\n",
            "Epoch 35/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 79.6714 - val_loss: 104.7575\n",
            "Epoch 36/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 79.0189 - val_loss: 108.0513\n",
            "Epoch 37/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 78.8077 - val_loss: 101.5636\n",
            "Epoch 38/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 78.5184 - val_loss: 95.8992\n",
            "Epoch 39/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 78.4081 - val_loss: 90.8319\n",
            "Epoch 40/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 78.0098 - val_loss: 93.7826\n",
            "Epoch 41/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.8355 - val_loss: 98.6898\n",
            "Epoch 42/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.4503 - val_loss: 100.0787\n",
            "Epoch 43/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.8802 - val_loss: 89.5143\n",
            "Epoch 44/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.2477 - val_loss: 90.0822\n",
            "Epoch 45/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.9020 - val_loss: 104.2654\n",
            "Epoch 46/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 78.3231 - val_loss: 100.9363\n",
            "Epoch 47/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 78.6895 - val_loss: 94.4810\n",
            "Epoch 48/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 78.2564 - val_loss: 89.4800\n",
            "Epoch 49/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.9580 - val_loss: 124.3633\n",
            "Epoch 50/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.5033 - val_loss: 141.3918\n",
            "Epoch 51/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 77.2939 - val_loss: 91.7848\n",
            "Epoch 52/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 77.2856 - val_loss: 95.1665\n",
            "Epoch 53/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 77.6342 - val_loss: 109.1492\n",
            "Epoch 54/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 77.0435 - val_loss: 86.2495\n",
            "Epoch 55/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.3278 - val_loss: 94.7007\n",
            "Epoch 56/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.9556 - val_loss: 98.6480\n",
            "Epoch 57/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 77.1621 - val_loss: 87.4774\n",
            "Epoch 58/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 76.9058 - val_loss: 137.2591\n",
            "Epoch 59/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 76.9070 - val_loss: 91.3342\n",
            "Epoch 60/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 76.6342 - val_loss: 89.9163\n",
            "Epoch 61/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 76.3634 - val_loss: 86.3165\n",
            "Epoch 62/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.0462 - val_loss: 124.7922\n",
            "Epoch 63/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.4508 - val_loss: 89.1493\n",
            "Epoch 64/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.1538 - val_loss: 82.0060\n",
            "Epoch 65/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.6358 - val_loss: 88.5882\n",
            "Epoch 66/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.5816 - val_loss: 90.5395\n",
            "Epoch 67/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.5454 - val_loss: 102.0257\n",
            "Epoch 68/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.3716 - val_loss: 140.0035\n",
            "Epoch 69/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.9719 - val_loss: 99.5123\n",
            "Epoch 70/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.7934 - val_loss: 156.4493\n",
            "Epoch 71/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.8968 - val_loss: 188.6467\n",
            "Epoch 72/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.9923 - val_loss: 100.5561\n",
            "Epoch 73/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.6911 - val_loss: 87.1855\n",
            "Epoch 74/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.2296 - val_loss: 125.3671\n",
            "Epoch 75/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.3279 - val_loss: 98.7902\n",
            "Epoch 76/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.4423 - val_loss: 91.3555\n",
            "Epoch 77/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.5759 - val_loss: 106.6130\n",
            "Epoch 78/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.4857 - val_loss: 116.9700\n",
            "Epoch 79/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.7121 - val_loss: 104.6571\n",
            "Epoch 80/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.8431 - val_loss: 84.9451\n",
            "Epoch 81/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.5048 - val_loss: 92.0040\n",
            "Epoch 82/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.9154 - val_loss: 81.2796\n",
            "Epoch 83/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.6184 - val_loss: 95.6935\n",
            "Epoch 84/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.0193 - val_loss: 138.2194\n",
            "Epoch 85/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.5516 - val_loss: 87.7245\n",
            "Epoch 86/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.0502 - val_loss: 95.8695\n",
            "Epoch 87/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.2047 - val_loss: 90.2299\n",
            "Epoch 88/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.8931 - val_loss: 124.8523\n",
            "Epoch 89/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.3646 - val_loss: 87.5147\n",
            "Epoch 90/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.1735 - val_loss: 83.4748\n",
            "Epoch 91/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.9982 - val_loss: 110.0218\n",
            "Epoch 92/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.9961 - val_loss: 111.7140\n",
            "Epoch 93/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.6364 - val_loss: 85.3392\n",
            "Epoch 94/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.5923 - val_loss: 95.5700\n",
            "Epoch 95/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.1701 - val_loss: 80.3677\n",
            "Epoch 96/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.2161 - val_loss: 85.7759\n",
            "Epoch 97/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 73.5280 - val_loss: 88.3128\n",
            "Epoch 98/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 74.1892 - val_loss: 96.2692\n",
            "Epoch 99/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.4248 - val_loss: 93.3849\n",
            "Epoch 100/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.9342 - val_loss: 103.5961\n",
            "Epoch 101/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.6374 - val_loss: 94.4789\n",
            "Epoch 102/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.3233 - val_loss: 96.7532\n",
            "Epoch 103/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.9392 - val_loss: 93.9733\n",
            "Epoch 104/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.9265 - val_loss: 112.8155\n",
            "Epoch 105/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.4902 - val_loss: 82.1855\n",
            "Epoch 106/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.7639 - val_loss: 81.1341\n",
            "Epoch 107/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.1150 - val_loss: 88.5557\n",
            "Epoch 108/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.3542 - val_loss: 84.3100\n",
            "Epoch 109/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.1493 - val_loss: 82.9839\n",
            "Epoch 110/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.6081 - val_loss: 81.4566\n",
            "Epoch 111/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.6767 - val_loss: 89.8991\n",
            "Epoch 112/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.7007 - val_loss: 82.0686\n",
            "Epoch 113/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.3451 - val_loss: 82.8644\n",
            "Epoch 114/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.4596 - val_loss: 78.4460\n",
            "Epoch 115/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.0649 - val_loss: 104.6270\n",
            "Epoch 116/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.6903 - val_loss: 108.1428\n",
            "Epoch 117/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 72.2068 - val_loss: 100.2202\n",
            "Epoch 118/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.5285 - val_loss: 84.9103\n",
            "Epoch 119/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.9710 - val_loss: 86.4527\n",
            "Epoch 120/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.6328 - val_loss: 80.2263\n",
            "Epoch 121/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.5972 - val_loss: 92.2841\n",
            "Epoch 122/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.8121 - val_loss: 87.9396\n",
            "Epoch 123/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.4443 - val_loss: 82.4907\n",
            "Epoch 124/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.4819 - val_loss: 85.8777\n",
            "Epoch 125/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.0954 - val_loss: 82.1470\n",
            "Epoch 126/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.6415 - val_loss: 91.2560\n",
            "Epoch 127/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.9256 - val_loss: 92.7338\n",
            "Epoch 128/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.3044 - val_loss: 93.6334\n",
            "Epoch 129/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.2218 - val_loss: 92.4768\n",
            "Epoch 130/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.3730 - val_loss: 85.6770\n",
            "Epoch 131/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.4170 - val_loss: 89.9293\n",
            "Epoch 132/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.2097 - val_loss: 81.3226\n",
            "Epoch 133/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.8425 - val_loss: 80.5406\n",
            "Epoch 134/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.2057 - val_loss: 83.4669\n",
            "Epoch 135/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.9830 - val_loss: 110.6058\n",
            "Epoch 136/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.7217 - val_loss: 79.8582\n",
            "Epoch 137/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.9352 - val_loss: 89.8924\n",
            "Epoch 138/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.1936 - val_loss: 88.1518\n",
            "Epoch 139/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.9038 - val_loss: 87.2891\n",
            "Epoch 140/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.4362 - val_loss: 82.6321\n",
            "Epoch 141/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.4113 - val_loss: 93.1175\n",
            "Epoch 142/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.5259 - val_loss: 82.7488\n",
            "Epoch 143/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.0734 - val_loss: 83.2017\n",
            "Epoch 144/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.7006 - val_loss: 77.6268\n",
            "Epoch 145/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.5429 - val_loss: 87.6388\n",
            "Epoch 146/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.5616 - val_loss: 83.5472\n",
            "Epoch 147/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.3726 - val_loss: 86.3427\n",
            "Epoch 148/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.3695 - val_loss: 83.0018\n",
            "Epoch 149/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.1593 - val_loss: 86.0239\n",
            "Epoch 150/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.3761 - val_loss: 84.4649\n",
            "Epoch 151/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.3183 - val_loss: 84.0961\n",
            "Epoch 152/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.7850 - val_loss: 81.5653\n",
            "Epoch 153/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.3758 - val_loss: 82.1392\n",
            "Epoch 154/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.2429 - val_loss: 83.4581\n",
            "Epoch 155/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.2525 - val_loss: 85.1668\n",
            "Epoch 156/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.1406 - val_loss: 93.4038\n",
            "Epoch 157/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.3591 - val_loss: 108.2550\n",
            "Epoch 158/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.8380 - val_loss: 85.5673\n",
            "Epoch 159/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.3647 - val_loss: 96.7681\n",
            "Epoch 160/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.7181 - val_loss: 89.7552\n",
            "Epoch 161/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.2259 - val_loss: 81.6630\n",
            "Epoch 162/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.7097 - val_loss: 87.3284\n",
            "Epoch 163/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.6474 - val_loss: 97.2415\n",
            "Epoch 164/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.5191 - val_loss: 91.0900\n",
            "Epoch 165/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.4892 - val_loss: 132.5862\n",
            "Epoch 166/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.4234 - val_loss: 96.2850\n",
            "Epoch 167/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.3047 - val_loss: 116.0593\n",
            "Epoch 168/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.2232 - val_loss: 84.2738\n",
            "Epoch 169/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.0098 - val_loss: 80.5496\n",
            "Epoch 170/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.5241 - val_loss: 100.7285\n",
            "Epoch 171/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.0465 - val_loss: 79.1130\n",
            "Epoch 172/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.9108 - val_loss: 90.6004\n",
            "Epoch 173/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.8085 - val_loss: 79.7724\n",
            "Epoch 174/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.3114 - val_loss: 178.0072\n",
            "Epoch 175/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.6552 - val_loss: 80.3956\n",
            "Epoch 176/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.8385 - val_loss: 92.5799\n",
            "Epoch 177/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.3398 - val_loss: 85.5734\n",
            "Epoch 178/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.3399 - val_loss: 94.6228\n",
            "Epoch 179/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.4078 - val_loss: 123.0952\n",
            "Epoch 180/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.2107 - val_loss: 79.1337\n",
            "Epoch 181/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.5811 - val_loss: 84.4741\n",
            "Epoch 182/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.6083 - val_loss: 98.2852\n",
            "Epoch 183/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.4764 - val_loss: 90.5434\n",
            "Epoch 184/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.3030 - val_loss: 98.2418\n",
            "Epoch 185/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.5990 - val_loss: 86.3781\n",
            "Epoch 186/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.2650 - val_loss: 94.2522\n",
            "Epoch 187/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.3734 - val_loss: 85.2261\n",
            "Epoch 188/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.0141 - val_loss: 107.6271\n",
            "Epoch 189/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.1021 - val_loss: 83.6183\n",
            "Epoch 190/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.0881 - val_loss: 78.7773\n",
            "Epoch 191/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.9561 - val_loss: 88.1109\n",
            "Epoch 192/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.9586 - val_loss: 81.0653\n",
            "Epoch 193/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.9660 - val_loss: 85.3257\n",
            "Epoch 194/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.8188 - val_loss: 78.4653\n",
            "Epoch 195/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.6699 - val_loss: 82.8137\n",
            "Epoch 196/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.8013 - val_loss: 79.7829\n",
            "Epoch 197/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.5672 - val_loss: 84.6050\n",
            "Epoch 198/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.6091 - val_loss: 86.6681\n",
            "Epoch 199/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.3878 - val_loss: 113.5219\n",
            "Epoch 200/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.9872 - val_loss: 99.4723\n",
            "Epoch 201/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.7827 - val_loss: 81.6512\n",
            "Epoch 202/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.8658 - val_loss: 84.6142\n",
            "Epoch 203/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.3630 - val_loss: 83.0794\n",
            "Epoch 204/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.8787 - val_loss: 93.3805\n",
            "Epoch 205/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.2992 - val_loss: 90.9711\n",
            "Epoch 206/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.6088 - val_loss: 87.7991\n",
            "Epoch 207/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.7552 - val_loss: 79.5379\n",
            "Epoch 208/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.6332 - val_loss: 93.9629\n",
            "Epoch 209/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.3989 - val_loss: 92.1899\n",
            "Epoch 210/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.6159 - val_loss: 87.4811\n",
            "Epoch 211/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.4406 - val_loss: 88.3411\n",
            "Epoch 212/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.2519 - val_loss: 79.2066\n",
            "Epoch 213/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.4230 - val_loss: 87.5581\n",
            "Epoch 214/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.9131 - val_loss: 92.8302\n",
            "Epoch 215/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.1966 - val_loss: 81.9644\n",
            "Epoch 216/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.0433 - val_loss: 90.6978\n",
            "Epoch 217/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.3042 - val_loss: 107.0621\n",
            "Epoch 218/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.5507 - val_loss: 97.0786\n",
            "Epoch 219/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.6060 - val_loss: 89.6363\n",
            "Epoch 220/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.1534 - val_loss: 83.5052\n",
            "Epoch 221/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.4239 - val_loss: 88.2395\n",
            "Epoch 222/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.9089 - val_loss: 91.2971\n",
            "Epoch 223/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.8245 - val_loss: 86.1116\n",
            "Epoch 224/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.6568 - val_loss: 84.8208\n",
            "Epoch 225/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.7607 - val_loss: 82.4010\n",
            "Epoch 226/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.9764 - val_loss: 80.5115\n",
            "Epoch 227/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.6088 - val_loss: 77.9713\n",
            "Epoch 228/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.8757 - val_loss: 104.3589\n",
            "Epoch 229/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.2007 - val_loss: 102.8638\n",
            "Epoch 230/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.3305 - val_loss: 98.8353\n",
            "Epoch 231/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.9651 - val_loss: 106.9591\n",
            "Epoch 232/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.7760 - val_loss: 82.8720\n",
            "Epoch 233/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.7272 - val_loss: 88.8349\n",
            "Epoch 234/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.6680 - val_loss: 84.3660\n",
            "Epoch 235/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.6443 - val_loss: 94.6924\n",
            "Epoch 236/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.6812 - val_loss: 89.2295\n",
            "Epoch 237/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.5463 - val_loss: 84.0361\n",
            "Epoch 238/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.9891 - val_loss: 87.0327\n",
            "Epoch 239/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.6683 - val_loss: 82.9679\n",
            "Epoch 240/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.8541 - val_loss: 81.1850\n",
            "Epoch 241/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.3668 - val_loss: 99.6615\n",
            "Epoch 242/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.8199 - val_loss: 83.0165\n",
            "Epoch 243/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.4951 - val_loss: 91.1923\n",
            "Epoch 244/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.4272 - val_loss: 87.4351\n",
            "Epoch 245/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.7713 - val_loss: 107.1351\n",
            "Epoch 246/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.7409 - val_loss: 81.8577\n",
            "Epoch 247/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.8573 - val_loss: 94.5829\n",
            "Epoch 248/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.7962 - val_loss: 91.2540\n",
            "Epoch 249/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.7222 - val_loss: 80.5010\n",
            "Epoch 250/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.5255 - val_loss: 92.3033\n",
            "Epoch 251/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.6547 - val_loss: 80.0119\n",
            "Epoch 252/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 67.7552 - val_loss: 81.7629\n",
            "Epoch 253/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 67.3344 - val_loss: 80.8823\n",
            "Epoch 254/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 67.3444 - val_loss: 92.3847\n",
            "Epoch 255/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 67.2527 - val_loss: 86.3329\n",
            "Epoch 256/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 67.3632 - val_loss: 89.2056\n",
            "Epoch 257/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.4484 - val_loss: 84.8831\n",
            "Epoch 258/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.4785 - val_loss: 77.6787\n",
            "Epoch 259/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.5769 - val_loss: 101.1763\n",
            "Epoch 260/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.3194 - val_loss: 81.8297\n",
            "Epoch 261/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.4349 - val_loss: 84.6288\n",
            "Epoch 262/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.2579 - val_loss: 87.7521\n",
            "Epoch 263/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.3513 - val_loss: 88.4867\n",
            "Epoch 264/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.5473 - val_loss: 82.8792\n",
            "Epoch 265/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.2971 - val_loss: 83.8708\n",
            "Epoch 266/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.1357 - val_loss: 87.5642\n",
            "Epoch 267/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.0080 - val_loss: 84.4142\n",
            "Epoch 268/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.4224 - val_loss: 82.9290\n",
            "Epoch 269/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.9652 - val_loss: 83.5376\n",
            "Epoch 270/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8558 - val_loss: 105.0426\n",
            "Epoch 271/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8893 - val_loss: 82.6926\n",
            "Epoch 272/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.0524 - val_loss: 86.7349\n",
            "Epoch 273/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.5987 - val_loss: 89.0422\n",
            "Epoch 274/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.9102 - val_loss: 78.7495\n",
            "Epoch 275/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.6401 - val_loss: 79.5164\n",
            "Epoch 276/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.7371 - val_loss: 81.4666\n",
            "Epoch 277/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.8079 - val_loss: 83.3559\n",
            "Epoch 278/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.5307 - val_loss: 80.8641\n",
            "Epoch 279/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8847 - val_loss: 90.3040\n",
            "Epoch 280/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.9249 - val_loss: 79.6104\n",
            "Epoch 281/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.3733 - val_loss: 84.8222\n",
            "Epoch 282/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.4436 - val_loss: 92.3358\n",
            "Epoch 283/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.5211 - val_loss: 90.2510\n",
            "Epoch 284/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.7222 - val_loss: 79.4551\n",
            "Epoch 285/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.6165 - val_loss: 82.2344\n",
            "Epoch 286/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.3720 - val_loss: 85.7225\n",
            "Epoch 287/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.5622 - val_loss: 80.7794\n",
            "Epoch 288/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.4593 - val_loss: 83.3119\n",
            "Epoch 289/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.4833 - val_loss: 82.0725\n",
            "Epoch 290/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.9340 - val_loss: 79.7769\n",
            "Epoch 291/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.7706 - val_loss: 81.1764\n",
            "Epoch 292/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.3963 - val_loss: 108.8582\n",
            "Epoch 293/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.4745 - val_loss: 81.5773\n",
            "Epoch 294/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.4394 - val_loss: 80.6206\n",
            "Epoch 295/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.4220 - val_loss: 83.5042\n",
            "Epoch 296/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.3719 - val_loss: 86.6078\n",
            "Epoch 297/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.4317 - val_loss: 82.4742\n",
            "Epoch 298/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.2763 - val_loss: 77.9081\n",
            "Epoch 299/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.5435 - val_loss: 96.2963\n",
            "Epoch 300/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.1604 - val_loss: 82.8566\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -1.971501554671245 \n",
            "MAE:  6.826280874615817 \n",
            "SD:  8.88649313219411\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su2Sj5jZCSwH",
        "outputId": "4a85f233-7730-4d19-a30a-66ad769ace4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_24 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPRh6v-mCSwH",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 7920.2974 - val_loss: 2244.9832\n",
            "Epoch 2/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 661.2776 - val_loss: 307.3690\n",
            "Epoch 3/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 157.8063 - val_loss: 138.5564\n",
            "Epoch 4/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 131.3609 - val_loss: 137.7027\n",
            "Epoch 5/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 116.9558 - val_loss: 122.2407\n",
            "Epoch 6/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 110.5673 - val_loss: 110.1981\n",
            "Epoch 7/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 108.8596 - val_loss: 135.1769\n",
            "Epoch 8/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 106.5135 - val_loss: 117.7561\n",
            "Epoch 9/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 101.4379 - val_loss: 151.3952\n",
            "Epoch 10/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 101.3305 - val_loss: 129.2046\n",
            "Epoch 11/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 100.5297 - val_loss: 148.5326\n",
            "Epoch 12/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 99.3164 - val_loss: 122.4560\n",
            "Epoch 13/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 94.7634 - val_loss: 139.5969\n",
            "Epoch 14/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 92.2203 - val_loss: 100.2526\n",
            "Epoch 15/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 90.2731 - val_loss: 103.1632\n",
            "Epoch 16/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 89.7764 - val_loss: 170.0602\n",
            "Epoch 17/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 88.2840 - val_loss: 99.2692\n",
            "Epoch 18/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 87.6333 - val_loss: 93.6165\n",
            "Epoch 19/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 86.4826 - val_loss: 163.8207\n",
            "Epoch 20/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 86.6883 - val_loss: 93.5300\n",
            "Epoch 21/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 85.3887 - val_loss: 100.8417\n",
            "Epoch 22/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 84.5785 - val_loss: 145.1441\n",
            "Epoch 23/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 83.4608 - val_loss: 120.1877\n",
            "Epoch 24/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 83.3114 - val_loss: 102.9657\n",
            "Epoch 25/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 82.4166 - val_loss: 92.0361\n",
            "Epoch 26/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 81.9079 - val_loss: 90.8246\n",
            "Epoch 27/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 81.4160 - val_loss: 94.5974\n",
            "Epoch 28/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 81.2314 - val_loss: 99.1408\n",
            "Epoch 29/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 80.1597 - val_loss: 86.8996\n",
            "Epoch 30/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 79.4084 - val_loss: 99.8081\n",
            "Epoch 31/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 79.5307 - val_loss: 96.5732\n",
            "Epoch 32/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 79.2828 - val_loss: 94.0582\n",
            "Epoch 33/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 79.1380 - val_loss: 89.1068\n",
            "Epoch 34/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 78.8707 - val_loss: 90.0883\n",
            "Epoch 35/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 78.2555 - val_loss: 112.3754\n",
            "Epoch 36/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 78.3860 - val_loss: 94.4724\n",
            "Epoch 37/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.6349 - val_loss: 89.6046\n",
            "Epoch 38/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.7755 - val_loss: 153.5480\n",
            "Epoch 39/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.1135 - val_loss: 87.0365\n",
            "Epoch 40/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 76.9826 - val_loss: 121.0957\n",
            "Epoch 41/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.8950 - val_loss: 86.2790\n",
            "Epoch 42/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.8732 - val_loss: 106.2859\n",
            "Epoch 43/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.0605 - val_loss: 92.6543\n",
            "Epoch 44/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.1056 - val_loss: 95.4710\n",
            "Epoch 45/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.2603 - val_loss: 106.7823\n",
            "Epoch 46/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.7028 - val_loss: 95.3813\n",
            "Epoch 47/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.9354 - val_loss: 100.5157\n",
            "Epoch 48/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.4484 - val_loss: 98.9280\n",
            "Epoch 49/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.6929 - val_loss: 90.8802\n",
            "Epoch 50/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.6827 - val_loss: 114.2238\n",
            "Epoch 51/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.7035 - val_loss: 102.6862\n",
            "Epoch 52/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.0102 - val_loss: 99.2385\n",
            "Epoch 53/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.7272 - val_loss: 121.2449\n",
            "Epoch 54/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.0194 - val_loss: 91.6428\n",
            "Epoch 55/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.2067 - val_loss: 110.0180\n",
            "Epoch 56/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.6841 - val_loss: 93.2319\n",
            "Epoch 57/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.8676 - val_loss: 86.5722\n",
            "Epoch 58/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.6579 - val_loss: 89.7910\n",
            "Epoch 59/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.1389 - val_loss: 109.5439\n",
            "Epoch 60/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 74.0941 - val_loss: 100.8006\n",
            "Epoch 61/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 73.9808 - val_loss: 95.7034\n",
            "Epoch 62/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 74.3075 - val_loss: 154.1552\n",
            "Epoch 63/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.5076 - val_loss: 94.3933\n",
            "Epoch 64/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.4730 - val_loss: 100.7398\n",
            "Epoch 65/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.0759 - val_loss: 130.3561\n",
            "Epoch 66/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.8711 - val_loss: 103.5271\n",
            "Epoch 67/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.2143 - val_loss: 87.6075\n",
            "Epoch 68/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 74.1205 - val_loss: 96.5307\n",
            "Epoch 69/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 74.0963 - val_loss: 84.8141\n",
            "Epoch 70/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.6703 - val_loss: 85.0301\n",
            "Epoch 71/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.2966 - val_loss: 117.3763\n",
            "Epoch 72/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.2951 - val_loss: 88.7824\n",
            "Epoch 73/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.8392 - val_loss: 164.6320\n",
            "Epoch 74/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.2106 - val_loss: 94.4779\n",
            "Epoch 75/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.9807 - val_loss: 93.9899\n",
            "Epoch 76/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.8350 - val_loss: 84.7469\n",
            "Epoch 77/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.7588 - val_loss: 84.5875\n",
            "Epoch 78/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.8738 - val_loss: 92.4849\n",
            "Epoch 79/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.0895 - val_loss: 93.4295\n",
            "Epoch 80/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 72.0370 - val_loss: 84.4268\n",
            "Epoch 81/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 72.8347 - val_loss: 88.0716\n",
            "Epoch 82/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 72.3590 - val_loss: 94.8522\n",
            "Epoch 83/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 72.0300 - val_loss: 92.0922\n",
            "Epoch 84/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 72.5240 - val_loss: 89.4815\n",
            "Epoch 85/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 72.1151 - val_loss: 84.7698\n",
            "Epoch 86/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 72.1827 - val_loss: 98.8165\n",
            "Epoch 87/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.9986 - val_loss: 103.7636\n",
            "Epoch 88/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 71.8788 - val_loss: 93.5904\n",
            "Epoch 89/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 72.1518 - val_loss: 92.0138\n",
            "Epoch 90/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.8644 - val_loss: 86.3709\n",
            "Epoch 91/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 72.0855 - val_loss: 85.3633\n",
            "Epoch 92/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 72.0524 - val_loss: 103.3829\n",
            "Epoch 93/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.8604 - val_loss: 90.7551\n",
            "Epoch 94/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.5792 - val_loss: 83.8893\n",
            "Epoch 95/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.6022 - val_loss: 83.6290\n",
            "Epoch 96/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.4085 - val_loss: 84.9431\n",
            "Epoch 97/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.5456 - val_loss: 99.7204\n",
            "Epoch 98/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.4441 - val_loss: 93.4362\n",
            "Epoch 99/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.4958 - val_loss: 101.7395\n",
            "Epoch 100/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.3899 - val_loss: 90.4713\n",
            "Epoch 101/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.2625 - val_loss: 81.9614\n",
            "Epoch 102/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.0487 - val_loss: 90.0750\n",
            "Epoch 103/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.4418 - val_loss: 86.3200\n",
            "Epoch 104/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.1868 - val_loss: 83.8037\n",
            "Epoch 105/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.8747 - val_loss: 141.3369\n",
            "Epoch 106/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.2929 - val_loss: 87.1139\n",
            "Epoch 107/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.4641 - val_loss: 107.8966\n",
            "Epoch 108/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.4398 - val_loss: 94.5756\n",
            "Epoch 109/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.8689 - val_loss: 118.2047\n",
            "Epoch 110/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 71.0227 - val_loss: 134.0205\n",
            "Epoch 111/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.3151 - val_loss: 83.3211\n",
            "Epoch 112/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.1557 - val_loss: 87.7561\n",
            "Epoch 113/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.1456 - val_loss: 85.8213\n",
            "Epoch 114/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.1889 - val_loss: 84.6834\n",
            "Epoch 115/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.9096 - val_loss: 87.9092\n",
            "Epoch 116/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.5936 - val_loss: 83.8821\n",
            "Epoch 117/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.5925 - val_loss: 86.0622\n",
            "Epoch 118/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.9814 - val_loss: 98.8147\n",
            "Epoch 119/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.5682 - val_loss: 82.9307\n",
            "Epoch 120/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.7338 - val_loss: 81.5305\n",
            "Epoch 121/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.6232 - val_loss: 144.2761\n",
            "Epoch 122/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.6046 - val_loss: 85.2563\n",
            "Epoch 123/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.4195 - val_loss: 99.2606\n",
            "Epoch 124/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.6324 - val_loss: 82.0932\n",
            "Epoch 125/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.3228 - val_loss: 82.1907\n",
            "Epoch 126/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.2292 - val_loss: 82.2118\n",
            "Epoch 127/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.5350 - val_loss: 92.2640\n",
            "Epoch 128/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.3988 - val_loss: 90.2870\n",
            "Epoch 129/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.1451 - val_loss: 82.5569\n",
            "Epoch 130/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.2231 - val_loss: 89.1514\n",
            "Epoch 131/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.0480 - val_loss: 88.3633\n",
            "Epoch 132/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.3890 - val_loss: 81.6195\n",
            "Epoch 133/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.8299 - val_loss: 106.1169\n",
            "Epoch 134/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.9341 - val_loss: 90.6614\n",
            "Epoch 135/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.8314 - val_loss: 94.6644\n",
            "Epoch 136/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.0367 - val_loss: 116.9330\n",
            "Epoch 137/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.7339 - val_loss: 85.0843\n",
            "Epoch 138/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.8685 - val_loss: 81.8135\n",
            "Epoch 139/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.1754 - val_loss: 95.8439\n",
            "Epoch 140/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.0614 - val_loss: 87.8203\n",
            "Epoch 141/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.0574 - val_loss: 83.6945\n",
            "Epoch 142/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.7569 - val_loss: 136.2239\n",
            "Epoch 143/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.7096 - val_loss: 86.0144\n",
            "Epoch 144/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.7382 - val_loss: 83.7098\n",
            "Epoch 145/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.5552 - val_loss: 91.1440\n",
            "Epoch 146/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.5436 - val_loss: 93.2544\n",
            "Epoch 147/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.3556 - val_loss: 92.7735\n",
            "Epoch 148/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.4040 - val_loss: 88.0375\n",
            "Epoch 149/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.4655 - val_loss: 100.6874\n",
            "Epoch 150/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.2057 - val_loss: 85.5100\n",
            "Epoch 151/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.4494 - val_loss: 86.8970\n",
            "Epoch 152/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.9328 - val_loss: 101.2300\n",
            "Epoch 153/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.8643 - val_loss: 90.1127\n",
            "Epoch 154/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.8266 - val_loss: 86.4109\n",
            "Epoch 155/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.4005 - val_loss: 93.9990\n",
            "Epoch 156/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.2101 - val_loss: 79.6559\n",
            "Epoch 157/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.9398 - val_loss: 84.2412\n",
            "Epoch 158/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.2150 - val_loss: 82.8347\n",
            "Epoch 159/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.9922 - val_loss: 95.3387\n",
            "Epoch 160/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.9581 - val_loss: 95.7246\n",
            "Epoch 161/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.6473 - val_loss: 85.0336\n",
            "Epoch 162/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.6170 - val_loss: 78.9657\n",
            "Epoch 163/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.5083 - val_loss: 83.0441\n",
            "Epoch 164/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.0744 - val_loss: 78.8818\n",
            "Epoch 165/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.4887 - val_loss: 80.1120\n",
            "Epoch 166/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.7338 - val_loss: 82.9775\n",
            "Epoch 167/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.8849 - val_loss: 87.4651\n",
            "Epoch 168/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.2763 - val_loss: 109.9873\n",
            "Epoch 169/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.3438 - val_loss: 87.5724\n",
            "Epoch 170/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.5033 - val_loss: 84.4459\n",
            "Epoch 171/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.2311 - val_loss: 86.2291\n",
            "Epoch 172/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.3864 - val_loss: 80.6072\n",
            "Epoch 173/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.2568 - val_loss: 88.2180\n",
            "Epoch 174/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.4638 - val_loss: 85.9319\n",
            "Epoch 175/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.5306 - val_loss: 86.2094\n",
            "Epoch 176/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.1174 - val_loss: 83.2052\n",
            "Epoch 177/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.0375 - val_loss: 138.9009\n",
            "Epoch 178/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.5596 - val_loss: 91.0584\n",
            "Epoch 179/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.2424 - val_loss: 83.2843\n",
            "Epoch 180/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.3561 - val_loss: 83.7986\n",
            "Epoch 181/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.3796 - val_loss: 79.6982\n",
            "Epoch 182/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.5197 - val_loss: 98.3048\n",
            "Epoch 183/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.3106 - val_loss: 92.4319\n",
            "Epoch 184/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.1930 - val_loss: 91.4896\n",
            "Epoch 185/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.9036 - val_loss: 90.6229\n",
            "Epoch 186/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.7965 - val_loss: 91.4137\n",
            "Epoch 187/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.1400 - val_loss: 81.2340\n",
            "Epoch 188/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.8547 - val_loss: 80.0050\n",
            "Epoch 189/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.0613 - val_loss: 84.2819\n",
            "Epoch 190/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.9450 - val_loss: 94.6555\n",
            "Epoch 191/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.8474 - val_loss: 82.1908\n",
            "Epoch 192/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.8596 - val_loss: 91.0675\n",
            "Epoch 193/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.6561 - val_loss: 91.3273\n",
            "Epoch 194/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.8492 - val_loss: 80.6758\n",
            "Epoch 195/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.5067 - val_loss: 80.7316\n",
            "Epoch 196/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.5529 - val_loss: 90.3677\n",
            "Epoch 197/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.4830 - val_loss: 83.0098\n",
            "Epoch 198/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.5473 - val_loss: 86.9082\n",
            "Epoch 199/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.5606 - val_loss: 83.9504\n",
            "Epoch 200/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.3808 - val_loss: 83.3800\n",
            "Epoch 201/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.4829 - val_loss: 77.6568\n",
            "Epoch 202/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.5622 - val_loss: 88.4147\n",
            "Epoch 203/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.8051 - val_loss: 90.8373\n",
            "Epoch 204/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.8628 - val_loss: 85.8931\n",
            "Epoch 205/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.7603 - val_loss: 84.3680\n",
            "Epoch 206/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.6075 - val_loss: 90.2200\n",
            "Epoch 207/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.6592 - val_loss: 85.3650\n",
            "Epoch 208/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.3612 - val_loss: 84.6490\n",
            "Epoch 209/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.4583 - val_loss: 86.1546\n",
            "Epoch 210/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.3951 - val_loss: 78.9145\n",
            "Epoch 211/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.6174 - val_loss: 87.1386\n",
            "Epoch 212/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.2016 - val_loss: 92.9784\n",
            "Epoch 213/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.2035 - val_loss: 83.4584\n",
            "Epoch 214/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.1879 - val_loss: 86.5878\n",
            "Epoch 215/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.1399 - val_loss: 88.5023\n",
            "Epoch 216/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.0359 - val_loss: 83.9264\n",
            "Epoch 217/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.3962 - val_loss: 86.1277\n",
            "Epoch 218/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.5539 - val_loss: 98.8385\n",
            "Epoch 219/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.0536 - val_loss: 84.6010\n",
            "Epoch 220/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.2698 - val_loss: 80.0568\n",
            "Epoch 221/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.1482 - val_loss: 84.0251\n",
            "Epoch 222/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.3918 - val_loss: 81.6017\n",
            "Epoch 223/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.1699 - val_loss: 95.3257\n",
            "Epoch 224/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.0154 - val_loss: 111.1867\n",
            "Epoch 225/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.3829 - val_loss: 90.2305\n",
            "Epoch 226/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.1369 - val_loss: 88.2089\n",
            "Epoch 227/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.4610 - val_loss: 96.7321\n",
            "Epoch 228/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.2453 - val_loss: 80.9307\n",
            "Epoch 229/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.0581 - val_loss: 98.4841\n",
            "Epoch 230/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.0407 - val_loss: 82.4544\n",
            "Epoch 231/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8051 - val_loss: 88.7421\n",
            "Epoch 232/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8601 - val_loss: 85.3513\n",
            "Epoch 233/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.0333 - val_loss: 91.1978\n",
            "Epoch 234/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.2236 - val_loss: 79.1146\n",
            "Epoch 235/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.6546 - val_loss: 144.3316\n",
            "Epoch 236/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.5386 - val_loss: 81.7399\n",
            "Epoch 237/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.7806 - val_loss: 89.9553\n",
            "Epoch 238/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.9678 - val_loss: 85.4068\n",
            "Epoch 239/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.9269 - val_loss: 83.7812\n",
            "Epoch 240/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.9686 - val_loss: 82.3603\n",
            "Epoch 241/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8134 - val_loss: 97.6941\n",
            "Epoch 242/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 67.0137 - val_loss: 80.1484\n",
            "Epoch 243/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.6897 - val_loss: 81.8670\n",
            "Epoch 244/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.4490 - val_loss: 97.0091\n",
            "Epoch 245/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8989 - val_loss: 83.9734\n",
            "Epoch 246/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8599 - val_loss: 84.1114\n",
            "Epoch 247/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.9049 - val_loss: 82.8246\n",
            "Epoch 248/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.4616 - val_loss: 78.5429\n",
            "Epoch 249/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8910 - val_loss: 78.6431\n",
            "Epoch 250/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.7713 - val_loss: 82.5388\n",
            "Epoch 251/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.7212 - val_loss: 82.6373\n",
            "Epoch 252/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8725 - val_loss: 86.2141\n",
            "Epoch 253/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.3470 - val_loss: 89.5212\n",
            "Epoch 254/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.5735 - val_loss: 82.2936\n",
            "Epoch 255/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.5344 - val_loss: 80.0174\n",
            "Epoch 256/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.4872 - val_loss: 84.2499\n",
            "Epoch 257/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.7027 - val_loss: 84.3531\n",
            "Epoch 258/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.6209 - val_loss: 89.9538\n",
            "Epoch 259/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.4473 - val_loss: 80.6849\n",
            "Epoch 260/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.4629 - val_loss: 81.4581\n",
            "Epoch 261/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.5060 - val_loss: 88.0094\n",
            "Epoch 262/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.6443 - val_loss: 91.0856\n",
            "Epoch 263/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.0180 - val_loss: 89.6649\n",
            "Epoch 264/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.3008 - val_loss: 84.9572\n",
            "Epoch 265/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.4528 - val_loss: 87.4847\n",
            "Epoch 266/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.6226 - val_loss: 84.0045\n",
            "Epoch 267/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.5911 - val_loss: 100.0753\n",
            "Epoch 268/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.2977 - val_loss: 90.5275\n",
            "Epoch 269/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.4034 - val_loss: 94.8894\n",
            "Epoch 270/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.5887 - val_loss: 78.7164\n",
            "Epoch 271/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.3252 - val_loss: 78.1089\n",
            "Epoch 272/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.3694 - val_loss: 150.4418\n",
            "Epoch 273/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.2477 - val_loss: 94.0824\n",
            "Epoch 274/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.0815 - val_loss: 81.4428\n",
            "Epoch 275/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.2427 - val_loss: 81.8757\n",
            "Epoch 276/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.1395 - val_loss: 93.5168\n",
            "Epoch 277/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.3161 - val_loss: 80.9156\n",
            "Epoch 278/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.5978 - val_loss: 85.5266\n",
            "Epoch 279/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.0813 - val_loss: 85.7403\n",
            "Epoch 280/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.0646 - val_loss: 79.5723\n",
            "Epoch 281/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.5319 - val_loss: 81.6331\n",
            "Epoch 282/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.3602 - val_loss: 121.4714\n",
            "Epoch 283/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.1064 - val_loss: 82.9530\n",
            "Epoch 284/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 65.9136 - val_loss: 87.3810\n",
            "Epoch 285/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.3221 - val_loss: 85.5098\n",
            "Epoch 286/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.0342 - val_loss: 86.4176\n",
            "Epoch 287/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.1072 - val_loss: 80.7813\n",
            "Epoch 288/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.1939 - val_loss: 111.8322\n",
            "Epoch 289/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.0424 - val_loss: 110.1393\n",
            "Epoch 290/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.1006 - val_loss: 86.2311\n",
            "Epoch 291/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.0504 - val_loss: 92.9944\n",
            "Epoch 292/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.3292 - val_loss: 83.8212\n",
            "Epoch 293/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.1408 - val_loss: 92.2285\n",
            "Epoch 294/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.0919 - val_loss: 90.5849\n",
            "Epoch 295/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.0492 - val_loss: 83.1986\n",
            "Epoch 296/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.2718 - val_loss: 82.4422\n",
            "Epoch 297/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 65.9605 - val_loss: 86.0317\n",
            "Epoch 298/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 65.8616 - val_loss: 86.2007\n",
            "Epoch 299/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.2319 - val_loss: 79.5815\n",
            "Epoch 300/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.1126 - val_loss: 81.8928\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  0.7852245135910326 \n",
            "MAE:  6.707571142214754 \n",
            "SD:  9.015337638807955\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "outputId": "4a85f233-7730-4d19-a30a-66ad769ace4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_36 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXAOqd2CTsa",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 6533.9707 - val_loss: 355.0341\n",
            "Epoch 2/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 185.6121 - val_loss: 163.7294\n",
            "Epoch 3/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 115.5572 - val_loss: 229.4985\n",
            "Epoch 4/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 105.5518 - val_loss: 107.1111\n",
            "Epoch 5/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 100.9250 - val_loss: 133.8469\n",
            "Epoch 6/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 98.2660 - val_loss: 128.4071\n",
            "Epoch 7/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 95.1736 - val_loss: 229.6508\n",
            "Epoch 8/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 92.8209 - val_loss: 130.2637\n",
            "Epoch 9/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 91.7616 - val_loss: 154.3812\n",
            "Epoch 10/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 89.7345 - val_loss: 96.6930\n",
            "Epoch 11/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 88.1708 - val_loss: 102.0752\n",
            "Epoch 12/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 86.6204 - val_loss: 169.3817\n",
            "Epoch 13/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 86.2838 - val_loss: 103.9691\n",
            "Epoch 14/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 84.8659 - val_loss: 118.0356\n",
            "Epoch 15/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 83.9321 - val_loss: 185.0070\n",
            "Epoch 16/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 83.7051 - val_loss: 102.1272\n",
            "Epoch 17/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 82.5482 - val_loss: 97.6100\n",
            "Epoch 18/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 82.1398 - val_loss: 97.2877\n",
            "Epoch 19/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 81.6063 - val_loss: 107.9462\n",
            "Epoch 20/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 81.0620 - val_loss: 120.3178\n",
            "Epoch 21/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 81.1502 - val_loss: 117.7828\n",
            "Epoch 22/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 80.4737 - val_loss: 101.6755\n",
            "Epoch 23/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 80.4027 - val_loss: 100.7500\n",
            "Epoch 24/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 79.7393 - val_loss: 102.3042\n",
            "Epoch 25/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 79.9923 - val_loss: 86.3127\n",
            "Epoch 26/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 79.5085 - val_loss: 95.4062\n",
            "Epoch 27/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 79.2772 - val_loss: 94.1212\n",
            "Epoch 28/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 78.5154 - val_loss: 100.3859\n",
            "Epoch 29/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 78.2599 - val_loss: 148.0621\n",
            "Epoch 30/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 78.0740 - val_loss: 93.2454\n",
            "Epoch 31/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.9216 - val_loss: 93.5252\n",
            "Epoch 32/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 77.6029 - val_loss: 87.1444\n",
            "Epoch 33/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 77.7640 - val_loss: 89.9620\n",
            "Epoch 34/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.2924 - val_loss: 87.2362\n",
            "Epoch 35/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.1561 - val_loss: 88.1631\n",
            "Epoch 36/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 77.0884 - val_loss: 130.9514\n",
            "Epoch 37/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.8252 - val_loss: 91.9194\n",
            "Epoch 38/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 76.7363 - val_loss: 107.0637\n",
            "Epoch 39/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.5913 - val_loss: 111.9065\n",
            "Epoch 40/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.6128 - val_loss: 107.9414\n",
            "Epoch 41/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.2915 - val_loss: 99.8633\n",
            "Epoch 42/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.5077 - val_loss: 100.9363\n",
            "Epoch 43/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.7315 - val_loss: 96.4164\n",
            "Epoch 44/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.1464 - val_loss: 108.0382\n",
            "Epoch 45/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.5964 - val_loss: 97.5953\n",
            "Epoch 46/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 75.1968 - val_loss: 89.7144\n",
            "Epoch 47/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.6768 - val_loss: 221.5494\n",
            "Epoch 48/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 75.7824 - val_loss: 120.5418\n",
            "Epoch 49/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.2332 - val_loss: 104.7765\n",
            "Epoch 50/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 75.6591 - val_loss: 88.7247\n",
            "Epoch 51/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 76.0002 - val_loss: 115.6881\n",
            "Epoch 52/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.2004 - val_loss: 99.8738\n",
            "Epoch 53/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.5116 - val_loss: 91.8236\n",
            "Epoch 54/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.4624 - val_loss: 97.4286\n",
            "Epoch 55/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.3179 - val_loss: 95.7228\n",
            "Epoch 56/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.2326 - val_loss: 97.3437\n",
            "Epoch 57/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 75.1683 - val_loss: 101.0275\n",
            "Epoch 58/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.8315 - val_loss: 100.0602\n",
            "Epoch 59/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.5326 - val_loss: 105.3932\n",
            "Epoch 60/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.6698 - val_loss: 104.9342\n",
            "Epoch 61/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.7962 - val_loss: 109.6159\n",
            "Epoch 62/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.0539 - val_loss: 142.3295\n",
            "Epoch 63/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.3466 - val_loss: 90.6890\n",
            "Epoch 64/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.7267 - val_loss: 128.8252\n",
            "Epoch 65/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.6111 - val_loss: 102.8309\n",
            "Epoch 66/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.8546 - val_loss: 94.3577\n",
            "Epoch 67/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.3556 - val_loss: 111.9921\n",
            "Epoch 68/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 74.0034 - val_loss: 242.5350\n",
            "Epoch 69/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.8477 - val_loss: 84.9022\n",
            "Epoch 70/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.2162 - val_loss: 91.4226\n",
            "Epoch 71/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.6016 - val_loss: 137.2241\n",
            "Epoch 72/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 73.1344 - val_loss: 99.8247\n",
            "Epoch 73/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.4098 - val_loss: 102.2663\n",
            "Epoch 74/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.6493 - val_loss: 84.7695\n",
            "Epoch 75/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.7791 - val_loss: 87.8394\n",
            "Epoch 76/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.8260 - val_loss: 133.4880\n",
            "Epoch 77/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.8668 - val_loss: 93.1551\n",
            "Epoch 78/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.7510 - val_loss: 99.1558\n",
            "Epoch 79/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.6744 - val_loss: 106.6798\n",
            "Epoch 80/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 73.3457 - val_loss: 87.9646\n",
            "Epoch 81/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 73.3597 - val_loss: 93.9511\n",
            "Epoch 82/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 73.3722 - val_loss: 102.6199\n",
            "Epoch 83/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.4996 - val_loss: 94.6340\n",
            "Epoch 84/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.4266 - val_loss: 95.4990\n",
            "Epoch 85/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 72.4045 - val_loss: 90.7398\n",
            "Epoch 86/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.2477 - val_loss: 112.6034\n",
            "Epoch 87/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.9874 - val_loss: 86.5167\n",
            "Epoch 88/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 72.1648 - val_loss: 85.4959\n",
            "Epoch 89/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 72.0886 - val_loss: 89.7809\n",
            "Epoch 90/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.1321 - val_loss: 105.4595\n",
            "Epoch 91/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 72.1366 - val_loss: 83.2917\n",
            "Epoch 92/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 72.2285 - val_loss: 98.5697\n",
            "Epoch 93/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.3590 - val_loss: 103.1292\n",
            "Epoch 94/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 72.1983 - val_loss: 93.9244\n",
            "Epoch 95/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.9088 - val_loss: 81.8122\n",
            "Epoch 96/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.7360 - val_loss: 96.3671\n",
            "Epoch 97/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 72.1723 - val_loss: 89.5835\n",
            "Epoch 98/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.5009 - val_loss: 82.5353\n",
            "Epoch 99/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.9213 - val_loss: 146.0702\n",
            "Epoch 100/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.6149 - val_loss: 95.4326\n",
            "Epoch 101/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 71.6344 - val_loss: 88.8230\n",
            "Epoch 102/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 71.8935 - val_loss: 108.7045\n",
            "Epoch 103/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.8324 - val_loss: 86.2463\n",
            "Epoch 104/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.5077 - val_loss: 111.7098\n",
            "Epoch 105/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.5485 - val_loss: 84.7686\n",
            "Epoch 106/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 71.6615 - val_loss: 81.6079\n",
            "Epoch 107/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.6176 - val_loss: 104.8656\n",
            "Epoch 108/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 72.2406 - val_loss: 84.3961\n",
            "Epoch 109/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.4347 - val_loss: 88.9307\n",
            "Epoch 110/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 70.7710 - val_loss: 89.5647\n",
            "Epoch 111/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.1444 - val_loss: 82.9819\n",
            "Epoch 112/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.1695 - val_loss: 113.5309\n",
            "Epoch 113/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.2319 - val_loss: 94.8085\n",
            "Epoch 114/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.0270 - val_loss: 135.3121\n",
            "Epoch 115/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.8785 - val_loss: 85.9523\n",
            "Epoch 116/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.1146 - val_loss: 90.7382\n",
            "Epoch 117/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.0934 - val_loss: 87.7730\n",
            "Epoch 118/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 71.0431 - val_loss: 86.8433\n",
            "Epoch 119/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.3349 - val_loss: 133.6086\n",
            "Epoch 120/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 71.0399 - val_loss: 107.7825\n",
            "Epoch 121/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.8749 - val_loss: 93.1887\n",
            "Epoch 122/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.9587 - val_loss: 92.0250\n",
            "Epoch 123/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.3836 - val_loss: 88.9102\n",
            "Epoch 124/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.5887 - val_loss: 86.0982\n",
            "Epoch 125/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.4351 - val_loss: 86.0893\n",
            "Epoch 126/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.5592 - val_loss: 92.0555\n",
            "Epoch 127/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.4936 - val_loss: 99.5966\n",
            "Epoch 128/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.7524 - val_loss: 87.6487\n",
            "Epoch 129/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.4825 - val_loss: 106.0187\n",
            "Epoch 130/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 70.2343 - val_loss: 88.1112\n",
            "Epoch 131/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.2468 - val_loss: 84.5125\n",
            "Epoch 132/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.0303 - val_loss: 98.1946\n",
            "Epoch 133/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.1626 - val_loss: 95.8405\n",
            "Epoch 134/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 70.0964 - val_loss: 101.3444\n",
            "Epoch 135/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.7236 - val_loss: 101.3662\n",
            "Epoch 136/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.8901 - val_loss: 89.9730\n",
            "Epoch 137/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.6493 - val_loss: 92.5554\n",
            "Epoch 138/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.7171 - val_loss: 98.5102\n",
            "Epoch 139/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.6944 - val_loss: 89.1585\n",
            "Epoch 140/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.4929 - val_loss: 84.0611\n",
            "Epoch 141/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.5669 - val_loss: 82.6650\n",
            "Epoch 142/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.2026 - val_loss: 120.0157\n",
            "Epoch 143/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.8610 - val_loss: 93.0934\n",
            "Epoch 144/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.6427 - val_loss: 93.0976\n",
            "Epoch 145/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.4106 - val_loss: 86.5625\n",
            "Epoch 146/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.5484 - val_loss: 82.1628\n",
            "Epoch 147/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.3198 - val_loss: 155.2495\n",
            "Epoch 148/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.3454 - val_loss: 125.5152\n",
            "Epoch 149/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.2837 - val_loss: 104.8855\n",
            "Epoch 150/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.3049 - val_loss: 118.4790\n",
            "Epoch 151/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.2234 - val_loss: 82.4379\n",
            "Epoch 152/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.5580 - val_loss: 88.0770\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 153/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.5704 - val_loss: 87.9373\n",
            "Epoch 154/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.6192 - val_loss: 84.7755\n",
            "Epoch 155/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.4669 - val_loss: 97.8014\n",
            "Epoch 156/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.3260 - val_loss: 84.6048\n",
            "Epoch 157/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.0706 - val_loss: 95.3526\n",
            "Epoch 158/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.1428 - val_loss: 101.6897\n",
            "Epoch 159/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.0073 - val_loss: 85.8569\n",
            "Epoch 160/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.2191 - val_loss: 83.3788\n",
            "Epoch 161/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.0033 - val_loss: 85.6516\n",
            "Epoch 162/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.1240 - val_loss: 86.3043\n",
            "Epoch 163/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.2512 - val_loss: 83.5191\n",
            "Epoch 164/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 69.0142 - val_loss: 81.7867\n",
            "Epoch 165/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.6743 - val_loss: 86.5568\n",
            "Epoch 166/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.8781 - val_loss: 112.6456\n",
            "Epoch 167/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.5531 - val_loss: 87.3283\n",
            "Epoch 168/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.3981 - val_loss: 89.2495\n",
            "Epoch 169/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.7038 - val_loss: 83.2964\n",
            "Epoch 170/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.9154 - val_loss: 86.4885\n",
            "Epoch 171/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.7809 - val_loss: 94.2922\n",
            "Epoch 172/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.4869 - val_loss: 82.3811\n",
            "Epoch 173/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.8257 - val_loss: 85.3422\n",
            "Epoch 174/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.0092 - val_loss: 83.9328\n",
            "Epoch 175/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.0567 - val_loss: 84.1215\n",
            "Epoch 176/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.4893 - val_loss: 99.6987\n",
            "Epoch 177/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 69.0927 - val_loss: 94.4916\n",
            "Epoch 178/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.6514 - val_loss: 97.6769\n",
            "Epoch 179/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.6592 - val_loss: 84.8909\n",
            "Epoch 180/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.3569 - val_loss: 85.6692\n",
            "Epoch 181/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.5479 - val_loss: 93.1580\n",
            "Epoch 182/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.4544 - val_loss: 93.1436\n",
            "Epoch 183/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.5422 - val_loss: 83.9011\n",
            "Epoch 184/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.6933 - val_loss: 82.9759\n",
            "Epoch 185/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.5314 - val_loss: 81.9717\n",
            "Epoch 186/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.7303 - val_loss: 87.1099\n",
            "Epoch 187/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.2621 - val_loss: 92.6949\n",
            "Epoch 188/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.7276 - val_loss: 104.5202\n",
            "Epoch 189/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.4036 - val_loss: 103.4874\n",
            "Epoch 190/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.3714 - val_loss: 125.6859\n",
            "Epoch 191/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.1305 - val_loss: 90.6179\n",
            "Epoch 192/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.0381 - val_loss: 94.0127\n",
            "Epoch 193/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.2808 - val_loss: 96.4319\n",
            "Epoch 194/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.6096 - val_loss: 99.1884\n",
            "Epoch 195/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.4823 - val_loss: 79.8263\n",
            "Epoch 196/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.0935 - val_loss: 97.9023\n",
            "Epoch 197/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.2324 - val_loss: 84.9428\n",
            "Epoch 198/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.7422 - val_loss: 84.1601\n",
            "Epoch 199/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.3268 - val_loss: 92.2148\n",
            "Epoch 200/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.0218 - val_loss: 86.1889\n",
            "Epoch 201/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.0777 - val_loss: 80.9654\n",
            "Epoch 202/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.0350 - val_loss: 85.0055\n",
            "Epoch 203/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.5783 - val_loss: 84.6946\n",
            "Epoch 204/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 68.1026 - val_loss: 97.8103\n",
            "Epoch 205/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.5996 - val_loss: 82.5684\n",
            "Epoch 206/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.5924 - val_loss: 84.7715\n",
            "Epoch 207/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 68.1247 - val_loss: 84.5618\n",
            "Epoch 208/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.7533 - val_loss: 87.5700\n",
            "Epoch 209/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.8163 - val_loss: 94.7371\n",
            "Epoch 210/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.7493 - val_loss: 81.0010\n",
            "Epoch 211/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.8386 - val_loss: 89.1726\n",
            "Epoch 212/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.5765 - val_loss: 81.6805\n",
            "Epoch 213/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.6601 - val_loss: 84.0674\n",
            "Epoch 214/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 67.5632 - val_loss: 86.5342\n",
            "Epoch 215/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.7779 - val_loss: 86.0453\n",
            "Epoch 216/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.4530 - val_loss: 101.3502\n",
            "Epoch 217/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.6224 - val_loss: 109.4371\n",
            "Epoch 218/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.5485 - val_loss: 91.4055\n",
            "Epoch 219/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.4843 - val_loss: 91.1011\n",
            "Epoch 220/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.3804 - val_loss: 88.5532\n",
            "Epoch 221/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.4526 - val_loss: 98.7176\n",
            "Epoch 222/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.4974 - val_loss: 92.6735\n",
            "Epoch 223/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.6759 - val_loss: 90.3536\n",
            "Epoch 224/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.6381 - val_loss: 96.8870\n",
            "Epoch 225/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.7452 - val_loss: 84.8788\n",
            "Epoch 226/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.7274 - val_loss: 83.7723\n",
            "Epoch 227/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.2159 - val_loss: 83.2337\n",
            "Epoch 228/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.3214 - val_loss: 85.4579\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 229/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.1803 - val_loss: 79.8391\n",
            "Epoch 230/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.1051 - val_loss: 82.1128\n",
            "Epoch 231/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.5750 - val_loss: 81.1315\n",
            "Epoch 232/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.6487 - val_loss: 95.4787\n",
            "Epoch 233/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 67.3189 - val_loss: 94.5444\n",
            "Epoch 234/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 66.9987 - val_loss: 81.3525\n",
            "Epoch 235/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.1435 - val_loss: 82.3112\n",
            "Epoch 236/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.9619 - val_loss: 79.6974\n",
            "Epoch 237/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 67.4733 - val_loss: 89.2328\n",
            "Epoch 238/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.2425 - val_loss: 83.6954\n",
            "Epoch 239/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.2067 - val_loss: 88.0911\n",
            "Epoch 240/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.5665 - val_loss: 80.9234\n",
            "Epoch 241/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.4891 - val_loss: 83.7250\n",
            "Epoch 242/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.3833 - val_loss: 88.0416\n",
            "Epoch 243/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.6618 - val_loss: 88.4842\n",
            "Epoch 244/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.1046 - val_loss: 96.0527\n",
            "Epoch 245/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.2798 - val_loss: 79.7901\n",
            "Epoch 246/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.1373 - val_loss: 93.3036\n",
            "Epoch 247/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.2317 - val_loss: 114.4480\n",
            "Epoch 248/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.0133 - val_loss: 89.8543\n",
            "Epoch 249/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 67.1183 - val_loss: 88.5138\n",
            "Epoch 250/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.0218 - val_loss: 93.1040\n",
            "Epoch 251/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.3666 - val_loss: 83.3085\n",
            "Epoch 252/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 67.1307 - val_loss: 102.5579\n",
            "Epoch 253/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 67.0226 - val_loss: 85.5071\n",
            "Epoch 254/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.9968 - val_loss: 81.6375\n",
            "Epoch 255/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.9359 - val_loss: 127.5138\n",
            "Epoch 256/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.9687 - val_loss: 83.5107\n",
            "Epoch 257/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.4999 - val_loss: 90.0371\n",
            "Epoch 258/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.7076 - val_loss: 81.1843\n",
            "Epoch 259/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.6030 - val_loss: 82.0599\n",
            "Epoch 260/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8267 - val_loss: 88.5337\n",
            "Epoch 261/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 67.4245 - val_loss: 86.0529\n",
            "Epoch 262/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8434 - val_loss: 85.0188\n",
            "Epoch 263/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 67.0930 - val_loss: 81.6365\n",
            "Epoch 264/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.8928 - val_loss: 86.5230\n",
            "Epoch 265/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 67.1815 - val_loss: 86.9385\n",
            "Epoch 266/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 67.1382 - val_loss: 93.1453\n",
            "Epoch 267/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.5658 - val_loss: 83.0932\n",
            "Epoch 268/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.8817 - val_loss: 88.2178\n",
            "Epoch 269/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.9724 - val_loss: 80.7345\n",
            "Epoch 270/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.7484 - val_loss: 82.7368\n",
            "Epoch 271/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.4901 - val_loss: 82.2431\n",
            "Epoch 272/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.8900 - val_loss: 83.8365\n",
            "Epoch 273/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.7174 - val_loss: 95.2422\n",
            "Epoch 274/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.7101 - val_loss: 98.9498\n",
            "Epoch 275/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8021 - val_loss: 78.3680\n",
            "Epoch 276/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.7980 - val_loss: 84.1496\n",
            "Epoch 277/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.4193 - val_loss: 83.2812\n",
            "Epoch 278/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.4748 - val_loss: 84.6810\n",
            "Epoch 279/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.2840 - val_loss: 80.2698\n",
            "Epoch 280/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.5540 - val_loss: 82.4851\n",
            "Epoch 281/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.6775 - val_loss: 84.6110\n",
            "Epoch 282/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.7375 - val_loss: 82.3660\n",
            "Epoch 283/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.8236 - val_loss: 85.8622\n",
            "Epoch 284/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.7100 - val_loss: 78.7544\n",
            "Epoch 285/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 66.9600 - val_loss: 83.3544\n",
            "Epoch 286/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.5763 - val_loss: 86.6490\n",
            "Epoch 287/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.7030 - val_loss: 89.7556\n",
            "Epoch 288/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.9316 - val_loss: 88.2493\n",
            "Epoch 289/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.7210 - val_loss: 88.8003\n",
            "Epoch 290/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.6508 - val_loss: 89.3320\n",
            "Epoch 291/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 66.6897 - val_loss: 80.8534\n",
            "Epoch 292/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.6369 - val_loss: 88.1928\n",
            "Epoch 293/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.5978 - val_loss: 81.5177\n",
            "Epoch 294/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.3281 - val_loss: 81.2994\n",
            "Epoch 295/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.1467 - val_loss: 86.9844\n",
            "Epoch 296/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.5393 - val_loss: 85.1469\n",
            "Epoch 297/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 66.2431 - val_loss: 83.1936\n",
            "Epoch 298/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.4517 - val_loss: 85.1813\n",
            "Epoch 299/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 65.9791 - val_loss: 89.2039\n",
            "Epoch 300/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 66.1924 - val_loss: 86.7037\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696v_fuFCTsa",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -0.3080055710067139 \n",
            "MAE:  6.948924093641964 \n",
            "SD:  9.306388251560083\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdZF2osWCUQS",
        "outputId": "9a69449e-8dd6-4ea6-ce0b-3e19d57fd027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  -0.49809420402897553 \n",
            "Ensemble_std:  9.06940634085405\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_48 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_54 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 1652.2218 - val_loss: 81.0266\n",
            "Epoch 2/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 54.5427 - val_loss: 169.5170\n",
            "Epoch 3/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 40.8865 - val_loss: 44.1501\n",
            "Epoch 4/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 37.4120 - val_loss: 51.0192\n",
            "Epoch 5/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 35.7181 - val_loss: 39.0581\n",
            "Epoch 6/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 34.7219 - val_loss: 49.0801\n",
            "Epoch 7/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 33.8845 - val_loss: 34.7510\n",
            "Epoch 8/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 33.2617 - val_loss: 37.5815\n",
            "Epoch 9/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 32.8888 - val_loss: 35.5794\n",
            "Epoch 10/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 32.2344 - val_loss: 39.8617\n",
            "Epoch 11/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 31.9957 - val_loss: 48.3830\n",
            "Epoch 12/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 31.5371 - val_loss: 57.5590\n",
            "Epoch 13/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 31.2990 - val_loss: 33.0202\n",
            "Epoch 14/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 31.2024 - val_loss: 36.5403\n",
            "Epoch 15/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 30.8939 - val_loss: 42.8175\n",
            "Epoch 16/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 31.1231 - val_loss: 44.5126\n",
            "Epoch 17/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 30.4300 - val_loss: 39.6330\n",
            "Epoch 18/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 30.7665 - val_loss: 36.3371\n",
            "Epoch 19/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 30.5102 - val_loss: 34.2319\n",
            "Epoch 20/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 30.5480 - val_loss: 36.0526\n",
            "Epoch 21/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 30.7002 - val_loss: 43.3077\n",
            "Epoch 22/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 30.8330 - val_loss: 46.6697\n",
            "Epoch 23/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 30.8362 - val_loss: 37.7583\n",
            "Epoch 24/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 30.3222 - val_loss: 64.9247\n",
            "Epoch 25/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.8960 - val_loss: 36.1882\n",
            "Epoch 26/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.9207 - val_loss: 38.5571\n",
            "Epoch 27/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.9182 - val_loss: 37.7942\n",
            "Epoch 28/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.9477 - val_loss: 38.2264\n",
            "Epoch 29/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 30.0134 - val_loss: 32.7542\n",
            "Epoch 30/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 29.9060 - val_loss: 38.9734\n",
            "Epoch 31/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 29.7982 - val_loss: 33.3028\n",
            "Epoch 32/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.4319 - val_loss: 33.5893\n",
            "Epoch 33/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.6691 - val_loss: 39.0928\n",
            "Epoch 34/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.4207 - val_loss: 45.1596\n",
            "Epoch 35/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.2653 - val_loss: 55.1756\n",
            "Epoch 36/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.2937 - val_loss: 34.0437\n",
            "Epoch 37/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 29.5362 - val_loss: 36.6606\n",
            "Epoch 38/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 29.3972 - val_loss: 36.7469\n",
            "Epoch 39/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 29.6720 - val_loss: 36.7962\n",
            "Epoch 40/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 29.0000 - val_loss: 36.6539\n",
            "Epoch 41/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 29.0837 - val_loss: 40.1715\n",
            "Epoch 42/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.9451 - val_loss: 47.6928\n",
            "Epoch 43/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 29.1196 - val_loss: 37.8778\n",
            "Epoch 44/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 29.2055 - val_loss: 34.4219\n",
            "Epoch 45/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 29.2491 - val_loss: 32.6011\n",
            "Epoch 46/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 29.1462 - val_loss: 39.0987\n",
            "Epoch 47/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.9435 - val_loss: 34.7929\n",
            "Epoch 48/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.7428 - val_loss: 38.8312\n",
            "Epoch 49/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.6463 - val_loss: 44.8774\n",
            "Epoch 50/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 28.7466 - val_loss: 39.7938\n",
            "Epoch 51/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.5143 - val_loss: 39.5077\n",
            "Epoch 52/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.5120 - val_loss: 36.5961\n",
            "Epoch 53/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.4358 - val_loss: 37.9999\n",
            "Epoch 54/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.5410 - val_loss: 44.4688\n",
            "Epoch 55/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.4798 - val_loss: 39.4286\n",
            "Epoch 56/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 28.2400 - val_loss: 33.2057\n",
            "Epoch 57/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 28.2636 - val_loss: 35.9898\n",
            "Epoch 58/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.9741 - val_loss: 41.2819\n",
            "Epoch 59/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.1708 - val_loss: 51.7185\n",
            "Epoch 60/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.1945 - val_loss: 32.7031\n",
            "Epoch 61/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.0776 - val_loss: 39.6184\n",
            "Epoch 62/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.0184 - val_loss: 39.8485\n",
            "Epoch 63/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.0767 - val_loss: 31.5654\n",
            "Epoch 64/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.0071 - val_loss: 35.0099\n",
            "Epoch 65/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.7498 - val_loss: 35.3857\n",
            "Epoch 66/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.8574 - val_loss: 37.5340\n",
            "Epoch 67/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.9121 - val_loss: 33.6494\n",
            "Epoch 68/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 27.6801 - val_loss: 46.1248\n",
            "Epoch 69/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.9136 - val_loss: 35.7773\n",
            "Epoch 70/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.8686 - val_loss: 33.4362\n",
            "Epoch 71/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.8414 - val_loss: 32.8434\n",
            "Epoch 72/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.7213 - val_loss: 34.8251\n",
            "Epoch 73/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.4950 - val_loss: 33.5570\n",
            "Epoch 74/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.6567 - val_loss: 32.3167\n",
            "Epoch 75/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.4921 - val_loss: 31.8426\n",
            "Epoch 76/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.6082 - val_loss: 32.6305\n",
            "Epoch 77/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.5918 - val_loss: 37.5849\n",
            "Epoch 78/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.5526 - val_loss: 31.9026\n",
            "Epoch 79/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.5434 - val_loss: 48.6350\n",
            "Epoch 80/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.5695 - val_loss: 32.8703\n",
            "Epoch 81/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.5347 - val_loss: 44.6808\n",
            "Epoch 82/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.4683 - val_loss: 32.3088\n",
            "Epoch 83/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.3637 - val_loss: 41.5944\n",
            "Epoch 84/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.3398 - val_loss: 34.5828\n",
            "Epoch 85/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.4059 - val_loss: 33.3413\n",
            "Epoch 86/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.3982 - val_loss: 32.0692\n",
            "Epoch 87/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.4400 - val_loss: 36.6930\n",
            "Epoch 88/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.3594 - val_loss: 32.7141\n",
            "Epoch 89/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 27.3507 - val_loss: 32.3486\n",
            "Epoch 90/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 27.2972 - val_loss: 32.3947s - loss:\n",
            "Epoch 91/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.2546 - val_loss: 32.4927\n",
            "Epoch 92/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.1956 - val_loss: 32.7194\n",
            "Epoch 93/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.1855 - val_loss: 32.7177\n",
            "Epoch 94/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 27.2440 - val_loss: 33.3442\n",
            "Epoch 95/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.1239 - val_loss: 33.4551\n",
            "Epoch 96/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.2145 - val_loss: 32.6723\n",
            "Epoch 97/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.0941 - val_loss: 32.6858\n",
            "Epoch 98/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 27.3601 - val_loss: 34.1630\n",
            "Epoch 99/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.0433 - val_loss: 33.9291\n",
            "Epoch 100/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 27.0858 - val_loss: 37.6745\n",
            "Epoch 101/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.1700 - val_loss: 32.7085\n",
            "Epoch 102/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.2068 - val_loss: 40.7739\n",
            "Epoch 103/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.1685 - val_loss: 33.4770\n",
            "Epoch 104/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.0245 - val_loss: 37.5269\n",
            "Epoch 105/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.9530 - val_loss: 50.4224\n",
            "Epoch 106/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.0202 - val_loss: 34.3934\n",
            "Epoch 107/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.9734 - val_loss: 34.1468\n",
            "Epoch 108/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.0144 - val_loss: 33.9104\n",
            "Epoch 109/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 27.0019 - val_loss: 35.4894\n",
            "Epoch 110/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 26.9578 - val_loss: 33.4789\n",
            "Epoch 111/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.0101 - val_loss: 33.4522\n",
            "Epoch 112/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.7956 - val_loss: 36.6727\n",
            "Epoch 113/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.7672 - val_loss: 33.9783\n",
            "Epoch 114/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.0376 - val_loss: 33.6589\n",
            "Epoch 115/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.2764 - val_loss: 41.9341\n",
            "Epoch 116/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.9750 - val_loss: 36.8830\n",
            "Epoch 117/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 27.0515 - val_loss: 33.8720\n",
            "Epoch 118/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.8734 - val_loss: 43.3746\n",
            "Epoch 119/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.8966 - val_loss: 43.8909\n",
            "Epoch 120/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.9201 - val_loss: 33.0872\n",
            "Epoch 121/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.8255 - val_loss: 35.4397\n",
            "Epoch 122/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.8646 - val_loss: 31.9200\n",
            "Epoch 123/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.8579 - val_loss: 34.1259\n",
            "Epoch 124/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.7871 - val_loss: 32.3831\n",
            "Epoch 125/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.8081 - val_loss: 34.6778\n",
            "Epoch 126/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.0224 - val_loss: 34.1644\n",
            "Epoch 127/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.9564 - val_loss: 37.8476\n",
            "Epoch 128/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.9380 - val_loss: 31.9574\n",
            "Epoch 129/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.7508 - val_loss: 44.4822\n",
            "Epoch 130/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.8062 - val_loss: 31.9366\n",
            "Epoch 131/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.7216 - val_loss: 32.3142\n",
            "Epoch 132/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.7149 - val_loss: 34.0069\n",
            "Epoch 133/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.8483 - val_loss: 35.3370\n",
            "Epoch 134/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.7155 - val_loss: 32.0024\n",
            "Epoch 135/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.7761 - val_loss: 42.5916\n",
            "Epoch 136/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.7758 - val_loss: 32.3666\n",
            "Epoch 137/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.7375 - val_loss: 37.1754\n",
            "Epoch 138/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.6911 - val_loss: 34.1575\n",
            "Epoch 139/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.6563 - val_loss: 33.3328\n",
            "Epoch 140/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.6186 - val_loss: 34.2652\n",
            "Epoch 141/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.6481 - val_loss: 34.4787\n",
            "Epoch 142/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.6323 - val_loss: 32.0846\n",
            "Epoch 143/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.5409 - val_loss: 32.2488\n",
            "Epoch 144/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.6820 - val_loss: 34.3271\n",
            "Epoch 145/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4537 - val_loss: 32.8562\n",
            "Epoch 146/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4536 - val_loss: 34.9551\n",
            "Epoch 147/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4407 - val_loss: 32.7021\n",
            "Epoch 148/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.4720 - val_loss: 36.8130\n",
            "Epoch 149/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.4639 - val_loss: 31.6048\n",
            "Epoch 150/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.4546 - val_loss: 43.2347\n",
            "Epoch 151/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4945 - val_loss: 31.4623\n",
            "Epoch 152/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.4911 - val_loss: 34.8116\n",
            "Epoch 153/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4922 - val_loss: 33.4719\n",
            "Epoch 154/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 26.5686 - val_loss: 30.9938\n",
            "Epoch 155/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.4441 - val_loss: 34.9768\n",
            "Epoch 156/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 26.4567 - val_loss: 34.0200\n",
            "Epoch 157/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 26.4091 - val_loss: 32.4844\n",
            "Epoch 158/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.4868 - val_loss: 35.7458\n",
            "Epoch 159/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.4218 - val_loss: 31.3255\n",
            "Epoch 160/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 26.3345 - val_loss: 32.2237\n",
            "Epoch 161/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 26.2825 - val_loss: 30.5023\n",
            "Epoch 162/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 26.3422 - val_loss: 36.4935\n",
            "Epoch 163/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 26.2112 - val_loss: 32.3561\n",
            "Epoch 164/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 26.3786 - val_loss: 34.1707\n",
            "Epoch 165/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 26.3642 - val_loss: 36.5897\n",
            "Epoch 166/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 26.2787 - val_loss: 36.1995\n",
            "Epoch 167/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.2933 - val_loss: 39.3970\n",
            "Epoch 168/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.1608 - val_loss: 32.8232\n",
            "Epoch 169/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.3408 - val_loss: 31.8735\n",
            "Epoch 170/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.2926 - val_loss: 31.5103\n",
            "Epoch 171/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.2149 - val_loss: 32.3243\n",
            "Epoch 172/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.3979 - val_loss: 32.5049\n",
            "Epoch 173/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.3750 - val_loss: 32.8182\n",
            "Epoch 174/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.2853 - val_loss: 32.5429\n",
            "Epoch 175/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.1466 - val_loss: 30.8366\n",
            "Epoch 176/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.2857 - val_loss: 31.9273\n",
            "Epoch 177/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.1177 - val_loss: 36.5654\n",
            "Epoch 178/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.1647 - val_loss: 35.9437\n",
            "Epoch 179/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0725 - val_loss: 30.4937\n",
            "Epoch 180/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.1167 - val_loss: 37.2697\n",
            "Epoch 181/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.1551 - val_loss: 36.5614\n",
            "Epoch 182/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.1639 - val_loss: 50.1451\n",
            "Epoch 183/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.2479 - val_loss: 33.7875\n",
            "Epoch 184/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.0245 - val_loss: 33.8910\n",
            "Epoch 185/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0543 - val_loss: 30.2025\n",
            "Epoch 186/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0326 - val_loss: 30.9146\n",
            "Epoch 187/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 25.9153 - val_loss: 30.3558\n",
            "Epoch 188/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0786 - val_loss: 30.8674\n",
            "Epoch 189/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0546 - val_loss: 34.8555\n",
            "Epoch 190/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0094 - val_loss: 30.7239\n",
            "Epoch 191/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0963 - val_loss: 30.5005\n",
            "Epoch 192/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.0237 - val_loss: 33.2221\n",
            "Epoch 193/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.0483 - val_loss: 32.3130\n",
            "Epoch 194/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 26.0927 - val_loss: 40.2981\n",
            "Epoch 195/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0900 - val_loss: 32.4047\n",
            "Epoch 196/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0084 - val_loss: 32.5992\n",
            "Epoch 197/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.9573 - val_loss: 35.6812\n",
            "Epoch 198/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.9192 - val_loss: 30.8117\n",
            "Epoch 199/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.9511 - val_loss: 40.6170\n",
            "Epoch 200/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7496 - val_loss: 30.3778\n",
            "Epoch 201/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9074 - val_loss: 33.0697\n",
            "Epoch 202/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8170 - val_loss: 32.4245\n",
            "Epoch 203/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9160 - val_loss: 39.7116\n",
            "Epoch 204/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8727 - val_loss: 33.7817\n",
            "Epoch 205/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0081 - val_loss: 32.5274\n",
            "Epoch 206/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8620 - val_loss: 34.8235\n",
            "Epoch 207/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7652 - val_loss: 31.4071\n",
            "Epoch 208/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6916 - val_loss: 32.5379\n",
            "Epoch 209/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8480 - val_loss: 30.8506\n",
            "Epoch 210/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9498 - val_loss: 32.5934\n",
            "Epoch 211/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9446 - val_loss: 34.0602\n",
            "Epoch 212/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7586 - val_loss: 37.4237\n",
            "Epoch 213/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8780 - val_loss: 31.2675\n",
            "Epoch 214/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7364 - val_loss: 35.3695\n",
            "Epoch 215/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.7171 - val_loss: 33.3797\n",
            "Epoch 216/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.7002 - val_loss: 34.1504\n",
            "Epoch 217/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6844 - val_loss: 33.6951\n",
            "Epoch 218/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8434 - val_loss: 34.3900\n",
            "Epoch 219/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.7695 - val_loss: 32.7814\n",
            "Epoch 220/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.8510 - val_loss: 33.2714\n",
            "Epoch 221/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7393 - val_loss: 41.8259\n",
            "Epoch 222/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.7287 - val_loss: 31.8346\n",
            "Epoch 223/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6558 - val_loss: 32.1411\n",
            "Epoch 224/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7014 - val_loss: 35.3420\n",
            "Epoch 225/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7296 - val_loss: 32.6250\n",
            "Epoch 226/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7477 - val_loss: 35.4528\n",
            "Epoch 227/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7256 - val_loss: 31.3533\n",
            "Epoch 228/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5275 - val_loss: 36.2636\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 229/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6628 - val_loss: 35.9043\n",
            "Epoch 230/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6269 - val_loss: 36.3331\n",
            "Epoch 231/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6357 - val_loss: 30.8158\n",
            "Epoch 232/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6533 - val_loss: 32.4103\n",
            "Epoch 233/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5860 - val_loss: 35.5381\n",
            "Epoch 234/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5979 - val_loss: 37.1897\n",
            "Epoch 235/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5205 - val_loss: 47.0634\n",
            "Epoch 236/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6911 - val_loss: 32.5514\n",
            "Epoch 237/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6680 - val_loss: 33.9641\n",
            "Epoch 238/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5855 - val_loss: 30.3553\n",
            "Epoch 239/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5926 - val_loss: 31.8085\n",
            "Epoch 240/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5703 - val_loss: 32.1312\n",
            "Epoch 241/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4679 - val_loss: 30.4938\n",
            "Epoch 242/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5666 - val_loss: 35.0784\n",
            "Epoch 243/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5474 - val_loss: 32.2912\n",
            "Epoch 244/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5812 - val_loss: 33.4075\n",
            "Epoch 245/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6039 - val_loss: 33.2092\n",
            "Epoch 246/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4499 - val_loss: 36.0837\n",
            "Epoch 247/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4169 - val_loss: 33.0539\n",
            "Epoch 248/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4242 - val_loss: 30.5448\n",
            "Epoch 249/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4678 - val_loss: 31.6415\n",
            "Epoch 250/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4934 - val_loss: 33.0488\n",
            "Epoch 251/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5597 - val_loss: 33.5229\n",
            "Epoch 252/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4623 - val_loss: 35.7650\n",
            "Epoch 253/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5421 - val_loss: 32.3166\n",
            "Epoch 254/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3332 - val_loss: 31.2641\n",
            "Epoch 255/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3830 - val_loss: 33.9050\n",
            "Epoch 256/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5096 - val_loss: 31.9733\n",
            "Epoch 257/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5009 - val_loss: 32.2007\n",
            "Epoch 258/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4615 - val_loss: 31.4818\n",
            "Epoch 259/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3948 - val_loss: 31.9874\n",
            "Epoch 260/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4582 - val_loss: 31.8806\n",
            "Epoch 261/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4484 - val_loss: 30.1655\n",
            "Epoch 262/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4267 - val_loss: 37.4335\n",
            "Epoch 263/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4436 - val_loss: 32.5049\n",
            "Epoch 264/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4539 - val_loss: 30.2094\n",
            "Epoch 265/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4375 - val_loss: 31.4090\n",
            "Epoch 266/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4893 - val_loss: 30.8994\n",
            "Epoch 267/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.2690 - val_loss: 42.0987\n",
            "Epoch 268/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4173 - val_loss: 34.2351\n",
            "Epoch 269/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4138 - val_loss: 32.5010\n",
            "Epoch 270/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3399 - val_loss: 32.8721\n",
            "Epoch 271/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3645 - val_loss: 30.9946\n",
            "Epoch 272/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3484 - val_loss: 31.1855\n",
            "Epoch 273/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3613 - val_loss: 32.8428\n",
            "Epoch 274/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4680 - val_loss: 30.7051\n",
            "Epoch 275/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 25.3783 - val_loss: 30.2677\n",
            "Epoch 276/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4427 - val_loss: 30.7895\n",
            "Epoch 277/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3126 - val_loss: 32.1568\n",
            "Epoch 278/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3421 - val_loss: 37.2001\n",
            "Epoch 279/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4438 - val_loss: 32.4023\n",
            "Epoch 280/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.2797 - val_loss: 31.1258\n",
            "Epoch 281/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3938 - val_loss: 32.1459\n",
            "Epoch 282/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3160 - val_loss: 30.2177\n",
            "Epoch 283/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3691 - val_loss: 30.3296\n",
            "Epoch 284/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3091 - val_loss: 36.8543\n",
            "Epoch 285/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3860 - val_loss: 31.1639\n",
            "Epoch 286/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4197 - val_loss: 33.5181\n",
            "Epoch 287/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3388 - val_loss: 35.9795\n",
            "Epoch 288/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4473 - val_loss: 35.7570\n",
            "Epoch 289/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3444 - val_loss: 32.6426\n",
            "Epoch 290/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.2522 - val_loss: 31.3345\n",
            "Epoch 291/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 25.4394 - val_loss: 33.6915\n",
            "Epoch 292/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 25.2593 - val_loss: 30.4645\n",
            "Epoch 293/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 25.3512 - val_loss: 34.6500\n",
            "Epoch 294/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.2225 - val_loss: 31.6624\n",
            "Epoch 295/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3493 - val_loss: 36.3851\n",
            "Epoch 296/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.2523 - val_loss: 33.4153\n",
            "Epoch 297/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.2457 - val_loss: 31.9492\n",
            "Epoch 298/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.2842 - val_loss: 34.7474\n",
            "Epoch 299/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.2932 - val_loss: 34.2275\n",
            "Epoch 300/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.2420 - val_loss: 33.6164\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  1.7721282522003154 \n",
            "MAE:  4.306871194640915 \n",
            "SD:  5.520500364995231\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cip38xZOZnV",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_60 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_55 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_57 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_65 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 1481.5334 - val_loss: 85.2369\n",
            "Epoch 2/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 57.6169 - val_loss: 50.9544\n",
            "Epoch 3/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 42.7021 - val_loss: 45.8476\n",
            "Epoch 4/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 38.7028 - val_loss: 40.5406\n",
            "Epoch 5/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 36.8555 - val_loss: 42.8897\n",
            "Epoch 6/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 35.6517 - val_loss: 38.6370\n",
            "Epoch 7/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 34.5758 - val_loss: 45.9770\n",
            "Epoch 8/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 33.8046 - val_loss: 43.7592\n",
            "Epoch 9/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 33.0503 - val_loss: 58.7530\n",
            "Epoch 10/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 32.6914 - val_loss: 41.5073\n",
            "Epoch 11/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 32.2057 - val_loss: 36.8717\n",
            "Epoch 12/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 31.8240 - val_loss: 55.0495\n",
            "Epoch 13/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 31.6403 - val_loss: 39.2725\n",
            "Epoch 14/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 31.2998 - val_loss: 36.9413\n",
            "Epoch 15/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 31.1633 - val_loss: 41.0711\n",
            "Epoch 16/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.9996 - val_loss: 39.6338\n",
            "Epoch 17/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.6953 - val_loss: 41.7603\n",
            "Epoch 18/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.5629 - val_loss: 37.4713\n",
            "Epoch 19/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.4471 - val_loss: 35.0856\n",
            "Epoch 20/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.2365 - val_loss: 91.2412\n",
            "Epoch 21/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.1594 - val_loss: 37.9432\n",
            "Epoch 22/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 29.9943 - val_loss: 45.7487\n",
            "Epoch 23/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 29.7748 - val_loss: 33.5748\n",
            "Epoch 24/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 29.7647 - val_loss: 37.3819\n",
            "Epoch 25/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 29.7197 - val_loss: 38.1810\n",
            "Epoch 26/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 29.4942 - val_loss: 38.4523\n",
            "Epoch 27/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 29.2696 - val_loss: 48.9671\n",
            "Epoch 28/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 29.2073 - val_loss: 40.5811\n",
            "Epoch 29/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.2671 - val_loss: 37.1518\n",
            "Epoch 30/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 29.3121 - val_loss: 53.3436\n",
            "Epoch 31/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.1337 - val_loss: 33.8040\n",
            "Epoch 32/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.9553 - val_loss: 38.4547\n",
            "Epoch 33/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.9121 - val_loss: 33.7450\n",
            "Epoch 34/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.8993 - val_loss: 34.6794\n",
            "Epoch 35/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.7944 - val_loss: 39.6089\n",
            "Epoch 36/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.9192 - val_loss: 35.0632\n",
            "Epoch 37/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.6756 - val_loss: 38.5104\n",
            "Epoch 38/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.6190 - val_loss: 32.7414\n",
            "Epoch 39/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.6467 - val_loss: 41.9425\n",
            "Epoch 40/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.5804 - val_loss: 35.3385\n",
            "Epoch 41/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.3274 - val_loss: 33.3630\n",
            "Epoch 42/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.3056 - val_loss: 33.8218\n",
            "Epoch 43/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.4522 - val_loss: 35.8088\n",
            "Epoch 44/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.2488 - val_loss: 33.5461\n",
            "Epoch 45/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.3230 - val_loss: 51.6985\n",
            "Epoch 46/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.1965 - val_loss: 42.1524\n",
            "Epoch 47/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.1199 - val_loss: 33.5630\n",
            "Epoch 48/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.1103 - val_loss: 36.7508\n",
            "Epoch 49/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.0034 - val_loss: 37.4142\n",
            "Epoch 50/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.9507 - val_loss: 33.7526\n",
            "Epoch 51/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.9109 - val_loss: 32.9041\n",
            "Epoch 52/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.8799 - val_loss: 38.0066\n",
            "Epoch 53/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.9597 - val_loss: 36.8891\n",
            "Epoch 54/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.7928 - val_loss: 45.0434\n",
            "Epoch 55/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.6893 - val_loss: 33.8644\n",
            "Epoch 56/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.7839 - val_loss: 39.1605\n",
            "Epoch 57/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.7325 - val_loss: 32.8403\n",
            "Epoch 58/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.7359 - val_loss: 33.8617\n",
            "Epoch 59/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.6901 - val_loss: 30.6618\n",
            "Epoch 60/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.5701 - val_loss: 35.4142\n",
            "Epoch 61/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.5534 - val_loss: 38.5318\n",
            "Epoch 62/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.5538 - val_loss: 35.7729\n",
            "Epoch 63/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.6247 - val_loss: 39.6199\n",
            "Epoch 64/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.4222 - val_loss: 35.6157\n",
            "Epoch 65/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.6115 - val_loss: 36.2298\n",
            "Epoch 66/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.3222 - val_loss: 40.3502\n",
            "Epoch 67/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.4848 - val_loss: 33.1333\n",
            "Epoch 68/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.2479 - val_loss: 35.0055\n",
            "Epoch 69/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.1326 - val_loss: 33.6682\n",
            "Epoch 70/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.3500 - val_loss: 33.9946\n",
            "Epoch 71/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.3512 - val_loss: 37.9589\n",
            "Epoch 72/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.2511 - val_loss: 36.0214\n",
            "Epoch 73/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.1693 - val_loss: 32.7878\n",
            "Epoch 74/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.2707 - val_loss: 44.3196\n",
            "Epoch 75/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.1157 - val_loss: 39.1834\n",
            "Epoch 76/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.0963 - val_loss: 36.2530\n",
            "Epoch 77/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.1845 - val_loss: 32.4681\n",
            "Epoch 78/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.0957 - val_loss: 32.5467\n",
            "Epoch 79/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.2245 - val_loss: 35.8505\n",
            "Epoch 80/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.9927 - val_loss: 33.1247\n",
            "Epoch 81/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.8398 - val_loss: 32.6032\n",
            "Epoch 82/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.9261 - val_loss: 44.8841\n",
            "Epoch 83/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.8732 - val_loss: 35.0448\n",
            "Epoch 84/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.9023 - val_loss: 37.8196\n",
            "Epoch 85/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.8632 - val_loss: 36.0139\n",
            "Epoch 86/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.9729 - val_loss: 51.1011\n",
            "Epoch 87/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.7795 - val_loss: 34.2551\n",
            "Epoch 88/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.8590 - val_loss: 35.2006\n",
            "Epoch 89/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.7469 - val_loss: 35.0393\n",
            "Epoch 90/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.8315 - val_loss: 37.3557\n",
            "Epoch 91/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.7183 - val_loss: 34.4539\n",
            "Epoch 92/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.6969 - val_loss: 38.4486\n",
            "Epoch 93/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.7939 - val_loss: 38.9371\n",
            "Epoch 94/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.7010 - val_loss: 34.2718\n",
            "Epoch 95/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5760 - val_loss: 32.8331\n",
            "Epoch 96/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.6081 - val_loss: 34.2774\n",
            "Epoch 97/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.3957 - val_loss: 32.3387\n",
            "Epoch 98/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5791 - val_loss: 31.7725\n",
            "Epoch 99/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4254 - val_loss: 37.0797\n",
            "Epoch 100/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4085 - val_loss: 32.9512\n",
            "Epoch 101/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5648 - val_loss: 32.7341\n",
            "Epoch 102/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5304 - val_loss: 34.2167\n",
            "Epoch 103/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4508 - val_loss: 31.3616\n",
            "Epoch 104/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4711 - val_loss: 31.8880\n",
            "Epoch 105/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4933 - val_loss: 31.7883\n",
            "Epoch 106/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2530 - val_loss: 35.2973\n",
            "Epoch 107/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.4894 - val_loss: 33.0285\n",
            "Epoch 108/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.3413 - val_loss: 32.5764\n",
            "Epoch 109/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4092 - val_loss: 34.9777\n",
            "Epoch 110/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4252 - val_loss: 32.0524\n",
            "Epoch 111/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3399 - val_loss: 35.3322\n",
            "Epoch 112/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2700 - val_loss: 30.8098\n",
            "Epoch 113/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3540 - val_loss: 33.6456\n",
            "Epoch 114/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3059 - val_loss: 32.7085\n",
            "Epoch 115/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2318 - val_loss: 34.2634\n",
            "Epoch 116/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3135 - val_loss: 57.1273\n",
            "Epoch 117/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3097 - val_loss: 32.0733\n",
            "Epoch 118/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1705 - val_loss: 31.3303\n",
            "Epoch 119/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3113 - val_loss: 39.8720\n",
            "Epoch 120/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0987 - val_loss: 45.8261\n",
            "Epoch 121/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2212 - val_loss: 33.4053\n",
            "Epoch 122/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2506 - val_loss: 33.2094\n",
            "Epoch 123/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0520 - val_loss: 37.7902\n",
            "Epoch 124/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.1896 - val_loss: 32.8681\n",
            "Epoch 125/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2212 - val_loss: 34.1540\n",
            "Epoch 126/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0749 - val_loss: 33.6562\n",
            "Epoch 127/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1865 - val_loss: 33.2829\n",
            "Epoch 128/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1670 - val_loss: 37.4845\n",
            "Epoch 129/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0769 - val_loss: 32.5929\n",
            "Epoch 130/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0421 - val_loss: 36.7113\n",
            "Epoch 131/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0181 - val_loss: 37.0829\n",
            "Epoch 132/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0725 - val_loss: 32.3105\n",
            "Epoch 133/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1412 - val_loss: 33.9025\n",
            "Epoch 134/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0761 - val_loss: 35.3004\n",
            "Epoch 135/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1020 - val_loss: 32.7306\n",
            "Epoch 136/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9749 - val_loss: 31.9185\n",
            "Epoch 137/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9995 - val_loss: 31.2351\n",
            "Epoch 138/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0870 - val_loss: 34.8433\n",
            "Epoch 139/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0422 - val_loss: 30.9291\n",
            "Epoch 140/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0377 - val_loss: 35.5095\n",
            "Epoch 141/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0052 - val_loss: 44.7544\n",
            "Epoch 142/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1139 - val_loss: 31.1108\n",
            "Epoch 143/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9635 - val_loss: 33.1088\n",
            "Epoch 144/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8455 - val_loss: 33.7086\n",
            "Epoch 145/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0450 - val_loss: 33.7927\n",
            "Epoch 146/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9906 - val_loss: 33.7868\n",
            "Epoch 147/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8367 - val_loss: 33.3886\n",
            "Epoch 148/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9823 - val_loss: 31.8939\n",
            "Epoch 149/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.9145 - val_loss: 38.2617\n",
            "Epoch 150/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8957 - val_loss: 40.1691\n",
            "Epoch 151/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8137 - val_loss: 30.9046\n",
            "Epoch 152/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8358 - val_loss: 31.9276\n",
            "Epoch 153/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9246 - val_loss: 31.7568\n",
            "Epoch 154/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8719 - val_loss: 33.1308\n",
            "Epoch 155/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7887 - val_loss: 32.9795\n",
            "Epoch 156/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9020 - val_loss: 38.0063\n",
            "Epoch 157/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7194 - val_loss: 34.5659\n",
            "Epoch 158/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7971 - val_loss: 34.1874\n",
            "Epoch 159/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8368 - val_loss: 31.7402\n",
            "Epoch 160/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8314 - val_loss: 33.4013\n",
            "Epoch 161/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8032 - val_loss: 31.5817\n",
            "Epoch 162/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7260 - val_loss: 33.8184\n",
            "Epoch 163/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7588 - val_loss: 34.5664\n",
            "Epoch 164/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7420 - val_loss: 33.1311\n",
            "Epoch 165/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7682 - val_loss: 31.2707\n",
            "Epoch 166/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7581 - val_loss: 33.9631\n",
            "Epoch 167/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.7651 - val_loss: 32.0184\n",
            "Epoch 168/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6836 - val_loss: 31.2605\n",
            "Epoch 169/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7075 - val_loss: 33.0494\n",
            "Epoch 170/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6782 - val_loss: 37.7898\n",
            "Epoch 171/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6594 - val_loss: 32.2909\n",
            "Epoch 172/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7194 - val_loss: 31.4689\n",
            "Epoch 173/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6130 - val_loss: 31.1371\n",
            "Epoch 174/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7266 - val_loss: 37.3523\n",
            "Epoch 175/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6563 - val_loss: 35.2046\n",
            "Epoch 176/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6347 - val_loss: 33.0922\n",
            "Epoch 177/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5536 - val_loss: 31.9256\n",
            "Epoch 178/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6721 - val_loss: 35.0834\n",
            "Epoch 179/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6327 - val_loss: 32.7442\n",
            "Epoch 180/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5334 - val_loss: 32.3655\n",
            "Epoch 181/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6840 - val_loss: 33.6991\n",
            "Epoch 182/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6199 - val_loss: 32.1746\n",
            "Epoch 183/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5810 - val_loss: 32.2201\n",
            "Epoch 184/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5322 - val_loss: 33.2564\n",
            "Epoch 185/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6442 - val_loss: 30.8009\n",
            "Epoch 186/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 25.6721 - val_loss: 37.5706\n",
            "Epoch 187/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5990 - val_loss: 32.9508\n",
            "Epoch 188/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6447 - val_loss: 33.3171\n",
            "Epoch 189/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4953 - val_loss: 33.2256\n",
            "Epoch 190/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4969 - val_loss: 36.3278\n",
            "Epoch 191/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5641 - val_loss: 32.4051\n",
            "Epoch 192/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5290 - val_loss: 31.7309\n",
            "Epoch 193/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5507 - val_loss: 36.3596\n",
            "Epoch 194/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.5183 - val_loss: 46.5630\n",
            "Epoch 195/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6000 - val_loss: 33.8481\n",
            "Epoch 196/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5188 - val_loss: 35.5582\n",
            "Epoch 197/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5659 - val_loss: 34.8613\n",
            "Epoch 198/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4885 - val_loss: 33.8863\n",
            "Epoch 199/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3041 - val_loss: 32.1935\n",
            "Epoch 200/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4990 - val_loss: 33.2177\n",
            "Epoch 201/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4195 - val_loss: 32.8089\n",
            "Epoch 202/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3845 - val_loss: 33.9785\n",
            "Epoch 203/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4897 - val_loss: 34.6917\n",
            "Epoch 204/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4454 - val_loss: 32.7493\n",
            "Epoch 205/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4801 - val_loss: 33.1703\n",
            "Epoch 206/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4750 - val_loss: 31.6147\n",
            "Epoch 207/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4702 - val_loss: 34.1643\n",
            "Epoch 208/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4235 - val_loss: 34.7721\n",
            "Epoch 209/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4939 - val_loss: 32.9652\n",
            "Epoch 210/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4132 - val_loss: 31.5632\n",
            "Epoch 211/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4626 - val_loss: 32.2920\n",
            "Epoch 212/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4440 - val_loss: 33.9800\n",
            "Epoch 213/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4138 - val_loss: 32.4323\n",
            "Epoch 214/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4090 - val_loss: 31.0724\n",
            "Epoch 215/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3273 - val_loss: 31.1865\n",
            "Epoch 216/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4164 - val_loss: 32.4729\n",
            "Epoch 217/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3613 - val_loss: 31.2637\n",
            "Epoch 218/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3562 - val_loss: 32.8900\n",
            "Epoch 219/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3295 - val_loss: 33.5370\n",
            "Epoch 220/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4200 - val_loss: 33.3213\n",
            "Epoch 221/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4330 - val_loss: 33.3548\n",
            "Epoch 222/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3360 - val_loss: 31.7362\n",
            "Epoch 223/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3691 - val_loss: 35.6734\n",
            "Epoch 224/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4490 - val_loss: 33.0542\n",
            "Epoch 225/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3154 - val_loss: 34.3241\n",
            "Epoch 226/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4040 - val_loss: 35.6458\n",
            "Epoch 227/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.2345 - val_loss: 34.6487\n",
            "Epoch 228/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3665 - val_loss: 33.4333\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 229/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3050 - val_loss: 38.3424\n",
            "Epoch 230/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4113 - val_loss: 37.5011\n",
            "Epoch 231/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4161 - val_loss: 32.2085\n",
            "Epoch 232/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3311 - val_loss: 33.4415\n",
            "Epoch 233/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.1634 - val_loss: 30.4081\n",
            "Epoch 234/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.2918 - val_loss: 32.7427\n",
            "Epoch 235/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3634 - val_loss: 31.1694\n",
            "Epoch 236/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3584 - val_loss: 33.7114\n",
            "Epoch 237/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3746 - val_loss: 31.2042\n",
            "Epoch 238/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.2516 - val_loss: 31.5918\n",
            "Epoch 239/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.1403 - val_loss: 37.2074\n",
            "Epoch 240/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.2141 - val_loss: 31.0384\n",
            "Epoch 241/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.2301 - val_loss: 30.6553\n",
            "Epoch 242/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.2685 - val_loss: 32.9965\n",
            "Epoch 243/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.2541 - val_loss: 36.9513\n",
            "Epoch 244/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3134 - val_loss: 33.7462\n",
            "Epoch 245/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.2338 - val_loss: 32.5900\n",
            "Epoch 246/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1718 - val_loss: 33.9807\n",
            "Epoch 247/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1736 - val_loss: 37.7562\n",
            "Epoch 248/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1773 - val_loss: 34.6843\n",
            "Epoch 249/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1977 - val_loss: 34.8074\n",
            "Epoch 250/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1542 - val_loss: 33.8613\n",
            "Epoch 251/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1874 - val_loss: 30.4105\n",
            "Epoch 252/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1316 - val_loss: 43.7303\n",
            "Epoch 253/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1858 - val_loss: 31.9574\n",
            "Epoch 254/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1758 - val_loss: 31.1102\n",
            "Epoch 255/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1179 - val_loss: 49.4701\n",
            "Epoch 256/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1860 - val_loss: 30.3067\n",
            "Epoch 257/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1424 - val_loss: 31.2326\n",
            "Epoch 258/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0563 - val_loss: 32.1298\n",
            "Epoch 259/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1328 - val_loss: 32.6446\n",
            "Epoch 260/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1795 - val_loss: 32.0677\n",
            "Epoch 261/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1306 - val_loss: 36.0389\n",
            "Epoch 262/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.2170 - val_loss: 35.4185\n",
            "Epoch 263/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1842 - val_loss: 31.6582\n",
            "Epoch 264/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.0816 - val_loss: 33.2938\n",
            "Epoch 265/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0552 - val_loss: 31.4608\n",
            "Epoch 266/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0201 - val_loss: 36.6313\n",
            "Epoch 267/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1010 - val_loss: 30.9442\n",
            "Epoch 268/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1110 - val_loss: 31.3866\n",
            "Epoch 269/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9812 - val_loss: 31.0719\n",
            "Epoch 270/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1667 - val_loss: 36.7088\n",
            "Epoch 271/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0827 - val_loss: 35.2018\n",
            "Epoch 272/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0823 - val_loss: 35.9355\n",
            "Epoch 273/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0668 - val_loss: 30.9184\n",
            "Epoch 274/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1094 - val_loss: 33.8426\n",
            "Epoch 275/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.0989 - val_loss: 32.5566\n",
            "Epoch 276/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9981 - val_loss: 34.0378\n",
            "Epoch 277/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9782 - val_loss: 32.6151\n",
            "Epoch 278/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0366 - val_loss: 32.7079\n",
            "Epoch 279/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0522 - val_loss: 32.1775\n",
            "Epoch 280/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9272 - val_loss: 31.4635\n",
            "Epoch 281/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0508 - val_loss: 31.5109\n",
            "Epoch 282/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9826 - val_loss: 31.3252\n",
            "Epoch 283/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1351 - val_loss: 31.1804\n",
            "Epoch 284/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9810 - val_loss: 36.0945\n",
            "Epoch 285/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.1196 - val_loss: 32.1919\n",
            "Epoch 286/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9745 - val_loss: 31.7137\n",
            "Epoch 287/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0143 - val_loss: 32.0348\n",
            "Epoch 288/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0113 - val_loss: 33.1697\n",
            "Epoch 289/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0362 - val_loss: 36.8242\n",
            "Epoch 290/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9625 - val_loss: 31.7833\n",
            "Epoch 291/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9663 - val_loss: 35.1073\n",
            "Epoch 292/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.8826 - val_loss: 30.6427\n",
            "Epoch 293/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9461 - val_loss: 31.7238\n",
            "Epoch 294/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0557 - val_loss: 30.7274\n",
            "Epoch 295/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0445 - val_loss: 34.3451\n",
            "Epoch 296/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9592 - val_loss: 30.8408\n",
            "Epoch 297/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0859 - val_loss: 31.9202\n",
            "Epoch 298/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9762 - val_loss: 34.5838\n",
            "Epoch 299/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.0040 - val_loss: 36.6502\n",
            "Epoch 300/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 24.9151 - val_loss: 31.9336\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  0.06656703574668575 \n",
            "MAE:  4.181968086550382 \n",
            "SD:  5.650587408508971\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_72 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_66 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_67 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_68 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_68 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_69 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_70 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_71 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_78 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_72 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_73 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_73 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_74 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_74 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_81 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_75 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_75 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_76 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_76 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 1654.8414 - val_loss: 87.6528\n",
            "Epoch 2/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 60.7237 - val_loss: 45.6165\n",
            "Epoch 3/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 43.0740 - val_loss: 46.2403\n",
            "Epoch 4/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 39.6420 - val_loss: 60.9226\n",
            "Epoch 5/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 37.3198 - val_loss: 133.4979\n",
            "Epoch 6/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 36.0424 - val_loss: 39.4369\n",
            "Epoch 7/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 35.0662 - val_loss: 54.9779\n",
            "Epoch 8/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 34.3632 - val_loss: 37.9750\n",
            "Epoch 9/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 34.0592 - val_loss: 67.9022\n",
            "Epoch 10/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 33.7452 - val_loss: 46.5561\n",
            "Epoch 11/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 33.5303 - val_loss: 42.4430\n",
            "Epoch 12/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 33.2842 - val_loss: 44.3043\n",
            "Epoch 13/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 32.9130 - val_loss: 34.9767\n",
            "Epoch 14/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 32.7355 - val_loss: 50.4477\n",
            "Epoch 15/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 32.1603 - val_loss: 73.9089\n",
            "Epoch 16/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 32.0545 - val_loss: 101.2029\n",
            "Epoch 17/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 32.0032 - val_loss: 43.8191\n",
            "Epoch 18/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 31.7655 - val_loss: 63.9604\n",
            "Epoch 19/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 31.6155 - val_loss: 36.5815\n",
            "Epoch 20/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 31.3356 - val_loss: 33.7504\n",
            "Epoch 21/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 31.2671 - val_loss: 46.0927\n",
            "Epoch 22/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 31.1813 - val_loss: 34.1572\n",
            "Epoch 23/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.9668 - val_loss: 40.1260\n",
            "Epoch 24/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.8859 - val_loss: 36.1856\n",
            "Epoch 25/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.5085 - val_loss: 40.1047\n",
            "Epoch 26/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.5959 - val_loss: 36.0008\n",
            "Epoch 27/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.3503 - val_loss: 34.8649\n",
            "Epoch 28/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.2196 - val_loss: 37.3243\n",
            "Epoch 29/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.0049 - val_loss: 43.9988\n",
            "Epoch 30/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.0244 - val_loss: 40.0337\n",
            "Epoch 31/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 30.0700 - val_loss: 37.9268\n",
            "Epoch 32/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.9171 - val_loss: 36.4551\n",
            "Epoch 33/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.7555 - val_loss: 35.1694\n",
            "Epoch 34/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.7653 - val_loss: 34.7116\n",
            "Epoch 35/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.5756 - val_loss: 38.4768\n",
            "Epoch 36/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.5847 - val_loss: 33.5224\n",
            "Epoch 37/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.4890 - val_loss: 32.6880\n",
            "Epoch 38/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.4459 - val_loss: 37.2105\n",
            "Epoch 39/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 29.3946 - val_loss: 36.5988\n",
            "Epoch 40/300\n",
            "2637/2637 [==============================] - 22s 8ms/step - loss: 29.4410 - val_loss: 37.5359\n",
            "Epoch 41/300\n",
            "2637/2637 [==============================] - 25s 10ms/step - loss: 29.1791 - val_loss: 43.9697\n",
            "Epoch 42/300\n",
            "2637/2637 [==============================] - 29s 11ms/step - loss: 29.1292 - val_loss: 32.4348\n",
            "Epoch 43/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 29.2090 - val_loss: 34.5440\n",
            "Epoch 44/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.8402 - val_loss: 41.5273\n",
            "Epoch 45/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.9526 - val_loss: 35.6854\n",
            "Epoch 46/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.9115 - val_loss: 33.4475\n",
            "Epoch 47/300\n",
            "2637/2637 [==============================] - 59s 22ms/step - loss: 29.0199 - val_loss: 33.5420\n",
            "Epoch 48/300\n",
            "2637/2637 [==============================] - 86s 33ms/step - loss: 28.8682 - val_loss: 35.3651\n",
            "Epoch 49/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.8246 - val_loss: 34.8381\n",
            "Epoch 50/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.6316 - val_loss: 38.5078\n",
            "Epoch 51/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.6415 - val_loss: 51.4206\n",
            "Epoch 52/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.5821 - val_loss: 32.5266\n",
            "Epoch 53/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.6877 - val_loss: 33.4654\n",
            "Epoch 54/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.4504 - val_loss: 32.4630\n",
            "Epoch 55/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.5262 - val_loss: 38.5702\n",
            "Epoch 56/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.6033 - val_loss: 34.0431\n",
            "Epoch 57/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.4599 - val_loss: 43.7663\n",
            "Epoch 58/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.4717 - val_loss: 33.6201\n",
            "Epoch 59/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.3809 - val_loss: 39.4304\n",
            "Epoch 60/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.4395 - val_loss: 54.6251\n",
            "Epoch 61/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.1431 - val_loss: 33.3913\n",
            "Epoch 62/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.2240 - val_loss: 36.7384\n",
            "Epoch 63/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.0823 - val_loss: 43.3880\n",
            "Epoch 64/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.0937 - val_loss: 36.2223\n",
            "Epoch 65/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.0563 - val_loss: 35.1715\n",
            "Epoch 66/300\n",
            "2637/2637 [==============================] - 17s 7ms/step - loss: 28.1044 - val_loss: 46.6283\n",
            "Epoch 67/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 28.1120 - val_loss: 43.0609\n",
            "Epoch 68/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.0679 - val_loss: 45.1776\n",
            "Epoch 69/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 28.0986 - val_loss: 36.0792\n",
            "Epoch 70/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.9569 - val_loss: 34.2926\n",
            "Epoch 71/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.9682 - val_loss: 36.6876\n",
            "Epoch 72/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.9781 - val_loss: 34.5408\n",
            "Epoch 73/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.8519 - val_loss: 39.9953\n",
            "Epoch 74/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.7617 - val_loss: 33.2538\n",
            "Epoch 75/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.7779 - val_loss: 34.5400\n",
            "Epoch 76/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.8090 - val_loss: 33.0038\n",
            "Epoch 77/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.6363 - val_loss: 33.8054\n",
            "Epoch 78/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.6546 - val_loss: 41.8167\n",
            "Epoch 79/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.7816 - val_loss: 34.7818\n",
            "Epoch 80/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.5883 - val_loss: 40.1211\n",
            "Epoch 81/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.5220 - val_loss: 33.6433\n",
            "Epoch 82/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.6269 - val_loss: 40.8143\n",
            "Epoch 83/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.7336 - val_loss: 32.3333\n",
            "Epoch 84/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.6328 - val_loss: 36.3784\n",
            "Epoch 85/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.4786 - val_loss: 32.8066\n",
            "Epoch 86/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.7287 - val_loss: 38.4945\n",
            "Epoch 87/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.5663 - val_loss: 31.9066\n",
            "Epoch 88/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.4634 - val_loss: 32.6387\n",
            "Epoch 89/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.4407 - val_loss: 33.6822\n",
            "Epoch 90/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.4970 - val_loss: 41.8070\n",
            "Epoch 91/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.4062 - val_loss: 41.3439\n",
            "Epoch 92/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.4287 - val_loss: 32.0728\n",
            "Epoch 93/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.3185 - val_loss: 33.9558\n",
            "Epoch 94/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.3058 - val_loss: 32.2055\n",
            "Epoch 95/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.2499 - val_loss: 38.5064\n",
            "Epoch 96/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.0973 - val_loss: 32.2490\n",
            "Epoch 97/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 27.2234 - val_loss: 34.7314\n",
            "Epoch 98/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.1851 - val_loss: 42.6056\n",
            "Epoch 99/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.1487 - val_loss: 34.5952\n",
            "Epoch 100/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.0987 - val_loss: 35.3029\n",
            "Epoch 101/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.0311 - val_loss: 36.2437\n",
            "Epoch 102/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.0811 - val_loss: 33.5393\n",
            "Epoch 103/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.9984 - val_loss: 33.2095\n",
            "Epoch 104/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.1075 - val_loss: 31.6199\n",
            "Epoch 105/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.1543 - val_loss: 38.2949\n",
            "Epoch 106/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.9319 - val_loss: 31.2438\n",
            "Epoch 107/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.0369 - val_loss: 42.3546\n",
            "Epoch 108/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.9928 - val_loss: 33.7632\n",
            "Epoch 109/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.0494 - val_loss: 41.4724\n",
            "Epoch 110/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.9764 - val_loss: 36.1562\n",
            "Epoch 111/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.9993 - val_loss: 33.0153\n",
            "Epoch 112/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 27.0607 - val_loss: 43.1530\n",
            "Epoch 113/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.8322 - val_loss: 35.6663\n",
            "Epoch 114/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.9443 - val_loss: 34.1604\n",
            "Epoch 115/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.9132 - val_loss: 33.6441\n",
            "Epoch 116/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.7577 - val_loss: 33.6378\n",
            "Epoch 117/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.9526 - val_loss: 31.7218\n",
            "Epoch 118/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.7536 - val_loss: 31.9254\n",
            "Epoch 119/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.8578 - val_loss: 31.7196\n",
            "Epoch 120/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.8425 - val_loss: 33.3248\n",
            "Epoch 121/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.9067 - val_loss: 35.3914\n",
            "Epoch 122/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.6487 - val_loss: 35.9753\n",
            "Epoch 123/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5938 - val_loss: 37.9033\n",
            "Epoch 124/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.6953 - val_loss: 32.9141\n",
            "Epoch 125/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.7692 - val_loss: 32.8040\n",
            "Epoch 126/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.7832 - val_loss: 33.6629\n",
            "Epoch 127/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.7327 - val_loss: 33.4717\n",
            "Epoch 128/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.6284 - val_loss: 31.5573\n",
            "Epoch 129/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.7637 - val_loss: 34.0804\n",
            "Epoch 130/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.6036 - val_loss: 31.4857\n",
            "Epoch 131/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.6018 - val_loss: 37.6756\n",
            "Epoch 132/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5268 - val_loss: 35.8835\n",
            "Epoch 133/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5038 - val_loss: 32.5539\n",
            "Epoch 134/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5226 - val_loss: 35.4669\n",
            "Epoch 135/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5874 - val_loss: 32.1857\n",
            "Epoch 136/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5342 - val_loss: 34.4334\n",
            "Epoch 137/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5030 - val_loss: 33.6690\n",
            "Epoch 138/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5870 - val_loss: 35.3397\n",
            "Epoch 139/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4579 - val_loss: 33.4413\n",
            "Epoch 140/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4733 - val_loss: 33.5536\n",
            "Epoch 141/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5399 - val_loss: 43.7369\n",
            "Epoch 142/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4476 - val_loss: 37.3129\n",
            "Epoch 143/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.6078 - val_loss: 37.2960\n",
            "Epoch 144/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4739 - val_loss: 32.4608\n",
            "Epoch 145/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3438 - val_loss: 54.9555\n",
            "Epoch 146/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4718 - val_loss: 35.1106\n",
            "Epoch 147/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3299 - val_loss: 34.9166\n",
            "Epoch 148/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5325 - val_loss: 47.6140\n",
            "Epoch 149/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4978 - val_loss: 33.3204\n",
            "Epoch 150/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4222 - val_loss: 31.6872\n",
            "Epoch 151/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3832 - val_loss: 53.0744\n",
            "Epoch 152/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4938 - val_loss: 31.8171\n",
            "Epoch 153/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5053 - val_loss: 32.9116\n",
            "Epoch 154/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4511 - val_loss: 32.3906\n",
            "Epoch 155/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3670 - val_loss: 33.2407\n",
            "Epoch 156/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3134 - val_loss: 31.6382\n",
            "Epoch 157/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4198 - val_loss: 33.0108\n",
            "Epoch 158/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4044 - val_loss: 32.2715\n",
            "Epoch 159/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3758 - val_loss: 32.2959\n",
            "Epoch 160/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3602 - val_loss: 35.1841\n",
            "Epoch 161/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1919 - val_loss: 37.3851\n",
            "Epoch 162/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1721 - val_loss: 33.2632\n",
            "Epoch 163/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2556 - val_loss: 32.2310\n",
            "Epoch 164/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2320 - val_loss: 32.0618\n",
            "Epoch 165/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.5011 - val_loss: 34.6328\n",
            "Epoch 166/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.4543 - val_loss: 40.9840\n",
            "Epoch 167/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2319 - val_loss: 32.5097\n",
            "Epoch 168/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2512 - val_loss: 40.0099\n",
            "Epoch 169/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1969 - val_loss: 35.6786\n",
            "Epoch 170/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2386 - val_loss: 31.6247\n",
            "Epoch 171/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2573 - val_loss: 33.8072\n",
            "Epoch 172/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1779 - val_loss: 32.3281\n",
            "Epoch 173/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.3593 - val_loss: 31.0471\n",
            "Epoch 174/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1606 - val_loss: 33.9604\n",
            "Epoch 175/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.2346 - val_loss: 32.0066\n",
            "Epoch 176/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.1033 - val_loss: 34.3230\n",
            "Epoch 177/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.1038 - val_loss: 34.4167\n",
            "Epoch 178/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0651 - val_loss: 40.6453\n",
            "Epoch 179/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0632 - val_loss: 31.7367\n",
            "Epoch 180/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 26.0710 - val_loss: 32.7198\n",
            "Epoch 181/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1071 - val_loss: 32.2692\n",
            "Epoch 182/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0854 - val_loss: 31.3090\n",
            "Epoch 183/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0832 - val_loss: 34.6041\n",
            "Epoch 184/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0329 - val_loss: 35.8704\n",
            "Epoch 185/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9878 - val_loss: 32.3855\n",
            "Epoch 186/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0296 - val_loss: 32.3330\n",
            "Epoch 187/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9014 - val_loss: 32.9427\n",
            "Epoch 188/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0948 - val_loss: 34.1351\n",
            "Epoch 189/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0071 - val_loss: 33.7800\n",
            "Epoch 190/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0599 - val_loss: 33.5334\n",
            "Epoch 191/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0030 - val_loss: 39.0367\n",
            "Epoch 192/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0195 - val_loss: 31.4585\n",
            "Epoch 193/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9916 - val_loss: 33.9897\n",
            "Epoch 194/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0363 - val_loss: 31.9479\n",
            "Epoch 195/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.9632 - val_loss: 37.2453\n",
            "Epoch 196/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9804 - val_loss: 32.8609\n",
            "Epoch 197/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0848 - val_loss: 32.1589\n",
            "Epoch 198/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.1023 - val_loss: 33.9247\n",
            "Epoch 199/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9946 - val_loss: 38.4044\n",
            "Epoch 200/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0220 - val_loss: 39.3957\n",
            "Epoch 201/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9169 - val_loss: 32.2240\n",
            "Epoch 202/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0613 - val_loss: 35.8153\n",
            "Epoch 203/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9740 - val_loss: 32.3742\n",
            "Epoch 204/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9518 - val_loss: 33.5404\n",
            "Epoch 205/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 26.0273 - val_loss: 32.8583\n",
            "Epoch 206/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9784 - val_loss: 43.7488\n",
            "Epoch 207/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9731 - val_loss: 31.1409\n",
            "Epoch 208/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9264 - val_loss: 33.0070\n",
            "Epoch 209/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9398 - val_loss: 40.1101\n",
            "Epoch 210/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8833 - val_loss: 32.1244\n",
            "Epoch 211/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8616 - val_loss: 33.4745\n",
            "Epoch 212/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8193 - val_loss: 45.8899\n",
            "Epoch 213/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9693 - val_loss: 72.2984\n",
            "Epoch 214/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9371 - val_loss: 35.3609\n",
            "Epoch 215/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9719 - val_loss: 33.6856\n",
            "Epoch 216/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8925 - val_loss: 32.0030\n",
            "Epoch 217/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9304 - val_loss: 31.8371\n",
            "Epoch 218/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9063 - val_loss: 32.8776\n",
            "Epoch 219/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7941 - val_loss: 31.2605\n",
            "Epoch 220/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8884 - val_loss: 31.7836\n",
            "Epoch 221/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.9615 - val_loss: 31.8961\n",
            "Epoch 222/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7824 - val_loss: 31.0394\n",
            "Epoch 223/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8580 - val_loss: 34.1726\n",
            "Epoch 224/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8272 - val_loss: 32.5713\n",
            "Epoch 225/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8717 - val_loss: 34.6627\n",
            "Epoch 226/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8359 - val_loss: 32.3107\n",
            "Epoch 227/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8581 - val_loss: 34.1232\n",
            "Epoch 228/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7637 - val_loss: 33.2409\n",
            "Epoch 229/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7514 - val_loss: 36.3367\n",
            "Epoch 230/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8687 - val_loss: 32.1262\n",
            "Epoch 231/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8354 - val_loss: 41.6654\n",
            "Epoch 232/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6953 - val_loss: 31.7716\n",
            "Epoch 233/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8672 - val_loss: 32.0009\n",
            "Epoch 234/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6084 - val_loss: 33.8159\n",
            "Epoch 235/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7759 - val_loss: 39.0628\n",
            "Epoch 236/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7708 - val_loss: 31.9761\n",
            "Epoch 237/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8243 - val_loss: 34.2421\n",
            "Epoch 238/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8432 - val_loss: 37.9296\n",
            "Epoch 239/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7608 - val_loss: 31.4918\n",
            "Epoch 240/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7167 - val_loss: 34.9606\n",
            "Epoch 241/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7960 - val_loss: 37.2284\n",
            "Epoch 242/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8794 - val_loss: 31.7377\n",
            "Epoch 243/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6154 - val_loss: 32.1819\n",
            "Epoch 244/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7613 - val_loss: 35.8784\n",
            "Epoch 245/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6678 - val_loss: 33.2276\n",
            "Epoch 246/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7428 - val_loss: 33.3800\n",
            "Epoch 247/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.8018 - val_loss: 37.0418\n",
            "Epoch 248/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7929 - val_loss: 37.2712\n",
            "Epoch 249/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7242 - val_loss: 37.0181\n",
            "Epoch 250/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6104 - val_loss: 38.1767\n",
            "Epoch 251/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7139 - val_loss: 31.4373\n",
            "Epoch 252/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7059 - val_loss: 34.3067\n",
            "Epoch 253/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6742 - val_loss: 37.7723\n",
            "Epoch 254/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6777 - val_loss: 31.9554\n",
            "Epoch 255/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.7560 - val_loss: 30.8165\n",
            "Epoch 256/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6520 - val_loss: 33.9575\n",
            "Epoch 257/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6577 - val_loss: 32.5137\n",
            "Epoch 258/300\n",
            "2637/2637 [==============================] - 18s 7ms/step - loss: 25.6965 - val_loss: 33.2532\n",
            "Epoch 259/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6624 - val_loss: 31.6528\n",
            "Epoch 260/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6471 - val_loss: 32.8891\n",
            "Epoch 261/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5959 - val_loss: 33.2872\n",
            "Epoch 262/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6368 - val_loss: 31.0225\n",
            "Epoch 263/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5367 - val_loss: 38.1695\n",
            "Epoch 264/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5949 - val_loss: 37.6511\n",
            "Epoch 265/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5432 - val_loss: 37.4749\n",
            "Epoch 266/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4991 - val_loss: 31.0615\n",
            "Epoch 267/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6316 - val_loss: 32.6710\n",
            "Epoch 268/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4909 - val_loss: 34.0630\n",
            "Epoch 269/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4376 - val_loss: 30.8492\n",
            "Epoch 270/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5559 - val_loss: 31.9823\n",
            "Epoch 271/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6213 - val_loss: 45.1982\n",
            "Epoch 272/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5190 - val_loss: 31.0171\n",
            "Epoch 273/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5687 - val_loss: 31.2475\n",
            "Epoch 274/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5893 - val_loss: 32.9669\n",
            "Epoch 275/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6140 - val_loss: 32.0636\n",
            "Epoch 276/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5577 - val_loss: 31.7928\n",
            "Epoch 277/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5431 - val_loss: 37.6788\n",
            "Epoch 278/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6502 - val_loss: 32.4764\n",
            "Epoch 279/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5101 - val_loss: 35.0967\n",
            "Epoch 280/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4884 - val_loss: 32.0473\n",
            "Epoch 281/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4217 - val_loss: 32.7539\n",
            "Epoch 282/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4663 - val_loss: 32.4068\n",
            "Epoch 283/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4172 - val_loss: 33.2512\n",
            "Epoch 284/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5518 - val_loss: 32.5373\n",
            "Epoch 285/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4552 - val_loss: 33.2836\n",
            "Epoch 286/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5506 - val_loss: 32.3743\n",
            "Epoch 287/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4399 - val_loss: 56.5501\n",
            "Epoch 288/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5263 - val_loss: 31.5826\n",
            "Epoch 289/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.6043 - val_loss: 30.8390\n",
            "Epoch 290/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4720 - val_loss: 38.8419\n",
            "Epoch 291/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5164 - val_loss: 31.2873\n",
            "Epoch 292/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.6118 - val_loss: 39.6598\n",
            "Epoch 293/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4907 - val_loss: 32.6598\n",
            "Epoch 294/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.3326 - val_loss: 32.4145\n",
            "Epoch 295/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.5114 - val_loss: 31.2136\n",
            "Epoch 296/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3618 - val_loss: 33.1787\n",
            "Epoch 297/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4543 - val_loss: 33.7768\n",
            "Epoch 298/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.4568 - val_loss: 32.2140\n",
            "Epoch 299/300\n",
            "2637/2637 [==============================] - 16s 6ms/step - loss: 25.3934 - val_loss: 30.9323\n",
            "Epoch 300/300\n",
            "2637/2637 [==============================] - 17s 6ms/step - loss: 25.4502 - val_loss: 33.0232\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -1.0240847036660259 \n",
            "MAE:  4.24585262097823 \n",
            "SD:  5.6545916631138216\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "outputId": "3af79ff2-e7d5-41fe-92f6-a18a7e0f3a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  0.27153686142699174 \n",
            "Ensemble_std:  5.608559812206008\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "_BP_hv3_4(2)(2).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}