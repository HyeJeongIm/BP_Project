{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/%08BP_hv3_6(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YTF6cMiY1Hw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiiiBla2-j1S"
      },
      "source": [
        "# batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCoux5AOZnK",
        "outputId": "af01081b-e0f0-43e8-d7fb-8c67538adde5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "TensorFlow version :  2.8.2\n",
            "Keras version :  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlHICkovd809",
        "outputId": "94386526-c8d7-4f65-9a8b-89d945053d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러z오기\n",
        "train_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "1019a468-0950-4058-dd6a-7854f44d3b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "27b9c89d-b034-4624-fbf5-80fb31d56ed9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee04c742-6e53-43d4-97eb-33ad3aff7a1c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee04c742-6e53-43d4-97eb-33ad3aff7a1c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee04c742-6e53-43d4-97eb-33ad3aff7a1c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee04c742-6e53-43d4-97eb-33ad3aff7a1c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "106f2938-58a8-41f4-887a-7ffca1388554"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c4b0b1ee-59eb-4225-a2ec-db9f32214599\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4b0b1ee-59eb-4225-a2ec-db9f32214599')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c4b0b1ee-59eb-4225-a2ec-db9f32214599 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c4b0b1ee-59eb-4225-a2ec-db9f32214599');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "outputs": [],
      "source": [
        "#parameter\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI8SHBwBOZnO",
        "outputId": "45027574-e7dc-487f-cf53-2134273e99a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 4)                 512       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 4)                16        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 641\n",
            "Trainable params: 609\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGT6-7NcOZnO",
        "scrolled": true,
        "outputId": "148ea9d7-d7fe-4f9a-daeb-cb90af8e15bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 5s 7ms/step - loss: 12382.0186 - val_loss: 12377.1367\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 12157.2168 - val_loss: 11894.2705\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11875.5430 - val_loss: 11471.2197\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11529.4893 - val_loss: 11012.8047\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11101.4756 - val_loss: 10675.3936\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 10569.8906 - val_loss: 10163.4629\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9948.4482 - val_loss: 9405.7422\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9270.9580 - val_loss: 8651.2168\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8562.7539 - val_loss: 7982.3301\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7843.3350 - val_loss: 7420.4585\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7127.9136 - val_loss: 6601.3140\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 6429.9302 - val_loss: 5866.8550\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5759.6221 - val_loss: 5344.7759\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5125.2939 - val_loss: 4790.9326\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4167.1426 - val_loss: 3143.1741\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 3013.0605 - val_loss: 2594.0771\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2364.3970 - val_loss: 1467.7946\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1855.5135 - val_loss: 1249.9042\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 1436.2390 - val_loss: 2155.7957\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 1094.9535 - val_loss: 1148.2404\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 827.9329 - val_loss: 1137.5051\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 612.6429 - val_loss: 462.7722\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 453.6192 - val_loss: 362.9473\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 339.8929 - val_loss: 229.8510\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 256.5475 - val_loss: 288.6340\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 201.0786 - val_loss: 160.7267\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 165.2801 - val_loss: 127.7193\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 143.7305 - val_loss: 193.3633\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 130.9088 - val_loss: 183.7268\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 121.8428 - val_loss: 133.9093\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 117.5970 - val_loss: 137.3938\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 114.6231 - val_loss: 116.9754\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 113.9395 - val_loss: 120.4721\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 113.1282 - val_loss: 128.3634\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 112.4447 - val_loss: 119.3866\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 112.4817 - val_loss: 142.1115\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.7670 - val_loss: 120.2553\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 111.6580 - val_loss: 137.1782\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 110.9177 - val_loss: 119.9452\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.8496 - val_loss: 124.9545\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.2453 - val_loss: 147.0021\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 110.4945 - val_loss: 114.9542\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.7968 - val_loss: 115.1312\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.7563 - val_loss: 117.3764\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.7372 - val_loss: 118.6049\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.3746 - val_loss: 118.6916\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.5579 - val_loss: 113.4399\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 108.8637 - val_loss: 121.6280\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.4896 - val_loss: 115.1199\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 108.5295 - val_loss: 122.0573\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 108.4479 - val_loss: 121.0427\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.1649 - val_loss: 123.6563\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.8358 - val_loss: 114.0147\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.6870 - val_loss: 123.1414\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 107.7163 - val_loss: 112.7455\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.4776 - val_loss: 114.7754\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 107.4770 - val_loss: 118.6976\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 107.3325 - val_loss: 119.2316\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.3088 - val_loss: 115.1262\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 107.0112 - val_loss: 114.4771\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.8686 - val_loss: 121.9086\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.7173 - val_loss: 118.6231\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.4659 - val_loss: 129.1258\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.2850 - val_loss: 121.0755\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.7133 - val_loss: 121.0122\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.8000 - val_loss: 117.3721\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.7358 - val_loss: 132.9320\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.4673 - val_loss: 119.3084\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.6916 - val_loss: 115.7759\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.2594 - val_loss: 112.4638\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.1344 - val_loss: 124.3261\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0210 - val_loss: 111.1669\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.8790 - val_loss: 112.9891\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8682 - val_loss: 114.3106\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0327 - val_loss: 124.0935\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.6400 - val_loss: 111.2808\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.5261 - val_loss: 115.7260\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.3536 - val_loss: 128.2646\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.2885 - val_loss: 116.4034\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1456 - val_loss: 111.1177\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.6597 - val_loss: 120.7188\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.9572 - val_loss: 113.3243\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7214 - val_loss: 120.0679\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.3851 - val_loss: 123.2736\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.4027 - val_loss: 134.2246\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.3635 - val_loss: 112.9973\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4684 - val_loss: 109.1874\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.3867 - val_loss: 112.6894\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.0893 - val_loss: 117.0179\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.0351 - val_loss: 117.0323\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.1975 - val_loss: 115.4264\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6374 - val_loss: 115.3489\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6765 - val_loss: 109.3561\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.2017 - val_loss: 114.9343\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.0949 - val_loss: 111.1549\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.1384 - val_loss: 113.2237\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0415 - val_loss: 110.9382\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9310 - val_loss: 115.4475\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.7289 - val_loss: 109.0765\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6461 - val_loss: 109.2477\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4356 - val_loss: 111.7112\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2484 - val_loss: 112.9499\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.3222 - val_loss: 108.5940\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.8624 - val_loss: 118.2106\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.7687 - val_loss: 108.0051\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.6484 - val_loss: 107.4195\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.1512 - val_loss: 111.3135\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.9246 - val_loss: 108.8032\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3638 - val_loss: 118.9125\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4765 - val_loss: 109.4312\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.0156 - val_loss: 109.6030\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.4937 - val_loss: 128.5464\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.5861 - val_loss: 118.2943\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.0106 - val_loss: 109.2166\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.8114 - val_loss: 105.2008\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.5004 - val_loss: 108.0969\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.4547 - val_loss: 105.1044\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.9690 - val_loss: 103.7045\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.8370 - val_loss: 109.6473\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.5233 - val_loss: 105.7214\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.5178 - val_loss: 107.3475\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.1230 - val_loss: 105.9793\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.8868 - val_loss: 103.0837\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.8874 - val_loss: 104.0254\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.5720 - val_loss: 130.6918\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.5276 - val_loss: 115.2428\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.5188 - val_loss: 106.8790\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.3449 - val_loss: 116.4550\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.3789 - val_loss: 109.3968\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.9841 - val_loss: 102.3851\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.2260 - val_loss: 133.5084\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.7620 - val_loss: 104.1131\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.7508 - val_loss: 157.4014\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.8249 - val_loss: 127.3321\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.5530 - val_loss: 101.9359\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.3989 - val_loss: 104.4010\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.6036 - val_loss: 130.3147\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.3012 - val_loss: 102.5766\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.3613 - val_loss: 118.4189\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.3475 - val_loss: 106.8072\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1236 - val_loss: 112.2662\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2861 - val_loss: 101.2904\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9843 - val_loss: 105.7213\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7590 - val_loss: 114.9458\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1844 - val_loss: 111.0690\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9698 - val_loss: 100.5309\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.9479 - val_loss: 116.8326\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.7368 - val_loss: 159.8330\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.8987 - val_loss: 109.6671\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.5710 - val_loss: 112.4672\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.7082 - val_loss: 103.3884\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6180 - val_loss: 246.7664\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5817 - val_loss: 100.6309\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.4701 - val_loss: 102.5619\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.4806 - val_loss: 104.8222\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.4012 - val_loss: 112.0145\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.2623 - val_loss: 106.0842\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.1190 - val_loss: 157.0372\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.2579 - val_loss: 110.3820\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.2908 - val_loss: 102.7381\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.2309 - val_loss: 99.7529\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.1203 - val_loss: 102.8011\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.1050 - val_loss: 100.1463\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.9974 - val_loss: 129.5801\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.9423 - val_loss: 104.7622\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.9363 - val_loss: 114.9819\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.0170 - val_loss: 126.8617\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.0655 - val_loss: 106.7838\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.8832 - val_loss: 112.5051\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7732 - val_loss: 98.4476\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.9191 - val_loss: 104.6733\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7803 - val_loss: 124.9356\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.6825 - val_loss: 102.3432\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.7162 - val_loss: 106.7419\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.9049 - val_loss: 113.8794\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7775 - val_loss: 106.4726\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.8643 - val_loss: 103.9320\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.5684 - val_loss: 107.0357\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.5520 - val_loss: 106.1546\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7374 - val_loss: 102.8278\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.6470 - val_loss: 101.3809\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.5895 - val_loss: 107.5335\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.4427 - val_loss: 104.9316\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.3982 - val_loss: 105.4587\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 92.4241 - val_loss: 117.9264\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 92.5399 - val_loss: 102.3082\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 92.4268 - val_loss: 110.7095\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.3361 - val_loss: 101.5600\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.4963 - val_loss: 104.8101\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.4721 - val_loss: 103.4091\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.2503 - val_loss: 102.1068\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.4797 - val_loss: 104.2398\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.2583 - val_loss: 105.1471\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.1055 - val_loss: 101.5079\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.2920 - val_loss: 111.0587\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.3186 - val_loss: 117.9934\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.2353 - val_loss: 110.3411\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.3452 - val_loss: 105.1834\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.3320 - val_loss: 114.2516\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.2315 - val_loss: 111.6763\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.2823 - val_loss: 100.0303\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.2418 - val_loss: 116.9148\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.9958 - val_loss: 111.0595\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.0580 - val_loss: 100.2259\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.2153 - val_loss: 109.9632\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.0304 - val_loss: 100.7053\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.9893 - val_loss: 102.8912\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.9865 - val_loss: 114.7471\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.9005 - val_loss: 98.5194\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.7926 - val_loss: 111.3381\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.2174 - val_loss: 101.8977\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.9306 - val_loss: 125.3081\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.9135 - val_loss: 102.8127\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.9037 - val_loss: 102.6182\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.0859 - val_loss: 102.3727\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.8252 - val_loss: 103.8115\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.8884 - val_loss: 103.2531\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.8867 - val_loss: 105.6776\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.8643 - val_loss: 99.5845\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.8303 - val_loss: 102.4228\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7948 - val_loss: 103.4142\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.9438 - val_loss: 111.8899\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.7330 - val_loss: 103.8058\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.6456 - val_loss: 130.1188\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.8233 - val_loss: 105.4546\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7989 - val_loss: 123.0915\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.7927 - val_loss: 100.8714\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.8703 - val_loss: 104.0884\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7471 - val_loss: 102.7118\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.7063 - val_loss: 105.1634\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.5949 - val_loss: 107.6382\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7475 - val_loss: 107.5696\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.5654 - val_loss: 105.2275\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.5303 - val_loss: 100.6088\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.6725 - val_loss: 99.3089\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7925 - val_loss: 102.4839\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7097 - val_loss: 103.7178\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.6783 - val_loss: 104.2981\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.6987 - val_loss: 101.2246\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.4704 - val_loss: 102.2851\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.4727 - val_loss: 97.9636\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.5217 - val_loss: 119.8825\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.3052 - val_loss: 115.0318\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.5921 - val_loss: 110.3573\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.3589 - val_loss: 118.6881\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.3878 - val_loss: 108.7397\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.6320 - val_loss: 113.1698\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.4270 - val_loss: 134.6587\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.5097 - val_loss: 119.1908\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.5668 - val_loss: 100.6448\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.4126 - val_loss: 115.2604\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.4606 - val_loss: 101.0965\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.3416 - val_loss: 109.1626\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.5448 - val_loss: 110.5249\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.3140 - val_loss: 101.9524\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.2369 - val_loss: 104.0402\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.3013 - val_loss: 123.5718\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.2018 - val_loss: 129.1955\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.1636 - val_loss: 106.4723\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.1684 - val_loss: 107.2542\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.1379 - val_loss: 114.9612\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.4854 - val_loss: 108.7154\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.2735 - val_loss: 100.2430\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.3965 - val_loss: 101.3908\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.3281 - val_loss: 111.4625\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.1804 - val_loss: 99.3037\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.2030 - val_loss: 102.0284\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.9551 - val_loss: 109.7065\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.2316 - val_loss: 103.5473\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.1095 - val_loss: 102.0581\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.1076 - val_loss: 124.3912\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.1554 - val_loss: 99.6662\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.0490 - val_loss: 99.5982\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.1283 - val_loss: 100.3096\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.3229 - val_loss: 105.7829\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.0280 - val_loss: 101.5313\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.1418 - val_loss: 129.0098\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.0897 - val_loss: 102.1015\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.0573 - val_loss: 99.8131\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.0600 - val_loss: 101.9808\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.9803 - val_loss: 102.1964\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.2365 - val_loss: 101.7996\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.1612 - val_loss: 100.4195\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.9608 - val_loss: 104.1769\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.8865 - val_loss: 107.4039\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.9016 - val_loss: 116.6678\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.0056 - val_loss: 103.4523\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8549 - val_loss: 124.6497\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.0164 - val_loss: 101.9265\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.8996 - val_loss: 100.1658\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.0893 - val_loss: 101.6213\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.9538 - val_loss: 101.6851\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8823 - val_loss: 99.1343\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.8738 - val_loss: 102.3240\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.7941 - val_loss: 111.1377\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.9359 - val_loss: 102.9901\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.7785 - val_loss: 113.3765\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.8836 - val_loss: 100.2627\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.9775 - val_loss: 102.5352\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.7837 - val_loss: 126.2060\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.8426 - val_loss: 100.7886\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.8007 - val_loss: 127.5725\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.7959 - val_loss: 102.6727\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.8277 - val_loss: 104.8301\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8090 - val_loss: 108.3552\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.9958 - val_loss: 160.1295\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5724 - val_loss: 117.8700\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.8381 - val_loss: 106.3752\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.7858 - val_loss: 104.6937\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.7554 - val_loss: 102.3241\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.7445 - val_loss: 100.7394\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.7038 - val_loss: 101.4146\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.6641 - val_loss: 99.3592\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.6953 - val_loss: 100.5214\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.6045 - val_loss: 113.1213\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5939 - val_loss: 99.6496\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.9078 - val_loss: 136.6552\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.7188 - val_loss: 106.6488\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5698 - val_loss: 130.6880\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5647 - val_loss: 101.3718\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4425 - val_loss: 105.4056\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.6706 - val_loss: 98.6987\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.9322 - val_loss: 119.2146\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.6382 - val_loss: 103.9907\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5022 - val_loss: 102.1674\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.7137 - val_loss: 101.2330\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.6683 - val_loss: 106.5675\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.6543 - val_loss: 120.2181\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5681 - val_loss: 106.1435\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5510 - val_loss: 105.1875\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.4318 - val_loss: 105.3626\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5334 - val_loss: 115.9566\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4884 - val_loss: 108.4881\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3683 - val_loss: 104.7286\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4217 - val_loss: 105.4336\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4920 - val_loss: 101.9083\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5847 - val_loss: 101.8341\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4117 - val_loss: 111.8742\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5468 - val_loss: 104.4361\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5302 - val_loss: 101.0262\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5870 - val_loss: 108.6379\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5180 - val_loss: 100.8449\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5421 - val_loss: 120.3242\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4791 - val_loss: 102.2788\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2859 - val_loss: 98.3870\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4532 - val_loss: 103.7419\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3417 - val_loss: 110.4296\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3018 - val_loss: 99.9272\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.6135 - val_loss: 101.8684\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3648 - val_loss: 109.9395\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.4276 - val_loss: 107.3145\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2952 - val_loss: 103.0167\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3948 - val_loss: 120.7123\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4962 - val_loss: 104.4404\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5219 - val_loss: 103.5769\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4766 - val_loss: 106.9975\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3815 - val_loss: 114.1819\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5574 - val_loss: 123.8818\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2409 - val_loss: 105.7205\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3314 - val_loss: 103.9005\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3379 - val_loss: 117.4568\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3707 - val_loss: 101.2783\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3699 - val_loss: 98.9942\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3147 - val_loss: 106.8000\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4637 - val_loss: 100.1166\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4146 - val_loss: 102.9433\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2397 - val_loss: 103.5493\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2347 - val_loss: 99.4083\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1057 - val_loss: 102.9168\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2629 - val_loss: 133.1836\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3112 - val_loss: 104.2520\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1725 - val_loss: 105.1928\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4497 - val_loss: 98.6364\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1212 - val_loss: 99.2261\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1356 - val_loss: 101.6320\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4485 - val_loss: 102.9735\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2991 - val_loss: 105.0302\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2224 - val_loss: 101.3164\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1129 - val_loss: 102.5281\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1947 - val_loss: 102.1934\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1033 - val_loss: 109.8634\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1314 - val_loss: 115.8907\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1613 - val_loss: 112.1381\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1034 - val_loss: 99.9080\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2447 - val_loss: 102.1442\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3278 - val_loss: 142.6138\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3619 - val_loss: 113.1400\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2086 - val_loss: 107.9174\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2277 - val_loss: 116.1348\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2221 - val_loss: 101.4723\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3367 - val_loss: 120.8352\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2494 - val_loss: 100.0825\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0995 - val_loss: 105.9866\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1694 - val_loss: 102.9131\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2183 - val_loss: 104.8441\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2993 - val_loss: 112.7200\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1581 - val_loss: 103.9145\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3427 - val_loss: 118.4990\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0794 - val_loss: 99.0277\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0891 - val_loss: 105.1183\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3313 - val_loss: 101.0533\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2769 - val_loss: 177.2535\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1524 - val_loss: 103.8132\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0953 - val_loss: 105.4920\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0690 - val_loss: 107.1183\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1266 - val_loss: 102.0140\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1373 - val_loss: 99.3209\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1419 - val_loss: 106.0124\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0798 - val_loss: 100.3973\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1069 - val_loss: 100.4173\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1985 - val_loss: 115.4798\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0138 - val_loss: 117.0905\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0550 - val_loss: 104.6668\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9702 - val_loss: 109.0129\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9989 - val_loss: 104.7779\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0942 - val_loss: 99.6791\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0813 - val_loss: 104.5616\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0186 - val_loss: 124.1594\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0979 - val_loss: 99.6872\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0671 - val_loss: 104.7209\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0666 - val_loss: 122.7083\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1106 - val_loss: 101.1615\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8954 - val_loss: 112.0065\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9480 - val_loss: 100.0443\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0894 - val_loss: 102.2570\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0301 - val_loss: 104.7098\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9614 - val_loss: 126.0994\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9429 - val_loss: 101.4573\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9545 - val_loss: 98.4356\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8930 - val_loss: 99.7470\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0665 - val_loss: 104.9778\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0174 - val_loss: 107.0137\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9784 - val_loss: 100.4794\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8412 - val_loss: 112.6853\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0052 - val_loss: 104.1583\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8076 - val_loss: 146.2726\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0957 - val_loss: 105.0426\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8120 - val_loss: 102.4754\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9979 - val_loss: 129.6284\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8936 - val_loss: 112.4050\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9784 - val_loss: 115.8934\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8936 - val_loss: 99.6225\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8140 - val_loss: 106.3988\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9779 - val_loss: 99.1673\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9191 - val_loss: 111.9390\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8569 - val_loss: 119.2599\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8232 - val_loss: 99.4765\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8714 - val_loss: 120.8314\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7291 - val_loss: 106.6817\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7226 - val_loss: 99.7058\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8530 - val_loss: 99.2579\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8260 - val_loss: 101.5183\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8263 - val_loss: 100.2785\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9493 - val_loss: 108.1571\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8427 - val_loss: 103.6946\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8074 - val_loss: 112.5704\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7526 - val_loss: 100.3673\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7036 - val_loss: 111.4437\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8286 - val_loss: 117.2117\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8030 - val_loss: 100.1785\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9355 - val_loss: 120.0108\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8381 - val_loss: 101.1691\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8429 - val_loss: 100.7574\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7895 - val_loss: 105.8604\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8311 - val_loss: 106.4624\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7515 - val_loss: 99.5379\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7625 - val_loss: 111.7760\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7729 - val_loss: 104.0443\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8980 - val_loss: 107.5498\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.6386 - val_loss: 107.5481\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8553 - val_loss: 128.4735\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7135 - val_loss: 100.6416\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7597 - val_loss: 102.8759\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.6843 - val_loss: 103.8325\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8073 - val_loss: 103.8821\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7491 - val_loss: 117.7587\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7063 - val_loss: 100.0215\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7585 - val_loss: 111.7170\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7648 - val_loss: 101.0339\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.6539 - val_loss: 160.9555\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8339 - val_loss: 103.1333\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7711 - val_loss: 111.2790\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.6544 - val_loss: 102.4181\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9137 - val_loss: 99.9538\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7754 - val_loss: 152.4314\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.6409 - val_loss: 105.2352\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8097 - val_loss: 104.7176\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.6588 - val_loss: 126.4514\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.6353 - val_loss: 99.2451\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.5846 - val_loss: 118.2463\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.6449 - val_loss: 117.1760\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.5669 - val_loss: 99.4473\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8167 - val_loss: 104.9892\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.6784 - val_loss: 126.7236\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.4830 - val_loss: 100.4252\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7822 - val_loss: 100.7185\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.6535 - val_loss: 100.9163\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 89.6314 - val_loss: 102.4609\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 89.6376 - val_loss: 108.8542\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 89.5209 - val_loss: 103.1780\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6Dc0xVwOZnO",
        "outputId": "ef0f533d-5c03-4ea4-be90-25a2dab3e95a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  2.2875979261863537 \n",
            "MAE:  7.528358194985919 \n",
            "SD:  9.896713723466501\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "qQZLKCzHOZnO",
        "outputId": "63da2cc1-db7c-4413-de9e-08a251ebda47"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAFBCAYAAAAsfIegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deXgV1fnHv2+SS8IqixERUFBQFIMBWYtbwRV3raKiFTdsSyvW/typu7Tu1tYqqLi0WEXFHatIKUhd2GSVLSIoYQkJEBIgCUne3x/vHGbuzdwluTO5Seb9PM997syZM2fOmeV73nnPMsTMUBRFUbwjLdUZUBRFaWqosCqKoniMCquiKIrHqLAqiqJ4jAqroiiKx6iwKoqieIxvwkpEWUQ0j4iWENEKIrrfCu9ORN8QUR4RvUlEzazwTGs9z9reza+8KYqi+ImfFms5gGHMfByAXABnEtFgAI8AeIqZewDYAeA6K/51AHZY4U9Z8RRFURodvgkrC6XWasj6MYBhAN62wl8FcIG1fL61Dmv7cCIiv/KnKIriF776WIkonYgWAygAMAPA9wB2MnOlFWUjgM7WcmcAPwGAtb0YQAc/86coiuIHGX4mzsxVAHKJqC2AdwH0SjZNIhoDYAwAtGzZ8vhevRJPcuvCjdiILsjNBdLTk82JoihNlYULFxYyc3Zd9/dVWA3MvJOIZgEYAqAtEWVYVmkXAPlWtHwAXQFsJKIMAAcAKHJJaxKASQDQv39/XrBgQWKZqK7GU+l/wC14Cv/9L3DAAUkWSlGUJgsRbUhmfz97BWRbliqIqDmA0wCsBDALwC+saFcDeN9a/sBah7X9P+zlDDFESEM1AKC62rNUFUVRauCnxdoJwKtElA4R8KnM/BERfQfgDSJ6CMC3AF6y4r8E4B9ElAdgO4DLPM2NCquiKPWEb8LKzEsB9HUJXwdgoEt4GYBL/MoPABVWRVHqhXrxsTYU0ggAq7AqqWPfvn3YuHEjysrKUp0VBUBWVha6dOmCUCjkabrBEla1WJUUs3HjRrRu3RrdunWDdtNOLcyMoqIibNy4Ed27d/c07UDNFZBG0hamwqqkirKyMnTo0EFFtQFAROjQoYMvbw8BE1a1WJXUo6LacPDrWgRLWGFZrHvLU5wTRVGaMoESVqraBwCo/vOjKc6JoiiRtGrVKuq29evX49hjj63H3CRHoITVNF5x/qYU50RRlKZMIIW1mnSiACW4rF+/Hr169cLo0aNx5JFHYtSoUfj8888xdOhQ9OzZE/PmzcPs2bORm5uL3Nxc9O3bFyUlJQCAxx57DAMGDECfPn1w7733Rj3GHXfcgWeffXb/+n333YfHH38cpaWlGD58OPr164ecnBy8//77UdOIRllZGa655hrk5OSgb9++mDVrFgBgxYoVGDhwIHJzc9GnTx+sXbsWu3fvxtlnn43jjjsOxx57LN58881aH68uBLO7VbDqE6WhcvPNwOLF3qaZmws8/XTcaHl5eXjrrbcwefJkDBgwAK+//jrmzp2LDz74ABMmTEBVVRWeffZZDB06FKWlpcjKysJnn32GtWvXYt68eWBmnHfeeZgzZw5OOumkGumPHDkSN998M8aOHQsAmDp1Kj799FNkZWXh3XffRZs2bVBYWIjBgwfjvPPOq1Uj0rPPPgsiwrJly7Bq1SqcfvrpWLNmDZ5//nmMGzcOo0aNQkVFBaqqqjB9+nQccsgh+PjjjwEAxcXFCR8nGQKlMGqxKorQvXt35OTkIC0tDb1798bw4cNBRMjJycH69esxdOhQ3HLLLXjmmWewc+dOZGRk4LPPPsNnn32Gvn37ol+/fli1ahXWrl3rmn7fvn1RUFCATZs2YcmSJWjXrh26du0KZsZdd92FPn364NRTT0V+fj62bt1aq7zPnTsXV155JQCgV69eOOyww7BmzRoMGTIEEyZMwCOPPIINGzagefPmyMnJwYwZM3D77bfjiy++wAH1NPtSMC1WFValIZCAZekXmZmZ+5fT0tL2r6elpaGyshJ33HEHzj77bEyfPh1Dhw7Fp59+CmbGnXfeiRtvvDGhY1xyySV4++23sWXLFowcORIAMGXKFGzbtg0LFy5EKBRCt27dPOtHesUVV2DQoEH4+OOPMWLECEycOBHDhg3DokWLMH36dIwfPx7Dhw/HPffc48nxYhFMYQ2Woa4oteb7779HTk4OcnJyMH/+fKxatQpnnHEG/vjHP2LUqFFo1aoV8vPzEQqFcNBBB7mmMXLkSNxwww0oLCzE7NmzAcir+EEHHYRQKIRZs2Zhw4baz8534oknYsqUKRg2bBjWrFmDH3/8EUcddRTWrVuHww8/HDfddBN+/PFHLF26FL169UL79u1x5ZVXom3btnjxxReTOi+JosKqKEoNnn76acyaNWu/q+Css85CZmYmVq5ciSFDhgCQ7lH//Oc/owpr7969UVJSgs6dO6NTp04AgFGjRuHcc89FTk4O+vfvj9pMVG/4zW9+g1//+tfIyclBRkYGXnnlFWRmZmLq1Kn4xz/+gVAohIMPPhh33XUX5s+fj1tvvRVpaWkIhUJ47rnn6n5SagF5OeVpfVOria4BvEcX4EK8h29H3I3cjx/2MWeK4s7KlStx9NFHpzobigO3a0JEC5m5f13TDJTppj5WRVHqA3UFKIpSZ4qKijB8+PAa4TNnzkSHDrX/FuiyZctw1VVXhYVlZmbim2++qXMeU0GghJXMXAEqrIriCR06dMBiD/vi5uTkeJpeqgiUwuwf0kqBKraiKPVMoBRGXQGKotQHgVIYFVZFUeqDQCmM9gpQFKU+CKawBqvYipISYs2v2tQJlMKoxaooSn0QqO5WarEqDYlUzRq4fv16nHnmmRg8eDC+/PJLDBgwANdccw3uvfdeFBQUYMqUKdi7dy/GjRsHQL4LNWfOHLRu3RqPPfYYpk6divLyclx44YW4//774+aJmXHbbbfhk08+ARFh/PjxGDlyJDZv3oyRI0di165dqKysxHPPPYef/exnuO6667BgwQIQEa699lr8/ve/9+LU1CsqrIoSQPyej9XJtGnTsHjxYixZsgSFhYUYMGAATjrpJLz++us444wzcPfdd6Oqqgp79uzB4sWLkZ+fj+XLlwMAdu7cWR+nw3NUWBUlRaRw1sD987ECcJ2P9bLLLsMtt9yCUaNG4aKLLkKXLl3C5mMFgNLSUqxduzausM6dOxeXX3450tPT0bFjR5x88smYP38+BgwYgGuvvRb79u3DBRdcgNzcXBx++OFYt24dfve73+Hss8/G6aef7vu58INAKcz+kVfqY1UCTiLzsb744ovYu3cvhg4dilWrVu2fj3Xx4sVYvHgx8vLycN1119U5DyeddBLmzJmDzp07Y/To0XjttdfQrl07LFmyBKeccgqef/55XH/99UmXNRUESlj3j7yCftddUWJh5mO9/fbbMWDAgP3zsU6ePBmlpaUAgPz8fBQUFMRN68QTT8Sbb76JqqoqbNu2DXPmzMHAgQOxYcMGdOzYETfccAOuv/56LFq0CIWFhaiursbFF1+Mhx56CIsWLfK7qL6grgBFUWrgxXyshgsvvBBfffUVjjvuOBARHn30URx88MF49dVX8dhjjyEUCqFVq1Z47bXXkJ+fj2uuuQbV1fKs/ulPf/K9rH4QqPlYF1E/HI9FeP/8yTjvvWt9zJmiuKPzsTY8dD7WJFGLVVGU+iCYrgCd3UpRPMHr+VibCsEUVmivAEXxAq/nY20qBMp0U1eA0hBozO0aTQ2/rkWgFMYIaxUHqthKAyIrKwtFRUUqrg0AZkZRURGysrI8TzugrgAVViU1dOnSBRs3bsS2bdtSnRUFUtF16dLF83R9E1Yi6grgNQAdATCAScz8FyK6D8ANAMyddRczT7f2uRPAdQCqANzEzJ96mad0VAEAqtTHqqSIUCiE7t27pzobis/4abFWAvgDMy8iotYAFhLRDGvbU8z8uDMyER0D4DIAvQEcAuBzIjqSmau8ytB+YVVXgKIoPuKbwjDzZmZeZC2XAFgJoHOMXc4H8AYzlzPzDwDyAAz0Mk/7hbVah7QqiuIf9WK6EVE3AH0BmI+D/5aIlhLRZCJqZ4V1BvCTY7eNiC3EtcYIa7W2GyiK4iO+CysRtQLwDoCbmXkXgOcAHAEgF8BmAE/UMr0xRLSAiBbUtgFgf68AtVgVRfERX4WViEIQUZ3CzNMAgJm3MnMVM1cDeAH2634+gK6O3btYYWEw8yRm7s/M/bOzs2uVn/2ugCoVVkVR/MM3YSUiAvASgJXM/KQjvJMj2oUAllvLHwC4jIgyiag7gJ4A5nmZJ/WxxqCiAnjhBcCaVUhRlLrjZ6+AoQCuArCMiMyYt7sAXE5EuZAuWOsB3AgAzLyCiKYC+A7So2Cslz0CABXWmDz0EPDgg0DLlsAVV6Q6N4rSqPFNWJl5LuA6o/T0GPs8DOBhv/KkwhoD468uLk5tPhSlCRCoDp3aKyAGOsRSUTwjUMKqvQIURakPAiWs2isgBqTnRFG8IpjCqharoig+EihhJQCEalSxCquiKP4RKGEFxGpVi1VRFD8JnLCmoRrVKqyKovhI4IRVLVZFUfxGhVVRFMVjgims2nilKIqPBFNYqwNX7PjoyCtF8YzAKUw6qnQCp1joQAFFSZrACWua9mNVFMVnAies6gqIg7oEFCVpgqUwBQXaeBUNdQEoimcES1izs5GernMFKIriL8ESVgDpqFZXgKIovhI4hUmnKlSrK0BRFB8JnLBqrwBFUfwmcMKaTuoKUBTFXwKnMOJjVYu1BtrNSlE8I3jCSlWo4sAVO3G025WiJE3gFCad1MeqKIq/BE9YUa29AhRF8ZXACWuaNl4piuIzgVMYdQUoiuI3wRNWVGvjVSy0d4CiJE3gFCaddHYrV7Q3gKJ4RuAURlwBgSu2oij1SOAUJo0Y1fq2qyiKjwROWNXHGgX1rSqKZwROYdQVEAf1tSpK0gROYVRYFUXxm8ApjM5upSiK3wROYdKpGtXQ111FUfwjcMKapq4ARVF8JnAKoz5WRVH8xjeFIaKuRDSLiL4johVENM4Kb09EM4horfXfzgonInqGiPKIaCkR9fMjX+nEKqyKoviKnwpTCeAPzHwMgMEAxhLRMQDuADCTmXsCmGmtA8BZAHpavzEAnvMjU2qxxkH7sypK0vimMMy8mZkXWcslAFYC6AzgfACvWtFeBXCBtXw+gNdY+BpAWyLq5HW+VFijoP1XFcUz6kVhiKgbgL4AvgHQkZk3W5u2AOhoLXcG8JNjt41WmKdkUBUqq9O9Trbxo5aqoniG78JKRK0AvAPgZmbe5dzGzAygVk80EY0hogVEtGDbtm21zk8orQqVarFGRy1XRUkaXxWGiEIQUZ3CzNOs4K3mFd/6L7DC8wF0dezexQoLg5knMXN/Zu6fnZ1d6zxlpFWhktViVRTFP/zsFUAAXgKwkpmfdGz6AMDV1vLVAN53hP/S6h0wGECxw2XgGRlUjX2c4XWyiqIo+/FTYYYCuArAMiJabIXdBeDPAKYS0XUANgC41No2HcAIAHkA9gC4xo9MhdIq1WJVFMVXfBNWZp4LRB07OtwlPgMY61d+DBlp1ajkDHBZOSgr0+/DKYoSQALXihOiKgBAVfOWwL59Kc5NA0R7ByhK0gROWDPSqgEAlcgAdu5McW4aICqsipI0ARRWsVj3IQTs2hUndgBRYVWUpAmcsBpXQCUygB07UpybBogKq6IkTeCENSNdXAH7EFJXgBMjqCqsipI0gRPWUJrDYlVhrYkKq6IkTeCENYMcFqu6AmzMUNbq6tTmQ1GaAMETVu0VEBu1WBUlaQInrKF0FdaYqLAqStIETli1u1UcVFgVJWkCJ6whpyugsjLFuWmAqLAqStIETljDLNaqqhTnpgGiwqooSRNAYXVYrCqsNVFhVZSkCZywhjVeqbDa6AABRfGMwAmrsVjVFRAFFVYlEebMARYsSHUuGiyBm0pfG6/ioMKqJMLJJ8u/3i+uqMWqhKMPiqIkTeCEVX2scVBhVZSkCZywaq+AKOhcAYriGcET1nR1BcRELVZFSZrACWtILdbYqLAqStIETljDGq+0V0BNVFgVJWkCJ6yhDBEOtVgj0AECiuIZgRNW7W4VBRVWRfGM4AlrulqsrpjeACqsipI0gRPWsG9eqbDaqMWqKJ4RPGG1fKzlyFRhdaLCqiieEThhzcwQMS1HpvYKcKLCqiieEThhTUsDQqhQizUSFVZF8YzACSuIkIlyFdZIVFiVpkphIXDJJUBxcb0dMnjCmp6OLJShDFkqrADw5ZfAXXfZgqpzBXjLe+/JPAybN6c6J8HlkUeAt98GJk2qt0MmJKxE1JKI0qzlI4noPCIK+Zs1n8jIUIvVydChwJ/+pBarXzz3nPwvWZLafCj1SqIW6xwAWUTUGcBnAK4C8IpfmfKVUChYwvrhh8CKFfHjRQprYSFQVuZfvoKCmTVMK6zUU4/XIFFhJWbeA+AiAH9n5ksA9PYvWz4SCtmugCD0Cvj1r4Gnn44fz1Qy5ubLzgbOOce/fAUFFdZAkrCwEtEQAKMAfGyFpfuTJZ8JmsVaUQGUl8ePZyoZpwDMnOlPnhSliZOosN4M4E4A7zLzCiI6HMAs/7LlI0HzsVZVibjGw01YFUWpEwkJKzPPZubzmPkRqxGrkJlvirUPEU0mogIiWu4Iu4+I8olosfUb4dh2JxHlEdFqIjqjziWKh2WxBqZXgApralFXQOox16AeSbRXwOtE1IaIWgJYDuA7Iro1zm6vADjTJfwpZs61ftOt9I8BcBnEb3smgL8TkT+uBsvHGiiLdd+++PFMHBUAb1FhTT3Ocz9hglwTn9tXEnUFHMPMuwBcAOATAN0hPQOiwsxzAGxPMP3zAbzBzOXM/AOAPAADE9y3dgTNx1oXi1X7snpHCqwlJQYPPyz/Pvd4SVRYQ1a/1QsAfMDM+wDUtQr+LREttVwF7aywzgB+csTZaIV5j1NYg9AroC7CGoQKp75RizV1OCu3enqDSFRYJwJYD6AlgDlEdBiAXXU43nMAjgCQC2AzgCdqmwARjSGiBUS0YNu2bbXPQUZGsEZeJeoKUGH1h6boCmisZXHm2+e3skQbr55h5s7MPIKFDQB+XtuDMfNWZq5i5moAL8B+3c8H0NURtYsV5pbGJGbuz8z9s7Oza5uFxFwBTaljfF0s1iBY8vWFCmvDpCH4WInoACJ60liKRPQExHqtFUTUybF6IaQhDAA+AHAZEWUSUXcAPQHMq236CRFPWL/8EmjeHJgxw5fD1yumVlZXQOpoisLqtbU3bx6wapW3abpBZF8Pn+/xjATjTYaI4KXW+lUAXoaMxHKFiP4F4BQABxLRRgD3AjiFiHIh/tn1AG4EAKtv7FQA3wGoBDCWmf0puXPkldvJ/e9/5f8//wFOO82XLNQbpny16RVQXa3C6gdN6Zx6LayDBsm/35WPM/0GIqxHMPPFjvX7iWhxrB2Y+XKX4JdixH8YwMMJ5qfuOC1WQG6SNIfhbm6atCYw8VdtLFZnd6umJAKpxlhITcm90hR6jfh8jyeqHnuJ6ASzQkRDAez1J0s+YzVeVSEDlUivecM3JWE1N4+6AlJHYxTWeMOgm4Kw+nw9ErVYfwXgNSI6wFrfAeBqf7LkM6EQWmI3AGA3WuKAqioRk/vvB37xC/umSW+cUyGEURtXgAqrvzQmYT3sMGDr1ugC2tiE1a0vcUOwWJl5CTMfB6APgD7M3BfAMF9z5hehEFqjBABQgtZygvfsEWE96SS1WFVYvaWeGks8ZcuW2P7OxiasbjQEYTUw8y5rBBYA3OJDfvwnFEIrlAIAStEqvJ9nWZkKqwqrtzRGV0A8GvP9UU/XI1FXgBuNc6xeRkZ0YQVUWFVYvaUpCqtarHFJRj0aZ8c8h8VagtbA6tW2o37vXmC3+F+blLCqjzX1NFVhbUz9c+uxu1VM9SCiEiLa5fIrAXCIrznzC4ePtRStgJ/9LLwF9Jln5L8pCWsifVO1u5U/RPOxlpQAV10FFBXVf56SxSmsjc16bQiuAGZu7evRU0GkjxVIbIb9xojzYa6okBFl0VCL1R+iPciTJgH//Kd8AufJJ+s/X8kQKayNpQdNQ7FYmySRrgDAXVgTeX2uKwsW1M+QWefNE688TdViHT0auPDCVOeiprA25qGujdVidea1gYy8ajpkZIS7Apo3905Y16+X/Xr2jB1vwAD59/Ohqq4GjjzSXo/XgOX8mGBT8ge++mqqcyA0FGH9+msgPx+4+OL4caPR2ITV6Y5pCK6AJklGxv4BAqWHHAUc2NNdWOty4rt3l/+GYIVUVNR0BSSCzhXgLdF8rKkS1iFDkj+ul8JaH+U3x3BeA3UFeEx6OjJQhSwqQ0nrQ6Tvan27AvyislIsErPsJNHyNDVXQKox5zKaxTpvXuKVXkPBS2Gtj3vN5LEeXQHBE1artb8NlWAXtxJRbSrCes89YpF8+23N/F98cXwXBeAurEVF0i0tlezdC5x8spStMRFPWL/+Gvj97+s3T4mSyJDWZIXVeV5mzfLnuTPXoB5dAYEV1nZUjB2VbcRiXbOmZrzaXOA+fYC//S369spKEQYvyM+PLi5Ll9pxIvO/cCGQlxc/fTdh7d0b6NWr9nn1kgULgDlzgN/9Lvm0Nm2SB+zzz+PHffdd4Ior6n4s50PtxDl+feHCuqfvJ9GeAS8tP6fADRsG3HFHcukxA199FR5m8quuAB+xuoa0T9uB7ftayWQTt91WM16iwlpdDSxbFvuBP/98oEULWU7Wp/Tgg9FbuTMsl3llZd1fL92EdevWuqXVUJk5U/5fijqLpc1FFwH/+lftj7FmDfDJJ/Et1sjlhkS0e8hPV8CyZcml9/e/S9/06dNrHkNdAT5y0EHAGWegXb/DsaMixkcQEhXWkpKaYZEXzXmRnZ99qctNWVwMFBS4bwuF5H/fvrq/UsXysV53HfDmm3VLN1mSFR/n+TDfSqvNp31qUyEWFgJHHQWMGNG4hTURi7W293BJSfgzEHlekhU8I8zr19dM0+kKmD0bWGxNKb1kCXDTTZ42pAVPWNPTgX//G+17dsCO8hbR4yUqTMXFNcPMsNhIKiuBXY5vMNbFqty7V35ufmGnxeqHsP7rX/JlBZMPP1p0580Dvvsudv7qgtMVU1go/7UR1to88M50ExHWhjrKzw9hbdMGOO44e91rYXWb68PNFfDkk0DfvrI8fDjw17/a94UHNNAr6j/t2gHby3wS1mXL5DWT2RYiQGprZ/y6fLTQ7LNzZ81tfgvr3r1y/A0bxLXxwgt1O0YsBg0Sn67X7NljLxuLNSvLPe4rr9i9KwyRleDs2ZLPeL7zRHysXluszPGHyiYiiNEqfmdZYqWzYEG45Whwtml4Laym4nUKa7RrYPChL25ghbV9e6C4LAtV0U5BpDBt3y4P4q9+Bfz4ox3uJqwnnABcfz3w/PNSGxpKSsIt1roMpU1EWCsq/BFWQPJsegi89VbdjlEXzAP45ZeJ+3ydD4ybxRrt/F9zjd3f0xApMuPGiWUdr7dENIvVmTevhXXiRODAA2PnrTbfQYskUYt1wADp2x3rzSzyXku2td7NYnXzsToxYuxhj4TACmu7dvJfjAPcIzhPcnExcMEF8iBOnCjOccMHH0Q/SGRr765dYu0Z6mKxGoHYsaPmNiOse/YkdpO43WjxhLWszL4R69M36Hw4nZVVLJzC6RRW46qpTcUWKQ6JngOzX6RgONe9Po/vviv/69bFz1ckTleLV70CMjOB999331bfrgC3c23K7GF/4sAK64EHyv9WdHSPYG6qqVOBtm2BL76wt+Xny39BAfDYY9EPEmlVLl4c7hrw2mI1jVexhDVel5PGIKwrVya2TzRhNeG1Of91tWbMcetDWJculWtn7pHMzOhxExHNigpxiWzeHD1Ooq/RH37oHh5PWD/9tHYGSCyLNZ4rQIU1eQ4/XP6/xxHuEcyNF+2GAKK3zhsixe+qq6Q7iMFrH6u5mWIJqzPc7bWrMQhrvMaeSZPEXeAUTqeP1YhdMharIZ6FZY4VGc95HZznsbwcGDgwvCKP5KqrgLPOCg9bsUIahe69N7H7KpHy7NsnLpE+fcLj1EVYo90rsYR1yRLgzDPF7eLGiy+Kr9stb87juTVeOfHBYg3eXAEWZhDSWjhGI40dKzc0s9xUM2eKxerGWWeF+/q6davpqHcTP0AmfonWsh+NnTuB++6zXQBurgBzk8YS1o0bgR49wuM7iRTWyFZ4p7DWJ4kK65YtwI03Ak88If8Gp8VqhKc2c0REcwXEu4ZG0ONZrP37Swf5X/4SmD8f+PWvgeXL3dP85z9rhm3aJP9ffWXnKZbARrs/nPkycZyt5atXh/d6iee3NKSluZ/baD7WZcuA3FxZXrHC/Rg33FDzWCY/zvI5fayRAm8+JgqosHpBhw5A25YVWLP7SGDwYGloMt1Ahg+Xm/vUU6Mn8O9/h6/37l1TWN3ED5CvYK5aFd+y2LULOOAAscBWrwb+8hd7m5tomxsjlrD27Ok+KYWhujr8AYiMU1ZmHyeaFXLvvXIuL7ooPPzhhyW9e+5x3y8WiQqrKfeaNcAf/mCHJyqs0YQy2kMXT1jNcSOvh/Mcp6WJP37hQrFGgbq/DTDbefr+e2DtWvehzNHK48xXZNnKymQE3sEH22HRhDVyX6LEKjKz/vrr0eMA0XtjOIV11Srgs89iuwLatrXTV1dA8hABvTruxAr0Bs44I7xvXSjk3vE/Glu2uA97jNZ6fdhh8v/229Jt6f773eNt3Cj/TzxR86aMJayLFtXsLuQkWks1UNNijYxTVmYLU7SH/4EH3KelGz9eRBeQ/qrHH5+Y9QOE3/SxRCea0HkprMuW2R3RExXWyEo0mo+1rpOuu6Vx003hU0c6ScRiNW0JBnOttmyxw5zXbN48ycfSpTXLS+RuSERzBTh7z7j1qGgRpauk0/ocNEjcCObauQlraal9vlRYvaH/GR2wqNlgVN12Z/gGMyO6mQYwHh06iLBu2hRupUUbKGCE9ZNP5MG77z65KZnFbzt5smx3ClikmMQS1q+/FnGLRuIg2yQAAB1zSURBVGmpHMut1o8UVjeLNZ6wJsLtt0sF4BzXHevGTtRijSZM//2vfT2cwhpZvkSE1XQsjxXfYFwBkRV1NB9raam9fMwxMtotEZzz6dbVxzpgAPDUU/Z6pLHgdj87hdWM8//mm5p5SEtLzGLNyxODw3m+IuPE6sjvbIgy4mzyHa1XgEGF1RsGDE7H7opmmL80ovW0lfXJlt/+1g6LfJidomu6OXXqVNPR78Yh1ufC1q61wwYNkj5gl1wiD9P69bZ4ugmrm5sh0Rtj1y6x0N0qjnjCWl4e/tBEHjNeAwEgXW9MeXbssCcHj+bPjjxOXYR14kTgyitl2eR/2jS5ds59EhFWZxkTtTCdFhgQ3WI1wkokvR9MJRsPN4s8FpEW69690qH/oYei7+NsADQ4z4XxhYZCiVusbvfL3XeHn6/IOOZNzg03H6sR1ngNbepj9YazzgI6dpSK+ZtvHCMRH3lEOomfcYbtpysslIjGt3rttfJQRnZFSURYWzrmKLjoIrGQ33pL+svOmWMfz7QME9VsDIhlscajpCT6ZBeJuAKMmEyfLl169uyxv6fltLii5feCC+zloiIZCZWXJ+c0Gsm6AgC5yKYMTrZuBQ491H2b2/GjHS8/P7zXh5PIoczTptnrznMc7S0nHkZYo72JmG3OPDiJvI/diGexmgovcj4Asy3RxsIjjohtsUa6KJy4ffLdabHGQi1Wb8jOlv79mzeL8TZxonWfdOsm3TyI5JX6/ffFmpw2DRg5UnY+6yx59XE2KAHA0UfHPugBBwBnn22v9+gBdOlSM97jj4uLAJB8RL5KJiOszkadSJjDG+Yib8adO20/qcEMEQWiC2s0f3NBgf0QxuqN4HQZxLJY41lrVVU1rTXTog4k13g1ejQwYYJ7PKewjh8f3tDpFCxz/iLzuHt3zektnYLj7C8beQ7cRhbFOgdubNwoPRcicRPW0tKaefj8c/dX+J//vGbYoYdG97Heeitw3nk196mokPvQXCfn9TJlU1dA/TFwoIhrx44yWnXIEDFM909d+sc/2heyeXPpOzdrljS8uHHUUe6vU8Zd8P33EsfQu7e4ECJxziK1YoWMX3fi5gpItBN7ZI8GJ1u3Ah99ZK+7WRSRw3idwuqsAJx5jCas+fk1X5OB8LK89578DHVxBQAiMG6vs3PnSmPI6tXR9492bk38Vatiz+9aUmILnNMFZLYZjMhGnpNLLpH7xln5OC1Ts+wmXibvyQjrkiXu4U5hNfeKm8W6ciVw+eXhYdG67VVWhlc2znvw8cfd97nySpm5zhzXWT4zb8KHH4bfq5GoK8BbTjtNtGvCBOkRNG+euJvuuEPaD/r0cTRCtmoFnHJK9MSIxEeUny/x0tJknoEuXaR1v23bcGHIyYneLSsarVrZFmthodxEnTp5c2NEDnpIZOy2cx+nxdq+vf3wRGtwmDYtvPuOobwcaNZMlletCt9WV2HdskVmV4rk8cdFmJ55Jrx3iJN4FuuJJ0Y/LiDnYfduuXaRguIUVnP+nMK6e7c0dALhk6s89phU4p98Io1zgHsFtnevnEtnGSLLE09YE5nAxNlQ5/bm4HyFr652P6c9ekg6zm2J3INm3gpzL9blWVBh9Z6sLHnr//3vpbfIOefYDbLHHy/P3pFH2u1OcXHztY0YUTPs6KNt4fjzn6W1PF5r+xFHSE1QXS2CXV4uD6sXN0bkg+k2yUwk0SxWJ9EGSxQWuvdgKC8HWlufJ488H4kI6/jxsRtinJgyR/OPAvGFNZEp53btchdWp4gaYXVWUNu22cLoHBL94IPyf+aZdpjbed6zR1xQsSzWeD7WaPeBEVZTcQDyNQ3nW5kbFRXu94oZPOO0WBOZP8C0Q5iGrboK67x5sa3aBAm8KyCSdu3k00pbtsjQ/rFjZQDMz38OdO0KnHuuPK8ffpjkxPoLF4pTNytL/LZTpgC33CLbHn9cvmoQ6W4YOVKskzFjpBbv0cN+sAsK6i6sr71mL0fexIncZCZOUZHd+BZJba1yp+UZ2Zqblib5nDix5mc4zH6jR0uldemltTtuNMy5jRQkc7xYlWH79vJvhCSWxeomjJs32/NARL5OJ4KxJJ15j7xX4l3naBWHuTamcjfE+4RONGFt0ULy69Z4NXp09PTMvAjm/NVlbgfT9/Wcc2q/bwRqsUahZUt5K/zb34A77xRxnT1bXJ1OF+RRR8lArZYtRXwjh3BHpV8/+QEiFM4+g6ZxqaJCXvHOOEPcC8YCmzVL/n/4wd6nY0egc+c6lBSx5z9NpLX4//5PfMdLlsi0fm7s3Cm9HxKdvcgprJHWEpFMzvGrX8lyZaVtxTonIDETZsfqxhULt1b0SAEy+TRi70Zurliabr5kILzicJt/YvVqW1jrwm23AXfdJf2tDZHCEymcN94oFVe07YahQ+UePfbY2uWpuNi9B02LFmJ0uFmsr77qnlZeXk3XQ4pdAWqxJkDnzqJtEyaIK6qgAPjf/4BHHxWj8ZVXxN01YoR83ur228VHW1rq3laSMM2aAaefLq8348fb4dFes/Lz5XtYY8bU7jixBNnpe3N2k4rkuefcRbWqyv4uWNu2ieepvFxqs7vvrjlps6lwABE/55h6I3SxZnYCxDIBYn+51mnJm4cuctx6vOG9gD3NoWkRjTXXglsfzdtvr50F1rZtuC952jSZI9iZhmlAq6yUa7Rtmz0zEVDzu2rRejsAUsGZ6eISZfp09+5bzZvXvN5mQEs0Iq9hjx4qrI2R7GyZkvXWW8V63bpVDJKLLxZj8tFHZRBL69ZiyY4YIY1i//tf7UbK7qdz5/DZ7t16ERg6dAi3NCJx29e8qhrS0oAZM2TZKazHHy+WT20491xg1CjpelFZmdgnuAF7lqcJE2o2XhUVyYnv2lXWzYn98MPEhfWBB+RBimWtO189zUNnvoTrzCcQ2+87erQ4581HCZ0WaqSrYvVqW+BM3+CCgtr1b+3Tp2Zl6ZzjAZAuc8xiTQ8ZIhapU1gzavEyG3l9EsH52ufE7bgVFYn5+g1ZWcnNdewBvgkrEU0mogIiWu4Ia09EM4horfXfzgonInqGiPKIaCkR9fMrX37QqpW4Ad5+W972fvgBeOcdEdPRo8VQGT9ejIY2bcQIvfZaaRybNAl4441auoSI5BX9/PNl/b33RIRMZmIRKaJAzdfM006TCWjatbOFtXt3cVc4BzcANSdaOfBA2d/wySdSQEAejkWLovthnTz4oH2jOwczmGGTK1cCV18tD1FenrhVzjvP7vIUT1hbtZJyu50PN4woLVsmbhdDIiOvDjlEztPnn0s6zgfYbUa04cNlkuoNG8InI0mU7GwZ4BJJpBX6449igc+fLxar22jC2nLDDYm18EabuStaP+h4X2ownHiivOlFm1gbCO9H7iRez4jawMy+/ACcBKAfgOWOsEcB3GEt3wHgEWt5BIBPABCAwQC+SeQYxx9/PDcWtm9nfvBB5muvZe7Zk7lrV+b27ZnFbGBu1oz55puZn3qKedYs5uLiBBLdtUsS3btXdgSYV6+WbSbhLVvsZYD51FPD1wGJf8UV9vpf/yphRx5phz3/vIT95S/h+5aWMs+YwTxihKz/3/8xX311zWM4j7V5c/Tt0X6nnsr8xRfMN95oh336KfMxxzC3bFkzfnW1fZ7c0tu7V7bdemtix7/oIubHH2c+9FDm006zy/jLX8rFirbfQQfJcd59V9Znz2YeMMDenp1dc58HHgi/zn36JH6exo1jnj+fuaqK+eGH3eNcdpn833ZbePj999vLX3xRc7/HHmPu0sVev/VW5nPOCY8zYwbzlCm1u7bffmsv9+/vHmfy5PjpHHkk8+7dzK1by/rw4e7x7ruP+c9/rhk+bNj+ZQALmJPQv2R2jps40C1CWFcD6GQtdwKw2lqeCOByt3ixfo1JWN2orGResYL5pZfCrikDzOnpzIMHM48dK89JZWWcxKqqmHfutNc//pj59ttluX175sxM5kcfrSm0RuyY7fV162T9/PPtsL//XcJefFHWQyHmMWPsfZ95RsJ/+1sRd2f6hx8efqzS0tgPyO9+J+L14Yd22Natsu9JJ8n6pZdKmc2DnZnpXiZnucyva1d7W+QDdsQR8R/gW26RfXNywsP/9KeacTdtkrg7d7qn5cy3WZ44MTz/sSoq81u+PPz6MzPn5bnHnTTJPfzzz8PT69jRXv/Nb6SyOuEEO2zBAqlInWn873/Mc+dGz+cpp9QMKypinjqV+eWXmY86KnY5H3gg+rYTTwy/3j/+GL79rrvk/w9/iF7pAMxff82NTVh3OpbJrAP4CMAJjm0zAfSPl35jF9ZIyspE96ZPZ77zTuZ+/exrPWiQ3K9OQyxhSkpEzAwPPsj8zjt24obI9bFj7bCbbpKwF16Q9dGja2Z+3Djm77+X9cpK5r/9jXnPHlmfNk1qCGYpRLSb+quvwtOMzNOECbJeWCjr48bJeq9e4Q+dk3btJCw7W86FyRNzTTHMy2N+5RV7fdgwqUQA5u7d5f/JJ2Vfp/UMMC9eHL5u4hmcbwXO8prl//6X+fTT7YrNUFrKPGcO82uvRT9vbjfG7t3ucZcvt5fvvpv58sul3M57oLqaee1ae/2992R779522LZtzG+9VfMc/PRTeJixIAHmf/yjZn4qKuw8d+oUvYyAVDqRYeb6Gj1w3gNm+fLLmf/9b1keMkSeAbf0zz/f2q2RCqu1vsP6T1hYAYwBsADAgkMPPbTmzdTE2LBB3szNG+PRR8s9Xl7uQeJr1zJ/+aW9vm6d/YAxi8IDYvGuXSthTzzBYUJbV/r1Y37oIRHoOXPEFwIw79gRHg+Q129DVZW4QAxGmAYPtuNHCuvWrczffeeejx9+YL7++nBB+egjWc7Kkn2NNb5xo1j95vg//MB87rnhAjFxolQiP/5YU+z27GHu2zf8QWa2hSE/P7FzV1ho73/YYeJDioaJd9559nJRkb38zTfh8QcNYr7wQnt94ULJs/FNHXKI7Pfjj7K+aZOd1h//yLxvn1SqzjLOni2ukMGD5RyZ8OeekwreyahR0UX1xBOlsn3kEXEDXX21WCLGlXDttfZ1MRV8377MTz8ty7t3M2dkML/xhlxHZ9rvvCOib7mIGpuwqiugjhQWMt9zjy2wubnydm4MN18P7OTNNyUDL7/s7XEuusgWJydr1oiDOhaTJjGvXCnLbsKaCF9+Ka4HZhHv2bPloWUWgXRauZF89JFYvonw00/iTtm2zS6XsbSdFUY82rRJrJxvvCECxCxvDAsXSnmaN2fu1i0BH1MEc+fKtXLu9/LLNa3sTZtEBDdsqJlGixaSdzfrYO9eqWhvuklcSkuXinvh8cdjv659/XXsa+R2nPHjxWD44osam5MVVpI0/IGIugH4iJmPtdYfA1DEzH8mojsAtGfm24jobAC/hTRiDQLwDDMPjJd+//79ecGCBb7lvyGybRvw8svSm+D776UR/pe/lC5diX4VOimYpXvT0KHefkxw715plT0iyscdE2XqVOn+U5fPv6QKZukWYuZGSIT8fOkZ4jbjVCIUFkqvDzOpe33yww+163qXAohoITPX8eTCP2Elon8BOAXAgQC2ArgXwHsApgI4FMAGAJcy83YiIgB/A3AmgD0ArmHmuIoZRGE1MMsghHvvlUFI1dWiSSefLCM5O3aUHkg9e9bueVUUpQELa30QZGF1UlQkXUWnT5cBCs5uku3bS1/x44+XSaS6dpUxBMccIwNuunSp/aAZRWnqqLCqsIbBLH2slywRoV27Vvqcz54dfRBCKCTi27GjfI7rwANlQJIZIHTwwTJgq2VLGaLbo4fs06aNzK3SokX0b7spSmMkWWHVSViaGEQylPaEE+TnZOdOGfm5bh3w7bcySjIjQ1ySmzeL22vRInG/RZvlz4mZ4S0tTQR51y5xR3TuLAKflmbPlNe1qxyrrEws5Q4dJPyAA2SQlBHrlStlKoTCQvmQA5EI+s6dMjBo1y6Jl5kpLo5mzSTdHTtkiHyzZt66fhWlLqiwBoi2beXXtav4YqPBLKMdmzUTcdyyRYR3zx4RtdWrxfotKBBrdtMmmTukfXtpl1izRvbbs0fEc+lSe3IrM+J2+/bEJ7qqDdnZItbV1dI2k58vx0xLEwu7vFwmYmrWTOKuXCn5bNFC3CKZmfZvxw45FwceKGnu3i3l6NtXKq/KSvmZYwFSxp075esipk1q+3Y5VmampNOmjSwvWSKVSFaWnFMTJz1d0m/e3K4kmLXCaEyoK0CpF8rKRDCc0xJs3WoPnf/pJxHqQw4R8SMSYQuFxLWRmSminZ1tTz5fXi7/FRVSYRQXi0Dt3i1pEYllHAqJABYXyz6bN4vgFRaKeHXpInn74QfbIt63TwSvvDx8zpTIifj9xCmwBQVyDiorJX/Nm8u29HSp1Lp0kfxu3CiVREWFNFyayfgrKqRCPfhg6Vmyfbu4fn74Qd4MOnSwpwfYt0/i79sn5zAjQ46Tni7n7OCD5a2nZUupUFq0sPc56CDJl/l2oKlg27e3XUmtW0tFUVVln09TuRkX065dck2zsiSf5tNolZX2G5H5eEbbtpJ2SYmk1bKlXRG2bGl3VjX3QLt2Uq7OneU8/vSTpFFUJOfhoIOATp3Ux5rqbChNhH375OFt1cr+7lxlpQhLSYkst20r4l1RIeEZGRJv+3YRkqoqEY5t20SI0tJkOSNDhKO4WERj924RjMJC2b9NG3mwQyERk5ISiVdSIqLTrp38t2kjx96xQ37V1SK45huJXbtK2kVFsr9xtWRkiIhu3iyV0549UmEdcYTkyVjnQLiLxQigmV2QSNLu0UPi7thhf/klI0NEat8+yVezZrJ/VpbkpXFJjfpYFcUTQiHbojbWm+nm6ZxKNtZMg4oQ6booLpZz2aKFCDqRrO/aZVdezCLGxcVSOe3ebb99dOxoC35+vsTt0EFEu7hY4rVuLRWReeNp2dI+1t69cmzjnjHWq/lye2mpWNVVVeLauvnm5MqvFquiKEoEyfYK0ImuFUVRPEaFVVEUxWNUWBVFUTxGhVVRFMVjVFgVRVE8RoVVURTFY1RYFUVRPEaFVVEUxWNUWBVFUTxGhVVRFMVjVFgVRVE8RoVVURTFY1RYFUVRPEaFVVEUxWNUWBVFUTxGhVVRFMVjVFgVRVE8RoVVURTFY1RYFUVRPEaFVVEUxWNUWBVFUTxGhVVRFMVjVFgVRVE8RoVVURTFY1RYFUVRPEaFVVEUxWNUWBVFUTxGhVVRFMVjVFgVRVE8RoVVURTFYzJScVAiWg+gBEAVgEpm7k9E7QG8CaAbgPUALmXmHanIn6IoSjKk0mL9OTPnMnN/a/0OADOZuSeAmda6oihKo6MhuQLOB/CqtfwqgAtSmBdFUZQ6kyphZQCfEdFCIhpjhXVk5s3W8hYAHVOTNUVRlORIiY8VwAnMnE9EBwGYQUSrnBuZmYmI3Xa0hHgMABx66KH+51RRFKWWpMRiZeZ8678AwLsABgLYSkSdAMD6L4iy7yRm7s/M/bOzs+sry4qiKAlT78JKRC2JqLVZBnA6gOUAPgBwtRXtagDv13feFEVRvCAVroCOAN4lInP815n530Q0H8BUIroOwAYAl6Ygb4qiKElT78LKzOsAHOcSXgRgeH3nR1EUxWsaUncrRVGUJoEKq6IoiseosCqKoniMCquiKIrHqLAqiqJ4jAqroiiKx6iwKoqieIwKq6IoiseosCqKoniMCquiKIrHqLAqiqJ4jAqroiiKx6iwKoqieIwKq6IoiseosCqKoniMCquiKIrHqLAqiqJ4jAqroiiKx6iwKoqieIwKq6IoiseosCqKoniMCquiKIrHqLAqiqJ4jAqroiiKx6iwKoqieIwKq6IoiseosCqKoniMCquiKIrHqLAqiqJ4jAqroiiKx6iwKoqieIwKq6IoiseosCqKoniMCquiKIrHqLAqiqJ4jAqroiiKxzQ4YSWiM4loNRHlEdEdqc6PoihKbWlQwkpE6QCeBXAWgGMAXE5Ex6Q2V4qiKLWjQQkrgIEA8ph5HTNXAHgDwPkpzpOiKEqtaGjC2hnAT471jVaYoihKoyEj1RmoLUQ0BsAYa7WciJanMj8+cyCAwlRnwke0fI2Xplw2ADgqmZ0bmrDmA+jqWO9ihe2HmScBmAQARLSAmfvXX/bqFy1f46Ypl68plw2Q8iWzf0NzBcwH0JOIuhNRMwCXAfggxXlSFEWpFQ3KYmXmSiL6LYBPAaQDmMzMK1KcLUVRlFrRoIQVAJh5OoDpCUaf5GdeGgBavsZNUy5fUy4bkGT5iJm9yoiiKIqChudjVRRFafQ0WmFtCkNfiWgyERU4u4wRUXsimkFEa63/dlY4EdEzVnmXElG/1OU8PkTUlYhmEdF3RLSCiMZZ4U2lfFlENI+Illjlu98K705E31jleNNqhAURZVrredb2bqnMfyIQUToRfUtEH1nrTaZsAEBE64loGREtNr0AvLo/G6WwNqGhr68AODMi7A4AM5m5J4CZ1jogZe1p/cYAeK6e8lhXKgH8gZmPATAYwFjrGjWV8pUDGMbMxwHIBXAmEQ0G8AiAp5i5B4AdAK6z4l8HYIcV/pQVr6EzDsBKx3pTKpvh58yc6+g65s39ycyN7gdgCIBPHet3Argz1fmqY1m6AVjuWF8NoJO13AnAamt5IoDL3eI1hh+A9wGc1hTLB6AFgEUABkE6zWdY4fvvU0hPlyHWcoYVj1Kd9xhl6mIJyzAAHwGgplI2RxnXAzgwIsyT+7NRWqxo2kNfOzLzZmt5C4CO1nKjLbP1atgXwDdoQuWzXpUXAygAMAPA9wB2MnOlFcVZhv3ls7YXA+hQvzmuFU8DuA1AtbXeAU2nbAYG8BkRLbRGdAIe3Z8NrruVYsPMTESNutsGEbUC8A6Am5l5FxHt39bYy8fMVQByiagtgHcB9EpxljyBiM4BUMDMC4nolFTnx0dOYOZ8IjoIwAwiWuXcmMz92Vgt1rhDXxsxW4moEwBY/wVWeKMrMxGFIKI6hZmnWcFNpnwGZt4JYBbk9bgtERmDxVmG/eWzth8AoKies5ooQwGcR0TrITPMDQPwFzSNsu2HmfOt/wJIxTgQHt2fjVVYm/LQ1w8AXG0tXw3xTZrwX1qtk4MBFDteWRocJKbpSwBWMvOTjk1NpXzZlqUKImoO8R+vhAjsL6xokeUz5f4FgP+w5axraDDznczchZm7QZ6t/zDzKDSBshmIqCURtTbLAE4HsBxe3Z+pdiAn4XgeAWANxK91d6rzU8cy/AvAZgD7ID6b6yC+qZkA1gL4HEB7Ky5BekJ8D2AZgP6pzn+csp0A8WEtBbDY+o1oQuXrA+Bbq3zLAdxjhR8OYB6APABvAci0wrOs9Txr++GpLkOC5TwFwEdNrWxWWZZYvxVGQ7y6P3XklaIoisc0VleAoihKg0WFVVEUxWNUWBVFUTxGhVVRFMVjVFgVRVE8RoVVUSyI6BQzk5OiJIMKq6IoiseosCqNDiK60poLdTERTbQmQykloqesuVFnElG2FTeXiL625tB81zG/Zg8i+tyaT3URER1hJd+KiN4molVENIWckxsoSoKosCqNCiI6GsBIAEOZORdAFYBRAFoCWMDMvQHMBnCvtctrAG5n5j6QETMmfAqAZ1nmU/0ZZAQcILNw3QyZ5/dwyLh5RakVOruV0tgYDuB4APMtY7I5ZKKMagBvWnH+CWAaER0AoC0zz7bCXwXwljVGvDMzvwsAzFwGAFZ685h5o7W+GDJf7lz/i6U0JVRYlcYGAXiVme8MCyT6Y0S8uo7VLncsV0GfEaUOqCtAaWzMBPALaw5N842iwyD3spl56QoAc5m5GMAOIjrRCr8KwGxmLgGwkYgusNLIJKIW9VoKpUmjtbHSqGDm74hoPGTm9zTIzGBjAewGMNDaVgDxwwIy9dvzlnCuA3CNFX4VgIlE9ICVxiX1WAyliaOzWylNAiIqZeZWqc6HogDqClAURfEctVgVRVE8Ri1WRVEUj1FhVRRF8RgVVkVRFI9RYVUURfEYFVZFURSPUWFVFEXxmP8Hqp3nj3p+SwsAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su2Sj5jZCSwH",
        "outputId": "151333f6-72f3-4cd9-86d2-c4709b3ed7ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 4)                 512       \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 641\n",
            "Trainable params: 609\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPRh6v-mCSwH",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1623bf-c474-4e96-c767-09f88ade7077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 7ms/step - loss: 12189.8994 - val_loss: 12307.1113\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11883.0332 - val_loss: 11848.8213\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11510.7676 - val_loss: 11294.7598\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 11045.8506 - val_loss: 10461.4609\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 10442.4082 - val_loss: 8694.3574\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9726.4443 - val_loss: 8208.3857\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 8986.9258 - val_loss: 7614.6782\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8216.9326 - val_loss: 6819.5703\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 7429.0269 - val_loss: 6824.7368\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 6641.0596 - val_loss: 5294.6211\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 5869.3550 - val_loss: 5503.4614\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5121.5181 - val_loss: 5819.6367\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4412.7866 - val_loss: 3584.8560\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 3749.6445 - val_loss: 3449.9973\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3141.5037 - val_loss: 2619.6829\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2591.4768 - val_loss: 2365.6199\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2103.9075 - val_loss: 2402.3149\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1680.0848 - val_loss: 1537.0637\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1317.1603 - val_loss: 956.2762\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1019.9664 - val_loss: 743.7672\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 777.5756 - val_loss: 474.8497\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 588.7877 - val_loss: 380.4886\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 442.2617 - val_loss: 391.0626\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 335.6135 - val_loss: 318.5037\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 262.4407 - val_loss: 196.1771\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 208.5224 - val_loss: 163.8031\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 171.7096 - val_loss: 160.7367\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 143.4922 - val_loss: 166.3325\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 131.3528 - val_loss: 199.7663\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 124.8522 - val_loss: 126.7452\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 121.3004 - val_loss: 148.3120\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 120.9308 - val_loss: 157.5291\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 117.2066 - val_loss: 140.7349\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 116.5643 - val_loss: 131.9845\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 116.0654 - val_loss: 138.5344\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 116.1974 - val_loss: 154.1949\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 114.6044 - val_loss: 160.4348\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 114.3129 - val_loss: 123.6101\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 112.9174 - val_loss: 161.0806\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 114.2534 - val_loss: 128.3387\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 112.8845 - val_loss: 135.8313\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 112.4867 - val_loss: 125.6263\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 112.2398 - val_loss: 127.5889\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 112.3282 - val_loss: 158.8290\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 111.1215 - val_loss: 242.0226\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 110.6599 - val_loss: 135.6508\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.7673 - val_loss: 124.9490\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 110.3068 - val_loss: 117.7720\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 110.1755 - val_loss: 118.6767\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.5417 - val_loss: 116.7350\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.4584 - val_loss: 144.1734\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.7861 - val_loss: 130.2337\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.2019 - val_loss: 131.3819\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 108.4134 - val_loss: 115.2903\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.3480 - val_loss: 126.6179\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.4900 - val_loss: 125.8118\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.0621 - val_loss: 113.7617\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 108.2749 - val_loss: 140.7178\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.8576 - val_loss: 120.3977\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.6964 - val_loss: 121.7977\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.4575 - val_loss: 118.6135\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 107.5410 - val_loss: 124.6209\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.4806 - val_loss: 123.0445\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.0747 - val_loss: 139.5350\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.7329 - val_loss: 120.1192\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.9931 - val_loss: 120.2102\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.4643 - val_loss: 137.1817\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.8128 - val_loss: 118.9546\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.3103 - val_loss: 121.0012\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.6796 - val_loss: 113.5243\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.4271 - val_loss: 123.3600\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.0447 - val_loss: 124.1933\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.1614 - val_loss: 120.7166\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.9698 - val_loss: 112.0250\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.7789 - val_loss: 117.7018\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.7501 - val_loss: 115.8255\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.4397 - val_loss: 128.5016\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.5485 - val_loss: 121.1235\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.6020 - val_loss: 123.5553\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.3787 - val_loss: 113.1578\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.1714 - val_loss: 120.3633\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8043 - val_loss: 121.6748\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8776 - val_loss: 118.7518\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.9461 - val_loss: 125.0199\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.6441 - val_loss: 120.0620\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.3686 - val_loss: 146.8400\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.4224 - val_loss: 121.4556\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.3964 - val_loss: 116.8203\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.0564 - val_loss: 126.9042\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2644 - val_loss: 143.4681\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7360 - val_loss: 117.3362\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.9341 - val_loss: 145.3461\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.5900 - val_loss: 114.6659\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.3647 - val_loss: 111.2826\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.1646 - val_loss: 118.2740\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1354 - val_loss: 123.3422\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.2400 - val_loss: 114.3562\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8466 - val_loss: 112.7672\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5662 - val_loss: 115.3132\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4184 - val_loss: 125.3426\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1888 - val_loss: 114.1476\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.1682 - val_loss: 113.5938\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.9879 - val_loss: 123.6226\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.9931 - val_loss: 113.8359\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.7169 - val_loss: 110.8532\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.4939 - val_loss: 109.2756\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4956 - val_loss: 120.0795\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.1426 - val_loss: 110.8660\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.2039 - val_loss: 131.5211\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.1747 - val_loss: 115.2423\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9220 - val_loss: 117.2535\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.8420 - val_loss: 114.2625\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.7642 - val_loss: 110.4340\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.5798 - val_loss: 125.5165\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.4448 - val_loss: 115.9665\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.3205 - val_loss: 122.3730\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.2437 - val_loss: 111.6768\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.1610 - val_loss: 114.2039\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.0515 - val_loss: 134.1971\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2035 - val_loss: 111.7746\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9548 - val_loss: 124.0284\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8887 - val_loss: 118.9597\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6352 - val_loss: 116.3820\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5950 - val_loss: 114.5771\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4056 - val_loss: 112.6402\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6294 - val_loss: 108.9907\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4758 - val_loss: 137.2472\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4861 - val_loss: 109.0307\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3253 - val_loss: 119.2841\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3224 - val_loss: 116.4605\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2889 - val_loss: 132.5697\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1741 - val_loss: 105.1487\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.2499 - val_loss: 123.2501\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1652 - val_loss: 132.4129\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0548 - val_loss: 111.9957\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0215 - val_loss: 113.6907\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.8264 - val_loss: 109.1314\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.9550 - val_loss: 117.0028\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9082 - val_loss: 107.3346\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8979 - val_loss: 115.4925\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.7732 - val_loss: 112.2042\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.6959 - val_loss: 122.2332\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.6673 - val_loss: 150.7470\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.7223 - val_loss: 106.5885\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7233 - val_loss: 110.4826\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4839 - val_loss: 109.7050\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.5250 - val_loss: 113.9749\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4024 - val_loss: 120.3591\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.4276 - val_loss: 120.4616\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2983 - val_loss: 107.1739\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.5361 - val_loss: 112.0463\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2519 - val_loss: 115.2096\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.1670 - val_loss: 186.9009\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.2111 - val_loss: 106.9077\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.0892 - val_loss: 115.1032\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.1191 - val_loss: 122.9394\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.9859 - val_loss: 107.0271\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.9867 - val_loss: 105.1886\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.8920 - val_loss: 106.4124\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.9015 - val_loss: 114.7361\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.9578 - val_loss: 113.1029\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.8453 - val_loss: 108.6455\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.8547 - val_loss: 123.9595\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.7598 - val_loss: 126.6866\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.7187 - val_loss: 114.9715\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.6657 - val_loss: 113.1944\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.7672 - val_loss: 108.1803\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.5602 - val_loss: 107.1058\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.4236 - val_loss: 109.9485\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.6108 - val_loss: 113.9278\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.2906 - val_loss: 113.6759\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.5109 - val_loss: 116.1966\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.3992 - val_loss: 159.8751\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.2616 - val_loss: 105.4388\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.2976 - val_loss: 118.4597\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.2888 - val_loss: 120.2254\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.0238 - val_loss: 119.1665\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.9524 - val_loss: 114.6549\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.9743 - val_loss: 105.6349\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.9027 - val_loss: 122.8696\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.5760 - val_loss: 104.2897\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.5844 - val_loss: 103.2555\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.2423 - val_loss: 113.5235\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.5088 - val_loss: 103.0542\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.0608 - val_loss: 117.3507\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.0362 - val_loss: 110.8218\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.1239 - val_loss: 113.1517\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.9981 - val_loss: 112.9591\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.8173 - val_loss: 104.6821\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.9552 - val_loss: 124.7915\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.9520 - val_loss: 107.9930\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.8304 - val_loss: 114.6318\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.6146 - val_loss: 105.3820\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.5814 - val_loss: 111.0398\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.4923 - val_loss: 103.6346\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.5046 - val_loss: 107.2813\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.4038 - val_loss: 111.1672\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.6112 - val_loss: 105.6104\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.3275 - val_loss: 105.7897\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.2552 - val_loss: 105.0781\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.1763 - val_loss: 120.4306\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.3083 - val_loss: 126.2362\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.3278 - val_loss: 136.3022\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.3704 - val_loss: 124.3107\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.1835 - val_loss: 111.8923\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.0663 - val_loss: 108.3480\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.2523 - val_loss: 113.6902\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.1163 - val_loss: 115.8873\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.0772 - val_loss: 123.1323\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.9717 - val_loss: 148.4220\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.2000 - val_loss: 115.0828\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.9477 - val_loss: 110.8907\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.1253 - val_loss: 104.8492\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.0151 - val_loss: 104.1952\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.1107 - val_loss: 110.5776\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.9462 - val_loss: 108.2330\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.8375 - val_loss: 117.1389\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.9941 - val_loss: 153.7530\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.9928 - val_loss: 106.0970\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.9813 - val_loss: 125.2272\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.8004 - val_loss: 103.7068\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.7585 - val_loss: 123.3520\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.8381 - val_loss: 108.1424\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.6927 - val_loss: 113.4405\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.6866 - val_loss: 117.6736\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.7841 - val_loss: 132.4396\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.7142 - val_loss: 104.0751\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.8312 - val_loss: 105.2170\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.7752 - val_loss: 115.7670\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.6940 - val_loss: 115.4471\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.5995 - val_loss: 104.6006\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.6095 - val_loss: 102.2457\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.6253 - val_loss: 106.8568\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.4675 - val_loss: 104.6150\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.6401 - val_loss: 141.4405\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.7672 - val_loss: 107.2400\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.7637 - val_loss: 107.7114\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.4607 - val_loss: 131.9593\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.6585 - val_loss: 111.8384\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.6228 - val_loss: 143.1466\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.6041 - val_loss: 106.9833\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.5836 - val_loss: 108.6391\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.5774 - val_loss: 106.6948\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.5440 - val_loss: 108.3981\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.4962 - val_loss: 123.2586\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4041 - val_loss: 129.6731\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.2617 - val_loss: 102.2387\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4024 - val_loss: 136.9239\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.4562 - val_loss: 103.4137\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4910 - val_loss: 129.3975\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.4844 - val_loss: 114.5509\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4591 - val_loss: 112.5130\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.5553 - val_loss: 103.5264\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4987 - val_loss: 144.0459\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.3232 - val_loss: 105.7146\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4844 - val_loss: 116.6949\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4566 - val_loss: 107.5407\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.4310 - val_loss: 104.7614\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.6354 - val_loss: 101.9041\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.3263 - val_loss: 123.4492\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.3180 - val_loss: 116.1227\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2924 - val_loss: 103.5174\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.5949 - val_loss: 142.5419\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.2453 - val_loss: 104.7215\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1813 - val_loss: 103.3414\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.2144 - val_loss: 116.3458\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.2291 - val_loss: 103.5767\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.3818 - val_loss: 104.0572\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.3302 - val_loss: 104.7589\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.4154 - val_loss: 110.1889\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1221 - val_loss: 105.7425\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.2380 - val_loss: 116.4119\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1747 - val_loss: 121.0597\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.2567 - val_loss: 107.9815\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2326 - val_loss: 105.5185\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1922 - val_loss: 113.7856\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2727 - val_loss: 108.8810\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2895 - val_loss: 103.9069\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2352 - val_loss: 116.3043\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1289 - val_loss: 105.4853\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.5573 - val_loss: 110.2829\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1989 - val_loss: 119.4057\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.0796 - val_loss: 113.1926\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1526 - val_loss: 120.3286\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2741 - val_loss: 104.2675\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9817 - val_loss: 100.7462\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2381 - val_loss: 101.6049\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.0381 - val_loss: 115.2572\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 94.0824 - val_loss: 119.1244\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 93.9714 - val_loss: 153.4086\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 94.2365 - val_loss: 122.9357\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.0197 - val_loss: 144.7707\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1369 - val_loss: 160.4058\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9896 - val_loss: 108.4621\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1191 - val_loss: 107.3937\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.3250 - val_loss: 111.9084\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.0240 - val_loss: 106.0751\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9464 - val_loss: 114.1844\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1515 - val_loss: 119.6962\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.0356 - val_loss: 104.2017\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.9991 - val_loss: 119.4952\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.0605 - val_loss: 102.7204\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9948 - val_loss: 133.1807\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.0458 - val_loss: 115.2215\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.0742 - val_loss: 134.6065\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.9806 - val_loss: 108.0236\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9495 - val_loss: 105.5322\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1032 - val_loss: 101.4603\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9565 - val_loss: 117.0402\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1361 - val_loss: 104.0410\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.0570 - val_loss: 107.1294\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.0043 - val_loss: 103.4278\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8162 - val_loss: 102.3902\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.0087 - val_loss: 111.6399\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9762 - val_loss: 117.6469\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8131 - val_loss: 109.7669\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2317 - val_loss: 110.3302\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.9484 - val_loss: 176.3930\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9558 - val_loss: 115.1800\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9233 - val_loss: 103.4725\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9824 - val_loss: 149.6854\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.8787 - val_loss: 116.1628\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8534 - val_loss: 110.3846\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1515 - val_loss: 108.7451\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9949 - val_loss: 113.5333\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9623 - val_loss: 110.2283\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.8704 - val_loss: 103.5375\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.8149 - val_loss: 106.2738\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.9081 - val_loss: 112.1214\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.0062 - val_loss: 106.6962\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8802 - val_loss: 112.4391\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1249 - val_loss: 108.5546\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8543 - val_loss: 150.5596\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.0297 - val_loss: 107.5876\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8469 - val_loss: 108.5947\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8807 - val_loss: 102.0768\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1086 - val_loss: 111.3810\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.0112 - val_loss: 107.2472\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8369 - val_loss: 106.4754\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9282 - val_loss: 103.2926\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9274 - val_loss: 138.8428\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9243 - val_loss: 100.4401\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.9503 - val_loss: 101.1074\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9133 - val_loss: 144.3083\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.0043 - val_loss: 101.3040\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9300 - val_loss: 112.3476\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8783 - val_loss: 129.9620\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9606 - val_loss: 102.7277\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9653 - val_loss: 105.1662\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.0832 - val_loss: 112.0659\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7697 - val_loss: 103.9535\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9260 - val_loss: 105.4743\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7869 - val_loss: 112.4866\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9103 - val_loss: 116.1566\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8202 - val_loss: 115.5479\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7162 - val_loss: 111.9100\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8797 - val_loss: 120.4074\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9363 - val_loss: 104.6617\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8783 - val_loss: 117.7894\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7341 - val_loss: 109.1802\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7833 - val_loss: 105.8967\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1545 - val_loss: 119.1588\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9014 - val_loss: 129.5023\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8473 - val_loss: 107.9082\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7658 - val_loss: 105.4304\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7534 - val_loss: 109.6304\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7033 - val_loss: 150.8183\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8591 - val_loss: 109.4492\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8700 - val_loss: 101.6879\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.0235 - val_loss: 103.3659\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7188 - val_loss: 105.3268\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9063 - val_loss: 116.0639\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7789 - val_loss: 118.1477\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7234 - val_loss: 119.4253\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8775 - val_loss: 104.3715\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8354 - val_loss: 104.5771\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7445 - val_loss: 101.4171\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8832 - val_loss: 112.8917\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8547 - val_loss: 117.8070\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8041 - val_loss: 111.0503\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8972 - val_loss: 116.6268\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7086 - val_loss: 111.0629\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8075 - val_loss: 109.1638\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8368 - val_loss: 108.3448\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7870 - val_loss: 101.5349\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7678 - val_loss: 101.5869\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8973 - val_loss: 124.5689\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9508 - val_loss: 114.0495\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9051 - val_loss: 114.9554\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9664 - val_loss: 104.4394\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7756 - val_loss: 112.9666\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7013 - val_loss: 113.4553\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9705 - val_loss: 105.0690\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8793 - val_loss: 106.6684\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8393 - val_loss: 119.6139\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7877 - val_loss: 107.8642\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7175 - val_loss: 116.7831\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6023 - val_loss: 103.8439\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6600 - val_loss: 126.8371\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6860 - val_loss: 106.7295\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7482 - val_loss: 127.7044\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8364 - val_loss: 103.6072\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7001 - val_loss: 125.8426\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7493 - val_loss: 107.0766\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7684 - val_loss: 100.7550\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7983 - val_loss: 103.6729\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6346 - val_loss: 118.4273\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6180 - val_loss: 114.0370\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9127 - val_loss: 101.8038\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9209 - val_loss: 103.4158\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8144 - val_loss: 109.2203\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5446 - val_loss: 110.2682\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7761 - val_loss: 134.9900\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8312 - val_loss: 122.6896\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7829 - val_loss: 109.7315\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6192 - val_loss: 108.5984\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6573 - val_loss: 108.4853\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7292 - val_loss: 105.2081\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7555 - val_loss: 102.7875\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7760 - val_loss: 102.5434\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8247 - val_loss: 108.0557\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6022 - val_loss: 104.5278\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7556 - val_loss: 136.9325\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7479 - val_loss: 120.2648\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6273 - val_loss: 109.7338\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7186 - val_loss: 148.1013\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6910 - val_loss: 115.4545\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7521 - val_loss: 101.3469\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6734 - val_loss: 109.0676\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6310 - val_loss: 103.7142\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6919 - val_loss: 111.3199\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6901 - val_loss: 101.4030\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6967 - val_loss: 117.3123\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5833 - val_loss: 103.3310\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5760 - val_loss: 105.7722\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7708 - val_loss: 106.7109\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6286 - val_loss: 110.3367\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6126 - val_loss: 104.5477\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7596 - val_loss: 100.8630\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6518 - val_loss: 118.9340\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7435 - val_loss: 103.0684\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6540 - val_loss: 102.6217\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5432 - val_loss: 109.2994\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6435 - val_loss: 109.5073\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5492 - val_loss: 100.2400\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5950 - val_loss: 102.3655\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6547 - val_loss: 121.8645\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7327 - val_loss: 106.8386\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7765 - val_loss: 101.6770\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6234 - val_loss: 104.8857\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5049 - val_loss: 115.5464\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5771 - val_loss: 108.0138\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7891 - val_loss: 119.7636\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 93.7181 - val_loss: 103.7992\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5650 - val_loss: 111.7644\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5900 - val_loss: 106.8936\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 93.7444 - val_loss: 124.9381\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6948 - val_loss: 103.1659\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.4793 - val_loss: 102.5278\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7267 - val_loss: 130.2228\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7013 - val_loss: 107.9582\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6728 - val_loss: 101.7427\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5619 - val_loss: 105.7507\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5575 - val_loss: 108.3308\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6638 - val_loss: 119.0230\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7299 - val_loss: 112.3643\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6190 - val_loss: 119.6749\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5639 - val_loss: 112.1823\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7486 - val_loss: 127.9548\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5714 - val_loss: 113.6253\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6350 - val_loss: 110.2335\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6589 - val_loss: 111.3687\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5747 - val_loss: 109.5668\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5297 - val_loss: 105.0164\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5474 - val_loss: 106.4691\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6774 - val_loss: 114.9662\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7228 - val_loss: 119.0875\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5845 - val_loss: 101.5089\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6298 - val_loss: 117.3426\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6244 - val_loss: 123.0719\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5484 - val_loss: 110.9293\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6528 - val_loss: 103.7370\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6633 - val_loss: 120.5409\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5391 - val_loss: 102.7764\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6475 - val_loss: 112.9301\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5498 - val_loss: 105.4462\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6118 - val_loss: 112.9297\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6186 - val_loss: 131.0245\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6157 - val_loss: 108.3130\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.3960 - val_loss: 112.7940\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6076 - val_loss: 127.3021\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5748 - val_loss: 104.5163\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6104 - val_loss: 112.3735\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5077 - val_loss: 141.2547\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7314 - val_loss: 115.1375\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5102 - val_loss: 111.7846\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5755 - val_loss: 110.0608\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7532 - val_loss: 104.6458\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6352 - val_loss: 101.9087\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5938 - val_loss: 115.7508\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca4b81e-b153-41f6-c2e9-ab97bbde966c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -2.8874741462769604 \n",
            "MAE:  8.224279387941328 \n",
            "SD:  10.364037563101284\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "94891ddb-fad0-4fe8-88d7-68aea39d2f73"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6302b947-05b2-480c-e039-f0cc34c274b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 4)                 512       \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 641\n",
            "Trainable params: 609\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXAOqd2CTsa",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb14975-2ead-4452-bd63-1287f9ebf50f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 7ms/step - loss: 12457.2637 - val_loss: 12490.5186\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 12291.5205 - val_loss: 12053.2812\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 12117.7549 - val_loss: 11663.5186\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11871.4570 - val_loss: 11377.5479\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11487.2158 - val_loss: 10792.9814\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 10994.0674 - val_loss: 10270.3965\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 10377.1582 - val_loss: 9286.4238\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 9707.9238 - val_loss: 10612.9844\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9011.1191 - val_loss: 10820.8301\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8294.3818 - val_loss: 6889.5771\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 7518.5547 - val_loss: 3579.0264\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 6693.4268 - val_loss: 6005.2407\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 5880.3506 - val_loss: 7963.3618\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 5106.0200 - val_loss: 7408.1738\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4365.3726 - val_loss: 7527.3994\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3671.9968 - val_loss: 1399.4282\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 3042.2378 - val_loss: 4344.9536\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2483.7922 - val_loss: 5994.0273\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1997.4437 - val_loss: 3085.9290\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1585.1252 - val_loss: 2757.8264\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1223.1995 - val_loss: 917.7424\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 936.3854 - val_loss: 1215.0405\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 705.5998 - val_loss: 637.9743\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 527.0099 - val_loss: 945.2633\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 392.6474 - val_loss: 435.2426\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 297.8263 - val_loss: 1913.8296\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 234.7933 - val_loss: 331.6911\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 191.2402 - val_loss: 2771.2588\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 163.9126 - val_loss: 196.2520\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 147.2141 - val_loss: 1608.9794\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 139.7711 - val_loss: 2353.0959\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 134.9627 - val_loss: 862.0689\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 129.4171 - val_loss: 727.9257\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 125.5943 - val_loss: 2125.1206\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 133.4249 - val_loss: 726.7718\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 126.2814 - val_loss: 405.1575\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 120.8116 - val_loss: 3393.9705\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 119.7841 - val_loss: 203.7595\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 119.2733 - val_loss: 143.3413\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 117.3773 - val_loss: 152.2157\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 115.9244 - val_loss: 383.3494\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 115.8237 - val_loss: 275.4742\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 115.3327 - val_loss: 124.2102\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 114.5523 - val_loss: 131.4077\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 114.0779 - val_loss: 4072.5508\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 114.9144 - val_loss: 2346.6448\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 114.0055 - val_loss: 300.4677\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 113.7409 - val_loss: 5947.9360\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 112.9481 - val_loss: 1030.3320\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 112.3182 - val_loss: 117.9441\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 112.1765 - val_loss: 123.1504\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.5047 - val_loss: 120.8973\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.8023 - val_loss: 239.1436\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.1836 - val_loss: 193.8054\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.4952 - val_loss: 225.5130\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.8208 - val_loss: 130.5363\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.5823 - val_loss: 235.8327\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.7812 - val_loss: 117.8745\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.3467 - val_loss: 174.8400\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.2042 - val_loss: 117.4119\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 110.4049 - val_loss: 131.0488\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 110.2577 - val_loss: 136.1783\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 110.1212 - val_loss: 123.2792\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.5898 - val_loss: 118.6698\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.7745 - val_loss: 147.3143\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.2984 - val_loss: 1186.5544\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.4428 - val_loss: 242.1312\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.4117 - val_loss: 116.7903\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.3775 - val_loss: 170.1279\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.2952 - val_loss: 149.5461\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.0021 - val_loss: 202.7123\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.0956 - val_loss: 172.9418\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.4921 - val_loss: 116.6865\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.6768 - val_loss: 152.4919\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.8309 - val_loss: 119.0562\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.4357 - val_loss: 206.8040\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 108.3818 - val_loss: 193.1342\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.2370 - val_loss: 146.7153\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.1203 - val_loss: 167.1373\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.0429 - val_loss: 140.8942\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.8203 - val_loss: 112.4443\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.2212 - val_loss: 118.0551\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 108.5771 - val_loss: 128.5527\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.7682 - val_loss: 225.0407\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.5493 - val_loss: 192.8830\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.9404 - val_loss: 152.8712\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 107.8976 - val_loss: 120.9242\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 107.2619 - val_loss: 140.4171\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.1660 - val_loss: 278.7821\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.8110 - val_loss: 189.4781\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.9095 - val_loss: 601.4095\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.8990 - val_loss: 358.4417\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.0737 - val_loss: 115.5553\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.7666 - val_loss: 550.5137\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.9499 - val_loss: 314.0367\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 107.0552 - val_loss: 135.5580\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.6656 - val_loss: 115.9145\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.4252 - val_loss: 142.6162\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.4021 - val_loss: 397.8187\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.9482 - val_loss: 420.3280\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.4707 - val_loss: 124.8229\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.3963 - val_loss: 214.0800\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.0297 - val_loss: 363.7131\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.5232 - val_loss: 144.4641\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.2809 - val_loss: 260.6008\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.5485 - val_loss: 117.6116\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.0070 - val_loss: 141.7319\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.9542 - val_loss: 128.2569\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.8637 - val_loss: 120.4643\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.9629 - val_loss: 125.1835\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.8345 - val_loss: 113.6225\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.6554 - val_loss: 135.0582\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.7892 - val_loss: 148.2447\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4612 - val_loss: 129.5442\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0557 - val_loss: 112.4295\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 104.9487 - val_loss: 117.5212\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 105.2141 - val_loss: 120.6557\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 104.9329 - val_loss: 141.8669\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.0907 - val_loss: 119.6661\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.7786 - val_loss: 111.0338\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.7342 - val_loss: 126.8341\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.6739 - val_loss: 127.0005\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.4997 - val_loss: 120.6360\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.4258 - val_loss: 133.9971\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.4188 - val_loss: 208.9135\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.2297 - val_loss: 133.7897\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.3468 - val_loss: 132.9096\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1836 - val_loss: 125.5017\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9098 - val_loss: 135.2857\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.0022 - val_loss: 111.4015\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.0476 - val_loss: 122.8604\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8263 - val_loss: 128.2536\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6015 - val_loss: 115.5744\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.5017 - val_loss: 148.3774\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.2848 - val_loss: 115.2229\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.2103 - val_loss: 124.4653\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.2121 - val_loss: 111.8322\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.0681 - val_loss: 131.9486\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.3104 - val_loss: 119.4480\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.9791 - val_loss: 118.4711\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.9082 - val_loss: 134.2384\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1065 - val_loss: 141.0746\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7691 - val_loss: 114.7310\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6382 - val_loss: 116.9844\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7836 - val_loss: 115.8411\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6158 - val_loss: 114.0104\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.5685 - val_loss: 114.1275\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5029 - val_loss: 112.9388\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.5416 - val_loss: 120.9437\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5173 - val_loss: 110.9072\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.4389 - val_loss: 117.9704\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.3491 - val_loss: 114.7647\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.4254 - val_loss: 112.2228\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.8920 - val_loss: 131.9253\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1988 - val_loss: 126.4156\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0973 - val_loss: 133.2327\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.2242 - val_loss: 129.6013\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0172 - val_loss: 112.2940\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.1264 - val_loss: 120.5679\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9304 - val_loss: 170.6562\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.9390 - val_loss: 153.6307\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9261 - val_loss: 132.1493\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.7530 - val_loss: 115.7312\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7732 - val_loss: 120.1188\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5461 - val_loss: 114.1350\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8338 - val_loss: 121.2169\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.6912 - val_loss: 113.9976\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.6680 - val_loss: 134.1540\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5463 - val_loss: 125.0214\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.5689 - val_loss: 140.1738\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.6171 - val_loss: 116.9543\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4745 - val_loss: 119.7468\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6721 - val_loss: 113.3603\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.4654 - val_loss: 115.3388\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4583 - val_loss: 122.7984\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5566 - val_loss: 113.1995\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.3252 - val_loss: 129.3671\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.3056 - val_loss: 116.6167\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.4215 - val_loss: 112.7274\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.5137 - val_loss: 111.8747\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.3376 - val_loss: 121.8004\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3896 - val_loss: 110.6589\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0386 - val_loss: 108.5227\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2934 - val_loss: 121.0560\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.1613 - val_loss: 108.3818\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.3854 - val_loss: 116.4977\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.0977 - val_loss: 118.3551\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.2257 - val_loss: 110.2561\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.1882 - val_loss: 111.4837\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1368 - val_loss: 116.7596\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9156 - val_loss: 114.7449\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.9706 - val_loss: 113.5088\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.8205 - val_loss: 123.9557\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9580 - val_loss: 121.1244\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.2222 - val_loss: 124.1667\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.9833 - val_loss: 115.5794\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9882 - val_loss: 127.7854\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.8269 - val_loss: 114.1004\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9214 - val_loss: 107.8721\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.9217 - val_loss: 122.1835\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.7761 - val_loss: 115.8863\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.8292 - val_loss: 110.4678\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.9270 - val_loss: 119.3505\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.6432 - val_loss: 113.9173\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.7013 - val_loss: 139.3430\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.7622 - val_loss: 113.7220\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.7168 - val_loss: 109.6781\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.7978 - val_loss: 109.3136\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.6086 - val_loss: 109.8103\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.7091 - val_loss: 110.1396\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.6326 - val_loss: 119.8407\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.7412 - val_loss: 111.0229\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.7539 - val_loss: 115.6375\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.6030 - val_loss: 112.3413\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.5702 - val_loss: 107.0838\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.7083 - val_loss: 127.0214\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.7473 - val_loss: 116.6906\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.5691 - val_loss: 138.1425\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.5881 - val_loss: 123.2310\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.5698 - val_loss: 119.8785\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.4651 - val_loss: 118.9266\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.4784 - val_loss: 124.0887\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.5266 - val_loss: 108.3557\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.4841 - val_loss: 117.4144\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.5993 - val_loss: 109.7252\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3036 - val_loss: 106.2147\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.4030 - val_loss: 128.3222\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.4057 - val_loss: 120.6102\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.6053 - val_loss: 108.5664\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.3809 - val_loss: 111.1369\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3891 - val_loss: 112.7379\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.3149 - val_loss: 131.5885\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3489 - val_loss: 113.6151\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3054 - val_loss: 105.7678\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.4523 - val_loss: 114.6318\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3143 - val_loss: 107.8995\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3986 - val_loss: 133.3518\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3149 - val_loss: 114.6523\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2530 - val_loss: 127.7899\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3133 - val_loss: 109.7807\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3295 - val_loss: 111.2536\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2515 - val_loss: 129.5016\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.1427 - val_loss: 109.6539\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.2804 - val_loss: 120.9323\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2864 - val_loss: 117.4075\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3100 - val_loss: 108.7508\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.1412 - val_loss: 117.9879\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.2403 - val_loss: 107.0624\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2342 - val_loss: 118.7009\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.2029 - val_loss: 114.3726\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.3005 - val_loss: 117.8389\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.1088 - val_loss: 120.4226\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.1963 - val_loss: 107.5619\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2120 - val_loss: 117.9575\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.2564 - val_loss: 111.1377\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2869 - val_loss: 119.3462\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.1024 - val_loss: 106.2781\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.2422 - val_loss: 112.7241\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.9210 - val_loss: 116.8165\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3605 - val_loss: 138.1903\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9746 - val_loss: 118.0831\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 100.1844 - val_loss: 118.3118\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 100.0237 - val_loss: 115.9614\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.0672 - val_loss: 129.3881\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.9966 - val_loss: 122.5330\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.0063 - val_loss: 109.4445\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.2354 - val_loss: 107.9738\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.0120 - val_loss: 107.3961\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.1581 - val_loss: 107.1070\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.0324 - val_loss: 108.1122\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.1929 - val_loss: 108.7953\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9082 - val_loss: 114.7019\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.0359 - val_loss: 118.4362\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.0539 - val_loss: 128.5536\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9331 - val_loss: 111.4533\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.9354 - val_loss: 116.2589\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8507 - val_loss: 108.6304\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.1884 - val_loss: 114.7007\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.0362 - val_loss: 116.0063\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.1019 - val_loss: 107.7528\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.0499 - val_loss: 124.4452\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9043 - val_loss: 131.2495\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9799 - val_loss: 119.3432\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9139 - val_loss: 105.4548\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.7724 - val_loss: 107.6106\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9915 - val_loss: 112.5223\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8739 - val_loss: 105.6754\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9934 - val_loss: 129.3253\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.7688 - val_loss: 121.3075\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9721 - val_loss: 116.7442\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.0704 - val_loss: 111.9871\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9896 - val_loss: 112.8077\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.8564 - val_loss: 116.3108\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9697 - val_loss: 111.0715\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.8566 - val_loss: 130.6648\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9202 - val_loss: 110.8195\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6970 - val_loss: 105.9391\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.8839 - val_loss: 114.1498\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8083 - val_loss: 108.9458\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8580 - val_loss: 113.1384\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8522 - val_loss: 133.0534\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.8480 - val_loss: 107.1954\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9286 - val_loss: 109.8714\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8863 - val_loss: 118.0106\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8340 - val_loss: 109.6357\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8400 - val_loss: 114.1737\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.8414 - val_loss: 111.6138\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.8482 - val_loss: 107.3308\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6728 - val_loss: 121.0501\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.7295 - val_loss: 109.8100\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.7049 - val_loss: 120.8545\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6940 - val_loss: 108.9264\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7243 - val_loss: 107.9067\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8101 - val_loss: 108.0121\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5958 - val_loss: 107.2674\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8497 - val_loss: 125.3730\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7690 - val_loss: 121.5346\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6140 - val_loss: 107.7690\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6176 - val_loss: 111.7498\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5371 - val_loss: 124.9844\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.7735 - val_loss: 118.4186\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.7683 - val_loss: 133.4484\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6710 - val_loss: 109.8410\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6073 - val_loss: 109.1570\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5172 - val_loss: 111.4953\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.9584 - val_loss: 116.2171\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6346 - val_loss: 111.2501\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6902 - val_loss: 113.2501\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5931 - val_loss: 112.4530\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6647 - val_loss: 126.7564\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6503 - val_loss: 109.3169\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6707 - val_loss: 107.6755\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6005 - val_loss: 114.4203\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7360 - val_loss: 113.0431\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6452 - val_loss: 108.5617\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5298 - val_loss: 109.7574\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.7139 - val_loss: 109.3350\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5368 - val_loss: 115.9403\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5100 - val_loss: 106.9386\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4738 - val_loss: 109.1867\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.7000 - val_loss: 106.1836\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.7634 - val_loss: 105.8999\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.7893 - val_loss: 108.2360\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6861 - val_loss: 107.1875\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 99.7135 - val_loss: 121.2234\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 99.5554 - val_loss: 106.5127\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5752 - val_loss: 104.8392\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.5854 - val_loss: 103.7214\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5591 - val_loss: 108.9808\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.5523 - val_loss: 117.0508\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4553 - val_loss: 114.2869\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5926 - val_loss: 109.2816\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6392 - val_loss: 108.1669\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5027 - val_loss: 111.3940\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6992 - val_loss: 112.1614\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4305 - val_loss: 117.5756\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6862 - val_loss: 104.1145\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5033 - val_loss: 118.0388\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6314 - val_loss: 103.8265\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5035 - val_loss: 115.5121\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4853 - val_loss: 107.6127\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5750 - val_loss: 105.4004\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5330 - val_loss: 135.8883\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5891 - val_loss: 106.8626\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5019 - val_loss: 122.8409\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4611 - val_loss: 111.2076\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4193 - val_loss: 118.4327\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5103 - val_loss: 117.5724\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4465 - val_loss: 106.3374\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5343 - val_loss: 156.3694\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4745 - val_loss: 107.6306\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4789 - val_loss: 125.0835\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5229 - val_loss: 105.0537\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5196 - val_loss: 111.6956\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5460 - val_loss: 108.4300\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4919 - val_loss: 112.9144\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4988 - val_loss: 110.7285\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5516 - val_loss: 107.9643\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6379 - val_loss: 108.7579\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6025 - val_loss: 118.1631\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5351 - val_loss: 108.0470\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4446 - val_loss: 106.7030\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5256 - val_loss: 108.1542\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5061 - val_loss: 108.6248\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5051 - val_loss: 112.7889\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5571 - val_loss: 107.6072\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5155 - val_loss: 110.3539\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5469 - val_loss: 114.6960\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2984 - val_loss: 123.0582\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4264 - val_loss: 127.1247\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5654 - val_loss: 135.2311\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4554 - val_loss: 105.0076\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5545 - val_loss: 111.7639\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6689 - val_loss: 111.8513\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5635 - val_loss: 112.2114\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3229 - val_loss: 139.9650\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3880 - val_loss: 120.5732\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4074 - val_loss: 123.4803\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3316 - val_loss: 111.3799\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5329 - val_loss: 111.6478\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5592 - val_loss: 114.7311\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3008 - val_loss: 107.1224\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6166 - val_loss: 115.4839\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4621 - val_loss: 106.7731\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3900 - val_loss: 107.6229\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4205 - val_loss: 122.7757\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4655 - val_loss: 107.2206\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.6285 - val_loss: 105.8585\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4188 - val_loss: 120.5729\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3834 - val_loss: 125.3067\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3133 - val_loss: 122.0561\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3394 - val_loss: 110.4887\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4589 - val_loss: 138.1853\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4412 - val_loss: 121.9882\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3775 - val_loss: 110.3984\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4149 - val_loss: 105.1419\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3900 - val_loss: 136.8237\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2618 - val_loss: 112.3643\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3744 - val_loss: 132.7292\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3062 - val_loss: 117.4909\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2981 - val_loss: 112.0014\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2236 - val_loss: 105.1795\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 99.3473 - val_loss: 106.6922\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 99.3305 - val_loss: 113.2471\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 99.4346 - val_loss: 140.2490\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 99.4191 - val_loss: 113.8821\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2779 - val_loss: 107.8224\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3765 - val_loss: 156.1523\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2783 - val_loss: 155.7959\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4359 - val_loss: 111.8034\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5526 - val_loss: 112.1928\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2238 - val_loss: 110.6177\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3426 - val_loss: 112.8037\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2087 - val_loss: 124.4157\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3904 - val_loss: 107.3025\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3192 - val_loss: 127.3958\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3931 - val_loss: 115.8086\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 99.3300 - val_loss: 116.8214\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4091 - val_loss: 112.9404\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3594 - val_loss: 107.2262\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3072 - val_loss: 114.4676\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3323 - val_loss: 118.5105\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4014 - val_loss: 108.5619\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3267 - val_loss: 113.0765\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1708 - val_loss: 110.4879\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2189 - val_loss: 117.0318\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2597 - val_loss: 114.7604\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3022 - val_loss: 116.8959\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1527 - val_loss: 110.3769\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4303 - val_loss: 110.0602\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3463 - val_loss: 103.1675\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2306 - val_loss: 114.4565\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2147 - val_loss: 106.7281\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2734 - val_loss: 128.1901\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1822 - val_loss: 118.2288\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1945 - val_loss: 107.3820\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3789 - val_loss: 109.3134\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4689 - val_loss: 105.5766\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2355 - val_loss: 113.2238\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3440 - val_loss: 107.6957\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1996 - val_loss: 117.2367\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2211 - val_loss: 114.9731\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4196 - val_loss: 138.7031\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2271 - val_loss: 115.2881\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3034 - val_loss: 159.6782\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3710 - val_loss: 124.2659\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2299 - val_loss: 107.7236\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2767 - val_loss: 112.4472\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2325 - val_loss: 106.8721\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2292 - val_loss: 108.6373\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1517 - val_loss: 108.2354\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2038 - val_loss: 118.5980\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2725 - val_loss: 111.0673\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2740 - val_loss: 109.0824\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1807 - val_loss: 118.8278\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1677 - val_loss: 114.5396\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1420 - val_loss: 108.6135\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3612 - val_loss: 104.6545\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2406 - val_loss: 108.9293\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1226 - val_loss: 104.0064\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2083 - val_loss: 109.7592\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.4723 - val_loss: 105.9226\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2242 - val_loss: 114.9638\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3753 - val_loss: 115.3435\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1519 - val_loss: 111.4350\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1382 - val_loss: 116.1106\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1219 - val_loss: 106.0078\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2983 - val_loss: 120.3134\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3397 - val_loss: 105.2015\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1545 - val_loss: 124.6770\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2995 - val_loss: 114.3101\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1694 - val_loss: 112.9672\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2088 - val_loss: 129.4317\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2115 - val_loss: 108.2772\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2357 - val_loss: 110.3221\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2429 - val_loss: 111.6438\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.1800 - val_loss: 107.8809\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2425 - val_loss: 111.0667\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.2479 - val_loss: 106.7556\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3422 - val_loss: 107.3422\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696v_fuFCTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283b9c28-e8ae-4336-bde9-a8ceac5accf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  1.4981092131351457 \n",
            "MAE:  7.67363025198544 \n",
            "SD:  10.251726054298446\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "efaf2966-2faa-47bc-cf19-a19a753e5b3d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdZF2osWCUQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1257abf9-abb7-4a7b-b9ca-b2636d9ba5fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  0.29941099768151297 \n",
            "Ensemble_std:  10.170825780288745\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c116e6ce-955d-42ac-f8cb-9bebce926214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_15 (Dense)            (None, 4)                 512       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 641\n",
            "Trainable params: 609\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9303a720-1991-47b5-e658-3d1ad873fb59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 7ms/step - loss: 3729.2969 - val_loss: 3775.6958\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3590.1421 - val_loss: 3296.3030\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3423.6001 - val_loss: 2382.8162\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3213.3982 - val_loss: 2680.8232\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2964.3264 - val_loss: 2481.9236\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2689.0559 - val_loss: 3296.2927\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2395.0532 - val_loss: 2340.6504\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2081.4497 - val_loss: 1714.7039\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1759.3340 - val_loss: 1314.8354\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1443.8628 - val_loss: 975.3718\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1151.2111 - val_loss: 950.9745\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 881.2000 - val_loss: 428.2656\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 658.2378 - val_loss: 887.7727\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 480.9109 - val_loss: 231.0556\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 344.5341 - val_loss: 305.3892\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 242.0483 - val_loss: 180.7426\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 172.1307 - val_loss: 153.3877\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 122.4688 - val_loss: 173.4198\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.0189 - val_loss: 118.5229\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.3695 - val_loss: 71.7890\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 60.0336 - val_loss: 66.2205\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 53.8493 - val_loss: 83.8434\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 50.8235 - val_loss: 99.9882\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 47.3857 - val_loss: 86.6847\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 45.7132 - val_loss: 51.7187\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 44.1724 - val_loss: 70.7502\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 43.2642 - val_loss: 127.7893\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 42.5142 - val_loss: 85.9539\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 42.1210 - val_loss: 46.6294\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 42.1000 - val_loss: 74.5388\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 41.4222 - val_loss: 49.3665\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 41.3245 - val_loss: 45.5977\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 41.1578 - val_loss: 62.0370\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 40.8913 - val_loss: 45.7861\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.9143 - val_loss: 60.7834\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.8561 - val_loss: 60.5502\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.4477 - val_loss: 49.8786\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 40.4493 - val_loss: 70.6563\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 40.0572 - val_loss: 44.3832\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.2228 - val_loss: 73.8253\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.8289 - val_loss: 64.3648\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.8897 - val_loss: 46.0570\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.5910 - val_loss: 46.2377\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.6735 - val_loss: 45.1461\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.5190 - val_loss: 45.5998\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 39.3045 - val_loss: 41.7968\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.1652 - val_loss: 61.3760\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.1168 - val_loss: 50.1482\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.9676 - val_loss: 43.1691\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.0881 - val_loss: 42.6994\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 39.0834 - val_loss: 45.7392\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.7763 - val_loss: 73.1852\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.8787 - val_loss: 43.7375\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.8008 - val_loss: 67.8110\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.7614 - val_loss: 48.1432\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.6373 - val_loss: 49.6843\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.5853 - val_loss: 75.7417\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.5262 - val_loss: 50.6169\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.4888 - val_loss: 41.8650\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.3671 - val_loss: 46.7666\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.3663 - val_loss: 43.5706\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.1955 - val_loss: 44.6141\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.0457 - val_loss: 43.4579\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.9312 - val_loss: 49.2915\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.8776 - val_loss: 43.8061\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.7168 - val_loss: 41.7399\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.6410 - val_loss: 51.7237\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.7171 - val_loss: 46.7347\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.4693 - val_loss: 40.7434\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.4525 - val_loss: 40.6537\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.4376 - val_loss: 52.7633\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.4220 - val_loss: 40.8652\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3291 - val_loss: 46.7680\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3800 - val_loss: 43.2121\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3287 - val_loss: 48.0251\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1574 - val_loss: 41.2626\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1326 - val_loss: 51.1641\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1603 - val_loss: 40.3872\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.2128 - val_loss: 41.5413\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0596 - val_loss: 39.5748\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0205 - val_loss: 44.2317\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.0552 - val_loss: 48.5056\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.0519 - val_loss: 57.5470\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.8660 - val_loss: 41.9942\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.9536 - val_loss: 48.6237\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8061 - val_loss: 53.0230\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.9461 - val_loss: 47.1645\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.8412 - val_loss: 39.6862\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8540 - val_loss: 39.5863\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.9542 - val_loss: 46.9085\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.7771 - val_loss: 42.1531\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8179 - val_loss: 43.3154\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.9132 - val_loss: 47.7293\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.9439 - val_loss: 46.0497\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7995 - val_loss: 40.5344\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7110 - val_loss: 48.0033\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7195 - val_loss: 41.2395\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7139 - val_loss: 44.8716\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6618 - val_loss: 53.8172\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6798 - val_loss: 53.1572\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7112 - val_loss: 43.0748\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6249 - val_loss: 44.7394\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6318 - val_loss: 60.5944\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6421 - val_loss: 47.3592\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.5536 - val_loss: 41.8563\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6059 - val_loss: 52.8697\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4992 - val_loss: 42.4223\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.4781 - val_loss: 41.9264\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4663 - val_loss: 42.3285\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5222 - val_loss: 62.5629\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.5694 - val_loss: 54.9537\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.4312 - val_loss: 39.9383\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4473 - val_loss: 40.5062\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.4347 - val_loss: 52.3955\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.4721 - val_loss: 43.2584\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4579 - val_loss: 45.9768\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.4078 - val_loss: 44.4002\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3910 - val_loss: 45.0354\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3673 - val_loss: 40.2608\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3428 - val_loss: 39.1315\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3897 - val_loss: 42.7214\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3629 - val_loss: 39.3904\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.2716 - val_loss: 42.8335\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3656 - val_loss: 40.4995\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.2954 - val_loss: 52.1761\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.2962 - val_loss: 45.5438\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3102 - val_loss: 41.2642\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3597 - val_loss: 43.3320\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.2400 - val_loss: 45.1290\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.2704 - val_loss: 40.8373\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.2474 - val_loss: 42.7581\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2454 - val_loss: 41.0350\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2161 - val_loss: 39.4977\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2425 - val_loss: 39.2515\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2947 - val_loss: 56.2631\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2566 - val_loss: 38.6436\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.3320 - val_loss: 39.6871\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2392 - val_loss: 43.7130\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1791 - val_loss: 42.7894\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2666 - val_loss: 41.8677\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2636 - val_loss: 41.5796\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2280 - val_loss: 41.2026\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1941 - val_loss: 40.6098\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1256 - val_loss: 40.0755\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1500 - val_loss: 39.3251\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1620 - val_loss: 45.2533\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0648 - val_loss: 40.2640\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1283 - val_loss: 43.3183\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1540 - val_loss: 42.7758\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1522 - val_loss: 43.6895\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0971 - val_loss: 46.7838\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0684 - val_loss: 41.4761\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1078 - val_loss: 44.9411\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1109 - val_loss: 44.6965\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0383 - val_loss: 39.4921\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1195 - val_loss: 41.1673\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0411 - val_loss: 39.3177\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0600 - val_loss: 44.5999\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0416 - val_loss: 46.5032\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0978 - val_loss: 46.5596\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0513 - val_loss: 44.4150\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0712 - val_loss: 41.5834\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0412 - val_loss: 39.4004\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0370 - val_loss: 44.7987\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0687 - val_loss: 39.7577\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0735 - val_loss: 41.4846\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0670 - val_loss: 41.6389\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9870 - val_loss: 42.8326\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9435 - val_loss: 44.2531\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9582 - val_loss: 39.5868\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0237 - val_loss: 43.3199\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9535 - val_loss: 41.1539\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9932 - val_loss: 43.5853\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9761 - val_loss: 46.9055\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9755 - val_loss: 43.6655\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9395 - val_loss: 43.9640\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9713 - val_loss: 46.8010\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0165 - val_loss: 44.8827\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9798 - val_loss: 42.1228\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9276 - val_loss: 40.2062\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9704 - val_loss: 43.3191\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9071 - val_loss: 40.0325\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9766 - val_loss: 40.0705\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9949 - val_loss: 42.5157\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9272 - val_loss: 39.4625\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9315 - val_loss: 38.7820\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9443 - val_loss: 45.5476\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9248 - val_loss: 41.9255\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9272 - val_loss: 48.7695\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8933 - val_loss: 40.1796\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9973 - val_loss: 43.6575\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9681 - val_loss: 43.1615\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8935 - val_loss: 39.0794\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 35.9393 - val_loss: 39.1124\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.8741 - val_loss: 42.9285\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8866 - val_loss: 39.6452\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9531 - val_loss: 38.8447\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9671 - val_loss: 44.2737\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8936 - val_loss: 41.3228\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9780 - val_loss: 38.8583\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8665 - val_loss: 42.3632\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8629 - val_loss: 45.0769\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8783 - val_loss: 38.2560\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9071 - val_loss: 40.4323\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9194 - val_loss: 39.4678\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8867 - val_loss: 39.8238\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8748 - val_loss: 42.7989\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8744 - val_loss: 44.6476\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8481 - val_loss: 41.6260\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8943 - val_loss: 44.9507\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8670 - val_loss: 43.1648\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8762 - val_loss: 40.0250\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9625 - val_loss: 46.4887\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9712 - val_loss: 43.0302\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8796 - val_loss: 38.1675\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8765 - val_loss: 46.2278\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9134 - val_loss: 39.7834\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9191 - val_loss: 43.0594\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8936 - val_loss: 41.3114\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9209 - val_loss: 40.0802\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8562 - val_loss: 44.6208\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9240 - val_loss: 55.5530\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8613 - val_loss: 43.3042\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9298 - val_loss: 41.0295\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8497 - val_loss: 47.2989\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8830 - val_loss: 42.1234\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8982 - val_loss: 44.6518\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8300 - val_loss: 40.4333\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8560 - val_loss: 42.4726\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8601 - val_loss: 42.1561\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9078 - val_loss: 42.5456\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8528 - val_loss: 39.6015\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8784 - val_loss: 40.1597\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8213 - val_loss: 52.4406\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8440 - val_loss: 58.6372\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8629 - val_loss: 40.1642\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8966 - val_loss: 41.5322\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8569 - val_loss: 41.4098\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8925 - val_loss: 39.1301\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8649 - val_loss: 40.8481\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8151 - val_loss: 40.3773\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8461 - val_loss: 47.2297\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8800 - val_loss: 42.8924\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8446 - val_loss: 39.9429\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8761 - val_loss: 40.3838\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8280 - val_loss: 41.7648\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8868 - val_loss: 41.4042\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8671 - val_loss: 38.9494\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8236 - val_loss: 43.4196\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8340 - val_loss: 45.5727\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8642 - val_loss: 43.0407\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8077 - val_loss: 44.3990\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8679 - val_loss: 47.9035\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8263 - val_loss: 51.8664\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8354 - val_loss: 42.5451\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8052 - val_loss: 39.8560\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8122 - val_loss: 41.7422\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8352 - val_loss: 40.4879\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8633 - val_loss: 42.0942\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8643 - val_loss: 39.8737\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8567 - val_loss: 38.9724\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8124 - val_loss: 41.8350\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8464 - val_loss: 40.7639\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8479 - val_loss: 39.2156\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7926 - val_loss: 38.7284\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7853 - val_loss: 39.3942\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7902 - val_loss: 41.6813\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8507 - val_loss: 41.1357\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8439 - val_loss: 46.4993\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8610 - val_loss: 40.8062\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8562 - val_loss: 40.3453\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7800 - val_loss: 40.9880\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8323 - val_loss: 39.0900\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8254 - val_loss: 45.0890\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8285 - val_loss: 43.2359\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8176 - val_loss: 43.3058\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8203 - val_loss: 39.8465\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8085 - val_loss: 40.8269\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8024 - val_loss: 45.2455\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7846 - val_loss: 43.9875\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8157 - val_loss: 40.4690\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7881 - val_loss: 42.4855\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8042 - val_loss: 41.8108\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7747 - val_loss: 41.9672\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8280 - val_loss: 48.2396\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7649 - val_loss: 40.4956\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8444 - val_loss: 41.0139\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7837 - val_loss: 46.7937\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7709 - val_loss: 38.8000\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8206 - val_loss: 40.7456\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7697 - val_loss: 44.4752\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8017 - val_loss: 41.3033\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7417 - val_loss: 40.5211\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7935 - val_loss: 42.1590\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8460 - val_loss: 40.9917\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7990 - val_loss: 38.6704\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7419 - val_loss: 39.4187\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8152 - val_loss: 38.2438\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7870 - val_loss: 41.1021\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7815 - val_loss: 41.9626\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7884 - val_loss: 44.2932\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7920 - val_loss: 41.7757\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7740 - val_loss: 41.2233\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7703 - val_loss: 39.0879\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7713 - val_loss: 42.6837\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7703 - val_loss: 39.0948\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8082 - val_loss: 39.8735\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7884 - val_loss: 40.9422\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7887 - val_loss: 43.8015\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8020 - val_loss: 39.4095\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7610 - val_loss: 40.0208\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7712 - val_loss: 42.7873\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7677 - val_loss: 38.4827\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8405 - val_loss: 47.0334\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7802 - val_loss: 41.5974\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7782 - val_loss: 40.0816\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7885 - val_loss: 39.7982\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7828 - val_loss: 39.6142\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7805 - val_loss: 40.5725\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7563 - val_loss: 42.9883\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 35.7893 - val_loss: 40.0737\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 35.7939 - val_loss: 42.8173\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7823 - val_loss: 38.8365\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 35.7013 - val_loss: 38.8154\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 35.7709 - val_loss: 46.8593\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 35.7692 - val_loss: 39.2992\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7900 - val_loss: 42.9794\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7639 - val_loss: 41.2138\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6906 - val_loss: 44.6237\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7445 - val_loss: 48.0460\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6995 - val_loss: 44.4183\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.8155 - val_loss: 38.9530\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7423 - val_loss: 40.6392\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7914 - val_loss: 39.4242\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7715 - val_loss: 41.3625\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.8086 - val_loss: 38.7182\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7412 - val_loss: 42.1926\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7634 - val_loss: 42.3849\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7556 - val_loss: 42.5668\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7393 - val_loss: 40.7718\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7323 - val_loss: 39.7018\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7725 - val_loss: 40.7239\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7570 - val_loss: 39.8065\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7884 - val_loss: 39.3151\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7168 - val_loss: 38.8078\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7855 - val_loss: 39.4462\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7689 - val_loss: 40.1623\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7846 - val_loss: 39.6614\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7341 - val_loss: 42.6647\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7756 - val_loss: 42.3008\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7494 - val_loss: 37.9978\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7475 - val_loss: 41.1411\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7019 - val_loss: 38.2473\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7500 - val_loss: 40.0025\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7235 - val_loss: 38.1964\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7726 - val_loss: 42.9730\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7589 - val_loss: 40.1020\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7389 - val_loss: 40.6657\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7018 - val_loss: 42.8097\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7210 - val_loss: 40.3433\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7154 - val_loss: 38.2667\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7147 - val_loss: 40.1608\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7348 - val_loss: 38.3954\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7600 - val_loss: 39.3189\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7448 - val_loss: 41.7661\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7070 - val_loss: 39.7802\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7419 - val_loss: 38.4688\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6977 - val_loss: 39.2343\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7053 - val_loss: 41.2255\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7413 - val_loss: 43.4980\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7407 - val_loss: 38.5838\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7031 - val_loss: 39.6816\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7033 - val_loss: 42.1083\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7244 - val_loss: 39.5114\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7415 - val_loss: 39.0226\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6800 - val_loss: 38.9294\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7289 - val_loss: 40.1356\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7031 - val_loss: 38.2353\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7284 - val_loss: 38.5845\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7133 - val_loss: 41.4026\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7149 - val_loss: 38.9357\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7220 - val_loss: 40.9393\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6860 - val_loss: 39.5314\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6772 - val_loss: 37.9149\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7043 - val_loss: 41.1651\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7330 - val_loss: 47.0054\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7120 - val_loss: 40.8162\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7014 - val_loss: 42.8862\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6855 - val_loss: 38.9742\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6979 - val_loss: 39.8797\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6932 - val_loss: 39.7483\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6799 - val_loss: 39.6623\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6728 - val_loss: 39.7524\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6912 - val_loss: 44.4080\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6884 - val_loss: 40.0569\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6762 - val_loss: 39.8527\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7234 - val_loss: 39.9728\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6730 - val_loss: 47.0145\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6938 - val_loss: 39.8058\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6151 - val_loss: 40.5150\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7227 - val_loss: 38.5299\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6372 - val_loss: 40.4813\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7091 - val_loss: 39.2224\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6900 - val_loss: 43.2880\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6779 - val_loss: 40.1460\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6354 - val_loss: 41.3073\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6428 - val_loss: 39.2090\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6481 - val_loss: 41.7305\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6482 - val_loss: 58.7387\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5744 - val_loss: 39.9732\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5837 - val_loss: 41.8035\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5997 - val_loss: 40.1222\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5696 - val_loss: 40.0017\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5535 - val_loss: 46.1440\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5911 - val_loss: 49.6108\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6096 - val_loss: 39.6449\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5717 - val_loss: 40.6218\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5776 - val_loss: 37.8796\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5146 - val_loss: 39.1213\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5296 - val_loss: 42.6275\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5627 - val_loss: 41.4892\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5358 - val_loss: 40.0278\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5963 - val_loss: 38.7912\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5102 - val_loss: 40.6810\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5631 - val_loss: 38.8256\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5385 - val_loss: 38.9689\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5551 - val_loss: 42.3796\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5697 - val_loss: 40.7127\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5513 - val_loss: 38.5112\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5737 - val_loss: 38.8582\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5117 - val_loss: 43.4098\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5853 - val_loss: 40.5621\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5056 - val_loss: 44.4844\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5199 - val_loss: 38.2952\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4928 - val_loss: 43.0918\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5454 - val_loss: 40.2107\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4882 - val_loss: 48.6235\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5108 - val_loss: 40.5595\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5219 - val_loss: 39.8436\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5614 - val_loss: 38.5377\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5023 - val_loss: 50.2966\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5001 - val_loss: 38.7255\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5335 - val_loss: 38.9830\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4801 - val_loss: 38.9378\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4639 - val_loss: 44.8548\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5099 - val_loss: 38.3294\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4993 - val_loss: 42.0123\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4848 - val_loss: 37.8306\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5174 - val_loss: 40.3423\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4829 - val_loss: 40.5488\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4693 - val_loss: 48.1525\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4992 - val_loss: 38.5514\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4622 - val_loss: 38.5543\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4762 - val_loss: 39.6967\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5397 - val_loss: 46.4336\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5289 - val_loss: 41.8633\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4797 - val_loss: 42.7940\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5072 - val_loss: 39.9238\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4860 - val_loss: 42.4730\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5029 - val_loss: 42.5292\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4211 - val_loss: 40.5027\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4834 - val_loss: 39.5363\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5025 - val_loss: 39.1777\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4683 - val_loss: 46.0729\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5066 - val_loss: 39.8960\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4939 - val_loss: 39.5757\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4428 - val_loss: 42.0382\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4848 - val_loss: 48.3091\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4656 - val_loss: 39.1075\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4734 - val_loss: 40.8276\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4809 - val_loss: 41.0686\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4908 - val_loss: 38.4079\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4327 - val_loss: 41.2974\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4255 - val_loss: 43.4126\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4693 - val_loss: 40.8439\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4743 - val_loss: 40.9753\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4590 - val_loss: 42.0247\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5027 - val_loss: 41.0799\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4610 - val_loss: 39.1539\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4746 - val_loss: 38.7282\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4776 - val_loss: 37.8281\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4220 - val_loss: 39.4144\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 35.4893 - val_loss: 41.0304\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.4933 - val_loss: 38.8355\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.4813 - val_loss: 43.0842\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.4199 - val_loss: 45.7451\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.4454 - val_loss: 38.8628\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.4683 - val_loss: 39.7637\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.4522 - val_loss: 41.9321\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.4364 - val_loss: 39.2113\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.4230 - val_loss: 39.8091\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.4831 - val_loss: 39.5256\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.4788 - val_loss: 37.9806\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.4429 - val_loss: 40.1206\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.3879 - val_loss: 38.5496\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.5032 - val_loss: 39.1180\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.4152 - val_loss: 39.8802\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 35.4589 - val_loss: 41.3302\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.4747 - val_loss: 39.6752\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.4350 - val_loss: 42.2865\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76a228f-8b2a-4494-98de-2efd4b156972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  1.876919839818609 \n",
            "MAE:  4.755846610926268 \n",
            "SD:  6.2260457788159265\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cip38xZOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "f8b0775e-d5ce-4636-b3ed-ffc4d49291df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAFBCAYAAAAsfIegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwU9f0/8Nc7yZIEwg0iAgoqFY9g0EBBFCl4QuutoMhXEaW2th71pwJqPWptFatWpYoHFRUqYKXyLVhBvhRKbTkNlyABBCUcIdxIAjnevz8+M9nZK2yS2Z1k8no+HvvYuXbmM7uzr/nsZz87K6oKIiJyT4rXBSAi8hsGKxGRyxisREQuY7ASEbmMwUpE5DIGKxGRyxIWrCKSISJLRGSliKwVkaes6V1EZLGIbBSRqSLSyJqebo1vtOZ3TlTZiIgSKZE11qMABqjquQByAFwhIr0BPAfgJVU9HcA+ACOt5UcC2GdNf8lajoio3klYsKpx2BoNWDcFMADAR9b0SQCusYavtsZhzR8oIpKo8hERJUpC21hFJFVE8gAUApgLYBOA/apaZi2yDUAHa7gDgO8AwJp/AEDrRJaPiCgR0hK5clUtB5AjIi0AzADQrbbrFJFRAEYBQJMmTc7v1i3KKvfuxfZvSrADJ+H882u7RSJqaJYvX16kqm1r+viEBqtNVfeLyHwAfQC0EJE0q1baEUCBtVgBgE4AtolIGoDmAPZEWdebAN4EgNzcXF22bFnkBidPxm9uXY9f4zdYvBhITU3EXhGRX4nI1to8PpG9AtpaNVWISCaASwGsAzAfwA3WYrcB+MQanmmNw5r/f1qLK8QIzEMrKmq6BiKimklkjbU9gEkikgoT4NNU9e8i8hWAD0XkGQBfAnjHWv4dAO+LyEYAewEMrc3GU2ASlRfvIqJkS1iwquoqAD2iTN8MoFeU6SUAbnRr+6yxEpFXktLGmnQirLFSnVRaWopt27ahpKTE66IQgIyMDHTs2BGBQMDV9fozWMEaK9VN27ZtQ9OmTdG5c2ewm7a3VBV79uzBtm3b0KVLF1fX7dtrBbDGSnVRSUkJWrduzVCtA0QErVu3TsinB98GK2usVFcxVOuORL0W/gxWtrESkYf8GaxgjZWovsnKyoo5b8uWLTjnnHOSWJra8W2wssZKRF7xfbCyxkoUasuWLejWrRtuv/12/OAHP8CwYcPw+eefo2/fvujatSuWLFmCBQsWICcnBzk5OejRowcOHToEABg3bhx69uyJ7t2744knnoi5jdGjR2P8+PGV408++SReeOEFHD58GAMHDsR5552H7OxsfPLJJzHXEUtJSQlGjBiB7Oxs9OjRA/PnzwcArF27Fr169UJOTg66d++O/Px8fP/99xg8eDDOPfdcnHPOOZg6dWq1t1cT/uxuJcKmAKr77r8fyMtzd505OcDLLx93sY0bN2L69OmYOHEievbsiSlTpmDRokWYOXMmnn32WZSXl2P8+PHo27cvDh8+jIyMDMyZMwf5+flYsmQJVBVXXXUVFi5ciH79+kWsf8iQIbj//vtxzz33AACmTZuGzz77DBkZGZgxYwaaNWuGoqIi9O7dG1dddVW1vkQaP348RASrV6/G+vXrcdlll2HDhg144403cN9992HYsGE4duwYysvLMXv2bJx00kmYNWsWAODAgQNxb6c2fF9jZVMAUaQuXbogOzsbKSkpOPvsszFw4ECICLKzs7Flyxb07dsXv/rVr/DKK69g//79SEtLw5w5czBnzhz06NED5513HtavX4/8/Pyo6+/RowcKCwuxfft2rFy5Ei1btkSnTp2gqhg7diy6d++OSy65BAUFBdi1a1e1yr5o0SLceuutAIBu3brhlFNOwYYNG9CnTx88++yzeO6557B161ZkZmYiOzsbc+fOxSOPPIJ//etfaN68ea2fu3j4s8YKfnlF9UAcNctESU9PrxxOSUmpHE9JSUFZWRlGjx6NwYMHY/bs2ejbty8+++wzqCrGjBmDn/70p3Ft48Ybb8RHH32EnTt3YsiQIQCAyZMnY/fu3Vi+fDkCgQA6d+7sWj/SW265BT/84Q8xa9YsDBo0CBMmTMCAAQOwYsUKzJ49G4899hgGDhyIX//6165sryq+DVbWWIlqbtOmTcjOzkZ2djaWLl2K9evX4/LLL8fjjz+OYcOGISsrCwUFBQgEAjjhhBOirmPIkCG46667UFRUhAULFgAwH8VPOOEEBAIBzJ8/H1u3Vv/qfBdddBEmT56MAQMGYMOGDfj2229xxhlnYPPmzTj11FNx77334ttvv8WqVavQrVs3tGrVCrfeeitatGiBt99+u1bPS7z8GaxsYyWqlZdffhnz58+vbCq48sorkZ6ejnXr1qFPnz4ATPeoDz74IGawnn322Th06BA6dOiA9u3bAwCGDRuGn/zkJ8jOzkZubi6iXqj+OH7+85/jZz/7GbKzs5GWloZ3330X6enpmDZtGt5//30EAgGceOKJGDt2LJYuXYqHHnoIKSkpCAQCeP3112v+pFSD1OKSp56LeaHrqVPxztA5uBPv4NtvgU6dkl82omjWrVuHM8880+tikEO010RElqtqbk3X6dsvr1hjJSKv+LMpAGxjJUqGPXv2YODAgRHT582bh9atq/9foKtXr8bw4cNDpqWnp2Px4sU1LqMX/BmsbGMlSorWrVsjz8W+uNnZ2a6uzyu+bQpgjZWIvOL7YGWNlYiSzbfByqYAIvKKP4OV12MlIg/5M1hVWWMl8lhV11f1O98GK2usROQVf3a3qqhgjZXqPK+uGrhlyxZcccUV6N27N7744gv07NkTI0aMwBNPPIHCwkJMnjwZxcXFuO+++wCY/4VauHAhmjZtinHjxmHatGk4evQorr32Wjz11FPHLZOq4uGHH8ann34KEcFjjz2GIUOGYMeOHRgyZAgOHjyIsrIyvP7667jgggswcuRILFu2DCKCO+64Aw888IAbT01S+TZYWWMlii3R12N1+vjjj5GXl4eVK1eiqKgIPXv2RL9+/TBlyhRcfvnlePTRR1FeXo4jR44gLy8PBQUFWLNmDQBg//79yXg6XOfbYGWNleo6D68aWHk9VgBRr8c6dOhQ/OpXv8KwYcNw3XXXoWPHjiHXYwWAw4cPIz8//7jBumjRItx8881ITU1Fu3btcPHFF2Pp0qXo2bMn7rjjDpSWluKaa65BTk4OTj31VGzevBm//OUvMXjwYFx22WUJfy4SwZ9trKyxElUpnuuxvv322yguLkbfvn2xfv36yuux5uXlIS8vDxs3bsTIkSNrXIZ+/fph4cKF6NChA26//Xa89957aNmyJVauXIn+/fvjjTfewJ133lnrffWCb4OVNVaimrOvx/rII4+gZ8+elddjnThxIg4fPgwAKCgoQGFh4XHXddFFF2Hq1KkoLy/H7t27sXDhQvTq1Qtbt25Fu3btcNddd+HOO+/EihUrUFRUhIqKClx//fV45plnsGLFikTvakL4timANVaimnPjeqy2a6+9Fv/5z39w7rnnQkTw/PPP48QTT8SkSZMwbtw4BAIBZGVl4b333kNBQQFGjBiBCqtG9Lvf/S7h+5oI/rwe69tvY9ZdM/BjzMLixUCvXskvG1E0vB5r3cPrscbLUWNlUwARJRubAoioxty+Hqtf+DZY+eUVUeK5fT1Wv/B9UwBrrFTX1OfvNfwmUa+Fb4OVNVaqizIyMrBnzx6Gax2gqtizZw8yMjJcX7dvmwJYY6W6qGPHjti2bRt2797tdVEI5kTXsWNH19ebsGAVkU4A3gPQDoACeFNV/ygiTwK4C4B9ZI1V1dnWY8YAGAmgHMC9qvpZjTbOGivVUYFAAF26dPG6GJRgiayxlgF4UFVXiEhTAMtFZK417yVVfcG5sIicBWAogLMBnATgcxH5gaqWV3vLvGwgEXkoYW2sqrpDVVdYw4cArAPQoYqHXA3gQ1U9qqrfANgIoGZd+1ljJSIPJeXLKxHpDKAHAPvPwX8hIqtEZKKItLSmdQDwneNh21B1EMfmbGOtYJWViJIr4cEqIlkA/grgflU9COB1AKcByAGwA8Afqrm+USKyTESWxfwCwFljLWOVlYiSK6HBKiIBmFCdrKofA4Cq7lLVclWtAPAWgh/3CwB0cjy8ozUthKq+qaq5qprbtm3b6BtmrwAi8lDCglVEBMA7ANap6ouO6e0di10LYI01PBPAUBFJF5EuALoCWFKjjTuvFVDOZCWi5Epkr4C+AIYDWC0i9m/exgK4WURyYLpgbQHwUwBQ1bUiMg3AVzA9Cu6pUY8AILQpgMFKREmWsGBV1UUAJMqs2VU85rcAflvrjfPLKyLykD9/0qrKGisRecafwcovr4jIQ74NVtZYicgrvg3WVJjvvRisRJRsvg1WdrciIq/4P1j5wysiSjLfB2t5GWusRJRcvg1WtrESkVd8G6xsCiAir/gzWB0XumaNlYiSzZ/ByjZWIvKQP4O1vDzYxsqmACJKMn8Ga79+bAogIs/48++vb7kFKV8WAC+wxkpEyefPGiuAlPYnAGAbKxEln2+DNdXaM9ZYiSjZfBusKZXByhorESWXf4M11fx5QUXN/tyFiKjGfB+sbGMlomTzbbCmppp7trESUbL5NlgrmwIYrESUZP4NVmvP2BRARMnm+2BljZWIks2/wcqmACLyiG+DFSkpSEE5+7ESUdL5N1hFkIIKlJd5XRAiamh8HaypKGdTABElna+DNQUVDFYiSjr/BmtKCoOViDzh32C121h5oWsiSjJfB2sqynkRFiJKOl8HK5sCiMgL/g1WtrESkUf8G6yVbaxeF4SIGhpfByv7sRKRF/wbrGwKICKP+DdYK7+8YncrIkquhAWriHQSkfki8pWIrBWR+6zprURkrojkW/ctrekiIq+IyEYRWSUi59WyAFYbq7iwN0RE8UtkjbUMwIOqehaA3gDuEZGzAIwGME9VuwKYZ40DwJUAulq3UQBer9XW2cZKRB5JWLCq6g5VXWENHwKwDkAHAFcDmGQtNgnANdbw1QDeU+O/AFqISPsaF4BtrETkkaS0sYpIZwA9ACwG0E5Vd1izdgJoZw13APCd42HbrGk13SiDlYg8kfBgFZEsAH8FcL+qHnTOU1UFUK1vl0RklIgsE5Flu3fvrmpB08bKYCWiJEtosIpIACZUJ6vqx9bkXfZHfOu+0JpeAKCT4+EdrWkhVPVNVc1V1dy2bdtWtXG2sRKRJxLZK0AAvANgnaq+6Jg1E8Bt1vBtAD5xTP8fq3dAbwAHHE0G1cc2ViLySFoC190XwHAAq0Ukz5o2FsDvAUwTkZEAtgK4yZo3G8AgABsBHAEwolZbZxsrEXkkYcGqqosAxOpEOjDK8grgHtcKwH6sROQRX//yim2sROQF/wYr21iJyCP+DVa7jZWXCiCiJPN9sLKNlYiSzb/BmpJi2lhZYyWiJPNvsLK7FRF5xPfByqYAIko23wcrmwKIKNn8G6x2GyubAogoyfwbrJVtrGwKIKLk8n2w8rKBRJRsvg5W0xTAGisRJZd/gzUtjd2tiMgT/g3WRo3499dE5An/BmsgYPVj9bogRNTQ+DdYGzVidysi8oSvg5VtrETkBQYrEZHLfB+s5UX7gD/+0evSEFED4t9gDQSQinKUIxV49VWvS0NEDYh/gzU11TQFIAUIBLwuDRE1IP4NVuuXV+VIZbASUVL5N1iBYLCmJexfvomIIvg6WNNQhjKkscZKREnl62BlUwAReYHBSkTkMgYrEZHLGKxERC7zdbCmoYzBSkRJ5+tgNTXWNGgqu1sRUfL4PlgBoCKVNVYiSp64glVEmohIijX8AxG5SkTqfFrZwVqelu5xSYioIYm3xroQQIaIdAAwB8BwAO8mqlBuYbASkRfiDVZR1SMArgPwJ1W9EcDZiSuWOyqDVX3d4kFEdUzcwSoifQAMAzDLmpaamCK5Jw1lAICyMo8LQkQNSrzBej+AMQBmqOpaETkVwPzEFcsdlTXWMv5TKxElT1zBqqoLVPUqVX3O+hKrSFXvreoxIjJRRApFZI1j2pMiUiAiedZtkGPeGBHZKCJfi8jlNd4jBwYrEXkh3l4BU0SkmYg0AbAGwFci8tBxHvYugCuiTH9JVXOs22xr/WcBGArTbnsFgD+JSK2bGlLHjgbAYCWi5Iq3KeAsVT0I4BoAnwLoAtMzICZVXQhgb5zrvxrAh6p6VFW/AbARQK84HxtTaudOABisRJRc8QZrwOq3eg2AmapaCqCmafULEVllNRW0tKZ1APCdY5lt1rRasa9vXV5e2zUREcUv3mCdAGALgCYAForIKQAO1mB7rwM4DUAOgB0A/lDdFYjIKBFZJiLLdu/eXeWyqVZjQlkpa6xElDzxfnn1iqp2UNVBamwF8KPqbkxVd6lquapWAHgLwY/7BQA6ORbtaE2Lto43VTVXVXPbtm1b5fbsYGWNlYiSKd4vr5qLyIt2TVFE/gBTe60WEWnvGL0W5oswAJgJYKiIpItIFwBdASyp7vrDVQYr21iJKInivezTRJgQvMkaHw7gzzC/xIpKRP4CoD+ANiKyDcATAPqLSA5M++wWAD8FAKtv7DQAXwEoA3CPqta6nskaKxF5Id5gPU1Vr3eMPyUieVU9QFVvjjL5nSqW/y2A38ZZnrgwWInIC/F+eVUsIhfaIyLSF0BxYorkHvYKICIvxFtjvRvAeyLS3BrfB+C2xBTJPZW9AnitACJKoriCVVVXAjhXRJpZ4wdF5H4AqxJZuNpiUwAReaFa19NT1YPWL7AA4FcJKI+rKoO1wttyEFHDUpsLlYprpUiQYI21zheViHykNsFa5zuHsh8rEXmhyjZWETmE6AEqADITUiIX2b0CylhjJaIkqjJYVbVpsgqSCME2VgYrESWPr/8Mir0CiMgLDSNYWWMloiRqGMHKGisRJZGvg7XyJ62ssRJREvk6WCt/0speAUSURA0iWFljJaJkanjBqgo88giwdq03hSIi32t4wbp7N/D888All3hTKCLyvQYSrFFm8lqCRJQgvg7WqL0CxBpWXj+AiBLD18Fa2SugIjU40Q5UBisRJUiDCNaQGmtFReg9EZHLfB2slVe3Qmqwhmr/DIs1ViJKEF8HayBg7ksRCAYqg5WIEozBSkTkMl8Hq93GGhKsdtsqg5WIEsTXwSoCBFLLWWMloqTydbACQCC1gsFKREnVcILV/qUVg5WIEqzhBCvbWIkoSRpesIYHLBGRyxpusLLGSkQJ4v9gTVMGKxEllf+DlTVWIkoy/wdreI2VX14RUYI1vGBljZWIEozBSkTkMt8Ha6NYwUpElCAJC1YRmSgihSKyxjGtlYjMFZF8676lNV1E5BUR2Sgiq0TkPLfKEbONlYgoQRJZY30XwBVh00YDmKeqXQHMs8YB4EoAXa3bKACvu1WIQECj/6SViChBEhasqroQwN6wyVcDmGQNTwJwjWP6e2r8F0ALEWnvRjkCaTGux0pElCDJbmNtp6o7rOGdANpZwx0AfOdYbps1rdYqa6zFxcCf/gQcO+bGaomIYkrzasOqqiJS7a/mRWQUTHMBTj755OMuHwhYNdbnnwc++wy4Irx1gojIXcmuse6yP+Jb94XW9AIAnRzLdbSmRVDVN1U1V1Vz27Zte9wNVjYFFBWZCfY9EVGCJDtYZwK4zRq+DcAnjun/Y/UO6A3ggKPJoFYqa6x2v1U2BRBRgiWsKUBE/gKgP4A2IrINwBMAfg9gmoiMBLAVwE3W4rMBDAKwEcARACPcKkdlsNoYrESUYAkLVlW9OcasgVGWVQD3JKIcETXW0tJEbIaIqJLvf3kVaBRWY2WwElGC+T9YA2KC1Q7Uo0e9LRAR+V4DCFarxnrokJlw8KC3BSIi3/N9sDZKF5SiEfTQYTOhuNjbAhGR7/k+WDMyzf3Rw2xbJaLk8H+wZggAoOSYeFwSImoofB+smY3NfTEyI2fyEoJElAC+D9bKGisyImfalxIkInKR74M1s7EJ1qg1VgYrESWA74M1o7HZRdZYiShZ/B+smayxElFy+T5YM5uwxkpEyeX7YGWNlYiSzf/B2iQVAGusRJQ8vg/WzCw2BRBRcvk+WDMamxpr1KYA/mMrESWA74M1MysBTQFHjgAvvshgJqKofB+sIV9eZWWFzpwypWYrffJJ4MEHgQ8/rF3hiMiX/B+sVkW1BBlAs2ahM595BliyJHTak08C48ZFX9mWLaaWum+fGf/+ezeLSkQ+4ftgTU0FAjhmaqzhwQpEXvj6qaeAhx+OXK6gAOjSBRg7NjEFJSLf8H2wAkAminEEjYGmTSNn2n8yeDy7dpn7OXPcKxgR+VKDCNamOIRDaBq9xup05pnJKRAR+VqDCdbDyIoerM4a6/r1sVdiLydibkREMTSMYJXv46uxOsXqivXll8Evr4iIomgYwdqoxARrtDbWWH1R9+8PHXf+28BHH7lXOCLynQYRrFnNUk2wpqVFzjx2zNyH11DDg9Vezol/7UJEUTSIYG168Xk41LoL0L175Ew7MEtKQqeHf9yPFqyl/OdXIooUpQrnP03bpONwSjoQCETOtAPz6NHQ6fHUWKNNI6IGr2HUWJsChw4h2BTg/Fb/8GETquE11uLi0PFotdNkB+ucOcD//m9yt0lE1dZggrWkBChLaWQmZDguyHL33WY8vMZqB+3f/w6sXh09RMMfk2iXXw5cdVVyt0lE1dZgghUADpVagZqeHrnQaaeFjts11p/8xLTNetEUMGGCCX4iqlcaRLA2b27u9x+1rsmaEeUSguHqQlPA3XebcA0X789wiZLlyBF+mevQIIL1xBPN/c6Djc1APMEa3uZal7684lW1glRNm/mYMV6XpGFr0gQYMMDrUtQZDSJY27c39zsOWMEarSkgXHFxaM3QrTbWtm2BXr2q95jwGuru3dXfrl/ZvTeee87bchCwaJHXJagzGlSwbt9jBWq8TQHOHw24VWMtKgKWLq3eY8IDvLrBeuutwM9/Xr3H1DXr1kWfvmOHuW/ZMnllITqOBhGsbdua67LuKLL6scbbFOBsZ/Wyu9Xhw6Hj1Q3WyZOB1193rzzJNmsWcNZZwNSpkfO2bzf3DFbvsM0/QoMI1pQU08663Q7WeJsCnO2sbtRY4/0J7B/+EPq3MeHBGn5xbr9budLc5+VFzmON1XvJ7nZYD3gSrCKyRURWi0ieiCyzprUSkbkikm/du/pOOe004OvvqvnllTNYw8MNiH1ALV1q/hMr/Ex+6FBwWAT4xz+iP/7//T9g2LDg+KBBwF//Ghz/7jvgX/+KfJxqsAbnJ85LNoZjjdV7R454XYI6x8sa649UNUdVc63x0QDmqWpXAPOscdfk5AArv2mGcqREv8pVuPAa6549kcuE11gnTABeeAG46CLzL67241evNlfR2rs3dPnnn49cZ7SPVevWATfcEBx/5BGgX7/I7b/6KtChQ+z2yPrKrumnRDlcCwvNfWrq8dezeDFw/vnRT5JUc+FdE6lONQVcDWCSNTwJwDVurrxHD+DI0TRsvO0Z4E9/Ov4D4gnW8Brr3XcDDz0U/NLr0CFg1SrzA4Nnn428sEtmZvTtxiv8egbz55v7r76Kfx31QVXBan8KiOd5mz0bWLHC/CkkucfNYJ0+Hdi50731ecSrYFUAc0RkuYiMsqa1U1WrwQw7AbRzc4M9epj7L68YA5xwwvEfEN4UYP/nlZNd89m5M/SfXe1a58GDwLffmuH//jeyxhrtojDVuYh2+LKNraYOZz9XL79YKCtz5ws++wQW7dq5drCG9zuOxj7hhJ+QqHbCg/Xdd4HBg6u/nkOHgJtuMk1f9ZxXwXqhqp4H4EoA94hIP+dMVVWY8I0gIqNEZJmILNtdjW/HzzzT5NiXX8ZYoHdvoJ0jy8NrrAUFkY+xg3LkyNB/drVrWAcOAH/5i13wyCB0hvWuXSagP/kkrv0BEDtYnV9ueflrmP794/ui8Hjs/XG2UduqU2O1g7U2/wChCjz9NLB1a83XEY9PPwW2bUvsNtwS3sY6YoT5dBDrIvLhvvzSvD/sv6LftCl0fkEB8NZbx1/P00+bCkwd4EmwqmqBdV8IYAaAXgB2iUh7ALDuC2M89k1VzVXV3LZt28a9zUaNgHPOAZYti7FAVpZpf7OF11idB3n79qZv6ObN5lYYtajA0KHBb/dFgrVXW0EB8J//ADNmmG4L7dsD99wT9z5FBEQj6yIzzsB2Bk5VtddNmyL3Y/fuqoN5zhyzrVgdw//979iPrQ47WKdPBx54IHReeI21vBz4xS8im0NKS4ENG8xwbYJ1/XrgiSdMm3fMg+k4/vY30+4ebsMGID/fBNWgQcCPflTzctbG4sXAwoXR533wQWQlI9ZJLdqJMJoPPzT3770Xff7gwcCoUVV3Mzx2zLwuffrEt80ES3qwikgTEWlqDwO4DMAaADMB3GYtdhuAalTd4nPZZcCCBcEeOlX6z3+AiROD486A2bwZOP10M3zaabH/XHDjxuCwSOg4YGqoF1wAXHddXOWP4GzPBYI1B2cblbM2UdXH8tNPD70Q+PTppsnkjjuiL793r7na1oknmi/r5s0Lne+sNVfnnxaWLzfP1cMPAz/7mZl24IC537kTePnl0P2wm2PsN/c33wDjxwNXXx1c5vvvTcjbz1WsYC0vN9t+6qnY5bObEZYtA3r2jDxZxuPaa6NfdP2MM4Af/CB4Ugg/XpKld2/g4osjpx88CAwfDlx5Zeh0Z7A6h+NtcrHfP7G6EdqfDuxjY/Hi4LySEuDee4G1a+Pblr2eaE17LvKixtoOwCIRWQlgCYBZqvoPAL8HcKmI5AO4xBp31YgR5r3z/vswb0BncN55Z/AFvvZacx+tQzpgaoatWgXH4/klVUpK5Eec2n5MX7vWfNS2v1izD8yvvzZn+dWrYx/0APD448AllwRrqrt2mbN+WZn1JAGYOdOsx74OrF3rtfuW2mbNCh23L9DgLFd42W+5JRiSCxaYbduhNm4c8MYbZnvhj//gg+BweI3Vfi6cte9LLgmt/cUKVvuM+/TTZplrrjHB+fnnJgj37o08K2/eHNl2DsQ+mcRzklm1KjhcVzrfFxUFn9NvviruU2YAABM5SURBVAmd5zx5O78YPF6wHj1qug7a7zv7BAqYphC7S6F9Qpw+3dy/805wualTTW+YUaMQl9JSIDcXGDgwvuVrSlXr7e3888/X6rrwQtUuXVQPH7YmbNkSnPnrX6sCqsuWqX7xhRmOdlNV/eCD2POj3QYPVj311Oo9Jt7b3/5mynTJJaHTf/971TVrguOjR6sWFAT3157+l7+EPu7DD1UbN1Zt3z50ekWFarduqrffrvrkk6HzundX3bnTLONcN6D67LOq5eXB57hvX9XOnc28lStV8/Nj71tRkeo550RO37XLrO/EE814VpYZ/+ST0NepvDz0cZmZqvfeG/3gsF/ztDTV6dPNcNeuqtdfb4afekr1tddiHxO2zZtDXxdV1TlzVKdOVd27N/pjnM/ZffcFhwsLYx7LNbJ9u+q6dbHnV1QEt33smJn22WfB/Xc+1yUl5o30/vvBx8yaFRyePz/2dvbvV+3TJ3R/zzrL3DdtGvocZWSY4ZtuMve33BJcz9tvm2nNm5v7jIzg+rdujdzukiWxn38HAMu0FtlU4wfWhVtNgnXePFUR1WHDzHMfoqzMPPE255sRUD3zTNWzzzbzZsyIHQbRbj/8oWpqqmqrVmb85JOr93j7lpISOe2mm0zYRVv+jTdCxxs3DoZcrG08+KC5f/TR0Ol22DhvHTuafbPHH31U9eOPI5d74AHVSy9VPf/80Onz56v+7nfV3+fbbzchFwhoZRiqqr75ZnCZigrVVauC46ecYm7Dh5vnoKws9PX/8EOzXKNGqhMnRm6zXTvVn/wkehmLioInFTt8L73UvPGXLg0ud8MNweHyctUpU0x4jxoVfb3Llpl15uervvKK6tdfh5a5sFD1iSdMYNoH+CWXqL78suqkSZFvgMGDzfaOHQuW16moKLjtm28208aMMeP2Ca5pUzM9N9eMO5/zV18NDs+YoTp3ruqnnwbX//XXqjt2RB4HzvdZeLCmpZnh884LTh83TvXddyNPdC1amMfYIR2+j3/8Y/D9F01FherMmQzWmrArWxkZ5liNdmJTVfNGAlQ/+ki1f3/Vb74JzrPDY+RI1X37VAcNqjoc7Fv//loZhs7pdq3Ivv3736r/+lcwOOxb167mftAg1T17TO0hnu2Gh9KuXaHT7BokoHrCCcE3hnN6+O2ii8xz8Y9/VL8M9u3DD1Uvv/z4y4XXnp03O3ifey60Jj18uOrpp5vhO+9U/fbbYC0JUG3TRnXECNW33jK1L3t6enpkjTye2+OPmzem/cnhpJOqXn779tjz7BPwjBkmQETM+DXXhB6jzz8fPK727jUnOud6KipMZWHYMNUDB4K1P8B8grHNmGGO45UrQx9fWqr68MNmuFkzc28Hq72M82ThvP35z8Fhu+Ydz/No7yugesEFweHGjSOXvfba0PE2bUKP7fR08ylJ1ZxInRWQbt1UN24MPgfl5SYQAGWw1tCcOebTq/0cZ2SoXnWV6osvmgrG0aOqumGD6mOPRT+zl5aaN59d7d2507xJnSEhEhlM//yn6jvvmBfUnvbaayZI7fFhw4LbWbtWdfx4U4OaOTP4pv3tb8387OzIgy1aDc++jRplagDOZok2bVTz8kLfdIDq8uWmLM5pzo/lzz1nyvD999G3Zdc0qrq1bWsO/i5dYtcGAfM6AKoDBhx/ndFu+/aZst57b/T5zo+zzpvzILFvdsBEu9knmYsvPn6Z7NpTtNtDD0VOO/NMc5+ba47NgwdNDdy5TPgnoQ8+CJ6MH3ggcp3btqkuWBAc79UrdH5enurQoZGPi/U8xrrl5kaGYDJvmzebGnj49LFjVYuLTZPZW29VTmew1kJ5ucmt5583JzK7mQYwJ8ezzgpWaKZMMZ+yNm60QjeWBQtUb7zRBF9+vqkJvfxycMX2RzZnGO3YYZa77jrz+NLS2Ou3Dw67/eq660IPFLsd76uvVH/zm8gDSdW0JTo/br3xRnD9zmV371Z94YXg+MknB2tIw4YFmxRUowdJddqhJ0yI3L7ztmmT6sKF5k0wfLg5A3bqZOZdemnoshddFH2/VUPePFFvn3wSGpyDB5v7SZNM7WnnTrOMc5vPPGNODM71HDhQ9XbS02PP69vXHAPh0+fNCw6fcUbwo3gibldeWfPH2p8SEnG78MLgfl94YdXLPvpo8OQ1duzx1203RYDBGjuAauDYMfPp+9VXTaAOHqzaunXk85+SYioKAwaYT5i/+53qtGmqq1ebTyHRKri6cqVpq3LOfO45054Z9QExbN8ebHdTDbbfNW5s3njhnG13F14YnL5okal9rl0buvzHH5sA7d/flGvvXhMiY8aYL74OHjS1lQMHQh9XWmqW/ec/gzWkvDzVQ4di1wbt2113mRONvX8TJgTnbd9uahvRfP656h/+YD7i7d9vwt0+JlatMiEImE8OtrIy82KVlQVrv717m3u7TVHVtNVdcIGp0f3jH9G3b5dRNTJIVYMfoQHTPj1wYHB81Spz4nvtNbMN52OXLw9dv32rqDCvz29+E6wF/OhHkR/FW7c2Z/9oz7WznTL8E4ozGO39Of101Z/9zHwR9/bboV9OAeY1aNHCfIRessTU/DZvDrbTA9Hbq4FgmzZg3nyxjo9OnYInot//3rQDL1tmKi7t25tPcXffHfxU43wNVM2JKtp6mzSJnDZmjOrUqbUOVlHVxHY7SKDc3FxdVtNO2nFSNf22jx413QuPHDE9SjZvNr2nNm2K7Lfcrp35n63MTHN/6qmml0fbtuZ3CG3amOvAlJaaf+Ru1sx0tQwETLdY+yfx6elmubQ007urUSPTY6pZs7Cus8eOBX8cEE1pqbke64gR8V2ApraOHAHmzjX/KCtinsSCAtMFa9Ei03Xpu++AP//ZTBsddr2dwkJzMZm5c82vt+Jl/9LHeUGWvXtNF6c2bSKXLyszP/xo0wb4+GPT6d/+9drRo2Y99l+mR7NqlbmqVqdOZnz6dHPJx+uvN32MVc0LVlFhXnjA/MBgyhTTrcz5In75pemvefXVwek7d5ryFBaaq3j1c/xAcetW0/Wrd2/giy+Avn2D8/72N7Oel14CXnvN9DcePdpcGOixx0w3w6FDzVXe/vtf073s1luBH//YdNPr08f8LHXLFrNv4Re42bHDdHF68smqjzsRc/Bv2mR+TJKSYg7kb781b6oHHwTuugu44grzU9YvvgD++U/g5JPNm+iXvzRdBx991EzbutWUP9o1NmxHj5rueMeOBftB79tn3nzNmpkryp1+ullv796mS1irVsApp5hfd116qVV0Wa7BC0RVG4PVBYcOmWNnzRqTF/n55v105Ih5T2zbZkJz5053rleRmWmO9ZQU874PvwHmPdOkiRmuqDD96Bs1Alq0MOVKTzfLZGSY6YGAuW/UyHQJTUkxwyLBoC8rM8dmkybBU3xFRXDYPt6Li81wRYW5lZebcmVmmuHycjO9ZUuzfns5VbOt778PZlFpqVnGzueUlMj3uV1Oex2pqWa4rMzc2/lo31JTQ8vu3Ad7uGnTYFnt59p5X1JitpmWZrZjZ2FGRujvF+yyO4ftLrfhmZSVZXKhrMw8V2Vl5nULBMw27f0oLzfbyMw09ykpgJQeQ0pGI6SmmsepBl87+7kTMeuyX89oZcPhw5BAGpCREX0+Yk/LzAz7BfPevWjULAPSpDFKSsxzWlZmbvYxkJkZepzZz2/luvPzIaefBpUUlJYGu37b++GML+exaj++8ranCFJWCjmpfUi5U1OBtJLDSG3aGPsPpqBJE1O25s1rF6xVnI4pXk2bmssS5uRUvZx9EBQUmDDOyjJvjP37TeAVF4f+kKekxLyxyspM/2y7hrt7dzCw7ABx3ioqTDjZb3ARUxMuKTF9sNu1M/NKSkzf+6NHUXnQHjtmDlrV4Lrs0ElLM2UtLg5909oHc0lJMGCLi4NvlJQUU1a7n7f9mFg/JbcP/Or8YIuqqDlWS5ZL6wGAVsdf5Li6urAOAIjyiaWSm/tsMFiTyD5LduwYe5lzzklOWRLBDuBoV/cDQmt2Iuak4QxoO0wzMsxwcbEJeWdt1q5V2ioqgrUYex12LdOuYdo1T+fJJ3y7zmERc+Kza7fOk5h9b9c27W3ZJ83i4mANzJ4Wfm/Xtpw/vFM127Q/jdgnr+bNgzU8u+x2zbW42DxX9nPi/HSQkhJaI7fLU1oa2qjofN3Cy1rd+cXFobV11eB4o0bmJG5/OrJPuPa/Zjv3wb45t2c/b/YF4ezH2K+7fSzEbqiNvt/2sVFaGvyElJpqWilqg8FKrnF+xIomvLmyWbOq12c3ZXihlRuVLaq3ahusdelC10REvsBgJSJyGYOViMhlDFYiIpcxWImIXMZgJSJyGYOViMhlDFYiIpcxWImIXMZgJSJyGYOViMhlDFYiIpcxWImIXMZgJSJyGYOViMhlDFYiIpcxWImIXMZgJSJyGYOViMhlDFYiIpcxWImIXMZgJSJyGYOViMhlDFYiIpcxWImIXMZgJSJyWZ0LVhG5QkS+FpGNIjLa6/IQEVVXnQpWEUkFMB7AlQDOAnCziJzlbamIiKqnTgUrgF4ANqrqZlU9BuBDAFd7XCYiomqpa8HaAcB3jvFt1jQionojzesCVJeIjAIwyho9KiJrvCxPgrUBUOR1IRKI+1d/+XnfAOCM2jy4rgVrAYBOjvGO1rRKqvomgDcBQESWqWpu8oqXXNy/+s3P++fnfQPM/tXm8XWtKWApgK4i0kVEGgEYCmCmx2UiIqqWOlVjVdUyEfkFgM8ApAKYqKprPS4WEVG11KlgBQBVnQ1gdpyLv5nIstQB3L/6zc/75+d9A2q5f6KqbhWEiIhQ99pYiYjqvXobrH746auITBSRQmeXMRFpJSJzRSTfum9pTRcRecXa31Uicp53JT8+EekkIvNF5CsRWSsi91nT/bJ/GSKyRERWWvv3lDW9i4gstvZjqvUlLEQk3RrfaM3v7GX54yEiqSLypYj83Rr3zb4BgIhsEZHVIpJn9wJw6/isl8Hqo5++vgvgirBpowHMU9WuAOZZ44DZ167WbRSA15NUxpoqA/Cgqp4FoDeAe6zXyC/7dxTAAFU9F0AOgCtEpDeA5wC8pKqnA9gHYKS1/EgA+6zpL1nL1XX3AVjnGPfTvtl+pKo5jq5j7hyfqlrvbgD6APjMMT4GwBivy1XDfekMYI1j/GsA7a3h9gC+toYnALg52nL14QbgEwCX+nH/ADQGsALAD2E6zadZ0yuPU5ieLn2s4TRrOfG67FXsU0crWAYA+DsA8cu+OfZxC4A2YdNcOT7rZY0V/v7paztV3WEN7wTQzhqut/tsfTTsAWAxfLR/1kflPACFAOYC2ARgv6qWWYs496Fy/6z5BwC0Tm6Jq+VlAA8DqLDGW8M/+2ZTAHNEZLn1i07ApeOzznW3oiBVVRGp1902RCQLwF8B3K+qB0Wkcl593z9VLQeQIyItAMwA0M3jIrlCRH4MoFBVl4tIf6/Lk0AXqmqBiJwAYK6IrHfOrM3xWV9rrMf96Ws9tktE2gOAdV9oTa93+ywiAZhQnayqH1uTfbN/NlXdD2A+zMfjFiJiV1ic+1C5f9b85gD2JLmo8eoL4CoR2QJzhbkBAP4If+xbJVUtsO4LYU6MveDS8Vlfg9XPP32dCeA2a/g2mLZJe/r/WN9O9gZwwPGRpc4RUzV9B8A6VX3RMcsv+9fWqqlCRDJh2o/XwQTsDdZi4ftn7/cNAP5Prca6ukZVx6hqR1XtDPPe+j9VHQYf7JtNRJqISFN7GMBlANbArePT6wbkWjQ8DwKwAaZd61Gvy1PDffgLgB0ASmHabEbCtE3NA5AP4HMAraxlBaYnxCYAqwHkel3+4+zbhTBtWKsA5Fm3QT7av+4AvrT2bw2AX1vTTwWwBMBGANMBpFvTM6zxjdb8U73ehzj3sz+Av/tt36x9WWnd1toZ4tbxyV9eERG5rL42BRAR1VkMViIilzFYiYhcxmAlInIZg5WIyGUMViKLiPS3r+REVBsMViIilzFYqd4RkVuta6HmicgE62Ioh0XkJevaqPNEpK21bI6I/Ne6huYMx/U1TxeRz63rqa4QkdOs1WeJyEcisl5EJovz4gZEcWKwUr0iImcCGAKgr6rmACgHMAxAEwDLVPVsAAsAPGE95D0Aj6hqd5hfzNjTJwMYr+Z6qhfA/AIOMFfhuh/mOr+nwvxunqhaeHUrqm8GAjgfwFKrMpkJc6GMCgBTrWU+APCxiDQH0EJVF1jTJwGYbv1GvIOqzgAAVS0BAGt9S1R1mzWeB3O93EWJ3y3yEwYr1TcCYJKqjgmZKPJ42HI1/a32UcdwOfgeoRpgUwDVN/MA3GBdQ9P+j6JTYI5l+8pLtwBYpKoHAOwTkYus6cMBLFDVQwC2icg11jrSRaRxUveCfI1nY6pXVPUrEXkM5srvKTBXBrsHwPcAelnzCmHaYQFz6bc3rODcDGCENX04gAki8rS1jhuTuBvkc7y6FfmCiBxW1Syvy0EEsCmAiMh1rLESEbmMNVYiIpcxWImIXMZgJSJyGYOViMhlDFYiIpcxWImIXPb/AWpQKfbAHCBjAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7988be36-3530-48e7-9ae6-072b541b6154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_20 (Dense)            (None, 4)                 512       \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 641\n",
            "Trainable params: 609\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b6bffb-f96f-4886-97bf-6dce82cfa09c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 3s 9ms/step - loss: 3690.1047 - val_loss: 3798.3389\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 3513.1602 - val_loss: 3637.1086\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 3323.2644 - val_loss: 3296.5078\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 3104.3997 - val_loss: 3195.1245\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 2858.8208 - val_loss: 2418.6023\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 2595.0181 - val_loss: 2258.2910\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 2320.5881 - val_loss: 1986.3799\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 2047.4026 - val_loss: 1667.3467\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 1780.0693 - val_loss: 3582.0295\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 1526.2506 - val_loss: 931.9971\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 1289.6040 - val_loss: 2514.5405\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 1079.3922 - val_loss: 2033.1564\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 888.5407 - val_loss: 427.7646\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 731.1600 - val_loss: 380.4256\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 581.1663 - val_loss: 413.3676\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 468.0509 - val_loss: 198.4050\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 376.4252 - val_loss: 1540.1223\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 302.3212 - val_loss: 2985.2664\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 234.3232 - val_loss: 954.0596\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 184.1415 - val_loss: 253.9900\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 147.4466 - val_loss: 363.0591\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 121.3746 - val_loss: 460.3747\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 100.6210 - val_loss: 681.1470\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 90.9480 - val_loss: 98.5624\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 76.9739 - val_loss: 144.4775\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 67.6836 - val_loss: 85.3331\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 62.6538 - val_loss: 42.3205\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 61.3540 - val_loss: 3053.9407\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 58.2979 - val_loss: 2796.4233\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 84.0806 - val_loss: 348.6482\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 59.9589 - val_loss: 69.9428\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 54.0337 - val_loss: 51.6148\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 64.5537 - val_loss: 285.3214\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 57.4984 - val_loss: 60.6317\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 54.2028 - val_loss: 54.9335\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 54.1952 - val_loss: 50.9802\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 49.6209 - val_loss: 721.8701\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 49.9500 - val_loss: 76.3203\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 53.3461 - val_loss: 50.8058\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 52.7679 - val_loss: 71.0787\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 50.5830 - val_loss: 82.5486\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 50.1686 - val_loss: 67.6454\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 49.6630 - val_loss: 54.8786\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 50.0027 - val_loss: 69.6263\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 50.1234 - val_loss: 49.5484\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 47.0272 - val_loss: 55.4649\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 47.9941 - val_loss: 54.4345\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 50.6716 - val_loss: 78.3795\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 46.4250 - val_loss: 43.5390\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 44.6201 - val_loss: 51.3890\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 43.4320 - val_loss: 62.4128\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 42.2942 - val_loss: 47.3444\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 41.6631 - val_loss: 44.8346\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 41.1883 - val_loss: 52.5697\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 40.9672 - val_loss: 47.9639\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 40.8885 - val_loss: 42.5019\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 38.3622 - val_loss: 3120.8818\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 38.0770 - val_loss: 95.3584\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 37.9339 - val_loss: 61.5771\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 37.8139 - val_loss: 47.4445\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 37.7069 - val_loss: 50.4244\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 37.5410 - val_loss: 53.8826\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 37.4848 - val_loss: 1160.2897\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 37.4009 - val_loss: 2763.1067\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 37.3091 - val_loss: 3135.1892\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 37.1957 - val_loss: 3451.5544\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 37.0869 - val_loss: 3450.4661\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 37.1425 - val_loss: 2987.9922\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 37.0697 - val_loss: 69.4999\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 37.0146 - val_loss: 56.6660\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.9321 - val_loss: 54.8793\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.8913 - val_loss: 885.1540\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.8075 - val_loss: 2791.1836\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.8087 - val_loss: 44.2257\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.7831 - val_loss: 1576.1575\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.7833 - val_loss: 53.9880\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.7054 - val_loss: 3287.8450\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.6978 - val_loss: 3339.5667\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.6661 - val_loss: 48.5550\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.5652 - val_loss: 545.9632\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.5898 - val_loss: 45.5330\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.5858 - val_loss: 3468.5776\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.5520 - val_loss: 67.9135\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.4523 - val_loss: 44.5180\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.6854 - val_loss: 77.8700\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.4651 - val_loss: 47.7540\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.4428 - val_loss: 44.8183\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2934 - val_loss: 41.4293\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1816 - val_loss: 50.9797\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9264 - val_loss: 48.4937\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.6252 - val_loss: 41.4866\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.6645 - val_loss: 63.3848\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.5258 - val_loss: 69.1787\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.5201 - val_loss: 42.0566\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.4186 - val_loss: 145.7791\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.4057 - val_loss: 39.3601\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2692 - val_loss: 41.0816\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2624 - val_loss: 40.4112\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3295 - val_loss: 980.5979\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1822 - val_loss: 44.2262\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1419 - val_loss: 47.3180\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1111 - val_loss: 42.0257\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1212 - val_loss: 46.9501\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0915 - val_loss: 38.7696\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0490 - val_loss: 41.3876\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9460 - val_loss: 38.1749\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9510 - val_loss: 39.4322\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9462 - val_loss: 37.4600\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9434 - val_loss: 40.3996\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.8858 - val_loss: 40.4018\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.8640 - val_loss: 37.7894\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.8363 - val_loss: 38.4128\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.7365 - val_loss: 740.4534\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.7848 - val_loss: 41.9477\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.6869 - val_loss: 37.7166\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.6947 - val_loss: 138.0622\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.6259 - val_loss: 48.7647\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.6676 - val_loss: 37.3466\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.6579 - val_loss: 37.2238\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.5866 - val_loss: 58.1590\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.6372 - val_loss: 42.5461\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.5791 - val_loss: 37.5988\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.5907 - val_loss: 2686.7756\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.5675 - val_loss: 45.4660\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.5161 - val_loss: 42.7373\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.4875 - val_loss: 3066.3621\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.4567 - val_loss: 73.8441\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.3975 - val_loss: 2876.5842\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.4321 - val_loss: 55.2377\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.4048 - val_loss: 37.5456\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.3822 - val_loss: 38.5680\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.3844 - val_loss: 1910.1475\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.3878 - val_loss: 39.5058\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.2894 - val_loss: 36.7926\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.3143 - val_loss: 37.6589\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.3128 - val_loss: 38.0914\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.3126 - val_loss: 41.9248\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.2873 - val_loss: 2437.2817\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.2311 - val_loss: 80.2398\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.2629 - val_loss: 101.3835\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.3526 - val_loss: 38.7121\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.2775 - val_loss: 43.7952\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.3086 - val_loss: 46.7673\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.2315 - val_loss: 45.7475\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1975 - val_loss: 60.7643\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.2298 - val_loss: 48.0098\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.2455 - val_loss: 41.7614\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1612 - val_loss: 215.0316\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1908 - val_loss: 39.2622\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.2009 - val_loss: 43.7164\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1741 - val_loss: 63.1546\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1836 - val_loss: 36.6575\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.2194 - val_loss: 37.5312\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1196 - val_loss: 37.4888\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1785 - val_loss: 54.7150\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1799 - val_loss: 44.8340\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1158 - val_loss: 38.6883\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.0997 - val_loss: 38.1806\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1254 - val_loss: 56.2571\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.2171 - val_loss: 37.1688\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1129 - val_loss: 38.5697\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1193 - val_loss: 37.7297\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.1476 - val_loss: 54.5352\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.0741 - val_loss: 43.4423\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.0665 - val_loss: 41.2899\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.0240 - val_loss: 39.0437\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.1064 - val_loss: 42.9578\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 34.1052 - val_loss: 37.4095\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 34.0779 - val_loss: 41.7184\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0791 - val_loss: 44.7415\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 34.0985 - val_loss: 40.2475\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0196 - val_loss: 38.7080\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0296 - val_loss: 45.5217\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0680 - val_loss: 37.1503\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0539 - val_loss: 37.0193\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0472 - val_loss: 40.9928\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0577 - val_loss: 39.9342\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0090 - val_loss: 51.8979\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0450 - val_loss: 40.6829\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0253 - val_loss: 37.6479\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0246 - val_loss: 41.4067\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0459 - val_loss: 45.5149\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0267 - val_loss: 43.5565\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0126 - val_loss: 37.2183\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0692 - val_loss: 37.8057\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9892 - val_loss: 38.3932\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0634 - val_loss: 38.1877\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9613 - val_loss: 41.1592\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9751 - val_loss: 37.8065\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0029 - val_loss: 39.2665\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9763 - val_loss: 37.0986\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9819 - val_loss: 38.1263\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9867 - val_loss: 54.4061\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9478 - val_loss: 42.6802\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9312 - val_loss: 37.6050\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9507 - val_loss: 42.1797\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9909 - val_loss: 36.7512\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9523 - val_loss: 37.9400\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9565 - val_loss: 43.6408\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9250 - val_loss: 37.4887\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9775 - val_loss: 41.2190\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9136 - val_loss: 41.1857\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0018 - val_loss: 37.9287\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9588 - val_loss: 37.9318\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9536 - val_loss: 38.0772\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9271 - val_loss: 37.5514\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9826 - val_loss: 36.8137\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9442 - val_loss: 48.7281\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9301 - val_loss: 37.4489\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9601 - val_loss: 39.0875\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9684 - val_loss: 42.3329\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9271 - val_loss: 44.2708\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9102 - val_loss: 42.8315\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9095 - val_loss: 39.7353\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9442 - val_loss: 36.6374\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9615 - val_loss: 39.1191\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9428 - val_loss: 51.3909\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9041 - val_loss: 38.2595\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9649 - val_loss: 38.5434\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8682 - val_loss: 53.8758\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9498 - val_loss: 38.0188\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9180 - val_loss: 40.5158\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8486 - val_loss: 38.4020\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8432 - val_loss: 37.5950\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8899 - val_loss: 48.0292\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9547 - val_loss: 37.3436\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8905 - val_loss: 65.3182\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9122 - val_loss: 36.9880\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8746 - val_loss: 38.9764\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8740 - val_loss: 36.9694\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8247 - val_loss: 38.8145\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8993 - val_loss: 40.8277\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8943 - val_loss: 38.5494\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.8588 - val_loss: 51.7777\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.8618 - val_loss: 39.9776\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 33.8870 - val_loss: 50.1049\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8288 - val_loss: 38.7910\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7920 - val_loss: 37.5715\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8429 - val_loss: 37.0302\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8557 - val_loss: 41.7164\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9021 - val_loss: 46.6006\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8116 - val_loss: 38.6589\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8252 - val_loss: 36.4488\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8288 - val_loss: 38.4550\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9178 - val_loss: 36.8738\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8503 - val_loss: 43.1930\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8447 - val_loss: 36.4075\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8403 - val_loss: 38.5237\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8570 - val_loss: 40.0490\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8158 - val_loss: 38.1232\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8011 - val_loss: 37.2594\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8637 - val_loss: 40.1458\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8506 - val_loss: 39.8112\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8740 - val_loss: 39.2019\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8295 - val_loss: 36.6729\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8339 - val_loss: 36.3872\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8592 - val_loss: 44.3342\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8232 - val_loss: 38.5360\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8151 - val_loss: 38.0137\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8460 - val_loss: 37.4694\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8321 - val_loss: 38.2200\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8217 - val_loss: 37.8229\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8142 - val_loss: 37.0706\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8455 - val_loss: 39.6290\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8942 - val_loss: 43.8926\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7959 - val_loss: 37.3017\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8355 - val_loss: 43.7063\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8242 - val_loss: 40.2010\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8502 - val_loss: 39.4337\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7978 - val_loss: 41.7755\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8582 - val_loss: 41.5036\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7796 - val_loss: 37.6636\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8553 - val_loss: 39.1856\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8103 - val_loss: 44.9813\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7849 - val_loss: 37.1507\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7833 - val_loss: 40.5561\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8321 - val_loss: 38.5987\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8064 - val_loss: 37.6816\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7988 - val_loss: 41.4311\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8105 - val_loss: 37.6228\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8210 - val_loss: 36.8655\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7955 - val_loss: 50.6346\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7938 - val_loss: 38.9279\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8220 - val_loss: 41.9503\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7919 - val_loss: 41.7702\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7623 - val_loss: 38.2679\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8121 - val_loss: 37.5961\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7423 - val_loss: 37.8719\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7967 - val_loss: 39.5693\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7915 - val_loss: 38.0330\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7924 - val_loss: 40.5795\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7400 - val_loss: 37.9090\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8595 - val_loss: 37.4677\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7910 - val_loss: 36.9764\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7955 - val_loss: 47.6207\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7562 - val_loss: 36.6859\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7712 - val_loss: 36.4002\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8047 - val_loss: 41.9456\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8093 - val_loss: 37.2260\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8125 - val_loss: 38.0689\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8145 - val_loss: 52.6672\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7567 - val_loss: 39.0250\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7542 - val_loss: 36.4734\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7396 - val_loss: 38.8858\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7302 - val_loss: 37.8029\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7505 - val_loss: 43.2520\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7977 - val_loss: 42.3880\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7844 - val_loss: 36.6027\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7791 - val_loss: 40.2233\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7633 - val_loss: 38.3768\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7741 - val_loss: 38.2080\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7700 - val_loss: 38.6249\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7859 - val_loss: 37.9121\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7668 - val_loss: 44.4594\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7514 - val_loss: 38.9634\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7506 - val_loss: 36.7109\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7483 - val_loss: 44.5297\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7680 - val_loss: 37.4673\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8265 - val_loss: 48.5897\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7268 - val_loss: 37.9799\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7689 - val_loss: 37.2070\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7476 - val_loss: 40.3066\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7263 - val_loss: 36.7683\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7350 - val_loss: 38.1987\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7380 - val_loss: 37.1953\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7585 - val_loss: 40.1868\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7576 - val_loss: 43.7495\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7728 - val_loss: 37.0553\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7520 - val_loss: 38.2532\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7723 - val_loss: 37.0705\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7148 - val_loss: 38.5740\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7877 - val_loss: 39.5842\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7232 - val_loss: 38.2858\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7463 - val_loss: 38.2018\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7804 - val_loss: 38.8032\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 33.7258 - val_loss: 39.3016\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7252 - val_loss: 37.9846\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7280 - val_loss: 42.4753\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7810 - val_loss: 36.4948\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7267 - val_loss: 41.0490\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7402 - val_loss: 39.4637\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7246 - val_loss: 38.8323\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7108 - val_loss: 44.1730\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6786 - val_loss: 36.5716\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7223 - val_loss: 38.7230\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6906 - val_loss: 36.2645\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7392 - val_loss: 37.8092\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7176 - val_loss: 42.1532\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7164 - val_loss: 51.2236\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7300 - val_loss: 36.8019\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7410 - val_loss: 36.9974\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7279 - val_loss: 38.0762\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7462 - val_loss: 41.1031\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6974 - val_loss: 39.2251\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7092 - val_loss: 41.1986\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7799 - val_loss: 36.9419\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7476 - val_loss: 38.3855\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7228 - val_loss: 46.3167\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7078 - val_loss: 42.8380\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7216 - val_loss: 43.0414\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6823 - val_loss: 37.1639\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7086 - val_loss: 37.1510\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7393 - val_loss: 44.0354\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6983 - val_loss: 38.2681\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7406 - val_loss: 45.2326\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7269 - val_loss: 36.8588\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7041 - val_loss: 37.9683\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7219 - val_loss: 37.5026\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7160 - val_loss: 49.2370\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7191 - val_loss: 40.0047\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6680 - val_loss: 46.2852\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7165 - val_loss: 37.5768\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7248 - val_loss: 52.6296\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7351 - val_loss: 38.9288\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7245 - val_loss: 36.6610\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7528 - val_loss: 37.7838\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7098 - val_loss: 53.3168\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6947 - val_loss: 46.7945\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7311 - val_loss: 36.9466\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6924 - val_loss: 36.8781\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7731 - val_loss: 40.4643\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7017 - val_loss: 36.8852\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7259 - val_loss: 39.2157\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6923 - val_loss: 40.1386\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7194 - val_loss: 37.0133\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7389 - val_loss: 38.1261\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7661 - val_loss: 42.3641\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6827 - val_loss: 40.2989\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6672 - val_loss: 37.0269\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7544 - val_loss: 50.2066\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6826 - val_loss: 39.9175\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7009 - val_loss: 40.2056\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7584 - val_loss: 39.7661\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7018 - val_loss: 37.4324\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7195 - val_loss: 38.7807\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6716 - val_loss: 38.3026\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7130 - val_loss: 38.8228\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6914 - val_loss: 36.5980\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7021 - val_loss: 47.0249\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7490 - val_loss: 36.6954\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7170 - val_loss: 40.4553\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6826 - val_loss: 40.9557\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7384 - val_loss: 37.8322\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7248 - val_loss: 37.8302\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7014 - val_loss: 40.2845\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7063 - val_loss: 36.2945\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7088 - val_loss: 36.6239\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7168 - val_loss: 36.7210\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7140 - val_loss: 41.6504\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6868 - val_loss: 38.5385\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6929 - val_loss: 43.1487\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6640 - val_loss: 37.1880\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7121 - val_loss: 36.5538\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6793 - val_loss: 36.8115\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7221 - val_loss: 43.7647\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7026 - val_loss: 40.7463\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7044 - val_loss: 37.5087\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7285 - val_loss: 39.0368\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6940 - val_loss: 37.6377\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6907 - val_loss: 38.9262\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6970 - val_loss: 47.4445\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7201 - val_loss: 43.6492\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7268 - val_loss: 38.0918\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7371 - val_loss: 45.7092\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7480 - val_loss: 36.7313\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6958 - val_loss: 38.0869\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6909 - val_loss: 38.3539\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7167 - val_loss: 38.1155\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7342 - val_loss: 39.7504\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6385 - val_loss: 38.7027\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6948 - val_loss: 40.2055\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6774 - val_loss: 38.2139\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7327 - val_loss: 59.0357\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7147 - val_loss: 36.8212\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7123 - val_loss: 36.7816\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7171 - val_loss: 37.9740\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7133 - val_loss: 42.0013\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6807 - val_loss: 39.9093\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7083 - val_loss: 40.3199\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6273 - val_loss: 36.9723\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7117 - val_loss: 38.1970\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7191 - val_loss: 40.4784\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6872 - val_loss: 36.7772\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7185 - val_loss: 36.9216\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6685 - val_loss: 39.3358\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6875 - val_loss: 36.9054\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6325 - val_loss: 36.9960\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7311 - val_loss: 39.8244\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6506 - val_loss: 40.6401\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6637 - val_loss: 37.3849\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6815 - val_loss: 36.6428\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6564 - val_loss: 40.5197\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6671 - val_loss: 38.9001\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6818 - val_loss: 40.1844\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6663 - val_loss: 36.1732\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6404 - val_loss: 40.0456\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6851 - val_loss: 37.5602\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6867 - val_loss: 45.4341\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6677 - val_loss: 42.2172\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6775 - val_loss: 40.3145\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6505 - val_loss: 37.0172\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7133 - val_loss: 40.9482\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6576 - val_loss: 38.1054\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6524 - val_loss: 36.3708\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6642 - val_loss: 41.5084\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6965 - val_loss: 36.4364\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6784 - val_loss: 36.1812\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7185 - val_loss: 37.3682\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6298 - val_loss: 39.5818\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7100 - val_loss: 38.9284\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6813 - val_loss: 41.5588\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6628 - val_loss: 43.6147\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6436 - val_loss: 52.0321\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7312 - val_loss: 36.8460\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6608 - val_loss: 37.5136\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.7062 - val_loss: 36.3517\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6726 - val_loss: 36.8728\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6312 - val_loss: 37.1152\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6953 - val_loss: 38.9246\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6384 - val_loss: 40.1645\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6516 - val_loss: 37.5099\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6958 - val_loss: 36.6621\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6750 - val_loss: 41.6918\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6300 - val_loss: 39.1368\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6613 - val_loss: 37.9642\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6748 - val_loss: 37.4315\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6611 - val_loss: 37.1876\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6582 - val_loss: 36.1672\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6377 - val_loss: 37.4442\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6544 - val_loss: 45.3729\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6649 - val_loss: 43.8288\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6900 - val_loss: 40.5021\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 33.6995 - val_loss: 36.4833\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 33.6594 - val_loss: 38.0079\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6544 - val_loss: 39.1506\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6944 - val_loss: 38.1420\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6928 - val_loss: 37.3289\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6786 - val_loss: 36.5359\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.6746 - val_loss: 36.5800\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.6542 - val_loss: 37.3215\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c90ba9b4-7669-4c91-8005-14b992f4a82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.15282905913532713 \n",
            "MAE:  4.413935241584604 \n",
            "SD:  6.107220828729802\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "bbef47ba-2cb5-4616-9ad8-8ee1f7a9cfc4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f253010-a11d-4ede-ca26-d4ff1d02015d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_25 (Dense)            (None, 4)                 512       \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 641\n",
            "Trainable params: 609\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae36ab7-bf8f-42f9-9a80-56b45945b260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 3s 9ms/step - loss: 3721.0176 - val_loss: 3674.2144\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 3571.0090 - val_loss: 3367.1306\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 3390.8511 - val_loss: 3264.6511\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 3175.4575 - val_loss: 2984.9292\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 2928.1899 - val_loss: 2507.0850\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 2659.6553 - val_loss: 2408.6824\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2378.8000 - val_loss: 2103.9077\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2094.1360 - val_loss: 1793.3373\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1805.9725 - val_loss: 1733.1212\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1512.9457 - val_loss: 1446.6246\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1224.8561 - val_loss: 1558.9938\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 941.4547 - val_loss: 655.1464\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 687.8027 - val_loss: 517.7307\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 492.5343 - val_loss: 366.4064\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 341.3883 - val_loss: 269.4906\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 230.5517 - val_loss: 205.3316\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 153.9599 - val_loss: 148.7620\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.3533 - val_loss: 177.8864\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.2294 - val_loss: 52.7836\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 57.7384 - val_loss: 69.7871\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 48.6133 - val_loss: 70.5917\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 43.9358 - val_loss: 43.1581\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 41.8252 - val_loss: 49.3705\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.9310 - val_loss: 54.5693\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 39.8574 - val_loss: 44.3960\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.1896 - val_loss: 44.7182\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.3721 - val_loss: 59.9847\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.5826 - val_loss: 48.7343\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.4807 - val_loss: 49.1225\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.4852 - val_loss: 64.3469\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.2050 - val_loss: 49.9959\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.1149 - val_loss: 40.9836\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.9908 - val_loss: 45.4163\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.9927 - val_loss: 67.1371\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.9061 - val_loss: 42.9258\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.9468 - val_loss: 92.8348\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.9693 - val_loss: 47.1994\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.8337 - val_loss: 50.9388\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.7347 - val_loss: 47.6601\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.8048 - val_loss: 41.6737\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5958 - val_loss: 56.6273\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.7543 - val_loss: 42.0894\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.6166 - val_loss: 45.0350\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.7895 - val_loss: 43.8676\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.5682 - val_loss: 53.7187\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.5373 - val_loss: 44.1801\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5374 - val_loss: 54.0648\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.5299 - val_loss: 48.5372\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5066 - val_loss: 46.3564\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.5824 - val_loss: 48.3957\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.5925 - val_loss: 47.7734\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5013 - val_loss: 64.8622\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.5609 - val_loss: 43.8805\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.5482 - val_loss: 49.4283\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.6061 - val_loss: 53.9628\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.5229 - val_loss: 42.4880\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.5386 - val_loss: 43.6442\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.4165 - val_loss: 42.9944\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.5052 - val_loss: 48.0735\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3714 - val_loss: 42.9330\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3234 - val_loss: 42.1485\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.3073 - val_loss: 49.0116\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.4912 - val_loss: 52.7228\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.4781 - val_loss: 46.3265\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3108 - val_loss: 56.6653\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.4598 - val_loss: 41.2145\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3588 - val_loss: 47.6759\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.3874 - val_loss: 48.4132\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3262 - val_loss: 48.6091\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.3230 - val_loss: 42.5898\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3046 - val_loss: 46.7755\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.2933 - val_loss: 54.2106\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3034 - val_loss: 44.3274\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1680 - val_loss: 44.3003\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.3130 - val_loss: 46.5455\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1584 - val_loss: 42.8325\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.1291 - val_loss: 44.3359\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.2272 - val_loss: 47.7471\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1363 - val_loss: 43.4486\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.2118 - val_loss: 42.1594\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1968 - val_loss: 46.3542\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1289 - val_loss: 51.3953\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.1590 - val_loss: 48.7622\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.1949 - val_loss: 44.8605\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.1440 - val_loss: 43.3854\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.1479 - val_loss: 41.6446\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0064 - val_loss: 43.1468\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.0192 - val_loss: 41.7966\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0655 - val_loss: 42.0807\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.0508 - val_loss: 51.1756\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.0384 - val_loss: 50.3295\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9923 - val_loss: 42.1240\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.9721 - val_loss: 39.4438\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0691 - val_loss: 44.2991\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0194 - val_loss: 41.4603\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9316 - val_loss: 40.5769\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0276 - val_loss: 44.0953\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9958 - val_loss: 44.0289\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9396 - val_loss: 52.3141\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.9524 - val_loss: 45.2194\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8932 - val_loss: 40.9505\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9197 - val_loss: 42.4971\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8888 - val_loss: 60.3292\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.8755 - val_loss: 42.9725\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8680 - val_loss: 54.3049\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8451 - val_loss: 42.3014\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7939 - val_loss: 42.2831\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7812 - val_loss: 42.3688\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8779 - val_loss: 41.1827\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.7330 - val_loss: 50.4788\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8098 - val_loss: 50.7037\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8252 - val_loss: 40.8196\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8572 - val_loss: 52.2879\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7656 - val_loss: 44.3345\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.7390 - val_loss: 52.5997\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.8208 - val_loss: 42.2496\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7655 - val_loss: 53.0147\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.7532 - val_loss: 46.8656\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7602 - val_loss: 46.5520\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6965 - val_loss: 49.4684\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6843 - val_loss: 64.6184\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7532 - val_loss: 47.8935\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6940 - val_loss: 48.5419\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6060 - val_loss: 43.3066\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6364 - val_loss: 40.4855\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6486 - val_loss: 41.6009\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6287 - val_loss: 47.2258\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6194 - val_loss: 39.5619\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6765 - val_loss: 51.1031\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6232 - val_loss: 43.8511\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6816 - val_loss: 41.7802\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6786 - val_loss: 44.0329\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6122 - val_loss: 47.4536\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6052 - val_loss: 39.7289\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6038 - val_loss: 40.8954\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6179 - val_loss: 46.7944\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6271 - val_loss: 41.4276\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5758 - val_loss: 45.6177\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5059 - val_loss: 40.8710\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.5986 - val_loss: 44.3215\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5135 - val_loss: 47.6880\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.5061 - val_loss: 45.9485\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.5821 - val_loss: 45.4500\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6541 - val_loss: 39.4722\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4905 - val_loss: 43.8986\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5270 - val_loss: 39.6807\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5596 - val_loss: 44.7796\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.5396 - val_loss: 44.7023\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5115 - val_loss: 41.0406\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5553 - val_loss: 45.0355\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.6506 - val_loss: 44.5881\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4602 - val_loss: 43.8996\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.4828 - val_loss: 47.5444\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5140 - val_loss: 41.8804\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.4681 - val_loss: 41.5002\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.4559 - val_loss: 40.0284\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4550 - val_loss: 41.4990\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4094 - val_loss: 42.3954\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4590 - val_loss: 48.8830\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.4330 - val_loss: 46.8293\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.4483 - val_loss: 41.2528\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3656 - val_loss: 41.8239\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3722 - val_loss: 41.1409\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4951 - val_loss: 42.2568\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4283 - val_loss: 42.3285\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3426 - val_loss: 46.9723\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.4556 - val_loss: 42.6153\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.2907 - val_loss: 39.6958\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4297 - val_loss: 47.9858\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3968 - val_loss: 45.5461\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3620 - val_loss: 38.8570\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3527 - val_loss: 41.6145\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3657 - val_loss: 39.2744\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3178 - val_loss: 41.3239\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.2678 - val_loss: 38.8637\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.4030 - val_loss: 47.4515\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3111 - val_loss: 41.3148\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.3535 - val_loss: 41.4037\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 36.2903 - val_loss: 41.2353\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.3359 - val_loss: 39.6093\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2323 - val_loss: 42.2253\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2340 - val_loss: 49.7861\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2620 - val_loss: 41.9367\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2196 - val_loss: 42.0945\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2312 - val_loss: 43.7015\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.2072 - val_loss: 42.5805\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1809 - val_loss: 41.7703\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1424 - val_loss: 42.8955\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1937 - val_loss: 43.5579\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0617 - val_loss: 40.7734\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0986 - val_loss: 47.9665\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1233 - val_loss: 40.7057\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1298 - val_loss: 41.1707\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1386 - val_loss: 40.0843\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1313 - val_loss: 41.2351\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.1122 - val_loss: 39.1661\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0598 - val_loss: 39.7104\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0559 - val_loss: 50.7478\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0899 - val_loss: 40.0030\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0439 - val_loss: 45.2342\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0311 - val_loss: 38.9317\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0087 - val_loss: 50.1915\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0472 - val_loss: 40.9247\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0101 - val_loss: 43.5115\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9904 - val_loss: 40.5079\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9768 - val_loss: 39.5650\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0164 - val_loss: 41.0571\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9997 - val_loss: 43.1976\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0006 - val_loss: 40.6076\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9369 - val_loss: 39.2709\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9047 - val_loss: 39.1606\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9146 - val_loss: 46.5447\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.9452 - val_loss: 38.2610\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8903 - val_loss: 38.4788\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8582 - val_loss: 39.6677\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8929 - val_loss: 39.7966\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8891 - val_loss: 38.3610\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8466 - val_loss: 40.5108\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8284 - val_loss: 39.2209\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7699 - val_loss: 39.1442\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7710 - val_loss: 43.9478\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7873 - val_loss: 39.6537\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7548 - val_loss: 51.6712\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7715 - val_loss: 40.1897\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7665 - val_loss: 41.1167\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.8048 - val_loss: 40.3570\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7216 - val_loss: 43.0653\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7006 - val_loss: 41.5694\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7153 - val_loss: 39.1002\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7095 - val_loss: 41.2171\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7161 - val_loss: 39.8354\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.7019 - val_loss: 38.1312\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.6347 - val_loss: 40.7409\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.6124 - val_loss: 41.6252\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.6902 - val_loss: 40.7669\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.5681 - val_loss: 40.9770\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.5584 - val_loss: 42.3619\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.6035 - val_loss: 38.9682\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.5227 - val_loss: 47.9809\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.5550 - val_loss: 39.7225\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.5560 - val_loss: 38.8776\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.5139 - val_loss: 40.1982\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.5023 - val_loss: 39.1834\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.5190 - val_loss: 42.5854\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.4648 - val_loss: 38.1889\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.4210 - val_loss: 42.5133\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3855 - val_loss: 38.4084\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.4046 - val_loss: 44.1984\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.4253 - val_loss: 39.2767\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3918 - val_loss: 39.4645\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3603 - val_loss: 41.4796\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.4379 - val_loss: 39.1238\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3411 - val_loss: 38.6995\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3963 - val_loss: 42.9043\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3296 - val_loss: 47.3055\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3508 - val_loss: 39.4481\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.4045 - val_loss: 39.2606\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3745 - val_loss: 39.7834\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3659 - val_loss: 38.5364\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3228 - val_loss: 43.5703\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2897 - val_loss: 38.7304\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3478 - val_loss: 39.8318\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3122 - val_loss: 39.6544\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3170 - val_loss: 41.0322\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2624 - val_loss: 37.7794\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.3451 - val_loss: 39.0808\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2244 - val_loss: 42.9923\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2947 - val_loss: 38.0846\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2815 - val_loss: 39.1489\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2998 - val_loss: 37.6659\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.2625 - val_loss: 38.9715\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 35.2927 - val_loss: 37.8922\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.2748 - val_loss: 40.3337\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2480 - val_loss: 42.8225\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2722 - val_loss: 42.4349\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2618 - val_loss: 40.0853\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2197 - val_loss: 39.0626\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2012 - val_loss: 48.9836\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2299 - val_loss: 38.2386\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2250 - val_loss: 41.7003\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2267 - val_loss: 55.4192\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2194 - val_loss: 44.5990\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2439 - val_loss: 38.2348\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1964 - val_loss: 39.6325\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1822 - val_loss: 38.0408\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2352 - val_loss: 38.2150\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1729 - val_loss: 40.0476\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1750 - val_loss: 43.3392\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1176 - val_loss: 38.3664\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1410 - val_loss: 38.8988\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1608 - val_loss: 38.0278\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1537 - val_loss: 38.7700\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1976 - val_loss: 41.3927\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1453 - val_loss: 38.4840\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.2020 - val_loss: 46.2748\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1365 - val_loss: 38.7594\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1572 - val_loss: 39.2181\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1481 - val_loss: 41.9220\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1567 - val_loss: 37.6426\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1570 - val_loss: 39.0019\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1475 - val_loss: 40.8307\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1059 - val_loss: 39.4968\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0964 - val_loss: 44.6289\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1150 - val_loss: 42.8335\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1143 - val_loss: 38.1583\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0810 - val_loss: 40.6272\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1318 - val_loss: 43.8453\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0824 - val_loss: 38.4791\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1225 - val_loss: 38.4026\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0401 - val_loss: 37.2440\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0814 - val_loss: 39.9779\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1083 - val_loss: 37.8211\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0951 - val_loss: 38.8282\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0737 - val_loss: 38.7416\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0600 - val_loss: 43.4944\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1041 - val_loss: 38.8341\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1017 - val_loss: 37.9754\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0683 - val_loss: 43.1532\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0965 - val_loss: 39.2826\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0574 - val_loss: 38.3156\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1199 - val_loss: 55.0820\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0485 - val_loss: 39.7126\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0176 - val_loss: 38.2179\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0741 - val_loss: 37.8898\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0329 - val_loss: 38.3276\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0332 - val_loss: 39.8801\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0448 - val_loss: 45.2685\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0316 - val_loss: 38.3564\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0405 - val_loss: 38.7023\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0586 - val_loss: 43.8188\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.1191 - val_loss: 40.6347\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0335 - val_loss: 38.0955\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.0269 - val_loss: 46.0164\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9617 - val_loss: 37.9246\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0011 - val_loss: 38.5798\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0467 - val_loss: 40.7556\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9747 - val_loss: 39.7020\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0000 - val_loss: 39.0500\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0049 - val_loss: 37.6941\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9984 - val_loss: 38.7769\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 34.9822 - val_loss: 43.9115\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0025 - val_loss: 38.1351\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0303 - val_loss: 40.8475\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0005 - val_loss: 39.0617\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0552 - val_loss: 38.2329\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0308 - val_loss: 40.4682\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9871 - val_loss: 39.1979\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0713 - val_loss: 41.6154\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9774 - val_loss: 40.4394\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9134 - val_loss: 37.6122\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9925 - val_loss: 38.6931\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0112 - val_loss: 40.2903\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0015 - val_loss: 40.0439\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9406 - val_loss: 41.7809\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9848 - val_loss: 38.2147\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9727 - val_loss: 37.6605\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0190 - val_loss: 38.3074\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 34.9748 - val_loss: 39.1879\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.0030 - val_loss: 48.9113\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9855 - val_loss: 39.5763\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0293 - val_loss: 41.9499\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 35.0246 - val_loss: 41.2630\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9799 - val_loss: 37.8464\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9681 - val_loss: 40.2077\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 34.9356 - val_loss: 38.5703\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 34.9872 - val_loss: 39.2981\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 34.9547 - val_loss: 38.1392\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 34.9650 - val_loss: 39.1547\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9929 - val_loss: 38.2059\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 34.9107 - val_loss: 42.8481\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9483 - val_loss: 40.5552\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9101 - val_loss: 38.0061\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9410 - val_loss: 37.5269\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9682 - val_loss: 45.0105\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9722 - val_loss: 37.9810\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9426 - val_loss: 39.8052\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9405 - val_loss: 37.8632\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9424 - val_loss: 41.3777\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9297 - val_loss: 39.9090\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8904 - val_loss: 37.6942\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9268 - val_loss: 37.5560\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9027 - val_loss: 38.1446\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9575 - val_loss: 42.6678\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8771 - val_loss: 38.6603\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9119 - val_loss: 47.0324\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9255 - val_loss: 43.4415\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9118 - val_loss: 38.4658\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8888 - val_loss: 50.9577\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8855 - val_loss: 39.4553\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8629 - val_loss: 37.5920\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8467 - val_loss: 38.5990\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8887 - val_loss: 37.9101\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8764 - val_loss: 44.7496\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9151 - val_loss: 38.7553\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9022 - val_loss: 38.4067\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8197 - val_loss: 41.7638\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8396 - val_loss: 38.5074\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7972 - val_loss: 39.4207\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7915 - val_loss: 37.3342\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8018 - val_loss: 39.5079\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8141 - val_loss: 42.3384\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7960 - val_loss: 40.3306\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7815 - val_loss: 42.9096\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7894 - val_loss: 42.2320\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7953 - val_loss: 40.3852\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8029 - val_loss: 38.0731\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8119 - val_loss: 43.8684\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8062 - val_loss: 38.9198\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7840 - val_loss: 43.3307\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8141 - val_loss: 42.5607\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7509 - val_loss: 39.0281\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7545 - val_loss: 38.3097\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7571 - val_loss: 41.5829\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7467 - val_loss: 39.0647\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7828 - val_loss: 42.6560\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7696 - val_loss: 39.6991\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7439 - val_loss: 38.6376\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7278 - val_loss: 39.4902\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6696 - val_loss: 38.9232\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7074 - val_loss: 38.0570\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7590 - val_loss: 38.6134\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6764 - val_loss: 38.4619\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6628 - val_loss: 39.1261\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7087 - val_loss: 43.2936\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6888 - val_loss: 40.2082\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6669 - val_loss: 40.4193\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6627 - val_loss: 40.5337\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6777 - val_loss: 38.0335\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7103 - val_loss: 39.7182\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6886 - val_loss: 38.3862\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6562 - val_loss: 38.4530\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6475 - val_loss: 38.6907\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7053 - val_loss: 47.8916\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6636 - val_loss: 39.6152\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7250 - val_loss: 39.4072\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6137 - val_loss: 38.8859\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6228 - val_loss: 42.7396\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6071 - val_loss: 40.2115\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6810 - val_loss: 39.1028\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6555 - val_loss: 45.6593\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6107 - val_loss: 39.9900\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6179 - val_loss: 38.9054\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6503 - val_loss: 38.4537\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6421 - val_loss: 39.1814\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6236 - val_loss: 42.9030\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5833 - val_loss: 38.0533\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6254 - val_loss: 38.1276\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6078 - val_loss: 37.8303\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6629 - val_loss: 37.8497\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6178 - val_loss: 40.5480\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5486 - val_loss: 39.0483\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5898 - val_loss: 38.9916\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6155 - val_loss: 38.2734\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5865 - val_loss: 39.5973\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6425 - val_loss: 39.5316\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5690 - val_loss: 38.9352\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5802 - val_loss: 43.9958\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5952 - val_loss: 40.6315\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5527 - val_loss: 54.5704\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5927 - val_loss: 40.0429\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5719 - val_loss: 38.0742\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5588 - val_loss: 40.0640\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5428 - val_loss: 39.1789\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5243 - val_loss: 38.4628\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5706 - val_loss: 42.0940\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5671 - val_loss: 41.6405\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5601 - val_loss: 39.4061\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5774 - val_loss: 39.4856\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5118 - val_loss: 40.8679\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5664 - val_loss: 38.9767\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5781 - val_loss: 37.7843\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5492 - val_loss: 41.3172\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5579 - val_loss: 43.6821\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5077 - val_loss: 37.3671\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5077 - val_loss: 38.9243\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5725 - val_loss: 37.8303\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5604 - val_loss: 43.7102\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5420 - val_loss: 54.3423\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5804 - val_loss: 43.8559\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5257 - val_loss: 38.8055\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5331 - val_loss: 38.1675\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5130 - val_loss: 38.3561\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4946 - val_loss: 40.4134\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5104 - val_loss: 38.1822\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5112 - val_loss: 37.8207\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4810 - val_loss: 43.2777\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5392 - val_loss: 39.1888\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4930 - val_loss: 52.3585\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5162 - val_loss: 37.7286\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4989 - val_loss: 37.8354\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4631 - val_loss: 38.4117\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4657 - val_loss: 38.5606\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4529 - val_loss: 40.6150\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5605 - val_loss: 38.1001\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5101 - val_loss: 41.2890\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4640 - val_loss: 38.1239\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4094 - val_loss: 38.9877\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4737 - val_loss: 38.5709\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4306 - val_loss: 41.5914\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4326 - val_loss: 48.2077\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f8cf60-ffba-41b5-bf21-6289cbdb1041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  3.1733003538810776 \n",
            "MAE:  5.246967769459968 \n",
            "SD:  6.175587791725244\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "046c89c7-4c6a-45c2-eca6-8e7f52740a0c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a31259-af06-46fd-85bf-21fe0345bbc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  1.7343497509450045 \n",
            "Ensemble_std:  6.169618133090324\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "\bBP_hv3_6(2).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}