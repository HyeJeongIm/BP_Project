{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/%08BP_hv3_3(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# batch_size"
      ],
      "metadata": {
        "id": "XiiiBla2-j1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WsCoux5AOZnK",
        "outputId": "b2341147-e928-4193-dca0-4bce583f3816",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "TensorFlow version :  2.8.2\n",
            "Keras version :  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlHICkovd809",
        "outputId": "5976faba-5d45-4177-8d8b-42dbb1c4ff8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러z오기\n",
        "train_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ],
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "95a42058-4181-4163-a7b4-04c5e689de01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "713cb0ea-035b-4b6b-9018-4e9d0773535a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b307f665-e6f8-4a34-a817-ead14f28647d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b307f665-e6f8-4a34-a817-ead14f28647d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b307f665-e6f8-4a34-a817-ead14f28647d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b307f665-e6f8-4a34-a817-ead14f28647d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "38e95e9a-d7d1-4165-8ac2-fc109c42ccbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb23d29b-a683-4ae6-9aff-3d08f2be17eb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb23d29b-a683-4ae6-9aff-3d08f2be17eb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cb23d29b-a683-4ae6-9aff-3d08f2be17eb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cb23d29b-a683-4ae6-9aff-3d08f2be17eb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ],
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ptBRJtSOZnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EI8SHBwBOZnO",
        "outputId": "d5594bd5-6e77-4c72-8c04-0943c358ad73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 32)                4096      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32)               128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,441\n",
            "Trainable params: 5,313\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "id": "dGT6-7NcOZnO",
        "outputId": "e2d6b1e5-f966-4971-c030-7ec83f9a535f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 3s 11ms/step - loss: 11924.5986 - val_loss: 11911.5732\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 10723.8135 - val_loss: 10646.3086\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9036.6387 - val_loss: 7912.9746\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7031.9771 - val_loss: 5323.8589\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 5005.4429 - val_loss: 3330.8450\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3218.7551 - val_loss: 2324.3374\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1845.1674 - val_loss: 1058.3154\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 941.5365 - val_loss: 497.9044\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 446.6321 - val_loss: 316.6319\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 219.1671 - val_loss: 384.2448\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 134.5920 - val_loss: 117.3447\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.7356 - val_loss: 139.6758\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 97.3702 - val_loss: 111.8888\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.0461 - val_loss: 111.2579\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.1801 - val_loss: 142.7395\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.6115 - val_loss: 113.6238\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.9490 - val_loss: 130.8369\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.9975 - val_loss: 120.2093\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.5655 - val_loss: 174.0384\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.1032 - val_loss: 120.2126\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.1550 - val_loss: 122.9943\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.9249 - val_loss: 106.3228\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.1737 - val_loss: 103.0961\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.9718 - val_loss: 109.9013\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 82.6529 - val_loss: 111.2636\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 82.7925 - val_loss: 114.5952\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.4614 - val_loss: 175.5821\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 81.8338 - val_loss: 119.4309\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 81.2213 - val_loss: 132.4887\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 80.4212 - val_loss: 105.2693\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 80.3538 - val_loss: 138.8324\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 80.1048 - val_loss: 116.9625\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 79.5888 - val_loss: 138.8559\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 79.3301 - val_loss: 145.1531\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 78.7558 - val_loss: 99.4339\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 78.7623 - val_loss: 93.7074\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 78.1229 - val_loss: 110.7758\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 78.0706 - val_loss: 138.3875\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9132 - val_loss: 104.1600\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 77.5079 - val_loss: 124.4436\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0442 - val_loss: 105.8359\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 76.9191 - val_loss: 98.9077\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 76.5723 - val_loss: 101.1177\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 76.3713 - val_loss: 104.1548\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 75.9931 - val_loss: 100.1003\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 75.7648 - val_loss: 111.4035\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 75.8424 - val_loss: 194.2221\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.4212 - val_loss: 175.5833\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.2422 - val_loss: 93.2001\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.0904 - val_loss: 112.0704\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 74.9805 - val_loss: 111.0823\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 74.5771 - val_loss: 121.9021\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 74.1485 - val_loss: 91.9688\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 74.1617 - val_loss: 107.4596\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 73.7031 - val_loss: 94.0866\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 73.6723 - val_loss: 125.7190\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5863 - val_loss: 99.8175\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 73.4326 - val_loss: 98.7083\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 73.0624 - val_loss: 98.5429\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0609 - val_loss: 98.3905\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8943 - val_loss: 110.1651\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0258 - val_loss: 108.2358\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3594 - val_loss: 122.8048\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2792 - val_loss: 99.5295\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1675 - val_loss: 93.8718\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1984 - val_loss: 114.3215\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2022 - val_loss: 92.8282\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.0591 - val_loss: 142.3035\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8214 - val_loss: 97.4307\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.6956 - val_loss: 98.8527\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.5398 - val_loss: 103.8791\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.5414 - val_loss: 122.7032\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4137 - val_loss: 95.6395\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2485 - val_loss: 108.8334\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.1750 - val_loss: 124.5842\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.9857 - val_loss: 186.8258\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9767 - val_loss: 91.6634\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.8647 - val_loss: 116.8251\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.7554 - val_loss: 93.3030\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.8499 - val_loss: 106.0876\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.6524 - val_loss: 155.6283\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.6437 - val_loss: 94.0308\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5543 - val_loss: 93.1669\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.7105 - val_loss: 96.0097\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.2645 - val_loss: 132.1609\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.2954 - val_loss: 114.4674\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.2209 - val_loss: 92.8235\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.1205 - val_loss: 95.9824\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.0398 - val_loss: 96.4822\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.0416 - val_loss: 139.6378\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.9679 - val_loss: 100.7344\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.8815 - val_loss: 138.8631\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.8112 - val_loss: 99.8220\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.7677 - val_loss: 115.3292\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.9434 - val_loss: 163.9084\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.6866 - val_loss: 100.7692\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.7644 - val_loss: 114.1089\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.4611 - val_loss: 102.4784\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.6351 - val_loss: 135.5335\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 69.5212 - val_loss: 168.0336\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 69.7458 - val_loss: 105.5070\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 69.3330 - val_loss: 124.1931\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.2762 - val_loss: 106.1368\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.4458 - val_loss: 89.1179\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.1412 - val_loss: 102.4653\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.2080 - val_loss: 103.8365\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.0653 - val_loss: 98.7828\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.1156 - val_loss: 98.9240\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.1301 - val_loss: 98.1602\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.9344 - val_loss: 96.9798\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.9782 - val_loss: 98.9064\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.6704 - val_loss: 103.1849\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.8179 - val_loss: 96.6018\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.8522 - val_loss: 88.7412\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.6445 - val_loss: 192.6344\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.8208 - val_loss: 118.6213\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.6638 - val_loss: 87.8199\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.5294 - val_loss: 93.1683\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.4401 - val_loss: 98.9580\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.4542 - val_loss: 135.6676\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.6636 - val_loss: 94.8632\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.5750 - val_loss: 97.8570\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.5327 - val_loss: 151.7088\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.1330 - val_loss: 114.4644\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.1404 - val_loss: 98.3173\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.2662 - val_loss: 100.0183\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.2445 - val_loss: 122.9195\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.3524 - val_loss: 94.7190\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.3033 - val_loss: 138.5479\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.3331 - val_loss: 89.0432\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.0725 - val_loss: 99.3582\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.3356 - val_loss: 88.9248\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.2130 - val_loss: 101.0267\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.7417 - val_loss: 104.0694\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.9245 - val_loss: 101.3863\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.0533 - val_loss: 87.0757\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.2857 - val_loss: 106.1584\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.7146 - val_loss: 93.1837\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.5905 - val_loss: 100.6724\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.5893 - val_loss: 103.0527\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.8563 - val_loss: 139.3617\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.7065 - val_loss: 95.5638\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.7984 - val_loss: 111.6264\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.8051 - val_loss: 106.6637\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.9409 - val_loss: 116.2374\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.4753 - val_loss: 94.6341\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.4764 - val_loss: 107.1313\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.7350 - val_loss: 94.8421\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.7088 - val_loss: 92.0725\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.3375 - val_loss: 104.0865\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.5041 - val_loss: 144.2427\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.4080 - val_loss: 111.5659\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.4552 - val_loss: 113.5196\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.4189 - val_loss: 90.3533\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.1220 - val_loss: 125.0888\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.3465 - val_loss: 104.8866\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.0236 - val_loss: 102.7194\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.3427 - val_loss: 98.5482\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.2236 - val_loss: 125.2972\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.2206 - val_loss: 93.7788\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.1531 - val_loss: 103.2299\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.1693 - val_loss: 143.3523\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.2044 - val_loss: 89.9995\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.9671 - val_loss: 103.3141\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.8516 - val_loss: 112.0022\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.0764 - val_loss: 124.1436\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.0398 - val_loss: 94.4036\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.1637 - val_loss: 89.3232\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.0328 - val_loss: 93.3446\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.8169 - val_loss: 119.9831\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.9632 - val_loss: 97.5329\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.8847 - val_loss: 86.8830\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.6943 - val_loss: 98.5723\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.7037 - val_loss: 103.3102\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.9073 - val_loss: 92.6521\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.6602 - val_loss: 101.0321\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.7604 - val_loss: 97.5997\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.7388 - val_loss: 98.3482\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.8577 - val_loss: 102.1133\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.7239 - val_loss: 106.9220\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.7128 - val_loss: 89.3381\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.5674 - val_loss: 117.0728\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.6932 - val_loss: 159.7195\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.6669 - val_loss: 94.3215\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.8073 - val_loss: 93.1415\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.4361 - val_loss: 98.0438\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.5602 - val_loss: 87.1595\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.3440 - val_loss: 105.6966\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.4497 - val_loss: 123.3535\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.5395 - val_loss: 103.3491\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.2401 - val_loss: 95.4654\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.3065 - val_loss: 94.2262\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.1767 - val_loss: 88.6316\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.2180 - val_loss: 93.3764\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 66.4668 - val_loss: 92.7032\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 66.7713 - val_loss: 90.3364\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 66.4594 - val_loss: 117.5819\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 66.2175 - val_loss: 102.5050\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.3230 - val_loss: 103.6112\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.3619 - val_loss: 94.1058\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.2943 - val_loss: 199.5519\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.2555 - val_loss: 89.5463\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.2099 - val_loss: 94.7543\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.1070 - val_loss: 92.5634\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.1971 - val_loss: 93.6078\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.2512 - val_loss: 99.7153\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.1192 - val_loss: 93.9740\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.1050 - val_loss: 108.4145\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.2731 - val_loss: 95.3401\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.0533 - val_loss: 100.0579\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.8130 - val_loss: 88.2721\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.9964 - val_loss: 123.0183\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.0960 - val_loss: 93.8426\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.9834 - val_loss: 86.4451\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.1612 - val_loss: 94.4899\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.0409 - val_loss: 92.0968\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.9641 - val_loss: 89.1476\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.8772 - val_loss: 109.1270\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.8270 - val_loss: 88.9423\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.9714 - val_loss: 91.5117\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.7255 - val_loss: 128.0984\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.8915 - val_loss: 100.1819\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.1782 - val_loss: 94.9140\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.7051 - val_loss: 97.5321\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.7723 - val_loss: 90.4937\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.7568 - val_loss: 95.4607\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.8491 - val_loss: 90.0020\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.6206 - val_loss: 91.1363\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.6234 - val_loss: 108.2104\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.6633 - val_loss: 113.0861\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.9607 - val_loss: 86.5784\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.7904 - val_loss: 100.8497\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.6484 - val_loss: 110.1649\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.7910 - val_loss: 154.9305\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.6898 - val_loss: 98.0447\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5406 - val_loss: 116.9678\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.6382 - val_loss: 103.6285\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.6719 - val_loss: 91.2311\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.4913 - val_loss: 91.4062\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.6963 - val_loss: 92.6669\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.4619 - val_loss: 102.5585\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5065 - val_loss: 89.7450\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5140 - val_loss: 98.0521\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.6076 - val_loss: 91.3053\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.4544 - val_loss: 116.4594\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3822 - val_loss: 100.8657\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.4408 - val_loss: 93.8222\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.5226 - val_loss: 91.9017\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.5268 - val_loss: 175.5080\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.2718 - val_loss: 92.2358\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.2802 - val_loss: 95.6772\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.4054 - val_loss: 94.0870\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3860 - val_loss: 90.6187\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.2937 - val_loss: 97.6721\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.3313 - val_loss: 93.8085\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3525 - val_loss: 86.9875\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3987 - val_loss: 99.0334\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.3659 - val_loss: 104.4676\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.4140 - val_loss: 101.5450\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3533 - val_loss: 86.7636\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.2826 - val_loss: 91.2375\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.1891 - val_loss: 90.3897\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3825 - val_loss: 93.5569\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.2886 - val_loss: 101.0147\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.1031 - val_loss: 116.0572\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.2933 - val_loss: 90.7637\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0014 - val_loss: 106.1088\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.2109 - val_loss: 112.1846\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.1867 - val_loss: 88.7033\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.1049 - val_loss: 91.3834\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.1989 - val_loss: 87.8626\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9448 - val_loss: 114.1828\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0961 - val_loss: 90.3386\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0714 - val_loss: 105.5286\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0840 - val_loss: 138.7878\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0701 - val_loss: 104.6996\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.1535 - val_loss: 92.7839\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9588 - val_loss: 119.0541\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0427 - val_loss: 90.8446\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9507 - val_loss: 107.7271\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9230 - val_loss: 95.0975\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9903 - val_loss: 104.1998\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9659 - val_loss: 125.0944\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9008 - val_loss: 91.9300\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8792 - val_loss: 107.9347\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9392 - val_loss: 99.4535\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9100 - val_loss: 107.7244\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0501 - val_loss: 121.0332\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9095 - val_loss: 99.5666\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9069 - val_loss: 88.4084\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.0463 - val_loss: 98.6463\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8535 - val_loss: 110.7704\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9023 - val_loss: 92.3141\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9470 - val_loss: 95.0572\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.7265 - val_loss: 90.8693\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9621 - val_loss: 98.5978\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9670 - val_loss: 96.2769\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9281 - val_loss: 89.4674\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6729 - val_loss: 95.1268\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8670 - val_loss: 91.9667\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6970 - val_loss: 91.1126\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8141 - val_loss: 106.4477\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8175 - val_loss: 111.3143\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6942 - val_loss: 114.2051\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6861 - val_loss: 99.3261\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5899 - val_loss: 90.4174\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.0095 - val_loss: 141.3172\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.7901 - val_loss: 95.1207\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6573 - val_loss: 134.7581\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8285 - val_loss: 100.8804\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.7479 - val_loss: 94.8004\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.7720 - val_loss: 96.4636\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6414 - val_loss: 95.3094\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4715 - val_loss: 115.4789\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5119 - val_loss: 91.7454\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6130 - val_loss: 88.1977\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.4039 - val_loss: 88.9556\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3974 - val_loss: 100.6922\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6069 - val_loss: 102.1353\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.3910 - val_loss: 87.3603\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.6861 - val_loss: 91.9040\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4892 - val_loss: 135.1486\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4272 - val_loss: 94.7257\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.5139 - val_loss: 96.0399\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4496 - val_loss: 97.0317\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4350 - val_loss: 110.3771\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5354 - val_loss: 104.0952\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4654 - val_loss: 103.9671\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.2863 - val_loss: 92.6079\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5556 - val_loss: 109.9848\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3499 - val_loss: 91.4039\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.3006 - val_loss: 88.1857\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4017 - val_loss: 93.9969\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5358 - val_loss: 114.2105\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.6145 - val_loss: 88.2578\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.3806 - val_loss: 124.8223\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5566 - val_loss: 96.0319\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.3289 - val_loss: 100.9692\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2882 - val_loss: 117.5080\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3294 - val_loss: 103.8360\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.3306 - val_loss: 109.3980\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2147 - val_loss: 95.4308\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3820 - val_loss: 119.5095\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.3987 - val_loss: 92.4503\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3946 - val_loss: 89.1819\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.3518 - val_loss: 98.4400\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4336 - val_loss: 93.4656\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3543 - val_loss: 101.0327\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4058 - val_loss: 88.0615\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.3203 - val_loss: 96.8609\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3594 - val_loss: 90.6974\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2389 - val_loss: 112.6535\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3470 - val_loss: 88.4774\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3191 - val_loss: 92.5362\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2859 - val_loss: 106.7695\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2940 - val_loss: 97.0821\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3654 - val_loss: 88.0766\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1646 - val_loss: 106.9525\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.0510 - val_loss: 102.2643\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0411 - val_loss: 87.6338\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2408 - val_loss: 106.7765\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2149 - val_loss: 89.0101\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2070 - val_loss: 105.4580\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2552 - val_loss: 96.0485\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4160 - val_loss: 137.3569\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2583 - val_loss: 92.3723\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8924 - val_loss: 92.6492\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.2649 - val_loss: 93.5510\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0803 - val_loss: 99.7893\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2014 - val_loss: 113.5014\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1640 - val_loss: 92.8375\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1151 - val_loss: 89.5351\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1047 - val_loss: 94.5028\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1709 - val_loss: 160.3053\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0711 - val_loss: 95.4753\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1483 - val_loss: 89.8193\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0186 - val_loss: 111.2933\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0955 - val_loss: 108.7148\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1764 - val_loss: 107.4360\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9008 - val_loss: 85.4049\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0907 - val_loss: 87.4657\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1267 - val_loss: 101.3530\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0593 - val_loss: 92.7292\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2691 - val_loss: 93.6575\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9683 - val_loss: 114.7581\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9167 - val_loss: 94.1893\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7456 - val_loss: 123.1096\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2275 - val_loss: 102.9713\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9210 - val_loss: 95.6498\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0054 - val_loss: 97.7720\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0807 - val_loss: 95.8674\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.0974 - val_loss: 95.7044\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9690 - val_loss: 99.4511\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9714 - val_loss: 88.3620\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7913 - val_loss: 87.4439\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0411 - val_loss: 88.3232\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8018 - val_loss: 92.1242\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9054 - val_loss: 98.3761\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7103 - val_loss: 111.5360\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8895 - val_loss: 90.4635\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7577 - val_loss: 95.6999\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9524 - val_loss: 92.5840\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7951 - val_loss: 92.6723\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8834 - val_loss: 92.3596\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7936 - val_loss: 121.5675\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9222 - val_loss: 106.5800\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8342 - val_loss: 88.6577\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7810 - val_loss: 93.5479\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7923 - val_loss: 93.8565\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7453 - val_loss: 102.3136\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0743 - val_loss: 89.6188\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9716 - val_loss: 91.3360\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8127 - val_loss: 94.3317\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7810 - val_loss: 90.5390\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8787 - val_loss: 92.0510\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9632 - val_loss: 98.2022\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5662 - val_loss: 95.6048\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8452 - val_loss: 100.8675\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8836 - val_loss: 123.4112\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8276 - val_loss: 94.8939\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6596 - val_loss: 101.0526\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8198 - val_loss: 93.9830\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7771 - val_loss: 108.8671\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7356 - val_loss: 104.1154\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7429 - val_loss: 106.0874\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6770 - val_loss: 87.3514\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7410 - val_loss: 91.1903\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9166 - val_loss: 129.8855\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6906 - val_loss: 143.1823\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.6212 - val_loss: 103.7678\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6736 - val_loss: 117.3366\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6914 - val_loss: 101.9266\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8216 - val_loss: 102.4761\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7002 - val_loss: 90.2875\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.7176 - val_loss: 99.4680\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7118 - val_loss: 98.7193\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4990 - val_loss: 96.9576\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8037 - val_loss: 118.5448\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8692 - val_loss: 92.2454\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6750 - val_loss: 91.0713\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6761 - val_loss: 93.6191\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4799 - val_loss: 91.5800\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5776 - val_loss: 104.1571\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6184 - val_loss: 102.7848\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5437 - val_loss: 88.3397\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5274 - val_loss: 93.1888\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5156 - val_loss: 93.5621\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6282 - val_loss: 94.8517\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4471 - val_loss: 124.6870\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4007 - val_loss: 90.7852\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4763 - val_loss: 90.2793\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5377 - val_loss: 92.1061\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7289 - val_loss: 95.2914\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4562 - val_loss: 90.6319\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5247 - val_loss: 90.3744\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3897 - val_loss: 108.3968\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5058 - val_loss: 93.0965\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5944 - val_loss: 98.0690\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4535 - val_loss: 96.4755\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3685 - val_loss: 93.5349\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5378 - val_loss: 98.2996\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6141 - val_loss: 128.1558\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4062 - val_loss: 108.5713\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4243 - val_loss: 87.3688\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5096 - val_loss: 86.6259\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3121 - val_loss: 109.3187\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3825 - val_loss: 91.7606\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4664 - val_loss: 114.0707\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7189 - val_loss: 95.5844\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5164 - val_loss: 117.7889\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5368 - val_loss: 99.9726\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3221 - val_loss: 90.9531\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3931 - val_loss: 90.8272\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3755 - val_loss: 92.7605\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4538 - val_loss: 101.3648\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5175 - val_loss: 91.0122\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3249 - val_loss: 88.7861\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2313 - val_loss: 90.1696\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5073 - val_loss: 100.2400\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3093 - val_loss: 95.0998\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3963 - val_loss: 87.3173\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2657 - val_loss: 89.3009\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3395 - val_loss: 89.6973\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4262 - val_loss: 117.0926\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3993 - val_loss: 123.4777\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3916 - val_loss: 96.3433\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3379 - val_loss: 98.1394\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4774 - val_loss: 94.7377\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6463 - val_loss: 104.0341\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3624 - val_loss: 102.9499\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3418 - val_loss: 112.8810\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2494 - val_loss: 88.3882\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2859 - val_loss: 165.6625\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3642 - val_loss: 98.6446\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2209 - val_loss: 97.7923\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2347 - val_loss: 91.0440\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5064 - val_loss: 89.7546\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3369 - val_loss: 102.4029\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2519 - val_loss: 130.2434\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1213 - val_loss: 89.1410\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "5d7f3517-8581-46f6-d3b1-1da85d57d4ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -1.2916378682260812 \n",
            "MAE:  7.019999851752993 \n",
            "SD:  9.352683942462814\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "c901ed96-8833-4a73-b0de-585df9f7bb6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "outputId": "a96c3be7-52b9-420f-82d5-36b7b973ee62",
        "id": "su2Sj5jZCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 32)                4096      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 32)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 32)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,441\n",
            "Trainable params: 5,313\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "scrolled": true,
        "outputId": "4f085bed-e760-4c21-d586-dba33ed48cbb",
        "id": "kPRh6v-mCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 5ms/step - loss: 12125.5205 - val_loss: 11946.8564\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 11152.6357 - val_loss: 10873.7129\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 9851.5166 - val_loss: 9242.2949\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 8105.2856 - val_loss: 7198.1646\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 6106.8794 - val_loss: 5884.0234\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 4176.9453 - val_loss: 3786.8691\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2574.7937 - val_loss: 1967.5020\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1428.4786 - val_loss: 1218.9319\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 720.3704 - val_loss: 626.6546\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 350.6794 - val_loss: 201.6563\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 188.6361 - val_loss: 172.5242\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 126.6680 - val_loss: 119.6653\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.1951 - val_loss: 175.2199\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.5955 - val_loss: 165.3512\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.2218 - val_loss: 203.7689\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.2647 - val_loss: 162.8394\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.8046 - val_loss: 130.4795\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.1473 - val_loss: 176.7885\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5135 - val_loss: 132.7259\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.1591 - val_loss: 149.9494\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8695 - val_loss: 125.7869\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.3183 - val_loss: 135.8516\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.5578 - val_loss: 117.9799\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.5141 - val_loss: 148.4123\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.3491 - val_loss: 129.6465\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.4441 - val_loss: 101.5784\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.0402 - val_loss: 116.9459\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.8318 - val_loss: 105.9410\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.2904 - val_loss: 113.8270\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.1533 - val_loss: 110.1281\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 82.5893 - val_loss: 105.7048\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 82.0061 - val_loss: 139.3563\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 81.4854 - val_loss: 114.6463\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.0899 - val_loss: 135.8218\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 81.2111 - val_loss: 133.6813\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 80.8382 - val_loss: 149.7990\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 80.5755 - val_loss: 118.9329\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 80.0826 - val_loss: 119.6362\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 79.9454 - val_loss: 101.3774\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 79.4835 - val_loss: 135.5848\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 79.5507 - val_loss: 106.2253\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 79.0108 - val_loss: 96.9521\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 78.9630 - val_loss: 113.1850\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7341 - val_loss: 130.5014\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.6615 - val_loss: 95.4821\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 78.0893 - val_loss: 92.9795\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 78.1851 - val_loss: 112.2673\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.7147 - val_loss: 112.1455\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 77.5567 - val_loss: 111.8217\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 77.1950 - val_loss: 116.8712\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 77.0470 - val_loss: 109.0770\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.8343 - val_loss: 101.0230\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 76.2825 - val_loss: 101.6262\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.6578 - val_loss: 110.9823\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 76.2516 - val_loss: 115.5526\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 75.9869 - val_loss: 106.0028\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2859 - val_loss: 101.0996\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.4456 - val_loss: 108.7674\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 75.3670 - val_loss: 110.7011\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 75.2557 - val_loss: 101.6972\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9322 - val_loss: 101.9337\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 75.0256 - val_loss: 113.1013\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 74.7416 - val_loss: 125.3560\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 74.8493 - val_loss: 100.5208\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 74.4073 - val_loss: 113.8510\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1644 - val_loss: 113.9505\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 73.8033 - val_loss: 103.2642\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2090 - val_loss: 137.8696\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 73.8387 - val_loss: 137.9642\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9351 - val_loss: 105.7702\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6364 - val_loss: 110.4530\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5013 - val_loss: 176.2730\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 73.4252 - val_loss: 110.6586\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 73.0531 - val_loss: 107.6457\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.9828 - val_loss: 114.4101\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 73.2262 - val_loss: 113.6614\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.9873 - val_loss: 91.4972\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.6425 - val_loss: 112.8612\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5676 - val_loss: 100.2502\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.5286 - val_loss: 144.7901\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4317 - val_loss: 95.7641\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.2023 - val_loss: 96.8025\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3832 - val_loss: 104.4764\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.2876 - val_loss: 107.4121\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.7898 - val_loss: 106.5209\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.7899 - val_loss: 98.4397\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6483 - val_loss: 123.4170\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7092 - val_loss: 107.4186\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.5656 - val_loss: 144.6849\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5418 - val_loss: 121.0715\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.4003 - val_loss: 96.3946\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.4125 - val_loss: 95.4744\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.3036 - val_loss: 106.5722\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3619 - val_loss: 128.4092\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9706 - val_loss: 102.4496\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.2034 - val_loss: 169.9244\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.0337 - val_loss: 113.4687\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9908 - val_loss: 109.3610\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9955 - val_loss: 91.6263\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6464 - val_loss: 108.5331\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7039 - val_loss: 106.4399\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.3872 - val_loss: 101.8820\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.2954 - val_loss: 109.5823\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.2781 - val_loss: 94.2066\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.3909 - val_loss: 93.4965\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.2625 - val_loss: 105.4290\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.1303 - val_loss: 103.8884\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.4873 - val_loss: 99.6949\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.0376 - val_loss: 104.6132\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.8679 - val_loss: 102.9498\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.9630 - val_loss: 119.0500\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.8044 - val_loss: 134.0279\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.8606 - val_loss: 140.0802\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.6779 - val_loss: 100.1120\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.5316 - val_loss: 106.3811\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.6556 - val_loss: 110.6110\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.3965 - val_loss: 128.1985\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.5119 - val_loss: 94.6854\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.3755 - val_loss: 144.8313\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.2797 - val_loss: 103.2453\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.1574 - val_loss: 123.6202\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.1222 - val_loss: 124.6775\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.2099 - val_loss: 98.0494\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.0242 - val_loss: 91.5788\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.7165 - val_loss: 84.4603\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.9281 - val_loss: 109.4636\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.8892 - val_loss: 109.9574\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.7735 - val_loss: 100.9452\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.7418 - val_loss: 147.2486\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.5565 - val_loss: 89.4303\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.7116 - val_loss: 89.3420\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.5801 - val_loss: 102.4418\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.5305 - val_loss: 106.3190\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.3721 - val_loss: 138.3282\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.2894 - val_loss: 109.8824\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.2714 - val_loss: 132.1363\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.3433 - val_loss: 93.8254\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.2281 - val_loss: 104.7752\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.2436 - val_loss: 107.8354\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.3074 - val_loss: 103.2372\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.1066 - val_loss: 104.8249\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.1699 - val_loss: 102.8273\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.9617 - val_loss: 93.5994\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.9481 - val_loss: 102.4361\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.8438 - val_loss: 101.0763\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.7721 - val_loss: 91.3140\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.6585 - val_loss: 95.2165\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.7630 - val_loss: 91.5525\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.6182 - val_loss: 111.5802\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.6364 - val_loss: 92.8797\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.8829 - val_loss: 115.8359\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.5126 - val_loss: 101.2021\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.8069 - val_loss: 103.1309\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.5369 - val_loss: 113.6427\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.3836 - val_loss: 107.8629\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.3997 - val_loss: 112.3946\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.5918 - val_loss: 99.4561\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.2669 - val_loss: 92.8702\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.3550 - val_loss: 107.4162\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.0593 - val_loss: 144.0917\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.1455 - val_loss: 104.2828\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.2543 - val_loss: 102.1056\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.9741 - val_loss: 92.4122\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.0528 - val_loss: 99.0596\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.9282 - val_loss: 100.9914\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.0919 - val_loss: 92.9193\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.9156 - val_loss: 124.8218\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.0778 - val_loss: 120.4995\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.9792 - val_loss: 92.7683\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.5066 - val_loss: 99.5359\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.8807 - val_loss: 108.6457\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.7443 - val_loss: 168.3139\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.5706 - val_loss: 102.6066\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.5767 - val_loss: 94.9193\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.6553 - val_loss: 109.2068\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.7304 - val_loss: 91.9282\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.5138 - val_loss: 110.0731\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.3830 - val_loss: 96.0884\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.5632 - val_loss: 113.5078\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.6188 - val_loss: 92.2870\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.5676 - val_loss: 93.9184\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.3736 - val_loss: 101.7798\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.2527 - val_loss: 104.3530\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.3014 - val_loss: 155.8392\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.3340 - val_loss: 108.1090\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.3026 - val_loss: 96.8559\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.2631 - val_loss: 145.8643\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.2871 - val_loss: 104.6365\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.2434 - val_loss: 93.3659\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.0844 - val_loss: 85.5494\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.0233 - val_loss: 88.6682\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.1656 - val_loss: 91.8304\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.1563 - val_loss: 96.9608\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.8749 - val_loss: 99.0011\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.0522 - val_loss: 100.5089\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.9616 - val_loss: 97.7994\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.9208 - val_loss: 87.8523\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.8647 - val_loss: 94.3229\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.8285 - val_loss: 99.5121\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.6774 - val_loss: 102.8988\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.9286 - val_loss: 97.4112\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.7030 - val_loss: 96.3470\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.6067 - val_loss: 96.7289\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.8428 - val_loss: 101.0804\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.6637 - val_loss: 98.6522\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.7236 - val_loss: 107.5862\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.8164 - val_loss: 94.8633\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.5878 - val_loss: 99.3892\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.6631 - val_loss: 92.1802\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.5970 - val_loss: 92.7335\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5721 - val_loss: 100.9979\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.4679 - val_loss: 94.1134\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.6208 - val_loss: 118.2892\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5407 - val_loss: 89.8660\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5124 - val_loss: 91.9953\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3924 - val_loss: 95.5722\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.5023 - val_loss: 102.7712\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5470 - val_loss: 93.3127\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.4845 - val_loss: 93.9302\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5221 - val_loss: 93.5486\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.4131 - val_loss: 87.8341\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3471 - val_loss: 132.1898\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.2341 - val_loss: 94.3633\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3325 - val_loss: 95.0590\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.1715 - val_loss: 101.4998\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.1711 - val_loss: 95.3717\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.2284 - val_loss: 94.8164\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.2264 - val_loss: 109.2571\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.2028 - val_loss: 108.5183\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.0973 - val_loss: 88.3792\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.1388 - val_loss: 90.7797\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9857 - val_loss: 96.0451\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.1327 - val_loss: 124.1589\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0155 - val_loss: 97.2379\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8031 - val_loss: 89.4880\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.1823 - val_loss: 112.8756\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0916 - val_loss: 96.7848\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8610 - val_loss: 94.9059\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0592 - val_loss: 102.0399\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9897 - val_loss: 102.6462\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0456 - val_loss: 91.7370\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.1956 - val_loss: 148.6456\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0406 - val_loss: 92.8215\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9560 - val_loss: 99.5992\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8319 - val_loss: 93.9429\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8790 - val_loss: 96.2236\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6849 - val_loss: 106.5346\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.7648 - val_loss: 123.9400\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9196 - val_loss: 132.8772\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5204 - val_loss: 104.3206\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8357 - val_loss: 95.8503\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6675 - val_loss: 119.3904\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5850 - val_loss: 106.3192\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5330 - val_loss: 89.8108\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.7723 - val_loss: 97.5290\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6239 - val_loss: 112.1295\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5809 - val_loss: 86.8648\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.5023 - val_loss: 92.6634\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5807 - val_loss: 97.8392\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5174 - val_loss: 96.3678\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4781 - val_loss: 90.3993\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5869 - val_loss: 103.9874\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5177 - val_loss: 98.2581\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6847 - val_loss: 112.5407\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.4602 - val_loss: 99.5743\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4947 - val_loss: 90.7260\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4106 - val_loss: 97.9137\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3665 - val_loss: 91.0317\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5872 - val_loss: 94.5925\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5483 - val_loss: 101.0033\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2727 - val_loss: 105.1635\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4457 - val_loss: 110.3555\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1305 - val_loss: 93.5345\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2409 - val_loss: 99.9143\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.4059 - val_loss: 123.1807\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4849 - val_loss: 94.4447\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.1418 - val_loss: 96.6531\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.2712 - val_loss: 99.2764\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1709 - val_loss: 107.3241\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2133 - val_loss: 95.7734\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.3102 - val_loss: 103.7610\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1430 - val_loss: 99.4081\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.2777 - val_loss: 92.2109\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0835 - val_loss: 102.5059\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0157 - val_loss: 108.4164\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2862 - val_loss: 92.4816\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.9628 - val_loss: 112.5474\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1023 - val_loss: 104.8697\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2168 - val_loss: 88.5608\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9759 - val_loss: 98.8900\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.2369 - val_loss: 96.3555\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7787 - val_loss: 96.7188\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.0242 - val_loss: 92.8443\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.9870 - val_loss: 108.2669\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.0919 - val_loss: 95.5121\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0248 - val_loss: 104.7161\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9584 - val_loss: 99.6989\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.1575 - val_loss: 91.5295\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9842 - val_loss: 100.4043\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9423 - val_loss: 113.6793\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.9344 - val_loss: 86.3158\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.0006 - val_loss: 90.1090\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.7905 - val_loss: 106.7932\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9416 - val_loss: 129.4662\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.8837 - val_loss: 108.0777\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9944 - val_loss: 106.7826\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.7316 - val_loss: 136.6356\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 63.7296 - val_loss: 94.7475\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 63.8911 - val_loss: 98.3048\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 63.8358 - val_loss: 92.6230\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6503 - val_loss: 100.4526\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8801 - val_loss: 105.5179\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9516 - val_loss: 116.6439\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7739 - val_loss: 107.1314\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7732 - val_loss: 92.1584\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7082 - val_loss: 91.7037\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7709 - val_loss: 89.7074\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7524 - val_loss: 93.5476\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5864 - val_loss: 92.6681\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6631 - val_loss: 96.2810\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7388 - val_loss: 98.0310\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6234 - val_loss: 90.7315\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6699 - val_loss: 93.0139\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6236 - val_loss: 102.2684\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7121 - val_loss: 90.9636\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6727 - val_loss: 99.7538\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.6909 - val_loss: 96.0721\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5200 - val_loss: 90.5720\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5729 - val_loss: 99.3445\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5850 - val_loss: 98.5903\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7029 - val_loss: 94.8291\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4226 - val_loss: 90.8082\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5831 - val_loss: 96.1217\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4698 - val_loss: 109.0013\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4736 - val_loss: 97.0575\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4247 - val_loss: 98.0366\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7797 - val_loss: 96.4311\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4511 - val_loss: 105.4662\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4426 - val_loss: 96.5482\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5344 - val_loss: 106.3012\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3718 - val_loss: 95.1277\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5522 - val_loss: 95.0362\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4390 - val_loss: 109.5893\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.5291 - val_loss: 97.8630\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2839 - val_loss: 92.0958\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.4357 - val_loss: 103.1554\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5695 - val_loss: 104.6651\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4288 - val_loss: 133.5958\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4579 - val_loss: 97.1603\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1253 - val_loss: 107.5115\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4762 - val_loss: 97.5738\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2127 - val_loss: 92.1329\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2316 - val_loss: 90.1691\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4425 - val_loss: 99.8429\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3828 - val_loss: 95.5444\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1865 - val_loss: 100.0133\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5177 - val_loss: 94.6115\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4218 - val_loss: 95.9405\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2915 - val_loss: 98.9477\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2888 - val_loss: 133.7172\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4654 - val_loss: 99.6992\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3433 - val_loss: 91.9109\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0862 - val_loss: 102.7827\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2744 - val_loss: 91.1483\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2759 - val_loss: 107.9681\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.3188 - val_loss: 111.2056\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1821 - val_loss: 97.9436\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2634 - val_loss: 88.3266\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2063 - val_loss: 113.4303\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1011 - val_loss: 89.9261\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0558 - val_loss: 90.7187\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1698 - val_loss: 100.1388\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3959 - val_loss: 103.7527\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0785 - val_loss: 114.2102\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0734 - val_loss: 110.8793\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3299 - val_loss: 100.5489\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0337 - val_loss: 89.0056\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0625 - val_loss: 119.5941\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4515 - val_loss: 111.2239\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0146 - val_loss: 89.7560\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1772 - val_loss: 99.2162\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2067 - val_loss: 95.2957\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0282 - val_loss: 93.4367\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1749 - val_loss: 111.5324\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0106 - val_loss: 118.4657\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1291 - val_loss: 88.8829\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3839 - val_loss: 116.4744\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9172 - val_loss: 118.8239\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1767 - val_loss: 99.7612\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0159 - val_loss: 98.1182\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0575 - val_loss: 124.6445\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8768 - val_loss: 92.3859\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8942 - val_loss: 93.0677\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8699 - val_loss: 92.9607\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8893 - val_loss: 93.9338\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0154 - val_loss: 95.5217\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1042 - val_loss: 102.7377\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9424 - val_loss: 105.5199\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8958 - val_loss: 89.4180\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8471 - val_loss: 91.9410\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8696 - val_loss: 89.1395\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9942 - val_loss: 97.7789\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9478 - val_loss: 94.4583\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0549 - val_loss: 104.0330\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8691 - val_loss: 100.4570\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1541 - val_loss: 97.9638\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8787 - val_loss: 103.8091\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8762 - val_loss: 111.3903\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1762 - val_loss: 92.3832\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7506 - val_loss: 94.9062\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0096 - val_loss: 94.3156\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9004 - val_loss: 91.5964\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9427 - val_loss: 121.3476\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8863 - val_loss: 93.8342\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7511 - val_loss: 126.9278\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9220 - val_loss: 118.1783\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8500 - val_loss: 92.9606\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8967 - val_loss: 94.5236\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7406 - val_loss: 107.4538\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7751 - val_loss: 94.2144\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7317 - val_loss: 96.5781\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7598 - val_loss: 85.9779\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5805 - val_loss: 92.8696\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9108 - val_loss: 96.5571\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8405 - val_loss: 96.7612\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8029 - val_loss: 97.7744\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9011 - val_loss: 98.4462\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7299 - val_loss: 96.7690\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4265 - val_loss: 99.8946\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.6057 - val_loss: 88.7049\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9417 - val_loss: 100.5983\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.6151 - val_loss: 96.9310\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8261 - val_loss: 95.6094\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9515 - val_loss: 130.6642\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8151 - val_loss: 116.7871\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7464 - val_loss: 85.7285\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8415 - val_loss: 92.2417\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4992 - val_loss: 94.5915\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5419 - val_loss: 104.2065\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7201 - val_loss: 99.8638\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5974 - val_loss: 88.4776\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7010 - val_loss: 92.2879\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5922 - val_loss: 86.6475\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.6204 - val_loss: 99.2253\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7165 - val_loss: 93.0836\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7042 - val_loss: 91.8210\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5823 - val_loss: 89.3393\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7575 - val_loss: 115.6229\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5963 - val_loss: 102.9424\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.6198 - val_loss: 93.7132\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.6392 - val_loss: 91.2568\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7200 - val_loss: 100.6330\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5099 - val_loss: 98.5561\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.6432 - val_loss: 99.2073\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7991 - val_loss: 91.0476\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3173 - val_loss: 109.1192\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.7126 - val_loss: 94.0538\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.6844 - val_loss: 88.8092\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5416 - val_loss: 95.3102\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.8210 - val_loss: 139.3139\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.6091 - val_loss: 90.1509\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4466 - val_loss: 91.0199\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4954 - val_loss: 86.8375\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5189 - val_loss: 118.0743\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5567 - val_loss: 91.5553\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4949 - val_loss: 93.8994\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3993 - val_loss: 111.8423\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5459 - val_loss: 94.1841\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.2392 - val_loss: 95.6232\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4165 - val_loss: 90.3299\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5602 - val_loss: 97.6006\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.6436 - val_loss: 91.6458\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.2989 - val_loss: 94.7424\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4599 - val_loss: 92.0678\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3496 - val_loss: 98.9258\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4222 - val_loss: 93.4735\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4467 - val_loss: 93.8283\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.2169 - val_loss: 91.9572\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3331 - val_loss: 90.2578\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3306 - val_loss: 100.9635\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3700 - val_loss: 88.8359\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4378 - val_loss: 95.6608\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4797 - val_loss: 107.2686\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5768 - val_loss: 95.2912\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4746 - val_loss: 98.2461\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4864 - val_loss: 88.8275\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.2965 - val_loss: 97.6232\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3731 - val_loss: 91.3912\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3437 - val_loss: 87.7722\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.2825 - val_loss: 108.9389\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.4829 - val_loss: 99.4677\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3877 - val_loss: 98.3781\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.2495 - val_loss: 90.9843\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3453 - val_loss: 101.2798\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.2871 - val_loss: 95.4275\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.5230 - val_loss: 95.3860\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.2061 - val_loss: 104.3725\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3818 - val_loss: 92.1127\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3647 - val_loss: 98.0185\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.3319 - val_loss: 102.7584\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531fbd97-6b4c-4971-ab7c-9b1ce083b0bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.9960508369253656 \n",
            "MAE:  7.585113053081205 \n",
            "SD:  10.08792844457644\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "cd2f399b-91c0-4c0b-d20c-ff111532b4ed"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e818a8f-0f5a-4f7c-f41a-45101e9aaa70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 32)                4096      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 32)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 32)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,441\n",
            "Trainable params: 5,313\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "scrolled": true,
        "id": "dcXAOqd2CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431cf1ae-89c1-481c-fcf4-ffbf7315faa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 12030.2725 - val_loss: 12115.6396\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10989.9619 - val_loss: 10805.0000\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9673.4609 - val_loss: 8940.3535\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 8019.8667 - val_loss: 7116.8359\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 6120.6797 - val_loss: 5385.1543\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 4192.4629 - val_loss: 3378.1206\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2530.7898 - val_loss: 2351.4080\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1325.2144 - val_loss: 776.3348\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 609.1476 - val_loss: 696.4110\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 270.8129 - val_loss: 393.4963\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 145.2674 - val_loss: 136.1790\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.8608 - val_loss: 119.8673\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 97.2414 - val_loss: 163.0080\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.4327 - val_loss: 110.6565\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.9954 - val_loss: 139.7354\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.2504 - val_loss: 109.6019\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.7397 - val_loss: 113.2187\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.5457 - val_loss: 102.2396\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.8222 - val_loss: 138.7263\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.1945 - val_loss: 120.7271\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.2999 - val_loss: 120.3632\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.0814 - val_loss: 111.0726\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.2729 - val_loss: 109.9717\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.9658 - val_loss: 112.4056\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.2717 - val_loss: 122.1550\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 82.9284 - val_loss: 110.4287\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.2143 - val_loss: 108.5996\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.3811 - val_loss: 116.3504\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 81.0049 - val_loss: 121.6075\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 81.0461 - val_loss: 130.9224\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.5379 - val_loss: 126.3805\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 80.3004 - val_loss: 108.1859\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.9335 - val_loss: 104.4409\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4297 - val_loss: 110.5280\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 78.8957 - val_loss: 98.5287\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.5247 - val_loss: 100.2859\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 78.3669 - val_loss: 100.9387\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 77.9592 - val_loss: 127.9622\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 77.8292 - val_loss: 109.8681\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 77.5612 - val_loss: 105.2393\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9591 - val_loss: 122.9477\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0923 - val_loss: 100.9780\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 76.4339 - val_loss: 110.6167\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1843 - val_loss: 138.0649\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 76.1741 - val_loss: 119.1071\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1848 - val_loss: 100.9452\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 75.6000 - val_loss: 96.8605\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 75.4551 - val_loss: 114.8497\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 75.4677 - val_loss: 103.8195\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9962 - val_loss: 104.1062\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.2940 - val_loss: 96.0574\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 74.8736 - val_loss: 94.6979\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5721 - val_loss: 112.9989\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 74.6419 - val_loss: 97.9363\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 74.2725 - val_loss: 118.6933\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 74.0852 - val_loss: 106.2102\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0325 - val_loss: 101.6964\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 73.9855 - val_loss: 125.1471\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 73.6540 - val_loss: 94.5854\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4835 - val_loss: 102.8120\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3600 - val_loss: 165.6470\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2350 - val_loss: 104.0498\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.9345 - val_loss: 121.8426\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.7543 - val_loss: 130.3768\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.8978 - val_loss: 128.4098\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4932 - val_loss: 113.7190\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.5021 - val_loss: 91.2640\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.6061 - val_loss: 104.9890\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.2786 - val_loss: 110.0176\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.9481 - val_loss: 111.7905\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0798 - val_loss: 110.7519\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 72.2770 - val_loss: 116.9773\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0420 - val_loss: 112.0633\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8446 - val_loss: 113.5829\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6382 - val_loss: 95.8239\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6181 - val_loss: 145.9083\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.3355 - val_loss: 126.8064\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.4729 - val_loss: 131.6892\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2422 - val_loss: 105.2855\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.3206 - val_loss: 102.8953\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.2671 - val_loss: 96.4561\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.2271 - val_loss: 101.5334\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.9951 - val_loss: 92.1265\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.0714 - val_loss: 100.2026\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7423 - val_loss: 114.6641\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8844 - val_loss: 108.6431\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.5233 - val_loss: 107.7813\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.3426 - val_loss: 92.8354\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5357 - val_loss: 104.4534\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2805 - val_loss: 106.4492\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0885 - val_loss: 98.4117\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5589 - val_loss: 110.8957\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.3101 - val_loss: 93.1677\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.0521 - val_loss: 96.6986\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.2571 - val_loss: 96.1626\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.9073 - val_loss: 114.0831\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.1401 - val_loss: 150.0034\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.9417 - val_loss: 96.4902\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.8924 - val_loss: 112.0733\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.0477 - val_loss: 97.5823\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.6904 - val_loss: 99.2692\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.7299 - val_loss: 91.9088\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.3328 - val_loss: 102.4854\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.5543 - val_loss: 110.9257\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.4821 - val_loss: 92.0075\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.5482 - val_loss: 98.3808\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.4012 - val_loss: 122.2007\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.3895 - val_loss: 107.4507\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.2350 - val_loss: 94.9395\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.2435 - val_loss: 108.0480\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.2146 - val_loss: 100.3018\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.2726 - val_loss: 99.2466\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.2326 - val_loss: 101.0192\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.1149 - val_loss: 105.9472\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 69.0473 - val_loss: 123.1259\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.8189 - val_loss: 105.1010\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.8906 - val_loss: 103.1476\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.7792 - val_loss: 140.3828\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.8769 - val_loss: 133.0751\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.7897 - val_loss: 103.4800\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.8680 - val_loss: 92.4025\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.6076 - val_loss: 115.9392\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.5963 - val_loss: 103.5044\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.7822 - val_loss: 95.7189\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.3975 - val_loss: 111.0194\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.5070 - val_loss: 103.9426\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.5678 - val_loss: 92.4031\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.3879 - val_loss: 94.2403\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.4750 - val_loss: 109.1106\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.6417 - val_loss: 106.3495\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.2432 - val_loss: 104.5666\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.4466 - val_loss: 103.8699\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.2558 - val_loss: 113.7852\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.2841 - val_loss: 94.8172\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.2173 - val_loss: 116.0515\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.2432 - val_loss: 152.1399\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.0848 - val_loss: 97.0528\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.2406 - val_loss: 106.2757\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 68.1022 - val_loss: 95.0299\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.9353 - val_loss: 108.6820\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.9191 - val_loss: 113.6436\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 68.1308 - val_loss: 117.6744\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.9597 - val_loss: 112.4589\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.6660 - val_loss: 102.6002\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.8368 - val_loss: 149.1485\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.7270 - val_loss: 102.2262\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.9643 - val_loss: 95.6487\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.7447 - val_loss: 99.4698\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.5718 - val_loss: 116.0487\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.6611 - val_loss: 92.4832\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.6069 - val_loss: 178.0079\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.7091 - val_loss: 107.4444\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.5910 - val_loss: 107.2491\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.5642 - val_loss: 104.9122\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.5486 - val_loss: 125.3583\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.4868 - val_loss: 94.2983\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.5084 - val_loss: 111.5801\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.4847 - val_loss: 91.8616\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.3533 - val_loss: 96.5129\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.3596 - val_loss: 95.8391\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.5475 - val_loss: 120.5574\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.4799 - val_loss: 103.5809\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.2062 - val_loss: 91.0762\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.3092 - val_loss: 102.1721\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.3251 - val_loss: 101.8596\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.1554 - val_loss: 112.7403\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.0017 - val_loss: 99.1222\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.2662 - val_loss: 130.8497\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.0912 - val_loss: 94.1401\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.0953 - val_loss: 95.6351\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 67.1549 - val_loss: 106.9665\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.9824 - val_loss: 112.5202\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.9962 - val_loss: 115.8588\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 67.0739 - val_loss: 107.8086\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.8379 - val_loss: 90.0359\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.9325 - val_loss: 98.6351\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.8531 - val_loss: 96.9405\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.7479 - val_loss: 100.4791\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.7731 - val_loss: 114.9914\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.7258 - val_loss: 93.2060\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 66.9870 - val_loss: 107.1874\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.8826 - val_loss: 121.1687\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 67.0057 - val_loss: 91.4742\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.7080 - val_loss: 98.4563\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.6733 - val_loss: 118.5341\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.7881 - val_loss: 145.0293\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.4875 - val_loss: 93.8360\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.5522 - val_loss: 96.7898\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.5468 - val_loss: 94.7613\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.5545 - val_loss: 112.8199\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.2344 - val_loss: 93.7411\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.5561 - val_loss: 102.7201\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.4362 - val_loss: 94.5782\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.4646 - val_loss: 93.6246\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.3795 - val_loss: 106.9563\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.3937 - val_loss: 115.5729\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.2795 - val_loss: 102.1319\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.4758 - val_loss: 91.7364\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.4699 - val_loss: 145.8692\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.3147 - val_loss: 104.7877\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.5500 - val_loss: 150.7495\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.5134 - val_loss: 114.7290\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.7201 - val_loss: 150.6669\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.2384 - val_loss: 97.5413\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.1943 - val_loss: 106.6581\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.4634 - val_loss: 133.4057\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.1721 - val_loss: 95.1374\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.2050 - val_loss: 103.4631\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.0676 - val_loss: 105.1796\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.2043 - val_loss: 94.9805\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.0842 - val_loss: 100.5873\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.0125 - val_loss: 102.2894\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.9881 - val_loss: 111.7433\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.9039 - val_loss: 183.8885\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.9150 - val_loss: 103.9938\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.9559 - val_loss: 111.0213\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.0551 - val_loss: 95.0712\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.2129 - val_loss: 104.7130\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 66.1966 - val_loss: 98.9446\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.7940 - val_loss: 128.2007\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.7561 - val_loss: 107.0343\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.0549 - val_loss: 104.2727\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.7905 - val_loss: 93.8100\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.7683 - val_loss: 95.2526\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.8470 - val_loss: 125.8751\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.9986 - val_loss: 88.0896\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.7081 - val_loss: 107.2996\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.7949 - val_loss: 95.3424\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.8871 - val_loss: 104.3787\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.8656 - val_loss: 99.1504\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5526 - val_loss: 114.4053\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.8534 - val_loss: 102.9849\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.9200 - val_loss: 93.4294\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5545 - val_loss: 101.3632\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.8924 - val_loss: 94.3683\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.7703 - val_loss: 96.6464\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.6073 - val_loss: 95.6123\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.6725 - val_loss: 114.0820\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.7548 - val_loss: 98.0488\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5557 - val_loss: 107.6170\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.5837 - val_loss: 109.4896\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.5756 - val_loss: 89.3027\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.8747 - val_loss: 117.8423\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5863 - val_loss: 145.9331\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.6638 - val_loss: 95.2255\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.4173 - val_loss: 111.8896\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.4071 - val_loss: 105.5007\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.5545 - val_loss: 99.2461\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.4076 - val_loss: 91.8741\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.4168 - val_loss: 106.0005\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.6008 - val_loss: 123.3633\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.4859 - val_loss: 91.6857\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3979 - val_loss: 104.1236\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.3740 - val_loss: 118.8536\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.4739 - val_loss: 90.0896\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3774 - val_loss: 117.1586\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.0573 - val_loss: 108.1605\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.5673 - val_loss: 97.4688\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.4317 - val_loss: 90.3696\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.2997 - val_loss: 120.2927\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.2237 - val_loss: 141.4334\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3530 - val_loss: 101.9387\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.2443 - val_loss: 111.8114\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.2079 - val_loss: 91.9158\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3983 - val_loss: 97.4701\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.1596 - val_loss: 103.0195\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9869 - val_loss: 97.7600\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.1921 - val_loss: 95.4886\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.2420 - val_loss: 97.3354\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.1942 - val_loss: 100.6047\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0555 - val_loss: 98.4778\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.3098 - val_loss: 117.1103\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9087 - val_loss: 95.9155\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.2133 - val_loss: 91.1300\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.2424 - val_loss: 97.6400\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9879 - val_loss: 93.7359\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9679 - val_loss: 94.5708\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.0743 - val_loss: 97.8393\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9409 - val_loss: 93.3772\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0360 - val_loss: 96.2411\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8946 - val_loss: 94.1006\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9658 - val_loss: 99.6164\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 65.0788 - val_loss: 110.5933\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.0855 - val_loss: 101.9159\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9505 - val_loss: 90.1455\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 65.1274 - val_loss: 94.9917\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8188 - val_loss: 97.0309\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.7603 - val_loss: 105.4639\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9073 - val_loss: 96.7522\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.7601 - val_loss: 99.6178\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.8872 - val_loss: 110.9456\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.8599 - val_loss: 96.8737\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.6743 - val_loss: 104.7227\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.8082 - val_loss: 98.1751\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6431 - val_loss: 98.7512\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.7873 - val_loss: 126.8033\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.7786 - val_loss: 99.3452\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.8767 - val_loss: 100.6852\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.8183 - val_loss: 105.7440\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5275 - val_loss: 105.3569\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.9153 - val_loss: 93.0538\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.6751 - val_loss: 95.6060\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.9241 - val_loss: 103.1486\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5378 - val_loss: 104.9277\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.6118 - val_loss: 99.4923\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.8376 - val_loss: 93.9524\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5454 - val_loss: 97.0389\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5891 - val_loss: 105.0179\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.8824 - val_loss: 90.1504\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.5939 - val_loss: 104.3988\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.7306 - val_loss: 117.9598\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.7689 - val_loss: 92.0359\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6630 - val_loss: 111.9996\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.7175 - val_loss: 96.7778\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5724 - val_loss: 99.9079\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3299 - val_loss: 92.0414\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5559 - val_loss: 105.5318\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5519 - val_loss: 99.2844\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.7078 - val_loss: 102.7866\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3309 - val_loss: 112.3165\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4689 - val_loss: 100.2725\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.5045 - val_loss: 94.8815\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4629 - val_loss: 146.8334\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.6130 - val_loss: 103.0628\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5832 - val_loss: 97.0054\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5146 - val_loss: 111.1811\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.4621 - val_loss: 102.5661\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.4454 - val_loss: 100.4301\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3579 - val_loss: 92.6232\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4652 - val_loss: 95.2841\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4113 - val_loss: 98.1104\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5029 - val_loss: 97.9853\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.6272 - val_loss: 94.8297\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5361 - val_loss: 103.8197\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.3911 - val_loss: 100.1770\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2570 - val_loss: 119.2349\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.2763 - val_loss: 89.3890\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.5122 - val_loss: 99.3716\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.2933 - val_loss: 109.8561\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.4369 - val_loss: 94.2885\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.4592 - val_loss: 108.2851\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3965 - val_loss: 110.2565\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2828 - val_loss: 95.3702\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.2662 - val_loss: 95.3755\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1686 - val_loss: 98.3591\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1324 - val_loss: 90.6467\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.2638 - val_loss: 92.5502\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3788 - val_loss: 89.2572\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3355 - val_loss: 93.1982\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.3649 - val_loss: 95.3574\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1911 - val_loss: 90.9622\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.0843 - val_loss: 113.3085\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3100 - val_loss: 93.3584\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3329 - val_loss: 93.4266\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.2351 - val_loss: 102.5314\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.3008 - val_loss: 88.9715\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2103 - val_loss: 122.1078\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1916 - val_loss: 96.2930\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1182 - val_loss: 119.3347\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2296 - val_loss: 86.1577\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2134 - val_loss: 93.8861\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0770 - val_loss: 121.4267\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9418 - val_loss: 95.1698\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9265 - val_loss: 94.6385\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.2285 - val_loss: 142.8970\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1770 - val_loss: 96.1025\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1022 - val_loss: 121.6381\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0258 - val_loss: 93.4740\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0541 - val_loss: 100.9241\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3866 - val_loss: 94.4359\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9543 - val_loss: 97.8821\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.3344 - val_loss: 104.2811\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 64.0325 - val_loss: 134.9736\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0437 - val_loss: 99.0077\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.9292 - val_loss: 120.1636\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1485 - val_loss: 87.2222\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1267 - val_loss: 153.8362\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0879 - val_loss: 132.5674\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.9215 - val_loss: 100.7116\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0307 - val_loss: 103.9011\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8865 - val_loss: 93.8571\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8231 - val_loss: 90.4427\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0658 - val_loss: 96.5493\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.1547 - val_loss: 87.0923\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9368 - val_loss: 90.9061\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9088 - val_loss: 92.2120\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8344 - val_loss: 102.5672\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9876 - val_loss: 114.1103\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7710 - val_loss: 103.9365\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0821 - val_loss: 143.7434\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9856 - val_loss: 95.8904\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7903 - val_loss: 96.7145\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6429 - val_loss: 98.5606\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7813 - val_loss: 183.5782\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9696 - val_loss: 91.9355\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9037 - val_loss: 89.9425\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0021 - val_loss: 92.3432\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9387 - val_loss: 94.3052\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0175 - val_loss: 94.4995\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7993 - val_loss: 219.8599\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9935 - val_loss: 109.5463\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9212 - val_loss: 97.2838\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7657 - val_loss: 105.9540\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6309 - val_loss: 91.6116\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7944 - val_loss: 90.5500\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7493 - val_loss: 118.7015\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6986 - val_loss: 111.5461\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8746 - val_loss: 128.9223\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7832 - val_loss: 134.1243\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5907 - val_loss: 89.9125\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7216 - val_loss: 97.5829\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7251 - val_loss: 88.5334\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6279 - val_loss: 132.7437\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.8325 - val_loss: 99.9386\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.9196 - val_loss: 95.3481\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6760 - val_loss: 91.8450\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 64.0716 - val_loss: 101.7934\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7573 - val_loss: 107.0745\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6320 - val_loss: 102.4596\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7088 - val_loss: 89.8629\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5281 - val_loss: 94.4254\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6943 - val_loss: 101.9579\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5442 - val_loss: 102.1787\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5538 - val_loss: 94.9166\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.5439 - val_loss: 94.1410\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4448 - val_loss: 93.4726\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6070 - val_loss: 88.7161\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6660 - val_loss: 93.4910\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6547 - val_loss: 92.6019\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5773 - val_loss: 92.4692\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5171 - val_loss: 89.0614\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5286 - val_loss: 93.4468\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6088 - val_loss: 93.3893\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6776 - val_loss: 107.4128\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7216 - val_loss: 102.1905\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4928 - val_loss: 99.1703\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.7450 - val_loss: 103.6283\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6241 - val_loss: 94.5212\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6306 - val_loss: 90.2477\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4933 - val_loss: 128.6275\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3871 - val_loss: 101.7809\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4979 - val_loss: 92.1334\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4989 - val_loss: 88.2006\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6103 - val_loss: 109.2180\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4449 - val_loss: 96.5271\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3548 - val_loss: 202.3026\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.4975 - val_loss: 101.2853\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6736 - val_loss: 95.5598\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.6960 - val_loss: 86.8493\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3647 - val_loss: 95.1258\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5471 - val_loss: 100.2753\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2848 - val_loss: 109.7181\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5415 - val_loss: 147.1059\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3482 - val_loss: 94.4933\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5010 - val_loss: 98.2190\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4630 - val_loss: 102.3622\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3438 - val_loss: 97.4160\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3556 - val_loss: 95.2187\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 63.3332 - val_loss: 88.7492\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4324 - val_loss: 96.7257\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4985 - val_loss: 157.8791\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3560 - val_loss: 96.7039\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4100 - val_loss: 92.0489\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2868 - val_loss: 109.4045\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2608 - val_loss: 121.9250\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.5085 - val_loss: 90.3868\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1999 - val_loss: 93.5187\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 63.4510 - val_loss: 87.7482\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 63.3616 - val_loss: 92.7845\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.2851 - val_loss: 104.3094\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2174 - val_loss: 94.3735\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1644 - val_loss: 93.6001\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3590 - val_loss: 106.1013\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3910 - val_loss: 88.2731\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2109 - val_loss: 113.2849\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2587 - val_loss: 104.3668\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3237 - val_loss: 101.3103\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3015 - val_loss: 92.6349\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2768 - val_loss: 96.9015\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2786 - val_loss: 94.5197\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2194 - val_loss: 108.9365\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.4126 - val_loss: 94.4841\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1173 - val_loss: 99.7174\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1744 - val_loss: 100.9395\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0475 - val_loss: 86.0280\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1839 - val_loss: 93.3237\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2627 - val_loss: 94.1752\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.3088 - val_loss: 109.4728\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1321 - val_loss: 108.2113\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2030 - val_loss: 94.2768\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1810 - val_loss: 97.6989\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0896 - val_loss: 98.8445\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.0521 - val_loss: 96.9881\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9876 - val_loss: 132.7672\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1717 - val_loss: 99.4356\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9675 - val_loss: 95.8663\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1216 - val_loss: 98.6375\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.1197 - val_loss: 90.8420\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9770 - val_loss: 112.1959\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 63.2519 - val_loss: 98.3512\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "696v_fuFCTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64fa364-507c-4d4b-eaf7-bdbc531d49b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.8939835739578302 \n",
            "MAE:  7.488419794226039 \n",
            "SD:  9.876839017877794\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "9915d5af-e14b-4686-c209-94f3d102c534"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "mdZF2osWCUQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f27edac-660d-4e6f-94db-a4b0c435c34b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -0.39652353508618193 \n",
            "Ensemble_std:  9.772483801639014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd27a397-b9e7-4c2a-dd27-f28e38700350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_9 (Dense)             (None, 32)                4096      \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 32)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 32)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,441\n",
            "Trainable params: 5,313\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa707486-a370-43db-a1a2-7806f855cd09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 3657.1125 - val_loss: 3593.8169\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3174.2769 - val_loss: 3021.8257\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2592.9380 - val_loss: 2278.4934\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1917.0446 - val_loss: 1417.3844\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1277.5571 - val_loss: 859.7516\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 775.1625 - val_loss: 542.1976\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 414.1298 - val_loss: 167.9816\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 180.9632 - val_loss: 134.1308\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.2761 - val_loss: 94.8239\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 41.5719 - val_loss: 51.7338\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.1550 - val_loss: 62.2315\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.5943 - val_loss: 40.6394\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2809 - val_loss: 43.3208\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3843 - val_loss: 43.0429\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1548 - val_loss: 62.5213\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5609 - val_loss: 48.7879\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.4849 - val_loss: 39.4944\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0560 - val_loss: 35.1859\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9335 - val_loss: 42.9916\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9262 - val_loss: 40.6107\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.4552 - val_loss: 39.7150\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3701 - val_loss: 43.2527\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.0976 - val_loss: 48.0012\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9229 - val_loss: 34.7205\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9656 - val_loss: 39.7164\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8747 - val_loss: 36.1032\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 29.9670 - val_loss: 38.0754\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4503 - val_loss: 37.2203\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 29.6708 - val_loss: 36.8574\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3592 - val_loss: 44.7733\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 29.1435 - val_loss: 63.7582\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.9911 - val_loss: 35.7454\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7611 - val_loss: 33.9337\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8537 - val_loss: 39.5899\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.6971 - val_loss: 41.0697\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.5412 - val_loss: 47.3858\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.4927 - val_loss: 38.4756\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3662 - val_loss: 37.8450\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5566 - val_loss: 41.0740\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.4148 - val_loss: 36.3436\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.2224 - val_loss: 34.0755\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.1738 - val_loss: 44.0141\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0234 - val_loss: 34.6776\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9325 - val_loss: 41.7547\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.9752 - val_loss: 35.6829\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.7687 - val_loss: 38.7495\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.7638 - val_loss: 37.2686\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.6787 - val_loss: 35.1721\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6364 - val_loss: 34.7574\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6432 - val_loss: 40.6842\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.5239 - val_loss: 39.4256\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3815 - val_loss: 72.2853\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.3341 - val_loss: 36.0308\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.3122 - val_loss: 40.9269\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2384 - val_loss: 40.9639\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.2094 - val_loss: 35.2770\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.0936 - val_loss: 37.4084\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.1653 - val_loss: 37.0765\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.1233 - val_loss: 36.7427\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0511 - val_loss: 36.0218\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9270 - val_loss: 37.4853\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8872 - val_loss: 52.3677\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.0121 - val_loss: 40.7459\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.8463 - val_loss: 41.5294\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.7596 - val_loss: 34.6265\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.7499 - val_loss: 32.4781\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8328 - val_loss: 46.3828\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.6770 - val_loss: 36.0588\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.7263 - val_loss: 39.8743\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.6187 - val_loss: 36.2955\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5925 - val_loss: 34.7199\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.5982 - val_loss: 42.9655\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6018 - val_loss: 34.5909\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.4892 - val_loss: 37.4272\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.4768 - val_loss: 39.7974\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.3783 - val_loss: 39.6946\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4353 - val_loss: 38.3309\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.3390 - val_loss: 32.8461\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.3119 - val_loss: 46.7644\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.3240 - val_loss: 50.3696\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.3052 - val_loss: 35.4831\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.3104 - val_loss: 36.4871\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.2356 - val_loss: 40.0494\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.2696 - val_loss: 40.4991\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.1811 - val_loss: 49.8083\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.2289 - val_loss: 41.4113\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.1021 - val_loss: 41.3140\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.1260 - val_loss: 37.8797\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.1612 - val_loss: 35.7960\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.1329 - val_loss: 37.3385\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.0069 - val_loss: 37.1131\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.0209 - val_loss: 35.5323\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.9827 - val_loss: 35.1270\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.0457 - val_loss: 36.8851\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.9910 - val_loss: 32.9403\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.9744 - val_loss: 34.5154\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.9231 - val_loss: 37.3553\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.9348 - val_loss: 41.8005\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.9066 - val_loss: 33.7747\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.8218 - val_loss: 41.7881\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.8238 - val_loss: 36.5327\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.7537 - val_loss: 37.8206\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.8011 - val_loss: 34.9062\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.7287 - val_loss: 41.7357\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.7215 - val_loss: 42.4497\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.7768 - val_loss: 33.9623\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.6502 - val_loss: 33.7612\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.6598 - val_loss: 33.2687\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.6563 - val_loss: 37.6405\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.6405 - val_loss: 44.2748\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5955 - val_loss: 39.7138\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5972 - val_loss: 39.7559\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5038 - val_loss: 35.0877\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5960 - val_loss: 48.8552\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.6203 - val_loss: 37.3158\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.5267 - val_loss: 38.0034\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5492 - val_loss: 37.8329\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4705 - val_loss: 35.0474\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5405 - val_loss: 45.6445\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.4376 - val_loss: 35.0865\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4011 - val_loss: 35.9851\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3939 - val_loss: 50.2999\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3730 - val_loss: 34.4378\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4153 - val_loss: 35.9970\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.4178 - val_loss: 36.5231\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3289 - val_loss: 41.7468\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3731 - val_loss: 44.9485\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3314 - val_loss: 34.3342\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.3790 - val_loss: 34.9760\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.2664 - val_loss: 34.8454\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3064 - val_loss: 34.4305\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.2511 - val_loss: 36.0501\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3128 - val_loss: 33.7614\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.2724 - val_loss: 33.7211\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.2878 - val_loss: 45.7075\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.2528 - val_loss: 40.6124\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.2196 - val_loss: 39.3420\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.2122 - val_loss: 35.5448\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1964 - val_loss: 37.0825\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1246 - val_loss: 35.7963\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.1753 - val_loss: 38.4699\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1095 - val_loss: 35.2174\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.2179 - val_loss: 35.8982\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.2073 - val_loss: 35.5493\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.0431 - val_loss: 36.4876\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.0534 - val_loss: 38.5205\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1041 - val_loss: 36.1024\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1442 - val_loss: 34.3679\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.0850 - val_loss: 33.4904\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.0076 - val_loss: 33.5923\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.9496 - val_loss: 38.9900\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.9605 - val_loss: 35.9846\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.9558 - val_loss: 39.6793\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.0401 - val_loss: 41.1362\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.9781 - val_loss: 39.1195\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.0102 - val_loss: 36.0571\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.0142 - val_loss: 38.5061\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8975 - val_loss: 45.6917\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.9994 - val_loss: 36.4582\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.9633 - val_loss: 34.9859\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8763 - val_loss: 41.0405\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.9340 - val_loss: 42.4134\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8762 - val_loss: 34.2734\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8433 - val_loss: 40.8542\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.9221 - val_loss: 38.4271\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8851 - val_loss: 36.6891\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8851 - val_loss: 33.0974\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8433 - val_loss: 35.0819\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8853 - val_loss: 33.8030\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8100 - val_loss: 38.1187\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8079 - val_loss: 35.6337\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8012 - val_loss: 38.6345\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7704 - val_loss: 32.2923\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.7095 - val_loss: 35.5921\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8053 - val_loss: 39.3051\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6831 - val_loss: 37.8762\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8015 - val_loss: 36.4598\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7931 - val_loss: 35.5089\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7228 - val_loss: 33.7627\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.7737 - val_loss: 44.0745\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7454 - val_loss: 39.6937\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6684 - val_loss: 36.7949\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6583 - val_loss: 34.6262\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.7111 - val_loss: 41.3980\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6759 - val_loss: 35.8487\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6580 - val_loss: 50.6380\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7145 - val_loss: 35.7076\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6892 - val_loss: 34.3293\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6694 - val_loss: 37.7488\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6769 - val_loss: 35.7651\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6567 - val_loss: 35.3751\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6816 - val_loss: 35.8226\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7003 - val_loss: 34.2463\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6127 - val_loss: 39.9706\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6520 - val_loss: 33.5657\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5719 - val_loss: 35.4946\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5785 - val_loss: 33.9009\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6050 - val_loss: 34.5111\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5693 - val_loss: 36.5989\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5762 - val_loss: 36.3984\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5720 - val_loss: 33.6246\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5962 - val_loss: 35.7188\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6062 - val_loss: 35.5972\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5415 - val_loss: 39.1756\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5915 - val_loss: 38.7640\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5026 - val_loss: 38.1002\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5219 - val_loss: 53.2612\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5836 - val_loss: 37.7185\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5787 - val_loss: 35.7841\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5177 - val_loss: 38.2370\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4767 - val_loss: 35.5021\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5557 - val_loss: 36.0193\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4894 - val_loss: 33.8825\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4335 - val_loss: 39.1433\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4628 - val_loss: 38.8436\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4857 - val_loss: 36.0447\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4882 - val_loss: 42.0037\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4753 - val_loss: 38.3191\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4825 - val_loss: 43.8174\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4769 - val_loss: 36.6850\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4165 - val_loss: 33.9883\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4155 - val_loss: 37.3628\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4369 - val_loss: 34.1957\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4965 - val_loss: 36.8330\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4719 - val_loss: 34.6325\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4346 - val_loss: 33.4090\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3896 - val_loss: 33.6994\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3280 - val_loss: 36.9355\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3509 - val_loss: 40.3594\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4316 - val_loss: 37.4636\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4684 - val_loss: 38.5317\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3624 - val_loss: 43.3957\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3989 - val_loss: 33.1140\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3966 - val_loss: 32.9073\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3867 - val_loss: 37.0442\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4190 - val_loss: 36.5469\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3561 - val_loss: 35.2493\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3206 - val_loss: 37.6316\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3602 - val_loss: 39.3432\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3548 - val_loss: 37.2275\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3220 - val_loss: 33.5557\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3024 - val_loss: 42.0180\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3737 - val_loss: 32.1356\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3851 - val_loss: 37.2541\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3890 - val_loss: 35.2428\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3410 - val_loss: 35.6545\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3320 - val_loss: 40.6342\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3511 - val_loss: 36.2883\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2244 - val_loss: 33.7571\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 24.3192 - val_loss: 48.8019\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 24.2918 - val_loss: 39.0188\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 24.3055 - val_loss: 41.7742\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2630 - val_loss: 38.9039\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2862 - val_loss: 37.7986\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2283 - val_loss: 34.9458\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2608 - val_loss: 37.2478\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2837 - val_loss: 44.3519\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1998 - val_loss: 51.4263\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2640 - val_loss: 34.9559\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2989 - val_loss: 35.4393\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2413 - val_loss: 40.1693\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2496 - val_loss: 34.8107\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2193 - val_loss: 42.2990\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2367 - val_loss: 37.2554\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2585 - val_loss: 32.9260\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3080 - val_loss: 35.2916\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2515 - val_loss: 37.2723\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2406 - val_loss: 35.5191\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1211 - val_loss: 35.1314\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1840 - val_loss: 37.6093\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2130 - val_loss: 33.0747\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2677 - val_loss: 35.2454\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2455 - val_loss: 38.8862\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2256 - val_loss: 34.7132\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1489 - val_loss: 32.4776\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1361 - val_loss: 44.6981\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3036 - val_loss: 34.1269\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1173 - val_loss: 36.0439\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1641 - val_loss: 33.8549\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2091 - val_loss: 41.3152\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1642 - val_loss: 32.8122\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1840 - val_loss: 35.8551\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1741 - val_loss: 41.6837\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2488 - val_loss: 38.2955\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1208 - val_loss: 35.3354\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1210 - val_loss: 37.8266\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1543 - val_loss: 35.9897\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1046 - val_loss: 38.4134\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0832 - val_loss: 41.9736\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1639 - val_loss: 33.5754\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1751 - val_loss: 32.3032\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1401 - val_loss: 35.9117\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1682 - val_loss: 34.0179\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1148 - val_loss: 38.2185\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1118 - val_loss: 44.8662\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0878 - val_loss: 40.1256\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0762 - val_loss: 41.9465\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0905 - val_loss: 46.9392\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.0718 - val_loss: 34.7147\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0893 - val_loss: 34.2346\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0689 - val_loss: 34.7413\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0329 - val_loss: 41.3269\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0997 - val_loss: 35.2656\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1160 - val_loss: 37.0786\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0498 - val_loss: 36.0066\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0776 - val_loss: 34.6799\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0549 - val_loss: 37.3504\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9621 - val_loss: 39.3009\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0551 - val_loss: 39.1666\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0304 - val_loss: 34.1963\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0366 - val_loss: 35.8605\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0678 - val_loss: 32.8373\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0552 - val_loss: 41.0404\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0870 - val_loss: 38.3889\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0080 - val_loss: 36.2676\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0552 - val_loss: 47.9050\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0811 - val_loss: 34.7698\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9591 - val_loss: 36.9590\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1017 - val_loss: 36.0971\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.0582 - val_loss: 39.4954\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1080 - val_loss: 40.6374\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0490 - val_loss: 32.7625\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.0057 - val_loss: 34.4327\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9403 - val_loss: 38.3602\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0003 - val_loss: 42.0734\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0541 - val_loss: 50.7488\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.9653 - val_loss: 40.4725\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0073 - val_loss: 36.2479\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.9814 - val_loss: 46.3041\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9692 - val_loss: 35.9489\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.0364 - val_loss: 33.3191\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9942 - val_loss: 56.2559\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9650 - val_loss: 54.0530\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0284 - val_loss: 37.9044\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0496 - val_loss: 41.1961\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9338 - val_loss: 33.2787\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8926 - val_loss: 38.9961\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9374 - val_loss: 40.0162\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9430 - val_loss: 37.6527\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.9691 - val_loss: 49.3681\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9996 - val_loss: 35.6205\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0210 - val_loss: 37.7741\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9362 - val_loss: 32.0595\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.9699 - val_loss: 33.7824\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.9002 - val_loss: 34.4504\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9900 - val_loss: 48.1411\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9637 - val_loss: 32.9506\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.9887 - val_loss: 37.7132\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9642 - val_loss: 37.0934\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9354 - val_loss: 35.2420\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0447 - val_loss: 34.5121\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9018 - val_loss: 35.0789\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9342 - val_loss: 37.4984\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9661 - val_loss: 38.2851\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9499 - val_loss: 34.2781\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8957 - val_loss: 36.3615\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9177 - val_loss: 33.6696\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9461 - val_loss: 34.1091\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9036 - val_loss: 39.8510\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9303 - val_loss: 34.7375\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9661 - val_loss: 38.7471\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8726 - val_loss: 40.5008\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8857 - val_loss: 37.7626\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8854 - val_loss: 35.3804\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9095 - val_loss: 35.9352\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9212 - val_loss: 34.0476\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8389 - val_loss: 32.2971\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9229 - val_loss: 33.7021\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8643 - val_loss: 34.8933\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.9093 - val_loss: 33.7279\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8444 - val_loss: 35.4092\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8776 - val_loss: 35.1187\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9310 - val_loss: 37.1014\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8930 - val_loss: 32.7715\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8502 - val_loss: 41.5240\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9414 - val_loss: 32.4073\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8751 - val_loss: 36.0364\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9052 - val_loss: 34.6530\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8711 - val_loss: 33.3881\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.8172 - val_loss: 38.9523\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8141 - val_loss: 39.5620\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8742 - val_loss: 34.6577\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9362 - val_loss: 58.2338\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8211 - val_loss: 39.9411\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7967 - val_loss: 33.6409\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8205 - val_loss: 32.9645\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9225 - val_loss: 39.0453\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8646 - val_loss: 39.0714\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8742 - val_loss: 35.7543\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9158 - val_loss: 33.6983\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8623 - val_loss: 34.6629\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8023 - val_loss: 40.8154\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8372 - val_loss: 34.6526\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8454 - val_loss: 36.2702\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9181 - val_loss: 34.0200\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8003 - val_loss: 41.7698\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8163 - val_loss: 35.1178\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8760 - val_loss: 38.5515\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8730 - val_loss: 39.4185\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7903 - val_loss: 35.0545\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7408 - val_loss: 36.2653\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7504 - val_loss: 35.6681\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7445 - val_loss: 40.0084\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7891 - val_loss: 35.0983\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7745 - val_loss: 32.9961\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7765 - val_loss: 36.2718\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8153 - val_loss: 33.7976\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7974 - val_loss: 34.0864\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7780 - val_loss: 37.5249\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7820 - val_loss: 31.9800\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8022 - val_loss: 39.7669\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7472 - val_loss: 37.2747\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8456 - val_loss: 33.4600\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8139 - val_loss: 32.7855\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8595 - val_loss: 40.6687\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7704 - val_loss: 37.8453\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7891 - val_loss: 34.6063\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8250 - val_loss: 46.4817\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7502 - val_loss: 35.6528\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7794 - val_loss: 37.7796\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7543 - val_loss: 34.5642\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7956 - val_loss: 37.7681\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6850 - val_loss: 34.5015\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7250 - val_loss: 40.9569\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7623 - val_loss: 33.3863\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7393 - val_loss: 37.6648\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6856 - val_loss: 37.0330\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7849 - val_loss: 36.7813\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8360 - val_loss: 33.0585\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8097 - val_loss: 33.9093\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7371 - val_loss: 34.5255\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7957 - val_loss: 46.1191\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7566 - val_loss: 34.1981\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6995 - val_loss: 34.5079\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7287 - val_loss: 33.2417\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7690 - val_loss: 32.7689\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7205 - val_loss: 35.4324\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7302 - val_loss: 34.5778\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8048 - val_loss: 37.6480\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7705 - val_loss: 35.8611\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7982 - val_loss: 38.8845\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7524 - val_loss: 37.0255\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7313 - val_loss: 34.2840\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7349 - val_loss: 36.4246\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7308 - val_loss: 33.6271\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7632 - val_loss: 43.3788\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7078 - val_loss: 46.0277\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7547 - val_loss: 35.2364\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7055 - val_loss: 46.1160\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7348 - val_loss: 34.3186\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6831 - val_loss: 35.4719\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7140 - val_loss: 33.4391\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7414 - val_loss: 42.3742\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6810 - val_loss: 41.4487\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7602 - val_loss: 32.9201\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6434 - val_loss: 33.7376\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7419 - val_loss: 38.0396\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7760 - val_loss: 37.3414\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7202 - val_loss: 43.7888\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6444 - val_loss: 34.9867\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7012 - val_loss: 33.3946\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6793 - val_loss: 36.0901\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7548 - val_loss: 36.4941\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6778 - val_loss: 41.2397\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7477 - val_loss: 40.4404\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6956 - val_loss: 35.1357\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7602 - val_loss: 37.2752\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6646 - val_loss: 39.2302\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6554 - val_loss: 40.9087\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7418 - val_loss: 35.8164\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.7418 - val_loss: 40.5146\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6954 - val_loss: 38.2575\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7127 - val_loss: 35.2900\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6856 - val_loss: 35.5547\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7270 - val_loss: 36.9889\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6545 - val_loss: 34.8805\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6272 - val_loss: 34.5001\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7000 - val_loss: 41.2899\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7219 - val_loss: 35.5517\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.5921 - val_loss: 35.2712\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6498 - val_loss: 37.6850\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6507 - val_loss: 37.3186\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.5738 - val_loss: 36.0499\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6392 - val_loss: 36.4303\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6989 - val_loss: 33.8981\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6505 - val_loss: 36.5247\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6660 - val_loss: 38.4439\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6971 - val_loss: 35.3572\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6548 - val_loss: 36.9448\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6426 - val_loss: 33.5512\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6791 - val_loss: 38.4061\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6257 - val_loss: 33.0835\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6321 - val_loss: 36.1584\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6719 - val_loss: 35.8949\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.5932 - val_loss: 34.7957\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6096 - val_loss: 33.2909\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6288 - val_loss: 34.4350\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6860 - val_loss: 35.6171\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.5928 - val_loss: 32.9655\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.6666 - val_loss: 37.5602\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4be9193-40be-4995-987d-e06e924471ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  1.8564019141810588 \n",
            "MAE:  4.636168160506653 \n",
            "SD:  5.840721313059367\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0cip38xZOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "ec353c0d-7cbd-4054-d7af-302150f02c10"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73996a53-04ee-46fb-de5c-52aa06d7b089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 32)                4096      \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 32)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 32)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,441\n",
            "Trainable params: 5,313\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146bef55-4b93-4a77-9953-f1eb7dfa7223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3509.4155 - val_loss: 3387.6411\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2895.6743 - val_loss: 2709.4790\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2153.1313 - val_loss: 1595.4729\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1378.4954 - val_loss: 688.0352\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 725.7821 - val_loss: 466.6091\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 307.7558 - val_loss: 148.6232\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 115.4797 - val_loss: 99.2164\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 53.5582 - val_loss: 46.3091\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.4787 - val_loss: 53.4473\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3050 - val_loss: 41.6935\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8322 - val_loss: 39.5275\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2421 - val_loss: 47.3679\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.6006 - val_loss: 54.1736\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.1124 - val_loss: 46.5484\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9813 - val_loss: 38.6426\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.5636 - val_loss: 36.9475\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2629 - val_loss: 54.0119\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1359 - val_loss: 37.7801\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8737 - val_loss: 43.9954\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6599 - val_loss: 48.1362\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.4825 - val_loss: 50.1755\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.2961 - val_loss: 41.0586\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1101 - val_loss: 37.0689\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7570 - val_loss: 56.1590\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5792 - val_loss: 39.0821\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5031 - val_loss: 47.4689\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3103 - val_loss: 37.0775\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2240 - val_loss: 39.8285\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9705 - val_loss: 42.6699\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0021 - val_loss: 44.0685\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.7712 - val_loss: 37.3579\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.6735 - val_loss: 39.4683\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.7068 - val_loss: 38.3041\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.4231 - val_loss: 38.5034\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.5291 - val_loss: 39.5044\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.2605 - val_loss: 52.2672\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2849 - val_loss: 54.4607\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.1687 - val_loss: 41.7219\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0935 - val_loss: 41.7389\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0810 - val_loss: 36.3644\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 28.0198 - val_loss: 42.3896\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.9084 - val_loss: 55.1955\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8922 - val_loss: 39.8420\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7580 - val_loss: 38.8883\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.7296 - val_loss: 37.1709\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.6567 - val_loss: 33.6293\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.5784 - val_loss: 34.8644\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.6019 - val_loss: 40.9920\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.5527 - val_loss: 51.9282\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5531 - val_loss: 36.9520\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3890 - val_loss: 43.1398\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3650 - val_loss: 40.7978\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.2649 - val_loss: 34.5931\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.1995 - val_loss: 35.7608\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.1963 - val_loss: 36.8351\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.2366 - val_loss: 34.1116\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.0955 - val_loss: 51.2889\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 27.1462 - val_loss: 42.5175\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0465 - val_loss: 45.8079\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0162 - val_loss: 36.1905\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.9439 - val_loss: 46.2595\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9479 - val_loss: 34.6106\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9805 - val_loss: 35.4647\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8696 - val_loss: 39.0931\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.7965 - val_loss: 39.7925\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8169 - val_loss: 40.0906\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.7965 - val_loss: 36.9412\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.6841 - val_loss: 33.9965\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.6630 - val_loss: 38.2423\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.6892 - val_loss: 42.1113\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6284 - val_loss: 36.2999\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.6655 - val_loss: 38.5457\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.6530 - val_loss: 38.1215\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.5898 - val_loss: 65.3946\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.5435 - val_loss: 44.1440\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4967 - val_loss: 52.3890\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.4541 - val_loss: 38.2869\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.4331 - val_loss: 74.2188\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.4178 - val_loss: 43.7636\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.4055 - val_loss: 37.6104\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.3737 - val_loss: 37.9667\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4025 - val_loss: 39.7609\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.4085 - val_loss: 46.9322\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.3055 - val_loss: 41.6104\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.3252 - val_loss: 35.5548\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.3620 - val_loss: 58.1890\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.2660 - val_loss: 37.1497\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.2433 - val_loss: 38.8559\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.2608 - val_loss: 33.1251\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.1909 - val_loss: 47.5805\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.1435 - val_loss: 42.4590\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.1288 - val_loss: 38.9834\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.1053 - val_loss: 36.5003\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.1361 - val_loss: 52.9560\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.1120 - val_loss: 36.3693\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 26.0727 - val_loss: 35.0299\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.0355 - val_loss: 45.9516\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.9831 - val_loss: 43.6895\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.9394 - val_loss: 36.2600\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.0866 - val_loss: 41.3608\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.9980 - val_loss: 37.0725\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.9888 - val_loss: 62.9396\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.9059 - val_loss: 39.6378\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.9073 - val_loss: 43.9005\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.8969 - val_loss: 36.9936\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.8715 - val_loss: 38.0567\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.8963 - val_loss: 36.0231\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.8857 - val_loss: 34.0153\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.8347 - val_loss: 37.4054\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.8327 - val_loss: 40.3543\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.7971 - val_loss: 38.4548\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.8159 - val_loss: 34.2098\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.7356 - val_loss: 37.1127\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.7629 - val_loss: 39.4350\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.7580 - val_loss: 35.4169\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.6688 - val_loss: 51.4278\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.6712 - val_loss: 36.3899\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.7535 - val_loss: 37.1958\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.7291 - val_loss: 34.3683\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.6482 - val_loss: 39.1009\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.7134 - val_loss: 34.3134\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.6231 - val_loss: 36.6410\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5812 - val_loss: 35.0913\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.6856 - val_loss: 34.8140\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.6960 - val_loss: 43.8961\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.5701 - val_loss: 37.9673\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.5330 - val_loss: 35.6234\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.5764 - val_loss: 36.0729\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.4884 - val_loss: 38.3019\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4769 - val_loss: 56.8614\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5012 - val_loss: 38.0282\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5259 - val_loss: 38.8313\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4896 - val_loss: 39.0037\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5686 - val_loss: 50.3090\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4646 - val_loss: 41.1502\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.5050 - val_loss: 77.5582\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.5386 - val_loss: 35.6403\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.4537 - val_loss: 36.5774\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3670 - val_loss: 43.1828\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4271 - val_loss: 57.2140\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.4647 - val_loss: 34.3893\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3753 - val_loss: 40.6885\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4283 - val_loss: 34.7292\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.3898 - val_loss: 35.0539\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4571 - val_loss: 37.7466\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3644 - val_loss: 32.5333\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3436 - val_loss: 40.1895\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.4078 - val_loss: 33.4484\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.3201 - val_loss: 39.8229\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.3101 - val_loss: 34.3448\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.2533 - val_loss: 37.0038\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.3509 - val_loss: 39.8062\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.2815 - val_loss: 49.2101\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.2632 - val_loss: 42.7404\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.2377 - val_loss: 41.2503\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.2862 - val_loss: 35.9157\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.2956 - val_loss: 40.3757\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.2466 - val_loss: 40.0425\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.2277 - val_loss: 35.5540\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.2291 - val_loss: 36.2530\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.2589 - val_loss: 38.6650\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.3744 - val_loss: 35.2409\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1981 - val_loss: 55.1252\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.2063 - val_loss: 46.1030\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1373 - val_loss: 34.9687\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.0859 - val_loss: 37.7530\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.2455 - val_loss: 39.0007\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.1515 - val_loss: 36.6847\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.2352 - val_loss: 38.1772\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.1033 - val_loss: 36.5383\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.0997 - val_loss: 32.9948\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1193 - val_loss: 34.6313\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.0700 - val_loss: 48.5108\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.1226 - val_loss: 44.5741\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1787 - val_loss: 37.0964\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.0365 - val_loss: 39.0592\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1018 - val_loss: 36.6140\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.9981 - val_loss: 33.9499\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1766 - val_loss: 33.9211\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.1184 - val_loss: 40.6664\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.0261 - val_loss: 41.1821\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.0324 - val_loss: 40.1114\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.9858 - val_loss: 35.5163\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.9740 - val_loss: 37.9849\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 25.0151 - val_loss: 37.1807\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.0160 - val_loss: 36.6098\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.9760 - val_loss: 38.5606\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.9110 - val_loss: 34.8587\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.9578 - val_loss: 42.7815\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.9964 - val_loss: 39.6586\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.9295 - val_loss: 34.9177\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.9453 - val_loss: 35.2309\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.0227 - val_loss: 35.7612\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.9127 - val_loss: 35.1379\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8804 - val_loss: 36.5144\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8710 - val_loss: 34.4104\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.9450 - val_loss: 41.4865\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 25.0117 - val_loss: 34.6325\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.9475 - val_loss: 35.1146\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.9307 - val_loss: 43.6820\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8738 - val_loss: 38.1938\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.9119 - val_loss: 36.2555\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8330 - val_loss: 57.0118\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8680 - val_loss: 42.3634\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.9041 - val_loss: 34.8939\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8683 - val_loss: 35.2385\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8729 - val_loss: 33.9189\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8743 - val_loss: 37.4557\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8547 - val_loss: 43.5597\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8392 - val_loss: 36.4245\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8055 - val_loss: 37.0402\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8374 - val_loss: 34.9827\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8074 - val_loss: 38.5018\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.8177 - val_loss: 33.6718\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.8021 - val_loss: 46.1900\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.7923 - val_loss: 33.8333\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7909 - val_loss: 37.1208\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7911 - val_loss: 34.5935\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7909 - val_loss: 36.3343\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6876 - val_loss: 37.1331\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.7302 - val_loss: 46.8443\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.7487 - val_loss: 36.4653\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7263 - val_loss: 35.7048\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.7229 - val_loss: 35.1681\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.7863 - val_loss: 33.6806\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7089 - val_loss: 42.7953\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6905 - val_loss: 35.7871\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.7218 - val_loss: 33.7303\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6792 - val_loss: 38.0142\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.7116 - val_loss: 38.3308\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6524 - val_loss: 38.0706\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6932 - val_loss: 39.2762\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.7443 - val_loss: 36.9661\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7058 - val_loss: 35.9269\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7084 - val_loss: 32.7453\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6669 - val_loss: 34.9219\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6800 - val_loss: 37.9693\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6800 - val_loss: 40.7156\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6778 - val_loss: 41.1007\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6046 - val_loss: 35.6603\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6103 - val_loss: 34.2985\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.7180 - val_loss: 35.0585\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5930 - val_loss: 42.6572\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6735 - val_loss: 34.9423\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6218 - val_loss: 35.9873\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5858 - val_loss: 37.0439\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6507 - val_loss: 33.9115\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6097 - val_loss: 36.5891\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6947 - val_loss: 40.7605\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.6044 - val_loss: 41.3010\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5522 - val_loss: 33.5982\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.6276 - val_loss: 36.5497\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5326 - val_loss: 39.1904\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5327 - val_loss: 36.5903\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5892 - val_loss: 32.9546\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4875 - val_loss: 35.5619\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5582 - val_loss: 35.5795\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5142 - val_loss: 36.8678\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5716 - val_loss: 35.5866\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5371 - val_loss: 38.0505\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4738 - val_loss: 32.2925\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5192 - val_loss: 33.5699\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5433 - val_loss: 43.5621\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5232 - val_loss: 33.9956\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4768 - val_loss: 35.1689\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5535 - val_loss: 33.9571\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.5614 - val_loss: 34.9553\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4839 - val_loss: 39.4283\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4885 - val_loss: 34.8935\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4795 - val_loss: 35.2540\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4923 - val_loss: 35.2952\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4262 - val_loss: 35.0969\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4959 - val_loss: 77.4627\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4843 - val_loss: 38.9273\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4671 - val_loss: 37.0699\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4555 - val_loss: 34.4883\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.5179 - val_loss: 38.9469\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4528 - val_loss: 58.5056\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4612 - val_loss: 37.4702\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4449 - val_loss: 33.7193\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4846 - val_loss: 36.7378\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3965 - val_loss: 43.9624\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4939 - val_loss: 36.5472\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4031 - val_loss: 49.4598\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4439 - val_loss: 35.2527\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3896 - val_loss: 48.7405\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3706 - val_loss: 34.6312\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4272 - val_loss: 34.1924\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4500 - val_loss: 33.9621\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4588 - val_loss: 40.4294\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4084 - val_loss: 38.5676\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3268 - val_loss: 33.4235\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3773 - val_loss: 32.6204\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.4153 - val_loss: 34.1037\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3718 - val_loss: 37.1557\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3389 - val_loss: 34.4063\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3910 - val_loss: 34.4200\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3315 - val_loss: 37.5406\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3185 - val_loss: 32.8025\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4013 - val_loss: 33.1017\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2965 - val_loss: 33.4341\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4175 - val_loss: 38.9511\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3286 - val_loss: 33.8729\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.4136 - val_loss: 37.6825\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3124 - val_loss: 34.5637\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3277 - val_loss: 38.5531\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3114 - val_loss: 36.7562\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3285 - val_loss: 36.7241\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3105 - val_loss: 34.8507\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2771 - val_loss: 35.5689\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2762 - val_loss: 37.0789\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2960 - val_loss: 41.0404\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2454 - val_loss: 36.7652\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3235 - val_loss: 36.3961\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3025 - val_loss: 34.6848\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2926 - val_loss: 39.4757\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2988 - val_loss: 34.3566\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2576 - val_loss: 33.2518\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2754 - val_loss: 47.0618\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2615 - val_loss: 35.1283\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3064 - val_loss: 34.3174\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2939 - val_loss: 33.7850\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3404 - val_loss: 39.2679\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2656 - val_loss: 51.0784\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3022 - val_loss: 34.1462\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2355 - val_loss: 37.6828\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.3132 - val_loss: 36.8264\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2970 - val_loss: 33.1140\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2510 - val_loss: 33.6872\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1704 - val_loss: 33.3042\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2079 - val_loss: 33.4756\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2215 - val_loss: 34.4472\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2233 - val_loss: 37.1319\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2417 - val_loss: 36.0140\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1851 - val_loss: 35.1530\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2095 - val_loss: 39.8264\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2353 - val_loss: 38.1714\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2966 - val_loss: 38.3003\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1457 - val_loss: 35.2745\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.3042 - val_loss: 35.8117\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2583 - val_loss: 37.2037\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1938 - val_loss: 35.8528\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2516 - val_loss: 34.8158\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2215 - val_loss: 33.1012\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.2333 - val_loss: 42.2228\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1495 - val_loss: 35.0785\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1130 - val_loss: 35.4313\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2283 - val_loss: 44.7773\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2127 - val_loss: 47.6154\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1978 - val_loss: 32.4393\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2207 - val_loss: 37.9694\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1982 - val_loss: 33.7205\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1267 - val_loss: 37.9106\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1627 - val_loss: 37.9172\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2307 - val_loss: 41.6023\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2218 - val_loss: 41.7181\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1451 - val_loss: 40.5874\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2163 - val_loss: 37.7803\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1609 - val_loss: 39.5053\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1832 - val_loss: 32.7049\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1683 - val_loss: 47.2793\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1715 - val_loss: 35.2066\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1866 - val_loss: 35.0242\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1773 - val_loss: 35.4843\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2066 - val_loss: 35.7405\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1411 - val_loss: 34.0941\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1493 - val_loss: 32.6975\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1535 - val_loss: 40.6271\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1523 - val_loss: 33.9988\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.2307 - val_loss: 33.5322\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1191 - val_loss: 33.9070\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1071 - val_loss: 37.1453\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.0850 - val_loss: 39.8162\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1026 - val_loss: 34.0784\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0775 - val_loss: 35.7715\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1060 - val_loss: 33.1268\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1372 - val_loss: 34.2927\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1164 - val_loss: 46.2497\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0628 - val_loss: 36.3001\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1001 - val_loss: 35.5302\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0906 - val_loss: 40.6668\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1385 - val_loss: 35.5875\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1345 - val_loss: 38.7975\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0629 - val_loss: 33.6003\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1095 - val_loss: 34.3976\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1361 - val_loss: 35.3519\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1211 - val_loss: 36.7253\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0945 - val_loss: 41.4390\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0914 - val_loss: 35.4374\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0649 - val_loss: 40.0626\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0789 - val_loss: 35.8917\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0897 - val_loss: 33.3086\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 24.0372 - val_loss: 35.6298\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 24.0692 - val_loss: 41.8322\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 24.0531 - val_loss: 34.7902\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 24.1489 - val_loss: 36.5377\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0650 - val_loss: 34.6998\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1031 - val_loss: 34.6025\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9777 - val_loss: 36.6706\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0847 - val_loss: 33.4249\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0441 - val_loss: 36.2767\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0145 - val_loss: 33.0046\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.1273 - val_loss: 32.9035\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0516 - val_loss: 39.1669\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9589 - val_loss: 33.5106\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.1259 - val_loss: 37.2112\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0794 - val_loss: 34.2428\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0547 - val_loss: 42.4300\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0475 - val_loss: 36.9768\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 24.0320 - val_loss: 33.3266\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0277 - val_loss: 32.8299\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9970 - val_loss: 36.6749\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0462 - val_loss: 35.1192\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0001 - val_loss: 36.5194\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0511 - val_loss: 33.7201\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0292 - val_loss: 34.9377\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0384 - val_loss: 40.3381\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0081 - val_loss: 33.4067\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9885 - val_loss: 43.3335\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0520 - val_loss: 36.4709\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0191 - val_loss: 40.9345\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9797 - val_loss: 34.5495\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0114 - val_loss: 37.2792\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9945 - val_loss: 43.5867\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9991 - val_loss: 34.9431\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.9938 - val_loss: 32.7694\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9762 - val_loss: 36.9426\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.9877 - val_loss: 32.9061\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.9622 - val_loss: 34.8061\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0486 - val_loss: 35.5871\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0095 - val_loss: 46.3371\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9836 - val_loss: 39.5862\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9780 - val_loss: 34.1902\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9792 - val_loss: 33.9690\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0630 - val_loss: 32.9163\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9819 - val_loss: 33.5231\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9852 - val_loss: 32.4831\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0213 - val_loss: 35.0849\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0045 - val_loss: 36.7283\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8988 - val_loss: 34.7282\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9811 - val_loss: 45.4837\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9721 - val_loss: 35.0568\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9426 - val_loss: 38.8139\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9834 - val_loss: 34.2343\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9445 - val_loss: 36.8943\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 23.9312 - val_loss: 33.4807\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 24.0091 - val_loss: 36.1439\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9835 - val_loss: 34.2205\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8989 - val_loss: 38.0771\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9926 - val_loss: 36.5535\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9163 - val_loss: 33.8578\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9288 - val_loss: 37.8621\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9978 - val_loss: 39.4089\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9367 - val_loss: 38.3772\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9501 - val_loss: 37.4506\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9755 - val_loss: 40.6709\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9524 - val_loss: 33.5523\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9310 - val_loss: 34.0583\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8884 - val_loss: 34.8940\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9300 - val_loss: 33.2082\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9329 - val_loss: 35.8498\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8790 - val_loss: 37.1032\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9407 - val_loss: 34.3923\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9759 - val_loss: 34.8539\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8665 - val_loss: 35.7174\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9687 - val_loss: 35.6211\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9139 - val_loss: 40.3726\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9324 - val_loss: 39.1309\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8783 - val_loss: 34.1450\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9036 - val_loss: 34.2705\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8899 - val_loss: 34.5010\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9184 - val_loss: 33.1512\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8473 - val_loss: 46.8287\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8961 - val_loss: 35.5477\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8760 - val_loss: 35.7287\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9115 - val_loss: 33.6520\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9138 - val_loss: 41.3313\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8891 - val_loss: 43.5786\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8879 - val_loss: 36.4073\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8623 - val_loss: 42.1152\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8610 - val_loss: 34.2916\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8699 - val_loss: 35.9158\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8552 - val_loss: 33.6629\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.9005 - val_loss: 38.1726\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8122 - val_loss: 39.5291\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8639 - val_loss: 32.3364\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8489 - val_loss: 34.3857\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8520 - val_loss: 34.7014\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.7945 - val_loss: 36.6888\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8070 - val_loss: 33.6292\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8817 - val_loss: 36.2574\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8441 - val_loss: 36.2685\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8229 - val_loss: 35.5415\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8582 - val_loss: 34.8663\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8563 - val_loss: 34.8185\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8382 - val_loss: 38.6287\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8293 - val_loss: 32.9722\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8276 - val_loss: 33.2316\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8071 - val_loss: 38.7949\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 23.8548 - val_loss: 33.4803\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f83fee1-2e17-4bf8-e94d-b7e1ffa3712f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.8668817995496243 \n",
            "MAE:  4.303234614046899 \n",
            "SD:  5.720908931519065\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "d60c7000-2475-444a-eab1-e802e6c84b99"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAFBCAYAAAAsfIegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de3wU1dnHf08uJAjITUQEKmgRFIMBA6IoIqh4aRW1FiyiIoqtVkX7VsVL1VZ9tdb7i3ilgkWLd6hgBQFFtAoBAS8gBOSSCCTcQ0Ku+7x/nBlmdrNJdpPd7Oz09/185jMzZ86cy+zMb57zzDlnRVVBCCEkdqQkugCEEOI3KKyEEBJjKKyEEBJjKKyEEBJjKKyEEBJjKKyEEBJj4iasIpIpIktEZKWIfCciD1jh3UXkKxHJE5EZItLMCs+w9vOs493iVTZCCIkn8bRYywEMVdUTAWQDOFdEBgJ4FMCTqvpzALsBjLPijwOw2wp/0opHCCFJR9yEVQ37rd10a1EAQwG8bYVPBTDC2r7I2od1fJiISLzKRwgh8SKuPlYRSRWRFQAKAcwDsB7AHlWtsqLkA+hsbXcGsAUArON7AbSPZ/kIISQepMUzcVWtBpAtIm0AvAegV2PTFJHxAMYDQIsWLU7q1bMn1i7fD22WgZ5ZzRqbPCGEYNmyZTtUtUNDz4+rsNqo6h4RWQjgFABtRCTNskq7ACiwohUA6AogX0TSALQGsDNMWi8CeBEAcnJyNPfzzzEsczHKOx2Lxbldm6I6hBCfIyKbGnN+PHsFdLAsVYhIcwBnA1gNYCGAX1nRrgIw09qeZe3DOr5AI5whRqDgXDKEEK8QT4u1E4CpIpIKI+BvquoHIvI9gH+KyIMAvgbwihX/FQCviUgegF0ARkWUi4gRVvA7FyHEG8RNWFV1FYC+YcI3ABgQJrwMwGUNyYsWKyHESzSJjzWuWBYrIclAZWUl8vPzUVZWluiiEACZmZno0qUL0tPTY5pu8gurBS1Wkgzk5+ejVatW6NatG9hNO7GoKnbu3In8/Hx07949pmkn/1wB9LGSJKKsrAzt27enqHoAEUH79u3j0npIfmEFfawkuaCoeod4/RbJL6y2xaq8WQkh3iD5hRWWxZroQhBCGkXLli1rPbZx40accMIJTViaxpH8wspeAYQQj5H8wmpBHyshkbFx40b06tULV199NY499liMHj0aH3/8MQYNGoQePXpgyZIl+PTTT5GdnY3s7Gz07dsXxcXFAIDHHnsM/fv3R58+fXDffffVmsedd96JSZMmHdy///778be//Q379+/HsGHD0K9fP2RlZWHmzJm1plEbZWVlGDt2LLKystC3b18sXLgQAPDdd99hwIAByM7ORp8+fbBu3TqUlJTgggsuwIknnogTTjgBM2bMiDq/hpD83a3oYyXJyoQJwIoVsU0zOxt46ql6o+Xl5eGtt97ClClT0L9/f7z++utYvHgxZs2ahYcffhjV1dWYNGkSBg0ahP379yMzMxNz587FunXrsGTJEqgqLrzwQixatAiDBw+ukf7IkSMxYcIE3HjjjQCAN998Ex999BEyMzPx3nvv4dBDD8WOHTswcOBAXHjhhVF9RJo0aRJEBN988w3WrFmDc845B2vXrsXzzz+PW265BaNHj0ZFRQWqq6sxZ84cHHnkkZg9ezYAYO/evRHn0xh8YbHSx0pIdHTv3h1ZWVlISUlB7969MWzYMIgIsrKysHHjRgwaNAi33XYbnnnmGezZswdpaWmYO3cu5s6di759+6Jfv35Ys2YN1q1bFzb9vn37orCwED/99BNWrlyJtm3bomvXrlBV3HXXXejTpw/OOussFBQUYPv27VGVffHixbjiiisAAL169cJRRx2FtWvX4pRTTsHDDz+MRx99FJs2bULz5s2RlZWFefPm4Y477sBnn32G1q1bN/raRQItVkISRQSWZbzIyMg4uJ2SknJwPyUlBVVVVbjzzjtxwQUXYM6cORg0aBA++ugjqComTpyI66+/PqI8LrvsMrz99tvYtm0bRo4cCQCYPn06ioqKsGzZMqSnp6Nbt24x60f6m9/8BieffDJmz56N888/Hy+88AKGDh2K5cuXY86cObjnnnswbNgw/OlPf4pJfnWR/MIKWB+vaLMSEivWr1+PrKwsZGVlYenSpVizZg2GDx+Oe++9F6NHj0bLli1RUFCA9PR0HH744WHTGDlyJK677jrs2LEDn376KQDTFD/88MORnp6OhQsXYtOm6GfnO/300zF9+nQMHToUa9euxebNm9GzZ09s2LABRx99NG6++WZs3rwZq1atQq9evdCuXTtcccUVaNOmDV5++eVGXZdI8YWwAuDIK0JiyFNPPYWFCxcedBWcd955yMjIwOrVq3HKKacAMN2j/vGPf9QqrL1790ZxcTE6d+6MTp06AQBGjx6NX/7yl8jKykJOTg569Yp+7vsbbrgBv/vd75CVlYW0tDS8+uqryMjIwJtvvonXXnsN6enpOOKII3DXXXdh6dKl+OMf/4iUlBSkp6dj8uTJDb8oUSARTnnqSXJycjQ3NxcXy3tYf/gpWLX9iEQXiZA6Wb16NY477rhEF4O4CPebiMgyVc1paJr++XhFHyshxCP4whXAXgGEJIadO3di2LBhNcLnz5+P9u2j/y/Qb775BmPGjAkKy8jIwFdffdXgMiYCnwgraLESkgDat2+PFTHsi5uVlRXT9BKFb1wB7BVACPEKvhBWgL0CCCHewRfCKsL5WAkh3sEfwspeAYQQD+EfYU10IQghQdQ1v6rf8YmwEkKId/BFdyuA3a1I8pGoWQM3btyIc889FwMHDsQXX3yB/v37Y+zYsbjvvvtQWFiI6dOn48CBA7jlllsAmP+FWrRoEVq1aoXHHnsMb775JsrLy3HxxRfjgQceqLdMqorbb78dH374IUQE99xzD0aOHImtW7di5MiR2LdvH6qqqjB58mSceuqpGDduHHJzcyEiuOaaa3DrrbfG4tI0Kb4QVhG6AgiJhnjPx+rm3XffxYoVK7By5Urs2LED/fv3x+DBg/H6669j+PDhuPvuu1FdXY3S0lKsWLECBQUF+PbbbwEAe/bsaYrLEXP8Iaz8l1aShCRw1sCD87ECCDsf66hRo3Dbbbdh9OjRuOSSS9ClS5eg+VgBYP/+/Vi3bl29wrp48WJcfvnlSE1NRceOHXHGGWdg6dKl6N+/P6655hpUVlZixIgRyM7OxtFHH40NGzbgpptuwgUXXIBzzjkn7tciHvjGx8p+rIRETiTzsb788ss4cOAABg0ahDVr1hycj3XFihVYsWIF8vLyMG7cuAaXYfDgwVi0aBE6d+6Mq6++GtOmTUPbtm2xcuVKDBkyBM8//zyuvfbaRtc1EfhEWGmxEhJL7PlY77jjDvTv3//gfKxTpkzB/v37AQAFBQUoLCysN63TTz8dM2bMQHV1NYqKirBo0SIMGDAAmzZtQseOHXHdddfh2muvxfLly7Fjxw4EAgFceumlePDBB7F8+fJ4VzUu+MYVQAiJHbGYj9Xm4osvxn/+8x+ceOKJEBH89a9/xRFHHIGpU6fiscceQ3p6Olq2bIlp06ahoKAAY8eORSAQAAD87//+b9zrGg98MR/rlanT8Vmr8/DjnnaJLhIhdcL5WL0H52OtBQ5pJYR4Cd+4AtiPlZCmJ9bzsfoFnwgrJw0kJBHEej5Wv+APVwBllSQRyfxdw2/E67fwhbBCOKSVJAeZmZnYuXMnxdUDqCp27tyJzMzMmKftE1cAh7SS5KBLly7Iz89HUVFRootCYF50Xbp0iXm6cRNWEekKYBqAjjAu0BdV9WkRuR/AdQDsO+suVZ1jnTMRwDgA1QBuVtWPIsoLtFhJcpCeno7u3bsnuhgkzsTTYq0C8AdVXS4irQAsE5F51rEnVfVv7sgicjyAUQB6AzgSwMcicqyqVteXES1WQoiXiJuPVVW3qupya7sYwGoAnes45SIA/1TVclX9EUAegAGR5GX6sdJiJYR4gyb5eCUi3QD0BWD/OfjvRWSViEwRkbZWWGcAW1yn5aNuIXbSp71KCPEQcRdWEWkJ4B0AE1R1H4DJAI4BkA1gK4DHo0xvvIjkikiu8wGA0koI8Q5xFVYRSYcR1emq+i4AqOp2Va1W1QCAl+A09wsAdHWd3sUKC0JVX1TVHFXN6dChg8mHI68IIR4ibsIqIgLgFQCrVfUJV3gnV7SLAXxrbc8CMEpEMkSkO4AeAJZElhc/XhFCvEM8ewUMAjAGwDciYo95uwvA5SKSDdMFayOA6wFAVb8TkTcBfA/To+DGSHoEALRYCSHeIm7CqqqLEf4PVOfUcc5DAB6KNi9KKiHES/hjSCs4CQshxDv4QliNj5V2KyHEG/hDWPmfV4QQD+ETYeW/tBJCvINPhJUWKyHEO/hDWIWqSgjxDr4QVoCuAEKId/CFsJr5WBNdCkIIMfhEWNndihDiHfwhrEKLlRDiHfwhrBx3RQjxEL4QVoAfrwgh3sEXwsohrYQQL+EPYQV9rIQQ7+ATYaXFSgjxDv4QVuGQVkKId/CHsCa6AIQQ4sIXwgqwVwAhxDv4Qlg5uxUhxEv4Q1iFFishxDv4Q1jZK4AQ4iH8IazsFUAI8RD+ENZEF4AQQlz4QlgBQP1TFUJIkuMLNeLsVoQQL+EPYbV8AfSzEkK8gD+E1bJYKayEEC/gD2Hlv7QSQjyEL4TVhhYrIcQL+EJY7e5WFFZCiBfwibDSx0oI8Q7+EFb2CiCEeAh/CCstVkKIh/CHsHJMKyHEQ/hCWG1osRJCvIAvhNXux0phJYR4AX8IK32shBAPETdhFZGuIrJQRL4Xke9E5BYrvJ2IzBORdda6rRUuIvKMiOSJyCoR6RdxXtaawkoI8QLxtFirAPxBVY8HMBDAjSJyPIA7AcxX1R4A5lv7AHAegB7WMh7A5Egz4uxWhBAvETdhVdWtqrrc2i4GsBpAZwAXAZhqRZsKYIS1fRGAaWr4EkAbEekUUWZWtwBarIQQL9AkPlYR6QagL4CvAHRU1a3WoW0AOlrbnQFscZ2Wb4XVnz59rIQQDxF3YRWRlgDeATBBVfe5j6mqAtG140VkvIjkikhuUVGRFWanF4sSE0JI44irsIpIOoyoTlfVd63g7XYT31oXWuEFALq6Tu9ihQWhqi+qao6q5nTo0MHkQ4uVEOIh4tkrQAC8AmC1qj7hOjQLwFXW9lUAZrrCr7R6BwwEsNflMqgnLworIcQ7pMUx7UEAxgD4RkRWWGF3AXgEwJsiMg7AJgC/to7NAXA+gDwApQDGRpoRR7QSQrxE3IRVVRejds0bFia+ArixcXk25mxCCIkN/hh5RVcAIcRD+ENYrTWFlRDiBXwirLRYCSHewR/Cyq9XhBAP4QthtaHFSgjxAr4QVo68IoR4CX8IK32shBAP4Q9hpcVKCPEQ/hBWWqyEEA/hD2FlrwBCiIfwhbDa0GIlhHgBXwgrh7QSQryEP4TVWlNYCSFewCfCSouVEOId/CGs/HhFCPEQvhBWG1qshBAv4AthpSuAEOIl/CGsHHlFCPEQ/hBWWqyEEA/hD2GlxUoI8RD+EFZQUQkh3sEXwmqbrLRYCSFewBfCSh8rIcRL+ENY6WMlhHgIfwgrLVZCiIfwh7AKFZUQ4h18Iaz2/Fa0WAkhXsAXwkpXACHES/hDWPnxihDiIfwhrLRYCSEewh/CSouVEOIhIhJWEWkhIinW9rEicqGIpMe3aJHDXgGEEC8RqcW6CECmiHQGMBfAGACvxqtQ0cNeAYQQ7xCpsIqqlgK4BMBzqnoZgN7xK1Z00MdKCPESEQuriJwCYDSA2VZYanyKFD30sRJCvESkwjoBwEQA76nqdyJyNICF8StWdNg+VgorIcQLRCSsqvqpql6oqo9aH7F2qOrNdZ0jIlNEpFBEvnWF3S8iBSKywlrOdx2bKCJ5IvKDiAyPphL8k1ZCiJeItFfA6yJyqIi0APAtgO9F5I/1nPYqgHPDhD+pqtnWMsdK/3gAo2D8tucCeE5EonY10GIlhHiBSF0Bx6vqPgAjAHwIoDtMz4BaUdVFAHZFmP5FAP6pquWq+iOAPAADIjyXPlZCiKeIVFjTrX6rIwDMUtVKoMH/h/J7EVlluQraWmGdAWxxxcm3wiKCPlZCiJeIVFhfALARQAsAi0TkKAD7GpDfZADHAMgGsBXA49EmICLjRSRXRHKLiopMmHWMwkoI8QKRfrx6RlU7q+r5atgE4MxoM1PV7aparaoBAC/Bae4XAOjqitrFCguXxouqmqOqOR06dABAVwAhxFtE+vGqtYg8YVuKIvI4jPUaFSLSybV7McyHMACYBWCUiGSISHcAPQAsiTjdVF9MeUAI8QlpEcabAiOCv7b2xwD4O8xIrLCIyBsAhgA4TETyAdwHYIiIZMP4ZzcCuB4ArL6xbwL4HkAVgBtVtTriWqQYYaXFSgjxApEK6zGqeqlr/wERWVHXCap6eZjgV+qI/xCAhyIsTxCSwrkCCCHeIdI29AEROc3eEZFBAA7Ep0jRY7sCKKyEEC8QqcX6WwDTRKS1tb8bwFXxKVL0UFgJIV4iImFV1ZUAThSRQ639fSIyAcCqeBYuUugKIIR4iag+p6vqPmsEFgDcFofyNAj2CiCEeInGKJJ35j5hrwBCiIdojLB6RsboCiCEeIk6fawiUozwAioAmselRA2AH68IIV6iTmFV1VZNVZDGQIuVEOIlfPHVhx+vCCFewh+KxI9XhBAP4QthpY+VEOIlfCKs9LESQryDP4SVrgBCiIfwh7Cmmf8dpLASQryAP4Q1xTuDwAghxBfCilRarIQQ7+ALYeUAAUKIl/CHsLK7FSHEQ/hLWKsDCS4JIYT4TFgRoLASQhKPL4T14JBWWqyEEA/gC2GlK4AQ4iUorIQQEmN8JqzVCS4JIYT4TViraLESQhKPr4SVvQIIIV7AV8IaoMVKCPEAvhDW1HQKKyHEO/hCWFOsia6rKymshJDE4wthpcVKCPESvhDWgxZrFWdhIYQkHl8IKy1WQoiX8IWwpqSZatBiJYR4AV8Ia2qacQXQYiWEeAFfCCstVkKIl/CFsNo+VgorIcQLxE1YRWSKiBSKyLeusHYiMk9E1lnrtla4iMgzIpInIqtEpF80eaU2M38mSFcAIcQLxNNifRXAuSFhdwKYr6o9AMy39gHgPAA9rGU8gMnRZERXACHES8RNWFV1EYBdIcEXAZhqbU8FMMIVPk0NXwJoIyKdIs2L3a0IIV6iqX2sHVV1q7W9DUBHa7szgC2uePlWWEQctFirabESQhJPwj5eqaoCiFoJRWS8iOSKSG5RUREAt8VKYSWEJJ6mFtbtdhPfWhda4QUAurridbHCaqCqL6pqjqrmdOjQAQCQkm4+XtFiJYR4gaYW1lkArrK2rwIw0xV+pdU7YCCAvS6XQb0ctFgprIQQD5AWr4RF5A0AQwAcJiL5AO4D8AiAN0VkHIBNAH5tRZ8D4HwAeQBKAYyNJi+nV0AsSk4IIY0jbsKqqpfXcmhYmLgK4MaG5mX3Y2V3K0KIF/DFyCvbYqUrgBDiBXwhrActVv77NSHEA/hCWCUtFYIALVZCiCfwhbAiLQ0pCNDHSgjxBP4Q1tRUpKIagQCFlRCSeHwjrMZiTXRBCCHER8JKi5UQ4hV8I6wpCLBXACHEE/hGWFNRjQCFlRDiAXwjrLRYCSFewTfCmopqCishxBP4RlhTEECAfyBACPEAvhFWWqyEEK/gK2GlxUoI8QK+EdYUBFBNYa2drRHPG04AoKqKs/qQBuMPYRVhd6u6+OQT4MgjgbfeSnRJkoc2bYBu3RJdCpKk+ENYAVqsdbF8uVl/8UViy5FMlJQA+fmJLgVJUnwjrKni4V4BFRXAvn2JLgUgkugSEPJfgW+ENQWK6mqPCsfw4UDr1onLXzmHAiFNiW+ENVU83Cvgk08Sm78trLRYCWkSfCOsKaKoDlA46oTCSkiT4BthTUWAwlobdAUQ0qT4R1i9/PHKhgJHyH8FvhHWFEkCizVRHc7pYyWkSfGNsKZKAJ7/A4HKysTkS2ElpEnxjbB68uPV/v3AgQPOfqKElRDSpKQlugCxIlUUAa8Ja6tWZiipTaKFlRYrIU0CLdZ489NPznaiXQGEkCbBN8KampIEPtaKisjjbt5sLMwvv2x8vvSxEtKk+EZYjcXq8epEY7HOm2fWL74Yu/wprIQ0CR5XoshJFUW1elw4ohFWWwRj0YynK4CQJsU3wprixY9XoSTKx1pVlZh8kxXPjzQhXsc3wpqaEkgeizUQAO6+O/jDViixtFhtYaXlGhmhL6KOHYFJkxJTFpKU+EdYRRHwurDaH6+++AJ4+GFg7Nja4zZWWLdsAT791Gzbgl5dDfz+98BnnzUszVgzcybwn/8kuhQ1cY+Qq6wECgvNdSMkQnzTjzUlRVGt9bwnnnjCTDh9//3xL1A4QXQLHBA8eCCUxgpr795AcbE537bAysqM5TVpkjes1xEjzNoLZXHjtlj3709cOUjS4h+LNSUCi/UPfwAeeCD8sYoK4IcfzPaNNwLDhjWuQOH8mrawRiOaDRWd4uKaZdm7N/p09uwBjjkGyM1tWDmSEQoraST+sVgFqC4tN1bgqFFAv35m1v7Fi4G3364/gUsvBT74ACgtBZ57rvEFCtdnNZqPV7GasEXVydcW1rQofvYvvgA2bADuvRf48MPYlMnruK89hZU0gIQIq4hsBFAMoBpAlarmiEg7ADMAdAOwEcCvVXV3pGmmH9iHShwF3HSTEdPKyuiE4IMPzDpW/01Vl7DaVqjbGp04EVi5EpgzJzhuYykrq2mxRiOszZubdV1uC7/htljdlj8hEZJIV8CZqpqtqjnW/p0A5qtqDwDzrf2IOaR8N0pxiPlH0l27gB07Ij/ZLYLhLJRhw4ALL3T2q6uNANbVTK9LWMvLax575JHgF4F9fm157NkDvPRS/a6CcMKanl73OW5SU826tDTyc5KdRLgCVPmvsD7CSz7WiwBMtbanAhgRzcmHVO0zwmo/CG5hLSsDPvqo9pPdVkk4C2XBAuBf/3L2n3gCuOACYNas2tMMJ6x2mC2skQhzbXHGjzdLfb7PnTuBV14x2w0RVrussbZYvfbByk0ihPWVV4CuXf+7fNk+JlHCqgDmisgyERlvhXVU1a3W9jYAHaNJsEVgH0rQAlocRlh/9Svg3HNduYc81O6Hx71dW3N8yxaz3rix9gKFO9cOi2TOgPribLUuVX2W5M03O9v1uQK+/BL4/PPgsLKyyPKJlkTP9FUXdflYZ84E/vrX2E9a/vHHZr12bWzTJQkhUcJ6mqr2A3AegBtFZLD7oKoqjPjWQETGi0iuiOQWFRUdDD8EpQggFRXFloXltjxnzw5OJPShdsd1P0i1iUkkfsdIXAGNsVgjHfe/aZOzXZ+wnnIKcNppwWG2sIbW9e9/B84+O7IyhMNO14vUZbGOGAHccUd0fupo8mxIuhwp5jkSIqyqWmCtCwG8B2AAgO0i0gkArHVhLee+qKo5qprToUOHg+EtUAIAKC2J4CY7/vjgfffD4/54VVoaXtgaI6y5ucYPbNOqFTB6dHAc9/n1Wa71WU7ul4gdN5qH134JhL5krrnGWFklJZGnFS7d0DKGY+3aulsHsSZSV0BjrW5V4OmngaKihgvrlCnGD14Y5nH55z+Bxx5rXBmTlU8/TdxfISEBwioiLUSklb0N4BwA3wKYBeAqK9pVAGZGk+4hMA9+CVrUH3n9+uB998PjvkFLS8NbVpmZZj1lCnDyycCiRTXj1Cas/fsDjz9u9lVN3q+/7sR57jkjJKH+2FBsi7W+Jnq4h78+H6vbAqrNFWALQEObru7rGpp2WRlw2WVAXp7Z79kT6N69Yfk0hEiFtaEvFZuVK4EJE4CrrnLyjNb6vPtus7ZdQ24uvxy4/fa6z9+929zDTe2CeOQRYMiQmuH79zfeAl+0yKT98MONS6cRJMJi7QhgsYisBLAEwGxV/TeARwCcLSLrAJxl7UdMiz+bTgSlOCSyE9w/ntsV4L5BS0rCd6q3RS8/H1iyBBg3rvY4dYWFs4YnTDBCYgtqbcJq0xBhrc8q2rXL2bYFMDSdrl3Nes2autOqDbewhgrUJ5+YvseJGkYaaT/WxgqrfT8UFjrCGu1Hwm3bgtOKllmzzD385z837PyGMnGiM+TaprzctODqexnUh/0Mf/tt49JpBE0urKq6QVVPtJbeqvqQFb5TVYepag9VPUtVd9WXlptDso4BEKHFCpheAmecYR4O98Nj36iAES23a6C2mz8lzGUMJ2ihIlnXrFN23HnzwouXbbG6H+78fDNqzP2QhRNeuwtVbeV1v1zcAuh+EXTqZNbr1oUvf11s2QJMn+7sl5QYyykRcxhUVAD//ndwWG39WJ9/PjheY4XVfrkHAk6eDf1I2NCy2PduvJrN9r1UWGhemKG4nz3biHn55cbl6a7TgQNOH/UmxEvdrRpFC0tPS3GImY2oPs4/3zQZ1qwJ/nHdolJaGmyx2unWJ6xVVeEtzc2bg/frehjcH53qmqzF/SBOmGBcCfYk2YARrFDCif6ePc52bcLqrrctBOF8ezZz5gDPPlszfNiw4KHFpaXAL38JDB4cnIdqcMtCFbjnHmDZstrzBEyz1u5iVh8TJwLnnQd89ZUTVpsr4He/Cz63sV2x7N9ONbL5I9yUlwf3fKlLkOt6gdsv2cY0v4uLa/YmAczL88gjzbU94wzgzDNr5rN9u7NtGzGN7YpnGx2BADBmjLm3mtjV4RthPcTyAJSghWlKR0phoWOVpKQEd9IPtVh37TLx6xLW/fuND9P27/z2t86xlSuDz6tr7L673224HgDhfKz2DVnf6LFwou8WYPc1cAurO137peDqmYG1a4MfnAsuMN293JbZ5Mk1rdx9+5wHs6go+Pq6LcYDB4CHHgJOPTV8vUpLzXLqqcC11zovkPz84A+GblavrlmPeLoCDhxwroeddnW1U9ZIhfXsswHXx9uDH1onTgSuvjr45VlXHezWTajFmpcHzJ9vtgsKgNNPD27NuRk3zvQmCX3J2i2Q3Fyn1RV6z7vTtI81VljtaxgIAEVOFKEAABVRSURBVO+8Y7Z79mzSPsK+E9ZSHGJusoyMyE7cvt3ceCI1nenhfKwLF5ofrls3JywlxdzI333niIbtP2rZ0om3YkVwWrsi9HYceqhZL1lihuuGltHG7q0Q7kOGm82bgRkzzPZXXxnxtIW1eXPgqaeAF16o+fHOfS1ChfW558zNG27mMNtamDULuOGGmsfdlpf7RQcEW9J2XuH8iXl5ptkyerQZFGGnu3u38QefdFLNc4DwE+JE+vGqsBB49NHIu45VVJgb9a67zL59DQMBp06RugJC3SYlJUakHnkEmDo12GK3r+dLL5lucm7sYwsWBP82PXoAZ51lXkrPPmvuu5deCl+Wr782a/f8wrt3OwNo3L5y+7excVuskUwS9NprwIMP1gzfsAF44w2zbV/X0JdFE7qafCOstiugBC2MsLQI8bV27hz+xLFjzYPfsiXw5JPBxzZurPmWzsszN78tYoD5IXv2BE44wfzwbtzCGvrQ1Pdhysa2Pk4+2VgOX3/t+KtKS42IV1Y6L5PQXg9DhwI5OcFho0aZh2bgQFOXm24y4XY/1t/+1ljdbtFYudKxWm3B+ewzI2AzrU4cTzxR0+KwrcXaHhy3tVhYGCxmbmEN7XK1dKnTR9nO//33neM7dgRbKQsWOIM7bGxhLS42AySeeipyYX3uOeDOO8N3aSotDRYNwPldHn8cePVV06sEMNfLFrhwFus995iXQ12WXGmpebHbuN0Wdtrjx5tucoEAkJ1txNc+tnu3aU2ECl/Xro5Qhfrm8/KMsWA/C+5r+5vfhH/BhxoT7ufLvrcqKmq6DH76CbjtNuDKK82EQKFcf73JMzfX+c1Cf7uMDNMy+u1va7+Woe6nBuIbYT3oChh+qem/5xbW1q2Bo46q/eQZM4zA/OxnTtiZZxprZMOG4Lhbt5qb3y2sP/5oFqCmsLZqFbzvFtpIWbAg2D/br5+z/cknxtK+/37nIQkV1hNOAA4/vGa6bj/u0qVmPXy4EzZtWvDM+SNHAu3bmxmv7Ietqsr4TO0HpKQEeOstxyoDHIu1ti/XbmFdudJ8gAPMTe4WVvsaA8B99wEDBgC/+IUpg+0jbdbMibNjR7DbYdgw83KyWbDAsdT37DEDJG69Nfhl4r5GodhCEPrxCzDW3hFHmJfgGWeYfOzmcGqqeaHbo60CAUfQwr18HnrIvATHjKm9LHPnGrEGara8Qodpr1tnrvMXX9R0Gx12WPDvATi/X6gY9egB9O3rPHxuYa3ti/yuXcHCVVDgbNt1Ly837gw7zYULjU/ebfiElsU2PqZPd+7NUGEvKTGGwwsvBN9XgLkWHTqY+rRpE77s0aCqSbucdNJJalNRodqsmertt1sBI0aomsuvesUVqpdfbrbvv98Jt5dzzlFdv141EHDCPvrIrFu3Do77i1+oHnqo6mmnBYe3a6fauXPNtAsKVPv2Vb3kErP/+9/XjNOiRc0wexk0yEm/tjiA6qmnqg4dWjN8+HDVkhLVCy6o+3xAtXdv1blz648XbunYMXz9AdUrrzS/ycMPhz/+u9/Vnu7jjzvb990XPs6XX6oedVTN8LFjVc84o2a4quobbwSHPfigs/3UU5HVuW1bsz76aNXVq1WfecZc59tvrxn3uuuc+qenBx8LLfvzz5sy7t2r+uc/B98ff/mL6r/+VXuZ0tNVn3giOGzevOB7e9o059644YaaacyfH7zfpo1Z33KLanW16q5dpnyh5915p2pRkTl2+OHhy/c//6O6cqWzP2bMwWdYb7stOO7Gjao5OWY7NTX42O7dphyPPmoe/t69TfiIESYPQDUjI/icq692ttesUd22zck75NkBkNsYbWrwiV5Y3MKqqtqvn+pZZ1k7ubmmepMnq5aVqS5bpiqimpen2r276muvqb70kurbbwelcfDiVlWptmpltsMJ33HHBe8PHqx60UXBYWvWOOkWFam+/ropSzjhqU2QXnop/LFIFvsBUFXt1q3++Nddp/r995GnP2xY8P5ll4WPl52t+sADtafTp09k+Z11VvB+fr55eOzwwYMjSydcWeyHEXCEPjs7svQyMlR/9rOG/07hFtVggY+0bu+/b+7dnj2dsOefV/3kE2d/zBizTkmpeU0B1Ztuqj39Hj3C/xaA6rHHRl/PIUOCn9e6lrvvdu7jH35Q/eUvzfaUKcH3Wl0vanu54w6zTk831yMrK+g4hdXFtdcaI6Kiwgqortao+eAD1QULzPYpp5hLdOGFZn/gwOAfZ9YsZ/uGG4w1AaieeaaxgAKB8HmE/sjff6/66qvO/qRJxmK2mTHDhLdsWf8N06mTWV96aXCetsV69NG1nztlirFugZp1tZd33nG2n3su2Pqwrcs+fVTPPrvucl57rerNNzsPKlD7A/HII6onnlgzXFV1woTg8oQ7v2/f+q9buOXWW826SxcnzG3ZNmSJ9Px9+1SvusrZf/TRyM7bscNcl+uvrz1OaCssUUtmpuoxx5jy3nJL/fHff99pUbmtT3tJSzPWtd3Ks5dnnom6bBRWFzNnmhpNnx5ez6LGtsgmTjT7bqsGMGF28+6771Q//NBsP/ts3elu26b6449O82bLFqcC48fXjL93r3mzut0Pc+ea80IfmGefNc3S0JfKnj2qW7cat0htN9QPPzjxDxxQ/fxzk8977xkXyIcfBjcpZ8ww+dj7b72lumSJyWv9elPme+9VPemkmk3yFStMPu7mfVlZ+Gb09u1mCQ1XNQ+bvV9S4jRBO3Z0tsePN2u3hb1ggWqvXjXTTE83ltDo0aaMQHCZAgHT1E5JifxBPessY22df75qaanqK6/Uf84VV6gedpixVK+/3riUXntN9YsvVAcMcOK5rTX7moTeq0OG1J3XqaeqLl6s+vXXTthTT6n++99OE7sxS//+NcOyspyX4v/9X/2uLsAYIPZvEro8+mjtL61du+pO96GHjB/RFUZhdVFVZYylzEzjhmo0r71mLlF+vtkvL1ctLDTW5aJFJqyy0vErlZQYK2Pz5sjS79fPpG9bGXXxt7+pvvuusRDfe88JX7dOdfZs09SLhP37zUOkGnxzzZ4d2fnu8/buNfsPPmisBPsFURvr1xvRP3AgOHzaNGOx2NjW7vDh5mVgc9NNqj//ubFAH3zQhG3a5JRH1bh67BbH5s2q551nytW+fXCrQFX14ovNdvPm5qX1wAM1y/zNN6YJNGCAsWBtVq1SHTUq+MXw4INOMxsw4vvGG+a+CGXvXifeww+bJuz77xuRcZ//n//UPHfSJCc/VSe+3axWdb4l3HOP2bf9wYBp+bzxhnlQACP6NgMHqp5wgvNi/uMfTZylS03YDz+EF87HHjPbqanG9WC7In72M9XiYmNMHHaYc84dd5h7we1f/stfTB579zpupXPOMa2k884zv0OoSP7738aosX9v90vw7rvNOhAwgnziicG+36FDHYs5EAgSVwprCEVFjr97yBCjN5WVNaJFTqNOrodt24yoJIqLLjI3U0FBdOfNnm1EqqkJ51oJBMwHi3feqfvcigoT96WXjICpOg+h21JvCBMmGKs0EFD96SfzYnj++dpdQTbr1hnfoptAQHXqVNW//92IejgqK41FuXu32U9JMS4eNz/+qNqhg+ratWb/88+NYBYUOOft36/69NOmpWBTUWEsFJvy8pov7S1bTMvo0kuNOAYCJt6tt5pvGjb5+ab1YvPVV+alOHKk4////nvzMRgwFnNoPuGwP1zV9pt//rkxHgKB8L8BYCyw0OPTp5sX0iefNFpYRVUb37UgQeTk5GhumNEUxcVmNrannzY9bjp2BH79a+CSS0zPl0inMiU+58AB040tmpF6XqSszAxScXc1SzZKS51uW/Hmp59Ml6o68hORZer8bVTU+FJYbfbuNV0M33rLzMNQXg4cd5zponryyaaPdFYWhZYQEgyFNcLxvyUlZtrTt982gy/sPsTNm5tBWb16maV7d9NHunt3M16gdWtn+lVCyH8HFNYGTKwQCJhRlqtWAd98Y9br1pnRlOFGmTZrBrRta1qMhx5qBlN16mS2O3c24tuihWlZtGrlxGnVyoTRIiYkuWissMb4j3uSg5QUM3Q+dPh8IGBGZu7caUay5uebEX/79pnw9evNCLx9+4xrrqrKfEKsL6+WLY3Atmhh8mjWzCxt2hhRto+npBgxbtvWjIBs29Ys6elmbuq0NLPdrJlJKyPDWTIzzUjJkhLjU05PN3EOHDBlbN7cHLfLkJJiwin6hMSe/0phrY2UFDN95JFHGt9rfVRUGMHdt88IWkmJ+XBWXGzC7O39+41vvqTE5FFebs7dvduId0mJOV5dbeKXlBihjMf/7WVkmBdCWpopx6GHGqE97DCTX4sWRrgrKkwZMjIcka6sNOGtWpmXQWpq7UtamlmnpztLaqqpX1kZ0K6dCauuNuKekeG8cGzBt18m9iJiytCqlfNyEDHb9gvOPrd1a5NWIGCuZ7t2zgumRQsTp6rKrFXNdiDg1L+2ehUXO5ONlZWZ89u0MWWprjZ1CgScuaurq8214gvsvwsKayNo1ix43pZYUVFhHtCKCvMBrqoqeCkvN0JcXm4ebntdWWmEobDQbJeUGEtY1cQpLzdibj/8GRlmLgoR03siI8OkU1Fh6mZvV1SYl4OqSX/vXpNHdbVZbAFxL3ZYZaWz2MKVmWnK8d/056LuF4yII8qpqUaU7bWI+W3T0sx1Sk834e7FjpuSYn4bO769pKaa38tu4bjjh6ZhvzDse6Sy0pxnl6W2BfDOsUDAqUdmptOStOvhvrbutOxrW1kZHC/SSefqgsLqQexeMxkZ4SelSlbcrgdbVG2XREWF82DbVqQt0PZiW4TFxY4YuC1XO+3qavPCqKoyx1q0MC+O1FSn5WBbxKrBD1VJiSlL6IvCXjIznZn9MjPN+bt2mfTsh9TuZW5b4MXFJk37BWNbznb5q6uD182bm3VZmWNJ24sdx95OSTEWtPsaVVYaK9nedp8fmp+92FZ+WpojTHZLIHSxf0v3EtpqiPS8WByzWwci5rexX16h1y60jPa1se8DO059/7UZCRRW0mS4m8PuP12wXQGRzk1OSLxprOvGN/OxEkKIV6CwEkJIjKGwEkJIjKGwEkJIjKGwEkJIjKGwEkJIjKGwEkJIjKGwEkJIjKGwEkJIjKGwEkJIjKGwEkJIjKGwEkJIjKGwEkJIjKGwEkJIjKGwEkJIjKGwEkJIjPGcsIrIuSLyg4jkicidiS4PIYREi6eEVURSAUwCcB6A4wFcLiLHJ7ZUhBASHZ4SVgADAOSp6gZVrQDwTwAXJbhMhBASFV4T1s4Atrj2860wQghJGpLuzwRFZDyA8dZuuYh8m8jyxJnDAOxIdCHiCOuXvPi5bgDQszEne01YCwB0de13scIOoqovAngRAEQkV1Vzmq54TQvrl9z4uX5+rhtg6teY873mClgKoIeIdBeRZgBGAZiV4DIRQkhUeMpiVdUqEfk9gI8ApAKYoqrfJbhYhBASFZ4SVgBQ1TkA5kQY/cV4lsUDsH7JjZ/r5+e6AY2sn6hqrApCCCEE3vOxEkJI0pO0wuqHoa8iMkVECt1dxkSknYjME5F11rqtFS4i8oxV31Ui0i9xJa8fEekqIgtF5HsR+U5EbrHC/VK/TBFZIiIrrfo9YIV3F5GvrHrMsD7CQkQyrP0863i3RJY/EkQkVUS+FpEPrH3f1A0ARGSjiHwjIivsXgCxuj+TUlh9NPT1VQDnhoTdCWC+qvYAMN/aB0xde1jLeACTm6iMDaUKwB9U9XgAAwHcaP1GfqlfOYChqnoigGwA54rIQACPAnhSVX8OYDeAcVb8cQB2W+FPWvG8zi0AVrv2/VQ3mzNVNdvVdSw296eqJt0C4BQAH7n2JwKYmOhyNbAu3QB869r/AUAna7sTgB+s7RcAXB4uXjIsAGYCONuP9QNwCIDlAE6G6TSfZoUfvE9herqcYm2nWfEk0WWvo05dLGEZCuADAOKXurnquBHAYSFhMbk/k9Jihb+HvnZU1a3W9jYAHa3tpK2z1TTsC+Ar+Kh+VlN5BYBCAPMArAewR1WrrCjuOhysn3V8L4D2TVviqHgKwO0AAtZ+e/inbjYKYK6ILLNGdAIxuj89192KOKiqikhSd9sQkZYA3gEwQVX3icjBY8leP1WtBpAtIm0AvAegV4KLFBNE5BcAClV1mYgMSXR54shpqlogIocDmCcia9wHG3N/JqvFWu/Q1yRmu4h0AgBrXWiFJ12dRSQdRlSnq+q7VrBv6mejqnsALIRpHrcREdtgcdfhYP2s460B7GziokbKIAAXishGmBnmhgJ4Gv6o20FUtcBaF8K8GAcgRvdnsgqrn4e+zgJwlbV9FYxv0g6/0vo6ORDAXleTxXOIMU1fAbBaVZ9wHfJL/TpYlipEpDmM/3g1jMD+yooWWj+73r8CsEAtZ53XUNWJqtpFVbvBPFsLVHU0fFA3GxFpISKt7G0A5wD4FrG6PxPtQG6E4/l8AGth/Fp3J7o8DazDGwC2AqiE8dmMg/FNzQewDsDHANpZcQWmJ8R6AN8AyEl0+eup22kwPqxVAFZYy/k+ql8fAF9b9fsWwJ+s8KMBLAGQB+AtABlWeKa1n2cdPzrRdYiwnkMAfOC3ull1WWkt39kaEqv7kyOvCCEkxiSrK4AQQjwLhZUQQmIMhZUQQmIMhZUQQmIMhZUQQmIMhZUQCxEZYs/kREhjoLASQkiMobCSpENErrDmQl0hIi9Yk6HsF5EnrblR54tIBytutoh8ac2h+Z5rfs2fi8jH1nyqy0XkGCv5liLytoisEZHp4p7cgJAIobCSpEJEjgMwEsAgVc0GUA1gNIAWAHJVtTeATwHcZ50yDcAdqtoHZsSMHT4dwCQ186meCjMCDjCzcE2Amef3aJhx84REBWe3IsnGMAAnAVhqGZPNYSbKCACYYcX5B4B3RaQ1gDaq+qkVPhXAW9YY8c6q+h4AqGoZAFjpLVHVfGt/Bcx8uYvjXy3iJyisJNkQAFNVdWJQoMi9IfEaOla73LVdDT4jpAHQFUCSjfkAfmXNoWn/R9FRMPeyPfPSbwAsVtW9AHaLyOlW+BgAn6pqMYB8ERlhpZEhIoc0aS2Ir+HbmCQVqvq9iNwDM/N7CszMYDcCKAEwwDpWCOOHBczUb89bwrkBwFgrfAyAF0Tkz1YalzVhNYjP4exWxBeIyH5VbZnochAC0BVACCExhxYrIYTEGFqshBASYyishBASYyishBASYyishBASYyishBASYyishBASY/4fGKWNx6JBw+UAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ab9fcb-172d-4c7d-e277-95d15c7f51c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_15 (Dense)            (None, 32)                4096      \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 32)                0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 32)                0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,441\n",
            "Trainable params: 5,313\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT"
      },
      "outputs": [],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fbf4d6e-d64b-43d0-bffc-4913a14080d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.5837919236392597 \n",
            "MAE:  4.263235818220608 \n",
            "SD:  5.783477155738345\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "3eeedfba-2b8b-4dcb-f8f7-73996363dca0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae043bb7-442b-4ccb-fc0a-6545c8a8ee89"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  0.5244373460902314 \n",
            "Ensemble_std:  5.781702466772259\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "\bBP_hv3_3(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}