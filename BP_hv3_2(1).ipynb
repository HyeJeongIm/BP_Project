{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/%08BP_hv3_2(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# batch_size"
      ],
      "metadata": {
        "id": "XiiiBla2-j1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsCoux5AOZnK",
        "outputId": "8e79d502-f7f1-4c24-9d1f-4c3542d58463",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "TensorFlow version :  2.8.2\n",
            "Keras version :  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlHICkovd809",
        "outputId": "54c17d2c-b4b4-44d6-e68e-6b178ce8cb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러z오기\n",
        "train_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ],
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "ae63e12b-c080-452f-af4c-76bdae5ef823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "cb00c773-0612-4d1f-d9d6-29122db7ee88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-480ca367-996f-46cf-a836-e7e490b70515\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-480ca367-996f-46cf-a836-e7e490b70515')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-480ca367-996f-46cf-a836-e7e490b70515 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-480ca367-996f-46cf-a836-e7e490b70515');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "fe2d5b5c-6f2c-46dc-817e-0857ab574e24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4966c7b2-c5dd-47bd-a1ae-28281e0e48f8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4966c7b2-c5dd-47bd-a1ae-28281e0e48f8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4966c7b2-c5dd-47bd-a1ae-28281e0e48f8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4966c7b2-c5dd-47bd-a1ae-28281e0e48f8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 400\n",
        "lrate = 0.001"
      ],
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8SHBwBOZnO",
        "outputId": "730a9555-8062-4265-b2f2-f003fe32001f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 8)                32        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,169\n",
            "Trainable params: 1,137\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dGT6-7NcOZnO",
        "outputId": "99042421-7ef8-4f87-fac4-b04bf2a46187",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "165/165 [==============================] - 3s 11ms/step - loss: 12252.1240 - val_loss: 12186.0439\n",
            "Epoch 2/400\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 11854.1523 - val_loss: 11197.2061\n",
            "Epoch 3/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 11330.2109 - val_loss: 10374.9121\n",
            "Epoch 4/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 10669.6699 - val_loss: 9576.8164\n",
            "Epoch 5/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 9883.8994 - val_loss: 8593.2822\n",
            "Epoch 6/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 8989.4580 - val_loss: 7712.7715\n",
            "Epoch 7/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8000.4761 - val_loss: 6652.8340\n",
            "Epoch 8/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6948.7432 - val_loss: 5561.3569\n",
            "Epoch 9/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 5881.6758 - val_loss: 4341.9551\n",
            "Epoch 10/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 4850.1348 - val_loss: 3495.7588\n",
            "Epoch 11/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3906.4531 - val_loss: 3405.4851\n",
            "Epoch 12/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3065.3210 - val_loss: 2377.5281\n",
            "Epoch 13/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2338.2600 - val_loss: 1909.1892\n",
            "Epoch 14/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1732.0281 - val_loss: 1682.2584\n",
            "Epoch 15/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1214.8777 - val_loss: 1454.0969\n",
            "Epoch 16/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 829.0779 - val_loss: 763.0923\n",
            "Epoch 17/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 560.4057 - val_loss: 365.2360\n",
            "Epoch 18/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 376.0238 - val_loss: 353.8429\n",
            "Epoch 19/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 258.5152 - val_loss: 203.9717\n",
            "Epoch 20/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 186.9097 - val_loss: 196.5966\n",
            "Epoch 21/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 149.2243 - val_loss: 226.4218\n",
            "Epoch 22/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 128.0717 - val_loss: 137.1886\n",
            "Epoch 23/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 117.9697 - val_loss: 223.9910\n",
            "Epoch 24/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 113.3246 - val_loss: 153.1803\n",
            "Epoch 25/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 110.7922 - val_loss: 183.4170\n",
            "Epoch 26/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.9862 - val_loss: 134.7549\n",
            "Epoch 27/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.9906 - val_loss: 154.3354\n",
            "Epoch 28/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 107.7132 - val_loss: 227.4718\n",
            "Epoch 29/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.6608 - val_loss: 127.4769\n",
            "Epoch 30/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4928 - val_loss: 150.8733\n",
            "Epoch 31/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.4627 - val_loss: 137.1419\n",
            "Epoch 32/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.8499 - val_loss: 198.9469\n",
            "Epoch 33/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.0612 - val_loss: 153.5992\n",
            "Epoch 34/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7043 - val_loss: 123.1946\n",
            "Epoch 35/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.0231 - val_loss: 139.3258\n",
            "Epoch 36/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.8699 - val_loss: 130.3303\n",
            "Epoch 37/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3731 - val_loss: 179.3967\n",
            "Epoch 38/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.6541 - val_loss: 134.3622\n",
            "Epoch 39/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.0231 - val_loss: 128.6542\n",
            "Epoch 40/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.5722 - val_loss: 134.1994\n",
            "Epoch 41/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.0739 - val_loss: 179.5926\n",
            "Epoch 42/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.6033 - val_loss: 119.1385\n",
            "Epoch 43/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.4725 - val_loss: 119.7776\n",
            "Epoch 44/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.3732 - val_loss: 154.4855\n",
            "Epoch 45/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.2962 - val_loss: 179.3896\n",
            "Epoch 46/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.1232 - val_loss: 176.5955\n",
            "Epoch 47/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.6692 - val_loss: 170.9051\n",
            "Epoch 48/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.6294 - val_loss: 144.3550\n",
            "Epoch 49/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.4359 - val_loss: 133.6891\n",
            "Epoch 50/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.1289 - val_loss: 124.6468\n",
            "Epoch 51/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.9955 - val_loss: 122.1964\n",
            "Epoch 52/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.7075 - val_loss: 118.6087\n",
            "Epoch 53/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6679 - val_loss: 216.9290\n",
            "Epoch 54/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.5098 - val_loss: 270.1038\n",
            "Epoch 55/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.0340 - val_loss: 139.8090\n",
            "Epoch 56/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.0088 - val_loss: 121.4818\n",
            "Epoch 57/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 97.7275 - val_loss: 121.6375\n",
            "Epoch 58/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.8826 - val_loss: 135.1803\n",
            "Epoch 59/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.7537 - val_loss: 116.5262\n",
            "Epoch 60/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 97.6179 - val_loss: 146.9890\n",
            "Epoch 61/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 97.8110 - val_loss: 127.2052\n",
            "Epoch 62/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.3048 - val_loss: 150.9723\n",
            "Epoch 63/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 97.2206 - val_loss: 115.5167\n",
            "Epoch 64/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 97.0629 - val_loss: 154.0258\n",
            "Epoch 65/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.9772 - val_loss: 124.3825\n",
            "Epoch 66/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 96.8495 - val_loss: 119.6518\n",
            "Epoch 67/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 96.3905 - val_loss: 192.9488\n",
            "Epoch 68/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 96.6112 - val_loss: 129.3950\n",
            "Epoch 69/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.5022 - val_loss: 214.8155\n",
            "Epoch 70/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 96.0717 - val_loss: 146.5962\n",
            "Epoch 71/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.0685 - val_loss: 128.4876\n",
            "Epoch 72/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.8856 - val_loss: 124.2838\n",
            "Epoch 73/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 96.1083 - val_loss: 151.2394\n",
            "Epoch 74/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.8975 - val_loss: 116.1937\n",
            "Epoch 75/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.1001 - val_loss: 129.3645\n",
            "Epoch 76/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.5947 - val_loss: 115.3475\n",
            "Epoch 77/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.4491 - val_loss: 119.8263\n",
            "Epoch 78/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.4728 - val_loss: 123.5033\n",
            "Epoch 79/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.3214 - val_loss: 133.9723\n",
            "Epoch 80/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.2472 - val_loss: 166.2718\n",
            "Epoch 81/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.0316 - val_loss: 123.4095\n",
            "Epoch 82/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.9550 - val_loss: 129.9334\n",
            "Epoch 83/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.7594 - val_loss: 118.8619\n",
            "Epoch 84/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.7604 - val_loss: 111.7761\n",
            "Epoch 85/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.8404 - val_loss: 159.6864\n",
            "Epoch 86/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.6011 - val_loss: 123.0510\n",
            "Epoch 87/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.3405 - val_loss: 115.9094\n",
            "Epoch 88/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4751 - val_loss: 108.9833\n",
            "Epoch 89/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.5658 - val_loss: 120.7668\n",
            "Epoch 90/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1512 - val_loss: 178.2208\n",
            "Epoch 91/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1525 - val_loss: 119.8740\n",
            "Epoch 92/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.0458 - val_loss: 114.9654\n",
            "Epoch 93/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.7488 - val_loss: 141.6573\n",
            "Epoch 94/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.7317 - val_loss: 108.4385\n",
            "Epoch 95/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.7757 - val_loss: 119.2359\n",
            "Epoch 96/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.8099 - val_loss: 128.5816\n",
            "Epoch 97/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.5353 - val_loss: 133.2088\n",
            "Epoch 98/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.0323 - val_loss: 116.4815\n",
            "Epoch 99/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.4178 - val_loss: 116.2950\n",
            "Epoch 100/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.4539 - val_loss: 132.7928\n",
            "Epoch 101/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.5578 - val_loss: 136.9324\n",
            "Epoch 102/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.5021 - val_loss: 140.5454\n",
            "Epoch 103/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.3556 - val_loss: 116.3368\n",
            "Epoch 104/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.1325 - val_loss: 128.4297\n",
            "Epoch 105/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.3113 - val_loss: 129.3317\n",
            "Epoch 106/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.9938 - val_loss: 112.7012\n",
            "Epoch 107/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.1869 - val_loss: 113.2791\n",
            "Epoch 108/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.1455 - val_loss: 133.6373\n",
            "Epoch 109/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.9523 - val_loss: 113.8138\n",
            "Epoch 110/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.9100 - val_loss: 111.0435\n",
            "Epoch 111/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.8815 - val_loss: 130.2108\n",
            "Epoch 112/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.6797 - val_loss: 124.2461\n",
            "Epoch 113/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.6324 - val_loss: 133.0980\n",
            "Epoch 114/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.9979 - val_loss: 141.9034\n",
            "Epoch 115/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.9016 - val_loss: 122.0745\n",
            "Epoch 116/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.7752 - val_loss: 110.1301\n",
            "Epoch 117/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.4508 - val_loss: 139.1333\n",
            "Epoch 118/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.8816 - val_loss: 110.5083\n",
            "Epoch 119/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.8127 - val_loss: 118.0705\n",
            "Epoch 120/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7730 - val_loss: 142.2910\n",
            "Epoch 121/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.3197 - val_loss: 108.3133\n",
            "Epoch 122/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.4573 - val_loss: 117.6054\n",
            "Epoch 123/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.4043 - val_loss: 105.9618\n",
            "Epoch 124/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.3582 - val_loss: 130.6631\n",
            "Epoch 125/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.0979 - val_loss: 113.1834\n",
            "Epoch 126/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.2243 - val_loss: 154.5343\n",
            "Epoch 127/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.2357 - val_loss: 117.1759\n",
            "Epoch 128/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.4148 - val_loss: 140.0105\n",
            "Epoch 129/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.0917 - val_loss: 128.6882\n",
            "Epoch 130/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.9855 - val_loss: 112.2760\n",
            "Epoch 131/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.0518 - val_loss: 134.3410\n",
            "Epoch 132/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.1002 - val_loss: 188.3782\n",
            "Epoch 133/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.0341 - val_loss: 104.9739\n",
            "Epoch 134/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.1470 - val_loss: 123.0147\n",
            "Epoch 135/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.2210 - val_loss: 105.6241\n",
            "Epoch 136/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.8278 - val_loss: 125.7274\n",
            "Epoch 137/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.0846 - val_loss: 131.4789\n",
            "Epoch 138/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.9997 - val_loss: 153.3102\n",
            "Epoch 139/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.8351 - val_loss: 126.6095\n",
            "Epoch 140/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.9579 - val_loss: 108.5793\n",
            "Epoch 141/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7574 - val_loss: 138.2558\n",
            "Epoch 142/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.0222 - val_loss: 143.3587\n",
            "Epoch 143/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.8576 - val_loss: 106.1389\n",
            "Epoch 144/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.6944 - val_loss: 122.7264\n",
            "Epoch 145/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.7246 - val_loss: 117.6659\n",
            "Epoch 146/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.6283 - val_loss: 110.9463\n",
            "Epoch 147/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.5568 - val_loss: 131.2140\n",
            "Epoch 148/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.7169 - val_loss: 223.0816\n",
            "Epoch 149/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.4091 - val_loss: 117.9689\n",
            "Epoch 150/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.5021 - val_loss: 103.9864\n",
            "Epoch 151/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.4505 - val_loss: 118.8172\n",
            "Epoch 152/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.6104 - val_loss: 117.9567\n",
            "Epoch 153/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.4230 - val_loss: 121.7192\n",
            "Epoch 154/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.3468 - val_loss: 178.9015\n",
            "Epoch 155/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.2959 - val_loss: 112.7961\n",
            "Epoch 156/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.2449 - val_loss: 130.0448\n",
            "Epoch 157/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.3081 - val_loss: 108.5929\n",
            "Epoch 158/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.1142 - val_loss: 110.1473\n",
            "Epoch 159/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.3004 - val_loss: 123.8005\n",
            "Epoch 160/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.1166 - val_loss: 107.9386\n",
            "Epoch 161/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.1006 - val_loss: 126.9666\n",
            "Epoch 162/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.1746 - val_loss: 123.5832\n",
            "Epoch 163/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.9565 - val_loss: 108.1834\n",
            "Epoch 164/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.9642 - val_loss: 107.3865\n",
            "Epoch 165/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.8705 - val_loss: 126.8647\n",
            "Epoch 166/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8814 - val_loss: 118.6185\n",
            "Epoch 167/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.7696 - val_loss: 117.2666\n",
            "Epoch 168/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.9592 - val_loss: 105.8322\n",
            "Epoch 169/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.7643 - val_loss: 106.1957\n",
            "Epoch 170/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.7746 - val_loss: 110.2448\n",
            "Epoch 171/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.6484 - val_loss: 116.3236\n",
            "Epoch 172/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.6475 - val_loss: 110.1228\n",
            "Epoch 173/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.6230 - val_loss: 106.8354\n",
            "Epoch 174/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.7135 - val_loss: 135.8534\n",
            "Epoch 175/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.7041 - val_loss: 162.6683\n",
            "Epoch 176/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.5811 - val_loss: 126.3260\n",
            "Epoch 177/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.7231 - val_loss: 150.5169\n",
            "Epoch 178/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.6376 - val_loss: 138.2096\n",
            "Epoch 179/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.5989 - val_loss: 110.6176\n",
            "Epoch 180/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.4828 - val_loss: 123.3213\n",
            "Epoch 181/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.6190 - val_loss: 127.5135\n",
            "Epoch 182/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5492 - val_loss: 108.1455\n",
            "Epoch 183/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.5465 - val_loss: 114.3005\n",
            "Epoch 184/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.3915 - val_loss: 116.4287\n",
            "Epoch 185/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.3896 - val_loss: 107.9511\n",
            "Epoch 186/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.3430 - val_loss: 217.1545\n",
            "Epoch 187/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.3091 - val_loss: 107.3328\n",
            "Epoch 188/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.2151 - val_loss: 107.9321\n",
            "Epoch 189/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.2177 - val_loss: 109.8395\n",
            "Epoch 190/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.1717 - val_loss: 107.2015\n",
            "Epoch 191/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.1407 - val_loss: 113.1155\n",
            "Epoch 192/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.1652 - val_loss: 142.2224\n",
            "Epoch 193/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.1641 - val_loss: 103.7107\n",
            "Epoch 194/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.4061 - val_loss: 115.2195\n",
            "Epoch 195/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.0501 - val_loss: 101.8972\n",
            "Epoch 196/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.0009 - val_loss: 146.0278\n",
            "Epoch 197/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.1267 - val_loss: 139.2198\n",
            "Epoch 198/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.0763 - val_loss: 112.8932\n",
            "Epoch 199/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.8655 - val_loss: 121.5050\n",
            "Epoch 200/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.1511 - val_loss: 116.2371\n",
            "Epoch 201/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.8766 - val_loss: 133.3113\n",
            "Epoch 202/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.9852 - val_loss: 105.2608\n",
            "Epoch 203/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.8740 - val_loss: 115.0540\n",
            "Epoch 204/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.9430 - val_loss: 103.6593\n",
            "Epoch 205/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.9580 - val_loss: 109.5769\n",
            "Epoch 206/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.8886 - val_loss: 102.2780\n",
            "Epoch 207/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.9774 - val_loss: 102.6333\n",
            "Epoch 208/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.6696 - val_loss: 104.9453\n",
            "Epoch 209/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.5848 - val_loss: 126.6657\n",
            "Epoch 210/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.5614 - val_loss: 110.9926\n",
            "Epoch 211/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.5855 - val_loss: 112.4862\n",
            "Epoch 212/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.4121 - val_loss: 120.2694\n",
            "Epoch 213/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.5349 - val_loss: 110.5605\n",
            "Epoch 214/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.5947 - val_loss: 115.3782\n",
            "Epoch 215/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.5921 - val_loss: 131.4032\n",
            "Epoch 216/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.4682 - val_loss: 101.0304\n",
            "Epoch 217/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.3849 - val_loss: 114.8947\n",
            "Epoch 218/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.2784 - val_loss: 100.0521\n",
            "Epoch 219/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.3480 - val_loss: 114.7932\n",
            "Epoch 220/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.0594 - val_loss: 106.5854\n",
            "Epoch 221/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.2918 - val_loss: 160.3457\n",
            "Epoch 222/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.3639 - val_loss: 103.9674\n",
            "Epoch 223/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.4069 - val_loss: 97.8574\n",
            "Epoch 224/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.9761 - val_loss: 123.4851\n",
            "Epoch 225/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.2048 - val_loss: 116.8438\n",
            "Epoch 226/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.1384 - val_loss: 109.9448\n",
            "Epoch 227/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.0083 - val_loss: 114.9643\n",
            "Epoch 228/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.9149 - val_loss: 107.0736\n",
            "Epoch 229/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.9661 - val_loss: 112.5626\n",
            "Epoch 230/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.7525 - val_loss: 104.1043\n",
            "Epoch 231/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.9412 - val_loss: 102.6792\n",
            "Epoch 232/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.8839 - val_loss: 130.4977\n",
            "Epoch 233/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.7743 - val_loss: 106.6384\n",
            "Epoch 234/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.9678 - val_loss: 130.1959\n",
            "Epoch 235/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.9171 - val_loss: 102.8708\n",
            "Epoch 236/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.7909 - val_loss: 112.5681\n",
            "Epoch 237/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.9389 - val_loss: 109.8318\n",
            "Epoch 238/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.6063 - val_loss: 120.3682\n",
            "Epoch 239/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.8184 - val_loss: 121.8653\n",
            "Epoch 240/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.8059 - val_loss: 122.6838\n",
            "Epoch 241/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.6891 - val_loss: 115.1753\n",
            "Epoch 242/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.5375 - val_loss: 113.1787\n",
            "Epoch 243/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.5122 - val_loss: 100.8800\n",
            "Epoch 244/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.5718 - val_loss: 119.6180\n",
            "Epoch 245/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.5691 - val_loss: 112.7685\n",
            "Epoch 246/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.2619 - val_loss: 99.7420\n",
            "Epoch 247/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.4926 - val_loss: 101.0144\n",
            "Epoch 248/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.3831 - val_loss: 110.1768\n",
            "Epoch 249/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.4206 - val_loss: 129.6105\n",
            "Epoch 250/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.3591 - val_loss: 102.6188\n",
            "Epoch 251/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.4091 - val_loss: 104.2206\n",
            "Epoch 252/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.2999 - val_loss: 124.3373\n",
            "Epoch 253/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.3158 - val_loss: 105.5536\n",
            "Epoch 254/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.2699 - val_loss: 111.8441\n",
            "Epoch 255/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0641 - val_loss: 105.9649\n",
            "Epoch 256/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.9205 - val_loss: 103.9742\n",
            "Epoch 257/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.9278 - val_loss: 101.9712\n",
            "Epoch 258/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.9218 - val_loss: 116.7956\n",
            "Epoch 259/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.1185 - val_loss: 126.9347\n",
            "Epoch 260/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8488 - val_loss: 103.7984\n",
            "Epoch 261/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.7506 - val_loss: 100.5152\n",
            "Epoch 262/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8220 - val_loss: 102.2608\n",
            "Epoch 263/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6577 - val_loss: 100.2210\n",
            "Epoch 264/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.6552 - val_loss: 112.6619\n",
            "Epoch 265/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.7372 - val_loss: 118.3264\n",
            "Epoch 266/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6634 - val_loss: 126.9815\n",
            "Epoch 267/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.5110 - val_loss: 109.1445\n",
            "Epoch 268/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6483 - val_loss: 113.6499\n",
            "Epoch 269/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.3782 - val_loss: 115.9540\n",
            "Epoch 270/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.5341 - val_loss: 118.6020\n",
            "Epoch 271/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.4411 - val_loss: 123.3773\n",
            "Epoch 272/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.3572 - val_loss: 105.3518\n",
            "Epoch 273/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.4229 - val_loss: 104.7705\n",
            "Epoch 274/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.1091 - val_loss: 101.6035\n",
            "Epoch 275/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.1734 - val_loss: 113.1905\n",
            "Epoch 276/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.2533 - val_loss: 98.5026\n",
            "Epoch 277/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.0485 - val_loss: 105.0303\n",
            "Epoch 278/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.9878 - val_loss: 113.4169\n",
            "Epoch 279/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.0245 - val_loss: 103.9787\n",
            "Epoch 280/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.9828 - val_loss: 108.5386\n",
            "Epoch 281/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.9175 - val_loss: 102.2918\n",
            "Epoch 282/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.9251 - val_loss: 115.8684\n",
            "Epoch 283/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.9827 - val_loss: 103.1694\n",
            "Epoch 284/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.8626 - val_loss: 100.7315\n",
            "Epoch 285/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.8933 - val_loss: 131.9708\n",
            "Epoch 286/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.9250 - val_loss: 106.9085\n",
            "Epoch 287/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.8800 - val_loss: 103.2200\n",
            "Epoch 288/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.7400 - val_loss: 97.3817\n",
            "Epoch 289/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.6661 - val_loss: 141.1814\n",
            "Epoch 290/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.6174 - val_loss: 122.6458\n",
            "Epoch 291/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.6247 - val_loss: 107.7381\n",
            "Epoch 292/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.7900 - val_loss: 115.3745\n",
            "Epoch 293/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.7025 - val_loss: 103.8035\n",
            "Epoch 294/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.7202 - val_loss: 108.0627\n",
            "Epoch 295/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.7880 - val_loss: 156.1571\n",
            "Epoch 296/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.6734 - val_loss: 131.3922\n",
            "Epoch 297/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.5403 - val_loss: 127.7982\n",
            "Epoch 298/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.6061 - val_loss: 108.2263\n",
            "Epoch 299/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.5133 - val_loss: 109.0354\n",
            "Epoch 300/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.5669 - val_loss: 121.9122\n",
            "Epoch 301/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.6233 - val_loss: 109.5835\n",
            "Epoch 302/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.6316 - val_loss: 113.3420\n",
            "Epoch 303/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.4310 - val_loss: 103.1508\n",
            "Epoch 304/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.4793 - val_loss: 100.1422\n",
            "Epoch 305/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.5343 - val_loss: 117.2318\n",
            "Epoch 306/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.5700 - val_loss: 146.9928\n",
            "Epoch 307/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.4783 - val_loss: 121.8777\n",
            "Epoch 308/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.6183 - val_loss: 116.2002\n",
            "Epoch 309/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.5412 - val_loss: 106.6068\n",
            "Epoch 310/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.2771 - val_loss: 103.8954\n",
            "Epoch 311/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.5738 - val_loss: 126.5783\n",
            "Epoch 312/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.4889 - val_loss: 99.1888\n",
            "Epoch 313/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.2331 - val_loss: 114.7715\n",
            "Epoch 314/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.3270 - val_loss: 102.0692\n",
            "Epoch 315/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.2647 - val_loss: 99.7326\n",
            "Epoch 316/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.3260 - val_loss: 108.5829\n",
            "Epoch 317/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.4280 - val_loss: 100.0286\n",
            "Epoch 318/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.4637 - val_loss: 106.1920\n",
            "Epoch 319/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.0629 - val_loss: 104.3323\n",
            "Epoch 320/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.2925 - val_loss: 103.4200\n",
            "Epoch 321/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.0753 - val_loss: 113.0746\n",
            "Epoch 322/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.2367 - val_loss: 107.6290\n",
            "Epoch 323/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.1314 - val_loss: 106.2155\n",
            "Epoch 324/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.1783 - val_loss: 100.4094\n",
            "Epoch 325/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.2569 - val_loss: 114.8111\n",
            "Epoch 326/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.0381 - val_loss: 100.1122\n",
            "Epoch 327/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.0375 - val_loss: 140.2438\n",
            "Epoch 328/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.2316 - val_loss: 102.7640\n",
            "Epoch 329/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.2455 - val_loss: 120.1492\n",
            "Epoch 330/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.0154 - val_loss: 109.5558\n",
            "Epoch 331/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9952 - val_loss: 103.0165\n",
            "Epoch 332/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.0430 - val_loss: 110.1541\n",
            "Epoch 333/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.1541 - val_loss: 102.3728\n",
            "Epoch 334/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.1874 - val_loss: 126.2850\n",
            "Epoch 335/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.1287 - val_loss: 107.3712\n",
            "Epoch 336/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.0225 - val_loss: 101.6560\n",
            "Epoch 337/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.0990 - val_loss: 101.6589\n",
            "Epoch 338/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8447 - val_loss: 111.0836\n",
            "Epoch 339/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.9881 - val_loss: 103.7273\n",
            "Epoch 340/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.0115 - val_loss: 111.4804\n",
            "Epoch 341/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.3073 - val_loss: 113.2185\n",
            "Epoch 342/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8279 - val_loss: 111.5330\n",
            "Epoch 343/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8205 - val_loss: 99.1819\n",
            "Epoch 344/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.9682 - val_loss: 109.5803\n",
            "Epoch 345/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9399 - val_loss: 101.4845\n",
            "Epoch 346/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9812 - val_loss: 95.9221\n",
            "Epoch 347/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8477 - val_loss: 107.3210\n",
            "Epoch 348/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.0350 - val_loss: 124.2490\n",
            "Epoch 349/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8106 - val_loss: 111.8166\n",
            "Epoch 350/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8632 - val_loss: 122.5741\n",
            "Epoch 351/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.9100 - val_loss: 99.8318\n",
            "Epoch 352/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8830 - val_loss: 104.1528\n",
            "Epoch 353/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.0445 - val_loss: 117.0756\n",
            "Epoch 354/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8146 - val_loss: 104.3103\n",
            "Epoch 355/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9526 - val_loss: 112.7552\n",
            "Epoch 356/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.7961 - val_loss: 174.9299\n",
            "Epoch 357/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.9493 - val_loss: 101.6552\n",
            "Epoch 358/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.9391 - val_loss: 107.0437\n",
            "Epoch 359/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8036 - val_loss: 97.1011\n",
            "Epoch 360/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8001 - val_loss: 107.9401\n",
            "Epoch 361/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.9076 - val_loss: 100.6890\n",
            "Epoch 362/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.6945 - val_loss: 104.2670\n",
            "Epoch 363/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8302 - val_loss: 109.9546\n",
            "Epoch 364/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8269 - val_loss: 98.8523\n",
            "Epoch 365/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8184 - val_loss: 138.7581\n",
            "Epoch 366/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.7696 - val_loss: 100.0409\n",
            "Epoch 367/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.6729 - val_loss: 109.9485\n",
            "Epoch 368/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8704 - val_loss: 106.4694\n",
            "Epoch 369/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.6261 - val_loss: 105.9312\n",
            "Epoch 370/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.6422 - val_loss: 109.7545\n",
            "Epoch 371/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.6206 - val_loss: 98.1499\n",
            "Epoch 372/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.6829 - val_loss: 109.1419\n",
            "Epoch 373/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8538 - val_loss: 114.5556\n",
            "Epoch 374/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8980 - val_loss: 138.0332\n",
            "Epoch 375/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4628 - val_loss: 100.1108\n",
            "Epoch 376/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.7860 - val_loss: 107.2730\n",
            "Epoch 377/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.6909 - val_loss: 121.7186\n",
            "Epoch 378/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8246 - val_loss: 101.4004\n",
            "Epoch 379/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.6199 - val_loss: 105.8269\n",
            "Epoch 380/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.7282 - val_loss: 100.7272\n",
            "Epoch 381/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.7854 - val_loss: 101.8792\n",
            "Epoch 382/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.7115 - val_loss: 103.6261\n",
            "Epoch 383/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.5978 - val_loss: 104.0899\n",
            "Epoch 384/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.7069 - val_loss: 99.1369\n",
            "Epoch 385/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.6332 - val_loss: 101.5005\n",
            "Epoch 386/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.4548 - val_loss: 100.6171\n",
            "Epoch 387/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.5959 - val_loss: 106.3707\n",
            "Epoch 388/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5677 - val_loss: 100.8493\n",
            "Epoch 389/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.7229 - val_loss: 124.8753\n",
            "Epoch 390/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.6597 - val_loss: 97.8162\n",
            "Epoch 391/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.4773 - val_loss: 103.8951\n",
            "Epoch 392/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5651 - val_loss: 142.4949\n",
            "Epoch 393/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.4684 - val_loss: 108.9448\n",
            "Epoch 394/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.6381 - val_loss: 106.3145\n",
            "Epoch 395/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5472 - val_loss: 99.9378\n",
            "Epoch 396/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5316 - val_loss: 129.1411\n",
            "Epoch 397/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5276 - val_loss: 109.4349\n",
            "Epoch 398/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.4607 - val_loss: 101.6094\n",
            "Epoch 399/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.5454 - val_loss: 110.8485\n",
            "Epoch 400/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.5716 - val_loss: 107.3945\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "a5c4e9de-8174-4c84-94f7-69689bf88600",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -3.339985904944486 \n",
            "MAE:  8.024856646590258 \n",
            "SD:  9.81014790330722\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "17e0f86a-3434-4674-a085-dfe2222a25e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "057170cb-380b-4ef5-d287-6ac2b61715b6",
        "id": "su2Sj5jZCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,169\n",
            "Trainable params: 1,137\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "outputId": "9845be64-1317-43bc-ac58-ece6f6bcf765",
        "id": "kPRh6v-mCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 12451.2803 - val_loss: 12332.2871\n",
            "Epoch 2/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 12121.6270 - val_loss: 11895.6396\n",
            "Epoch 3/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11716.9111 - val_loss: 11189.6455\n",
            "Epoch 4/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 11209.5361 - val_loss: 10496.0469\n",
            "Epoch 5/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 10605.6260 - val_loss: 9888.2383\n",
            "Epoch 6/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9927.9482 - val_loss: 9347.2539\n",
            "Epoch 7/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9201.1514 - val_loss: 8656.1572\n",
            "Epoch 8/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 8447.4072 - val_loss: 7940.4429\n",
            "Epoch 9/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7669.7222 - val_loss: 6938.3062\n",
            "Epoch 10/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 6897.0542 - val_loss: 6168.9546\n",
            "Epoch 11/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6183.4199 - val_loss: 5641.7280\n",
            "Epoch 12/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 5523.2173 - val_loss: 4882.9976\n",
            "Epoch 13/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4922.2793 - val_loss: 4747.1948\n",
            "Epoch 14/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4381.4097 - val_loss: 3954.6003\n",
            "Epoch 15/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3902.0979 - val_loss: 3267.0615\n",
            "Epoch 16/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3479.5295 - val_loss: 3036.3850\n",
            "Epoch 17/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3052.1121 - val_loss: 3237.4934\n",
            "Epoch 18/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1849.4465 - val_loss: 3787.3979\n",
            "Epoch 19/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1237.1089 - val_loss: 1484.7975\n",
            "Epoch 20/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 850.8420 - val_loss: 802.8773\n",
            "Epoch 21/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 597.7106 - val_loss: 990.3762\n",
            "Epoch 22/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 426.1681 - val_loss: 546.6083\n",
            "Epoch 23/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 313.4090 - val_loss: 814.1276\n",
            "Epoch 24/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 240.0526 - val_loss: 320.8993\n",
            "Epoch 25/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 193.6242 - val_loss: 654.4221\n",
            "Epoch 26/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 164.0681 - val_loss: 310.2357\n",
            "Epoch 27/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 143.8019 - val_loss: 342.4783\n",
            "Epoch 28/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 132.8389 - val_loss: 287.9990\n",
            "Epoch 29/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 125.4952 - val_loss: 140.4220\n",
            "Epoch 30/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 121.2238 - val_loss: 182.6408\n",
            "Epoch 31/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 118.8146 - val_loss: 168.1301\n",
            "Epoch 32/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 117.1892 - val_loss: 158.3132\n",
            "Epoch 33/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 116.3821 - val_loss: 201.0984\n",
            "Epoch 34/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 115.8900 - val_loss: 139.3287\n",
            "Epoch 35/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 114.5889 - val_loss: 139.6372\n",
            "Epoch 36/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 114.1500 - val_loss: 155.8548\n",
            "Epoch 37/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 113.5064 - val_loss: 128.4434\n",
            "Epoch 38/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 112.6248 - val_loss: 127.5920\n",
            "Epoch 39/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.8861 - val_loss: 131.1982\n",
            "Epoch 40/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 110.4576 - val_loss: 194.3757\n",
            "Epoch 41/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 109.5923 - val_loss: 145.9202\n",
            "Epoch 42/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.9318 - val_loss: 165.0742\n",
            "Epoch 43/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.2793 - val_loss: 200.8046\n",
            "Epoch 44/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.2488 - val_loss: 128.5488\n",
            "Epoch 45/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.9899 - val_loss: 116.9607\n",
            "Epoch 46/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.5808 - val_loss: 135.1062\n",
            "Epoch 47/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.4695 - val_loss: 110.8037\n",
            "Epoch 48/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3031 - val_loss: 119.1768\n",
            "Epoch 49/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.0451 - val_loss: 122.5439\n",
            "Epoch 50/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.6181 - val_loss: 108.1021\n",
            "Epoch 51/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.6420 - val_loss: 122.0885\n",
            "Epoch 52/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.9567 - val_loss: 135.5045\n",
            "Epoch 53/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.4920 - val_loss: 121.5686\n",
            "Epoch 54/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.5114 - val_loss: 109.3161\n",
            "Epoch 55/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0179 - val_loss: 109.0319\n",
            "Epoch 56/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.1369 - val_loss: 114.2522\n",
            "Epoch 57/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.0862 - val_loss: 108.0754\n",
            "Epoch 58/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7655 - val_loss: 112.0217\n",
            "Epoch 59/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.1833 - val_loss: 114.6226\n",
            "Epoch 60/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 97.7924 - val_loss: 117.6572\n",
            "Epoch 61/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 97.9524 - val_loss: 108.5542\n",
            "Epoch 62/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 97.7136 - val_loss: 134.3496\n",
            "Epoch 63/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.3658 - val_loss: 115.2728\n",
            "Epoch 64/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.2794 - val_loss: 136.3080\n",
            "Epoch 65/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.8034 - val_loss: 110.2343\n",
            "Epoch 66/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.8446 - val_loss: 112.9096\n",
            "Epoch 67/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 96.7371 - val_loss: 111.4871\n",
            "Epoch 68/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.5302 - val_loss: 118.8085\n",
            "Epoch 69/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.6686 - val_loss: 118.7815\n",
            "Epoch 70/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.2719 - val_loss: 105.6719\n",
            "Epoch 71/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 96.2855 - val_loss: 117.2676\n",
            "Epoch 72/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 96.3826 - val_loss: 105.9319\n",
            "Epoch 73/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.2171 - val_loss: 108.4543\n",
            "Epoch 74/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 96.0080 - val_loss: 143.1089\n",
            "Epoch 75/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.5994 - val_loss: 117.0449\n",
            "Epoch 76/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.7628 - val_loss: 114.8199\n",
            "Epoch 77/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.5876 - val_loss: 133.2608\n",
            "Epoch 78/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.5292 - val_loss: 123.1321\n",
            "Epoch 79/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.2209 - val_loss: 120.5603\n",
            "Epoch 80/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.1964 - val_loss: 106.2102\n",
            "Epoch 81/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.2386 - val_loss: 124.9600\n",
            "Epoch 82/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.1227 - val_loss: 112.8534\n",
            "Epoch 83/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.0250 - val_loss: 120.5680\n",
            "Epoch 84/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.0252 - val_loss: 115.9212\n",
            "Epoch 85/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.8082 - val_loss: 118.2468\n",
            "Epoch 86/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4684 - val_loss: 129.6102\n",
            "Epoch 87/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.4200 - val_loss: 125.3105\n",
            "Epoch 88/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.3438 - val_loss: 106.6071\n",
            "Epoch 89/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.1931 - val_loss: 104.4848\n",
            "Epoch 90/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.9954 - val_loss: 118.7473\n",
            "Epoch 91/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.6873 - val_loss: 121.5835\n",
            "Epoch 92/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.8149 - val_loss: 130.9409\n",
            "Epoch 93/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.6842 - val_loss: 108.6540\n",
            "Epoch 94/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.4591 - val_loss: 108.8728\n",
            "Epoch 95/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.3148 - val_loss: 113.9543\n",
            "Epoch 96/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.0298 - val_loss: 104.2106\n",
            "Epoch 97/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.0891 - val_loss: 128.3465\n",
            "Epoch 98/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.8033 - val_loss: 121.4612\n",
            "Epoch 99/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.8928 - val_loss: 109.0346\n",
            "Epoch 100/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.0623 - val_loss: 115.8137\n",
            "Epoch 101/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.5658 - val_loss: 147.1659\n",
            "Epoch 102/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.2651 - val_loss: 105.2498\n",
            "Epoch 103/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.1711 - val_loss: 109.8734\n",
            "Epoch 104/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.0590 - val_loss: 138.0196\n",
            "Epoch 105/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.9826 - val_loss: 107.0459\n",
            "Epoch 106/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.0466 - val_loss: 115.7636\n",
            "Epoch 107/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.6171 - val_loss: 113.4743\n",
            "Epoch 108/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.6620 - val_loss: 107.5875\n",
            "Epoch 109/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.8645 - val_loss: 103.7041\n",
            "Epoch 110/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.5671 - val_loss: 108.8011\n",
            "Epoch 111/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.5128 - val_loss: 113.7816\n",
            "Epoch 112/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.3548 - val_loss: 104.7674\n",
            "Epoch 113/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.3548 - val_loss: 112.3851\n",
            "Epoch 114/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.4033 - val_loss: 100.1353\n",
            "Epoch 115/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.9719 - val_loss: 105.4588\n",
            "Epoch 116/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.1039 - val_loss: 121.5369\n",
            "Epoch 117/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.0049 - val_loss: 102.6592\n",
            "Epoch 118/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.0349 - val_loss: 101.2083\n",
            "Epoch 119/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.8581 - val_loss: 109.0624\n",
            "Epoch 120/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.0694 - val_loss: 109.8008\n",
            "Epoch 121/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.1420 - val_loss: 101.1602\n",
            "Epoch 122/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8018 - val_loss: 106.1074\n",
            "Epoch 123/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8047 - val_loss: 101.5251\n",
            "Epoch 124/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5411 - val_loss: 99.5242\n",
            "Epoch 125/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5660 - val_loss: 116.5194\n",
            "Epoch 126/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.6637 - val_loss: 115.5838\n",
            "Epoch 127/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.7758 - val_loss: 116.2853\n",
            "Epoch 128/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.2819 - val_loss: 113.1349\n",
            "Epoch 129/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.3832 - val_loss: 110.0302\n",
            "Epoch 130/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.1718 - val_loss: 136.3682\n",
            "Epoch 131/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.3048 - val_loss: 106.6054\n",
            "Epoch 132/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.2867 - val_loss: 102.9061\n",
            "Epoch 133/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.0874 - val_loss: 137.5358\n",
            "Epoch 134/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.3498 - val_loss: 127.5661\n",
            "Epoch 135/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.0512 - val_loss: 105.7504\n",
            "Epoch 136/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.1501 - val_loss: 98.7071\n",
            "Epoch 137/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.1158 - val_loss: 99.7743\n",
            "Epoch 138/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.2275 - val_loss: 111.2800\n",
            "Epoch 139/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.9898 - val_loss: 157.1179\n",
            "Epoch 140/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.0515 - val_loss: 109.0594\n",
            "Epoch 141/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.9754 - val_loss: 149.4078\n",
            "Epoch 142/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.7527 - val_loss: 101.7014\n",
            "Epoch 143/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.7860 - val_loss: 123.7056\n",
            "Epoch 144/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.7153 - val_loss: 185.2195\n",
            "Epoch 145/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.5613 - val_loss: 125.8117\n",
            "Epoch 146/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.4322 - val_loss: 114.0424\n",
            "Epoch 147/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.5716 - val_loss: 140.5475\n",
            "Epoch 148/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.5867 - val_loss: 104.3789\n",
            "Epoch 149/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.6432 - val_loss: 109.9469\n",
            "Epoch 150/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.6228 - val_loss: 109.7604\n",
            "Epoch 151/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.3761 - val_loss: 99.2597\n",
            "Epoch 152/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.4441 - val_loss: 107.3834\n",
            "Epoch 153/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.4274 - val_loss: 123.2751\n",
            "Epoch 154/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.4496 - val_loss: 99.9921\n",
            "Epoch 155/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.4778 - val_loss: 109.3430\n",
            "Epoch 156/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.0724 - val_loss: 100.9189\n",
            "Epoch 157/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.3585 - val_loss: 98.4199\n",
            "Epoch 158/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.1929 - val_loss: 107.1540\n",
            "Epoch 159/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.2532 - val_loss: 105.0181\n",
            "Epoch 160/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.3001 - val_loss: 99.5557\n",
            "Epoch 161/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.1961 - val_loss: 107.8398\n",
            "Epoch 162/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.1114 - val_loss: 105.8859\n",
            "Epoch 163/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.9788 - val_loss: 107.3201\n",
            "Epoch 164/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.8852 - val_loss: 123.2223\n",
            "Epoch 165/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.9310 - val_loss: 144.0682\n",
            "Epoch 166/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.8935 - val_loss: 242.1070\n",
            "Epoch 167/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.1162 - val_loss: 98.0702\n",
            "Epoch 168/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.8262 - val_loss: 100.6951\n",
            "Epoch 169/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.7209 - val_loss: 112.3865\n",
            "Epoch 170/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.7385 - val_loss: 117.2572\n",
            "Epoch 171/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.6301 - val_loss: 112.1553\n",
            "Epoch 172/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.7666 - val_loss: 106.3026\n",
            "Epoch 173/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.7875 - val_loss: 98.3198\n",
            "Epoch 174/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.7542 - val_loss: 115.7795\n",
            "Epoch 175/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.5707 - val_loss: 111.2540\n",
            "Epoch 176/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.6070 - val_loss: 134.3730\n",
            "Epoch 177/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.5463 - val_loss: 101.6902\n",
            "Epoch 178/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.5201 - val_loss: 104.9433\n",
            "Epoch 179/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.5190 - val_loss: 107.7162\n",
            "Epoch 180/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.3478 - val_loss: 106.2646\n",
            "Epoch 181/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.3532 - val_loss: 117.4671\n",
            "Epoch 182/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.2926 - val_loss: 118.8080\n",
            "Epoch 183/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.2577 - val_loss: 106.8897\n",
            "Epoch 184/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.1149 - val_loss: 98.3504\n",
            "Epoch 185/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.0951 - val_loss: 96.6330\n",
            "Epoch 186/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.1006 - val_loss: 107.5093\n",
            "Epoch 187/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0942 - val_loss: 106.4629\n",
            "Epoch 188/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.1891 - val_loss: 113.8092\n",
            "Epoch 189/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.1038 - val_loss: 98.4533\n",
            "Epoch 190/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.9056 - val_loss: 107.9973\n",
            "Epoch 191/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.9820 - val_loss: 124.3410\n",
            "Epoch 192/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.7659 - val_loss: 118.9459\n",
            "Epoch 193/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.8818 - val_loss: 152.5952\n",
            "Epoch 194/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.7576 - val_loss: 96.5719\n",
            "Epoch 195/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.7019 - val_loss: 101.2016\n",
            "Epoch 196/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6562 - val_loss: 102.8950\n",
            "Epoch 197/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.4279 - val_loss: 117.1137\n",
            "Epoch 198/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.6329 - val_loss: 107.8911\n",
            "Epoch 199/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.4041 - val_loss: 129.9511\n",
            "Epoch 200/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.5313 - val_loss: 103.5999\n",
            "Epoch 201/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.4156 - val_loss: 125.9359\n",
            "Epoch 202/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.3250 - val_loss: 133.4314\n",
            "Epoch 203/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.3729 - val_loss: 97.3876\n",
            "Epoch 204/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.2318 - val_loss: 112.4045\n",
            "Epoch 205/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.1136 - val_loss: 106.7947\n",
            "Epoch 206/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.1889 - val_loss: 113.7685\n",
            "Epoch 207/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.2846 - val_loss: 117.3507\n",
            "Epoch 208/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.0108 - val_loss: 119.6362\n",
            "Epoch 209/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.9630 - val_loss: 105.4594\n",
            "Epoch 210/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.9576 - val_loss: 108.8902\n",
            "Epoch 211/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.8514 - val_loss: 110.0887\n",
            "Epoch 212/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.9015 - val_loss: 104.8787\n",
            "Epoch 213/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.8059 - val_loss: 99.9169\n",
            "Epoch 214/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.6693 - val_loss: 99.2799\n",
            "Epoch 215/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.7658 - val_loss: 106.1983\n",
            "Epoch 216/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.5930 - val_loss: 112.8330\n",
            "Epoch 217/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.5808 - val_loss: 105.2798\n",
            "Epoch 218/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.5282 - val_loss: 98.2321\n",
            "Epoch 219/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.7175 - val_loss: 100.7385\n",
            "Epoch 220/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.5253 - val_loss: 95.6550\n",
            "Epoch 221/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 86.2529 - val_loss: 127.8363\n",
            "Epoch 222/400\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.3985 - val_loss: 98.4832\n",
            "Epoch 223/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.2333 - val_loss: 114.1380\n",
            "Epoch 224/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.2741 - val_loss: 114.9021\n",
            "Epoch 225/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.2765 - val_loss: 95.6621\n",
            "Epoch 226/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.0780 - val_loss: 112.3272\n",
            "Epoch 227/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.1462 - val_loss: 113.9123\n",
            "Epoch 228/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.2390 - val_loss: 100.8653\n",
            "Epoch 229/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.0351 - val_loss: 97.9910\n",
            "Epoch 230/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9836 - val_loss: 145.3780\n",
            "Epoch 231/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.7321 - val_loss: 97.6544\n",
            "Epoch 232/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8915 - val_loss: 109.0338\n",
            "Epoch 233/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.7846 - val_loss: 164.0656\n",
            "Epoch 234/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.7946 - val_loss: 104.4560\n",
            "Epoch 235/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.6659 - val_loss: 104.0242\n",
            "Epoch 236/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5674 - val_loss: 105.3225\n",
            "Epoch 237/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8613 - val_loss: 109.7828\n",
            "Epoch 238/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.6106 - val_loss: 121.0858\n",
            "Epoch 239/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.6842 - val_loss: 96.4025\n",
            "Epoch 240/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.6149 - val_loss: 125.0473\n",
            "Epoch 241/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.3937 - val_loss: 97.6853\n",
            "Epoch 242/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.3799 - val_loss: 107.5462\n",
            "Epoch 243/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.4093 - val_loss: 95.2306\n",
            "Epoch 244/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.2819 - val_loss: 98.4416\n",
            "Epoch 245/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.3529 - val_loss: 94.6946\n",
            "Epoch 246/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.4357 - val_loss: 108.8468\n",
            "Epoch 247/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.2403 - val_loss: 98.7904\n",
            "Epoch 248/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.4621 - val_loss: 100.2673\n",
            "Epoch 249/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.1613 - val_loss: 130.9915\n",
            "Epoch 250/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.2266 - val_loss: 112.2228\n",
            "Epoch 251/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.2590 - val_loss: 176.0715\n",
            "Epoch 252/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.2446 - val_loss: 99.0381\n",
            "Epoch 253/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1768 - val_loss: 103.3576\n",
            "Epoch 254/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.9893 - val_loss: 119.6064\n",
            "Epoch 255/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.0848 - val_loss: 101.4423\n",
            "Epoch 256/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1636 - val_loss: 110.9368\n",
            "Epoch 257/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.0898 - val_loss: 97.9203\n",
            "Epoch 258/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.8115 - val_loss: 97.0982\n",
            "Epoch 259/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.9886 - val_loss: 103.6151\n",
            "Epoch 260/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.9512 - val_loss: 105.6741\n",
            "Epoch 261/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.9208 - val_loss: 102.2769\n",
            "Epoch 262/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.8453 - val_loss: 112.8483\n",
            "Epoch 263/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.8348 - val_loss: 104.1130\n",
            "Epoch 264/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.7562 - val_loss: 119.0556\n",
            "Epoch 265/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.9240 - val_loss: 108.5839\n",
            "Epoch 266/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.9977 - val_loss: 99.7189\n",
            "Epoch 267/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.7042 - val_loss: 96.3882\n",
            "Epoch 268/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.6901 - val_loss: 118.1639\n",
            "Epoch 269/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.6512 - val_loss: 105.6801\n",
            "Epoch 270/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.6803 - val_loss: 104.9136\n",
            "Epoch 271/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.6472 - val_loss: 102.5341\n",
            "Epoch 272/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.6287 - val_loss: 129.3977\n",
            "Epoch 273/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.6248 - val_loss: 103.8731\n",
            "Epoch 274/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.4403 - val_loss: 97.0413\n",
            "Epoch 275/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.4539 - val_loss: 97.4744\n",
            "Epoch 276/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.5265 - val_loss: 97.1619\n",
            "Epoch 277/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.4195 - val_loss: 102.4331\n",
            "Epoch 278/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.3700 - val_loss: 102.5754\n",
            "Epoch 279/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.7203 - val_loss: 140.5337\n",
            "Epoch 280/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.5499 - val_loss: 94.4952\n",
            "Epoch 281/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.5647 - val_loss: 114.1588\n",
            "Epoch 282/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.4243 - val_loss: 101.3725\n",
            "Epoch 283/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.4381 - val_loss: 94.6496\n",
            "Epoch 284/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.6214 - val_loss: 97.4745\n",
            "Epoch 285/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.3414 - val_loss: 98.3673\n",
            "Epoch 286/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.5957 - val_loss: 105.4002\n",
            "Epoch 287/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.4175 - val_loss: 113.1624\n",
            "Epoch 288/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.3247 - val_loss: 130.5366\n",
            "Epoch 289/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.3671 - val_loss: 95.1069\n",
            "Epoch 290/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.3657 - val_loss: 113.1087\n",
            "Epoch 291/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.4679 - val_loss: 94.4792\n",
            "Epoch 292/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.3507 - val_loss: 124.3836\n",
            "Epoch 293/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1727 - val_loss: 94.6479\n",
            "Epoch 294/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.3120 - val_loss: 93.9642\n",
            "Epoch 295/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1374 - val_loss: 112.5598\n",
            "Epoch 296/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.3240 - val_loss: 103.4682\n",
            "Epoch 297/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.1446 - val_loss: 95.4893\n",
            "Epoch 298/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.1916 - val_loss: 103.8229\n",
            "Epoch 299/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.0554 - val_loss: 100.0245\n",
            "Epoch 300/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.1378 - val_loss: 101.9062\n",
            "Epoch 301/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.0772 - val_loss: 97.2831\n",
            "Epoch 302/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1304 - val_loss: 108.5614\n",
            "Epoch 303/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.1323 - val_loss: 102.4350\n",
            "Epoch 304/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.9608 - val_loss: 101.2669\n",
            "Epoch 305/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.2187 - val_loss: 115.9978\n",
            "Epoch 306/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.0085 - val_loss: 116.5683\n",
            "Epoch 307/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.0110 - val_loss: 104.6241\n",
            "Epoch 308/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1529 - val_loss: 102.8607\n",
            "Epoch 309/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1727 - val_loss: 104.9269\n",
            "Epoch 310/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.9111 - val_loss: 105.4333\n",
            "Epoch 311/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.9151 - val_loss: 94.0148\n",
            "Epoch 312/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1654 - val_loss: 113.4797\n",
            "Epoch 313/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1443 - val_loss: 99.8642\n",
            "Epoch 314/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.9338 - val_loss: 96.9321\n",
            "Epoch 315/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.8364 - val_loss: 99.7366\n",
            "Epoch 316/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.9426 - val_loss: 93.4259\n",
            "Epoch 317/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.8690 - val_loss: 137.0153\n",
            "Epoch 318/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.9354 - val_loss: 95.5210\n",
            "Epoch 319/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.8855 - val_loss: 106.9552\n",
            "Epoch 320/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.8251 - val_loss: 95.4140\n",
            "Epoch 321/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.8818 - val_loss: 104.2220\n",
            "Epoch 322/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.8740 - val_loss: 121.2577\n",
            "Epoch 323/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.8298 - val_loss: 95.5933\n",
            "Epoch 324/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.8246 - val_loss: 95.0835\n",
            "Epoch 325/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.7820 - val_loss: 97.5375\n",
            "Epoch 326/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.6608 - val_loss: 121.0146\n",
            "Epoch 327/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.8619 - val_loss: 96.7736\n",
            "Epoch 328/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.8561 - val_loss: 93.8324\n",
            "Epoch 329/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.8659 - val_loss: 95.6762\n",
            "Epoch 330/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.6600 - val_loss: 98.8163\n",
            "Epoch 331/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.7499 - val_loss: 94.7136\n",
            "Epoch 332/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.7208 - val_loss: 99.9011\n",
            "Epoch 333/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.6556 - val_loss: 101.5937\n",
            "Epoch 334/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.6924 - val_loss: 98.5655\n",
            "Epoch 335/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.6770 - val_loss: 109.6285\n",
            "Epoch 336/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.7584 - val_loss: 102.8917\n",
            "Epoch 337/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.6600 - val_loss: 112.7994\n",
            "Epoch 338/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.5269 - val_loss: 104.9875\n",
            "Epoch 339/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.7540 - val_loss: 108.9774\n",
            "Epoch 340/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.6446 - val_loss: 102.5961\n",
            "Epoch 341/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.6527 - val_loss: 97.2840\n",
            "Epoch 342/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.5774 - val_loss: 99.0893\n",
            "Epoch 343/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.5629 - val_loss: 99.6325\n",
            "Epoch 344/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.5452 - val_loss: 103.6693\n",
            "Epoch 345/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.5812 - val_loss: 98.3528\n",
            "Epoch 346/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.5726 - val_loss: 154.5006\n",
            "Epoch 347/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.5339 - val_loss: 96.0655\n",
            "Epoch 348/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3905 - val_loss: 98.7381\n",
            "Epoch 349/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.4665 - val_loss: 117.2475\n",
            "Epoch 350/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.5577 - val_loss: 114.3870\n",
            "Epoch 351/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.5103 - val_loss: 101.2369\n",
            "Epoch 352/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.5290 - val_loss: 96.0014\n",
            "Epoch 353/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.5624 - val_loss: 92.8421\n",
            "Epoch 354/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.5793 - val_loss: 116.2859\n",
            "Epoch 355/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.4184 - val_loss: 108.2999\n",
            "Epoch 356/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3635 - val_loss: 91.4586\n",
            "Epoch 357/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.4568 - val_loss: 119.3798\n",
            "Epoch 358/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.5208 - val_loss: 101.4166\n",
            "Epoch 359/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.2019 - val_loss: 95.8894\n",
            "Epoch 360/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.5008 - val_loss: 102.7555\n",
            "Epoch 361/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.4827 - val_loss: 102.4235\n",
            "Epoch 362/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.3415 - val_loss: 102.9546\n",
            "Epoch 363/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3835 - val_loss: 93.2865\n",
            "Epoch 364/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1460 - val_loss: 98.5701\n",
            "Epoch 365/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.4516 - val_loss: 109.7183\n",
            "Epoch 366/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.2027 - val_loss: 128.9128\n",
            "Epoch 367/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.2928 - val_loss: 113.3722\n",
            "Epoch 368/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3268 - val_loss: 106.3802\n",
            "Epoch 369/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3047 - val_loss: 105.3750\n",
            "Epoch 370/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.1300 - val_loss: 103.1062\n",
            "Epoch 371/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3412 - val_loss: 93.1962\n",
            "Epoch 372/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.2695 - val_loss: 120.5771\n",
            "Epoch 373/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.2645 - val_loss: 108.3769\n",
            "Epoch 374/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3543 - val_loss: 124.3108\n",
            "Epoch 375/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.1593 - val_loss: 163.2098\n",
            "Epoch 376/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.3309 - val_loss: 95.9164\n",
            "Epoch 377/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.2596 - val_loss: 98.8991\n",
            "Epoch 378/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.2305 - val_loss: 118.4571\n",
            "Epoch 379/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0143 - val_loss: 93.4523\n",
            "Epoch 380/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1921 - val_loss: 92.9968\n",
            "Epoch 381/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3298 - val_loss: 98.5973\n",
            "Epoch 382/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.9624 - val_loss: 95.4626\n",
            "Epoch 383/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.1027 - val_loss: 93.8477\n",
            "Epoch 384/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.2117 - val_loss: 93.6007\n",
            "Epoch 385/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.2593 - val_loss: 95.8278\n",
            "Epoch 386/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1361 - val_loss: 98.2175\n",
            "Epoch 387/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1552 - val_loss: 103.4393\n",
            "Epoch 388/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0389 - val_loss: 95.0816\n",
            "Epoch 389/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.0266 - val_loss: 95.1923\n",
            "Epoch 390/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0301 - val_loss: 99.0305\n",
            "Epoch 391/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0662 - val_loss: 93.8052\n",
            "Epoch 392/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 82.9790 - val_loss: 119.5091\n",
            "Epoch 393/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1057 - val_loss: 102.0297\n",
            "Epoch 394/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 83.0862 - val_loss: 106.0753\n",
            "Epoch 395/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1015 - val_loss: 94.8427\n",
            "Epoch 396/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.9840 - val_loss: 108.7019\n",
            "Epoch 397/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1268 - val_loss: 96.3233\n",
            "Epoch 398/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1425 - val_loss: 99.1933\n",
            "Epoch 399/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0168 - val_loss: 110.0571\n",
            "Epoch 400/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.9804 - val_loss: 94.3523\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "1aad764b-ea99-4072-c669-de0e2120bc6e",
        "id": "lYDcggm8CSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.8612087732977046 \n",
            "MAE:  7.260075238007743 \n",
            "SD:  9.675257497331652\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "1e02b933-6bb0-49ee-ccf2-0fd310312b99",
        "id": "VpKjAxdPCSwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "75b13b11-2f50-4074-c027-1e022d943e71",
        "id": "U0vAhaD0CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,169\n",
            "Trainable params: 1,137\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "outputId": "58481a0b-5e33-4872-baea-b67409bcb057",
        "id": "dcXAOqd2CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 12372.0127 - val_loss: 12233.7705\n",
            "Epoch 2/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11969.7129 - val_loss: 12024.2061\n",
            "Epoch 3/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11439.7842 - val_loss: 10729.0371\n",
            "Epoch 4/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10762.8408 - val_loss: 11157.2900\n",
            "Epoch 5/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 9950.5459 - val_loss: 10083.4736\n",
            "Epoch 6/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9047.3506 - val_loss: 9902.1934\n",
            "Epoch 7/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8100.0928 - val_loss: 7910.9722\n",
            "Epoch 8/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7144.3091 - val_loss: 9732.7500\n",
            "Epoch 9/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 6210.0225 - val_loss: 10028.2588\n",
            "Epoch 10/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 5316.8853 - val_loss: 5098.0835\n",
            "Epoch 11/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4490.4277 - val_loss: 2691.4421\n",
            "Epoch 12/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3746.1306 - val_loss: 3409.1746\n",
            "Epoch 13/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3085.5791 - val_loss: 2172.0374\n",
            "Epoch 14/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2509.8416 - val_loss: 2628.8467\n",
            "Epoch 15/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2026.0696 - val_loss: 1572.8987\n",
            "Epoch 16/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1617.4694 - val_loss: 1393.0601\n",
            "Epoch 17/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1289.3101 - val_loss: 1689.1368\n",
            "Epoch 18/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1024.7220 - val_loss: 1288.4858\n",
            "Epoch 19/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 802.0248 - val_loss: 4201.2192\n",
            "Epoch 20/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 633.1888 - val_loss: 1242.6320\n",
            "Epoch 21/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 507.3847 - val_loss: 586.4042\n",
            "Epoch 22/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 404.5567 - val_loss: 934.3970\n",
            "Epoch 23/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 335.3246 - val_loss: 4581.5444\n",
            "Epoch 24/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 304.9933 - val_loss: 1111.8289\n",
            "Epoch 25/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 243.3228 - val_loss: 506.5079\n",
            "Epoch 26/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 216.5117 - val_loss: 286.3486\n",
            "Epoch 27/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 185.0408 - val_loss: 480.5355\n",
            "Epoch 28/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 171.6176 - val_loss: 1655.3745\n",
            "Epoch 29/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 162.2163 - val_loss: 1043.3564\n",
            "Epoch 30/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 155.1489 - val_loss: 250.3799\n",
            "Epoch 31/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 137.1183 - val_loss: 826.4810\n",
            "Epoch 32/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 125.5762 - val_loss: 300.8013\n",
            "Epoch 33/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 123.3306 - val_loss: 722.4577\n",
            "Epoch 34/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 119.7850 - val_loss: 221.8686\n",
            "Epoch 35/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 116.7053 - val_loss: 212.7902\n",
            "Epoch 36/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 116.1937 - val_loss: 120.5183\n",
            "Epoch 37/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 114.7989 - val_loss: 155.8968\n",
            "Epoch 38/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 113.4453 - val_loss: 127.9614\n",
            "Epoch 39/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 113.2195 - val_loss: 124.8490\n",
            "Epoch 40/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 112.6727 - val_loss: 142.7045\n",
            "Epoch 41/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 111.9679 - val_loss: 172.2906\n",
            "Epoch 42/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 111.8939 - val_loss: 129.0253\n",
            "Epoch 43/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 112.1048 - val_loss: 706.4354\n",
            "Epoch 44/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.5642 - val_loss: 150.5634\n",
            "Epoch 45/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.7005 - val_loss: 223.5667\n",
            "Epoch 46/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.9790 - val_loss: 288.1618\n",
            "Epoch 47/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.0713 - val_loss: 180.8289\n",
            "Epoch 48/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.1151 - val_loss: 153.1637\n",
            "Epoch 49/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.7544 - val_loss: 137.1444\n",
            "Epoch 50/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 110.1294 - val_loss: 205.5378\n",
            "Epoch 51/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.6587 - val_loss: 140.7957\n",
            "Epoch 52/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.8452 - val_loss: 133.3077\n",
            "Epoch 53/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.9616 - val_loss: 420.1498\n",
            "Epoch 54/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.4181 - val_loss: 141.5307\n",
            "Epoch 55/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.1279 - val_loss: 163.6902\n",
            "Epoch 56/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.1151 - val_loss: 127.0718\n",
            "Epoch 57/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.3272 - val_loss: 233.7420\n",
            "Epoch 58/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 107.4852 - val_loss: 125.3073\n",
            "Epoch 59/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 107.5034 - val_loss: 156.8912\n",
            "Epoch 60/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 107.5131 - val_loss: 140.3517\n",
            "Epoch 61/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.2390 - val_loss: 252.3680\n",
            "Epoch 62/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 107.7136 - val_loss: 129.6607\n",
            "Epoch 63/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.2735 - val_loss: 132.0712\n",
            "Epoch 64/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.4729 - val_loss: 134.0343\n",
            "Epoch 65/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.6046 - val_loss: 158.8305\n",
            "Epoch 66/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.0255 - val_loss: 150.2757\n",
            "Epoch 67/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.1408 - val_loss: 120.6262\n",
            "Epoch 68/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.6996 - val_loss: 115.6192\n",
            "Epoch 69/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.1245 - val_loss: 341.4470\n",
            "Epoch 70/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4389 - val_loss: 121.0048\n",
            "Epoch 71/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.9981 - val_loss: 137.4111\n",
            "Epoch 72/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.3246 - val_loss: 144.1036\n",
            "Epoch 73/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.5464 - val_loss: 117.1123\n",
            "Epoch 74/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.3271 - val_loss: 231.1591\n",
            "Epoch 75/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.8726 - val_loss: 111.8674\n",
            "Epoch 76/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.9987 - val_loss: 372.9773\n",
            "Epoch 77/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.1204 - val_loss: 176.2830\n",
            "Epoch 78/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.8529 - val_loss: 193.8618\n",
            "Epoch 79/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.7960 - val_loss: 339.0571\n",
            "Epoch 80/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.6803 - val_loss: 157.1403\n",
            "Epoch 81/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.0596 - val_loss: 444.3315\n",
            "Epoch 82/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.6278 - val_loss: 122.5062\n",
            "Epoch 83/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.9079 - val_loss: 141.4541\n",
            "Epoch 84/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.4532 - val_loss: 144.8819\n",
            "Epoch 85/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2854 - val_loss: 113.7699\n",
            "Epoch 86/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.3966 - val_loss: 345.5022\n",
            "Epoch 87/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1357 - val_loss: 120.0350\n",
            "Epoch 88/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0967 - val_loss: 130.0893\n",
            "Epoch 89/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0103 - val_loss: 122.1243\n",
            "Epoch 90/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0046 - val_loss: 153.3670\n",
            "Epoch 91/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.0565 - val_loss: 119.8018\n",
            "Epoch 92/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.9821 - val_loss: 111.8867\n",
            "Epoch 93/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.8575 - val_loss: 204.2582\n",
            "Epoch 94/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.1339 - val_loss: 111.1003\n",
            "Epoch 95/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.7512 - val_loss: 330.7473\n",
            "Epoch 96/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.9298 - val_loss: 115.9326\n",
            "Epoch 97/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1155 - val_loss: 172.2641\n",
            "Epoch 98/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.9251 - val_loss: 114.1576\n",
            "Epoch 99/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.6579 - val_loss: 130.2130\n",
            "Epoch 100/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6178 - val_loss: 121.7033\n",
            "Epoch 101/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7653 - val_loss: 144.8900\n",
            "Epoch 102/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.4593 - val_loss: 120.6796\n",
            "Epoch 103/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5325 - val_loss: 201.8588\n",
            "Epoch 104/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5672 - val_loss: 133.6616\n",
            "Epoch 105/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4798 - val_loss: 139.5264\n",
            "Epoch 106/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5162 - val_loss: 198.7733\n",
            "Epoch 107/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5901 - val_loss: 121.0316\n",
            "Epoch 108/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6908 - val_loss: 123.2654\n",
            "Epoch 109/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.9872 - val_loss: 131.9782\n",
            "Epoch 110/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4391 - val_loss: 127.8232\n",
            "Epoch 111/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.3565 - val_loss: 210.3458\n",
            "Epoch 112/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1075 - val_loss: 118.2402\n",
            "Epoch 113/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.4654 - val_loss: 129.1027\n",
            "Epoch 114/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.9919 - val_loss: 112.2909\n",
            "Epoch 115/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.6142 - val_loss: 122.5051\n",
            "Epoch 116/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.3247 - val_loss: 163.1362\n",
            "Epoch 117/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.1077 - val_loss: 115.1019\n",
            "Epoch 118/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.8821 - val_loss: 121.3792\n",
            "Epoch 119/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8150 - val_loss: 153.5858\n",
            "Epoch 120/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1735 - val_loss: 118.4596\n",
            "Epoch 121/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.1032 - val_loss: 154.6943\n",
            "Epoch 122/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.8978 - val_loss: 141.4079\n",
            "Epoch 123/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.1513 - val_loss: 111.2921\n",
            "Epoch 124/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.8127 - val_loss: 172.9305\n",
            "Epoch 125/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.6962 - val_loss: 111.8815\n",
            "Epoch 126/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8112 - val_loss: 113.3838\n",
            "Epoch 127/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.6148 - val_loss: 114.3541\n",
            "Epoch 128/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.4468 - val_loss: 110.9141\n",
            "Epoch 129/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3512 - val_loss: 114.1384\n",
            "Epoch 130/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6795 - val_loss: 133.5777\n",
            "Epoch 131/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7108 - val_loss: 120.0545\n",
            "Epoch 132/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.5361 - val_loss: 117.0795\n",
            "Epoch 133/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3108 - val_loss: 111.4792\n",
            "Epoch 134/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1185 - val_loss: 113.7045\n",
            "Epoch 135/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9843 - val_loss: 111.7322\n",
            "Epoch 136/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.2197 - val_loss: 146.2488\n",
            "Epoch 137/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0314 - val_loss: 135.7998\n",
            "Epoch 138/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0369 - val_loss: 115.0391\n",
            "Epoch 139/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.7481 - val_loss: 240.9401\n",
            "Epoch 140/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7047 - val_loss: 146.3451\n",
            "Epoch 141/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.7728 - val_loss: 138.3263\n",
            "Epoch 142/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.5215 - val_loss: 183.9609\n",
            "Epoch 143/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.2863 - val_loss: 172.4777\n",
            "Epoch 144/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.5064 - val_loss: 115.7341\n",
            "Epoch 145/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.1163 - val_loss: 135.0887\n",
            "Epoch 146/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9997 - val_loss: 231.8993\n",
            "Epoch 147/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.9252 - val_loss: 154.5833\n",
            "Epoch 148/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.7730 - val_loss: 109.6319\n",
            "Epoch 149/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.7444 - val_loss: 125.8868\n",
            "Epoch 150/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.3294 - val_loss: 110.0152\n",
            "Epoch 151/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.4254 - val_loss: 119.1965\n",
            "Epoch 152/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.8842 - val_loss: 124.7210\n",
            "Epoch 153/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.9194 - val_loss: 175.5406\n",
            "Epoch 154/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6018 - val_loss: 131.1366\n",
            "Epoch 155/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.1050 - val_loss: 115.1517\n",
            "Epoch 156/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.5118 - val_loss: 152.7245\n",
            "Epoch 157/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.1327 - val_loss: 119.0960\n",
            "Epoch 158/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.9957 - val_loss: 118.3904\n",
            "Epoch 159/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 97.2960 - val_loss: 212.4277\n",
            "Epoch 160/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 97.2892 - val_loss: 197.6578\n",
            "Epoch 161/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.9838 - val_loss: 138.5274\n",
            "Epoch 162/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.8551 - val_loss: 106.5602\n",
            "Epoch 163/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 96.5868 - val_loss: 115.8262\n",
            "Epoch 164/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.5771 - val_loss: 120.0208\n",
            "Epoch 165/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 96.1578 - val_loss: 126.9568\n",
            "Epoch 166/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.2672 - val_loss: 111.2132\n",
            "Epoch 167/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.9994 - val_loss: 109.4097\n",
            "Epoch 168/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.9254 - val_loss: 118.7052\n",
            "Epoch 169/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.6371 - val_loss: 116.7757\n",
            "Epoch 170/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.4154 - val_loss: 264.5714\n",
            "Epoch 171/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.6800 - val_loss: 132.9181\n",
            "Epoch 172/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.1972 - val_loss: 117.8929\n",
            "Epoch 173/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.2941 - val_loss: 103.0976\n",
            "Epoch 174/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.0183 - val_loss: 108.8079\n",
            "Epoch 175/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.9047 - val_loss: 105.3096\n",
            "Epoch 176/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 95.0123 - val_loss: 109.2838\n",
            "Epoch 177/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.9568 - val_loss: 138.3410\n",
            "Epoch 178/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.5531 - val_loss: 185.3966\n",
            "Epoch 179/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.7430 - val_loss: 135.4872\n",
            "Epoch 180/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.3389 - val_loss: 142.6049\n",
            "Epoch 181/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1858 - val_loss: 106.9806\n",
            "Epoch 182/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 94.1698 - val_loss: 117.3306\n",
            "Epoch 183/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.3355 - val_loss: 114.2751\n",
            "Epoch 184/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.2945 - val_loss: 111.5093\n",
            "Epoch 185/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.0762 - val_loss: 130.7442\n",
            "Epoch 186/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.7988 - val_loss: 108.3111\n",
            "Epoch 187/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.8211 - val_loss: 132.7083\n",
            "Epoch 188/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.5063 - val_loss: 109.4881\n",
            "Epoch 189/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.4612 - val_loss: 112.6893\n",
            "Epoch 190/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.5940 - val_loss: 110.4075\n",
            "Epoch 191/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.6376 - val_loss: 153.9750\n",
            "Epoch 192/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.3657 - val_loss: 105.2849\n",
            "Epoch 193/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.2131 - val_loss: 107.1481\n",
            "Epoch 194/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.3446 - val_loss: 107.6373\n",
            "Epoch 195/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.8809 - val_loss: 108.8557\n",
            "Epoch 196/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.6956 - val_loss: 119.1938\n",
            "Epoch 197/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.4635 - val_loss: 103.6877\n",
            "Epoch 198/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.2140 - val_loss: 115.7717\n",
            "Epoch 199/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 92.3818 - val_loss: 136.1331\n",
            "Epoch 200/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.0539 - val_loss: 135.8181\n",
            "Epoch 201/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7372 - val_loss: 104.3390\n",
            "Epoch 202/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 91.3770 - val_loss: 119.4491\n",
            "Epoch 203/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.4164 - val_loss: 114.4312\n",
            "Epoch 204/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.2273 - val_loss: 118.9554\n",
            "Epoch 205/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.9400 - val_loss: 139.7846\n",
            "Epoch 206/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.0646 - val_loss: 103.7218\n",
            "Epoch 207/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.8759 - val_loss: 112.2103\n",
            "Epoch 208/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.6731 - val_loss: 108.9492\n",
            "Epoch 209/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5635 - val_loss: 105.8750\n",
            "Epoch 210/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.4538 - val_loss: 105.7279\n",
            "Epoch 211/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.3922 - val_loss: 100.7308\n",
            "Epoch 212/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.0834 - val_loss: 99.1295\n",
            "Epoch 213/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 90.1882 - val_loss: 103.7918\n",
            "Epoch 214/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.9781 - val_loss: 178.2672\n",
            "Epoch 215/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.9091 - val_loss: 117.8016\n",
            "Epoch 216/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.7677 - val_loss: 117.5461\n",
            "Epoch 217/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.7894 - val_loss: 104.8247\n",
            "Epoch 218/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.6291 - val_loss: 108.3070\n",
            "Epoch 219/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.5333 - val_loss: 104.5342\n",
            "Epoch 220/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.5387 - val_loss: 118.5432\n",
            "Epoch 221/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.1847 - val_loss: 133.1327\n",
            "Epoch 222/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.4419 - val_loss: 140.5039\n",
            "Epoch 223/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.1277 - val_loss: 105.6652\n",
            "Epoch 224/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.3120 - val_loss: 115.2035\n",
            "Epoch 225/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.0942 - val_loss: 159.8105\n",
            "Epoch 226/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.9024 - val_loss: 147.6086\n",
            "Epoch 227/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.9524 - val_loss: 110.6292\n",
            "Epoch 228/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.0458 - val_loss: 98.5085\n",
            "Epoch 229/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.0075 - val_loss: 96.1879\n",
            "Epoch 230/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.8282 - val_loss: 97.9608\n",
            "Epoch 231/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.6472 - val_loss: 163.3214\n",
            "Epoch 232/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.7454 - val_loss: 109.2084\n",
            "Epoch 233/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.5929 - val_loss: 98.0261\n",
            "Epoch 234/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.4763 - val_loss: 100.0433\n",
            "Epoch 235/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.5938 - val_loss: 132.4467\n",
            "Epoch 236/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.4788 - val_loss: 120.6091\n",
            "Epoch 237/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.4681 - val_loss: 104.2404\n",
            "Epoch 238/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.3413 - val_loss: 162.9689\n",
            "Epoch 239/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.4828 - val_loss: 101.7151\n",
            "Epoch 240/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.3047 - val_loss: 142.8992\n",
            "Epoch 241/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.5076 - val_loss: 110.5911\n",
            "Epoch 242/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.3923 - val_loss: 122.2592\n",
            "Epoch 243/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0816 - val_loss: 109.5095\n",
            "Epoch 244/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0596 - val_loss: 123.4302\n",
            "Epoch 245/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.1530 - val_loss: 119.3024\n",
            "Epoch 246/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 88.0180 - val_loss: 97.4133\n",
            "Epoch 247/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.9436 - val_loss: 96.3386\n",
            "Epoch 248/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.9154 - val_loss: 104.0870\n",
            "Epoch 249/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8826 - val_loss: 110.8764\n",
            "Epoch 250/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8984 - val_loss: 185.5424\n",
            "Epoch 251/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8002 - val_loss: 101.6674\n",
            "Epoch 252/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.7312 - val_loss: 97.6977\n",
            "Epoch 253/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.9008 - val_loss: 122.2628\n",
            "Epoch 254/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.7129 - val_loss: 136.8246\n",
            "Epoch 255/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.7561 - val_loss: 97.9144\n",
            "Epoch 256/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.6289 - val_loss: 102.7498\n",
            "Epoch 257/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6059 - val_loss: 99.9246\n",
            "Epoch 258/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.5080 - val_loss: 101.5475\n",
            "Epoch 259/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.4860 - val_loss: 100.1647\n",
            "Epoch 260/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.5732 - val_loss: 122.3723\n",
            "Epoch 261/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.4796 - val_loss: 113.8852\n",
            "Epoch 262/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.4769 - val_loss: 102.7906\n",
            "Epoch 263/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.4392 - val_loss: 99.3850\n",
            "Epoch 264/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.3265 - val_loss: 148.0245\n",
            "Epoch 265/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.4775 - val_loss: 95.7801\n",
            "Epoch 266/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.4603 - val_loss: 104.1954\n",
            "Epoch 267/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.3401 - val_loss: 108.4156\n",
            "Epoch 268/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.4596 - val_loss: 101.3639\n",
            "Epoch 269/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.2300 - val_loss: 107.4599\n",
            "Epoch 270/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.3667 - val_loss: 99.7683\n",
            "Epoch 271/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.2649 - val_loss: 104.3988\n",
            "Epoch 272/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.0340 - val_loss: 119.5910\n",
            "Epoch 273/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.1704 - val_loss: 117.9196\n",
            "Epoch 274/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.3125 - val_loss: 96.2621\n",
            "Epoch 275/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.1039 - val_loss: 146.5543\n",
            "Epoch 276/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.3273 - val_loss: 99.1577\n",
            "Epoch 277/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.1213 - val_loss: 120.7950\n",
            "Epoch 278/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.9719 - val_loss: 102.7583\n",
            "Epoch 279/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.1687 - val_loss: 105.3706\n",
            "Epoch 280/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.9886 - val_loss: 94.2526\n",
            "Epoch 281/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 87.1379 - val_loss: 106.1850\n",
            "Epoch 282/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.9942 - val_loss: 98.2390\n",
            "Epoch 283/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.9523 - val_loss: 123.0422\n",
            "Epoch 284/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.9924 - val_loss: 129.7901\n",
            "Epoch 285/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.8594 - val_loss: 99.9000\n",
            "Epoch 286/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.8564 - val_loss: 124.7807\n",
            "Epoch 287/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.8620 - val_loss: 96.1676\n",
            "Epoch 288/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.9575 - val_loss: 98.1793\n",
            "Epoch 289/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.6779 - val_loss: 105.5130\n",
            "Epoch 290/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.7327 - val_loss: 95.7363\n",
            "Epoch 291/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.8558 - val_loss: 121.8915\n",
            "Epoch 292/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.7958 - val_loss: 108.9270\n",
            "Epoch 293/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.7111 - val_loss: 97.2984\n",
            "Epoch 294/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.6900 - val_loss: 98.6439\n",
            "Epoch 295/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.8479 - val_loss: 104.9652\n",
            "Epoch 296/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.6731 - val_loss: 120.7170\n",
            "Epoch 297/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.5166 - val_loss: 113.6676\n",
            "Epoch 298/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.5799 - val_loss: 179.1283\n",
            "Epoch 299/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.5941 - val_loss: 108.6850\n",
            "Epoch 300/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.5449 - val_loss: 126.3970\n",
            "Epoch 301/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.5828 - val_loss: 102.2896\n",
            "Epoch 302/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.6356 - val_loss: 99.0963\n",
            "Epoch 303/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.7240 - val_loss: 138.8956\n",
            "Epoch 304/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.6243 - val_loss: 99.1933\n",
            "Epoch 305/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.4448 - val_loss: 96.6695\n",
            "Epoch 306/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.4747 - val_loss: 93.2734\n",
            "Epoch 307/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.3667 - val_loss: 96.2026\n",
            "Epoch 308/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.3095 - val_loss: 131.3492\n",
            "Epoch 309/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.2944 - val_loss: 101.1725\n",
            "Epoch 310/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.5352 - val_loss: 100.2989\n",
            "Epoch 311/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.2879 - val_loss: 101.6561\n",
            "Epoch 312/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.3083 - val_loss: 99.6321\n",
            "Epoch 313/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.1720 - val_loss: 95.2532\n",
            "Epoch 314/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.2741 - val_loss: 94.1230\n",
            "Epoch 315/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.1048 - val_loss: 95.5370\n",
            "Epoch 316/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.1797 - val_loss: 106.7584\n",
            "Epoch 317/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.0727 - val_loss: 113.0181\n",
            "Epoch 318/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.0651 - val_loss: 123.7243\n",
            "Epoch 319/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.0933 - val_loss: 97.6357\n",
            "Epoch 320/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9698 - val_loss: 103.7097\n",
            "Epoch 321/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.0012 - val_loss: 114.2914\n",
            "Epoch 322/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9758 - val_loss: 101.8770\n",
            "Epoch 323/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.1701 - val_loss: 98.9925\n",
            "Epoch 324/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9838 - val_loss: 116.8809\n",
            "Epoch 325/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.8808 - val_loss: 136.6512\n",
            "Epoch 326/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.9736 - val_loss: 104.0036\n",
            "Epoch 327/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9943 - val_loss: 99.1111\n",
            "Epoch 328/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9917 - val_loss: 100.5998\n",
            "Epoch 329/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9489 - val_loss: 101.2895\n",
            "Epoch 330/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.1017 - val_loss: 106.9542\n",
            "Epoch 331/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9149 - val_loss: 118.4067\n",
            "Epoch 332/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.9417 - val_loss: 107.8208\n",
            "Epoch 333/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.7541 - val_loss: 119.0745\n",
            "Epoch 334/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8201 - val_loss: 138.8598\n",
            "Epoch 335/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.7944 - val_loss: 105.2313\n",
            "Epoch 336/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.7862 - val_loss: 104.1360\n",
            "Epoch 337/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4929 - val_loss: 130.6819\n",
            "Epoch 338/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.6192 - val_loss: 107.4800\n",
            "Epoch 339/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.4586 - val_loss: 97.5716\n",
            "Epoch 340/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.6810 - val_loss: 127.5809\n",
            "Epoch 341/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.5156 - val_loss: 100.6177\n",
            "Epoch 342/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.5782 - val_loss: 135.5296\n",
            "Epoch 343/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5324 - val_loss: 96.1831\n",
            "Epoch 344/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.5391 - val_loss: 98.1794\n",
            "Epoch 345/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.3989 - val_loss: 96.9475\n",
            "Epoch 346/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.6288 - val_loss: 110.2418\n",
            "Epoch 347/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.3165 - val_loss: 116.0616\n",
            "Epoch 348/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.3019 - val_loss: 96.1355\n",
            "Epoch 349/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4127 - val_loss: 115.3277\n",
            "Epoch 350/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.3280 - val_loss: 98.4194\n",
            "Epoch 351/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.4456 - val_loss: 100.3600\n",
            "Epoch 352/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.3453 - val_loss: 102.2554\n",
            "Epoch 353/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4615 - val_loss: 101.7489\n",
            "Epoch 354/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.3645 - val_loss: 102.5665\n",
            "Epoch 355/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.2687 - val_loss: 105.3716\n",
            "Epoch 356/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.3427 - val_loss: 93.5925\n",
            "Epoch 357/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4127 - val_loss: 97.9378\n",
            "Epoch 358/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.3139 - val_loss: 102.5983\n",
            "Epoch 359/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.2586 - val_loss: 114.9482\n",
            "Epoch 360/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.3526 - val_loss: 101.2365\n",
            "Epoch 361/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.3701 - val_loss: 153.7351\n",
            "Epoch 362/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1106 - val_loss: 111.5457\n",
            "Epoch 363/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1159 - val_loss: 115.5988\n",
            "Epoch 364/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1312 - val_loss: 99.4779\n",
            "Epoch 365/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1894 - val_loss: 118.2151\n",
            "Epoch 366/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1216 - val_loss: 107.8584\n",
            "Epoch 367/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.9942 - val_loss: 103.3576\n",
            "Epoch 368/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.0710 - val_loss: 113.6605\n",
            "Epoch 369/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1799 - val_loss: 100.6135\n",
            "Epoch 370/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.9888 - val_loss: 98.6921\n",
            "Epoch 371/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.0923 - val_loss: 95.9163\n",
            "Epoch 372/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1400 - val_loss: 103.9207\n",
            "Epoch 373/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.9678 - val_loss: 120.4399\n",
            "Epoch 374/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.1414 - val_loss: 105.6777\n",
            "Epoch 375/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.0925 - val_loss: 93.9581\n",
            "Epoch 376/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.0375 - val_loss: 103.8479\n",
            "Epoch 377/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.9262 - val_loss: 97.0153\n",
            "Epoch 378/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.0494 - val_loss: 152.0030\n",
            "Epoch 379/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.9260 - val_loss: 95.6655\n",
            "Epoch 380/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.6964 - val_loss: 121.0699\n",
            "Epoch 381/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 85.0741 - val_loss: 93.7906\n",
            "Epoch 382/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.8046 - val_loss: 104.7278\n",
            "Epoch 383/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.9487 - val_loss: 96.7220\n",
            "Epoch 384/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.8225 - val_loss: 105.5121\n",
            "Epoch 385/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.9659 - val_loss: 108.2599\n",
            "Epoch 386/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.8433 - val_loss: 120.4572\n",
            "Epoch 387/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.8878 - val_loss: 131.9329\n",
            "Epoch 388/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.8945 - val_loss: 97.0337\n",
            "Epoch 389/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.7878 - val_loss: 107.5449\n",
            "Epoch 390/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.8034 - val_loss: 97.2969\n",
            "Epoch 391/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.6732 - val_loss: 98.5977\n",
            "Epoch 392/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.7500 - val_loss: 105.3185\n",
            "Epoch 393/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.6911 - val_loss: 94.6552\n",
            "Epoch 394/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.5815 - val_loss: 101.1578\n",
            "Epoch 395/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.7733 - val_loss: 113.7074\n",
            "Epoch 396/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.6078 - val_loss: 135.7235\n",
            "Epoch 397/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.7575 - val_loss: 105.7698\n",
            "Epoch 398/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.5855 - val_loss: 92.8109\n",
            "Epoch 399/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 84.8013 - val_loss: 132.1690\n",
            "Epoch 400/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.5885 - val_loss: 106.1744\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696v_fuFCTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3f245e-dbb0-445e-fa50-afc257d0db98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  1.1399949100915245 \n",
            "MAE:  7.534886542381784 \n",
            "SD:  10.240839004411272\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "da11b378-0612-4453-e20f-243276faa02f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "mdZF2osWCUQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b13e82-3f25-4a41-d376-4f3ba8abab2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -0.44626074051841896 \n",
            "Ensemble_std:  9.908748135016715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7eab46-fd8a-43f9-9d06-a0ec9a0dead7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_9 (Dense)             (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,169\n",
            "Trainable params: 1,137\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "outputId": "d1e19d34-c130-4df3-e170-0d54146d38a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 3764.9905 - val_loss: 3608.8088\n",
            "Epoch 2/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3569.7507 - val_loss: 3411.5325\n",
            "Epoch 3/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3344.0356 - val_loss: 3157.7480\n",
            "Epoch 4/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3070.2632 - val_loss: 2947.6929\n",
            "Epoch 5/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2748.8516 - val_loss: 2623.4919\n",
            "Epoch 6/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2407.0603 - val_loss: 2123.9514\n",
            "Epoch 7/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2058.8647 - val_loss: 1850.5997\n",
            "Epoch 8/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1718.7457 - val_loss: 1552.2251\n",
            "Epoch 9/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1398.1814 - val_loss: 969.1258\n",
            "Epoch 10/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1107.7526 - val_loss: 814.1357\n",
            "Epoch 11/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 855.7174 - val_loss: 892.5109\n",
            "Epoch 12/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 643.9118 - val_loss: 552.6289\n",
            "Epoch 13/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 472.2743 - val_loss: 386.4450\n",
            "Epoch 14/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 338.3835 - val_loss: 295.6239\n",
            "Epoch 15/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 229.4886 - val_loss: 161.0377\n",
            "Epoch 16/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 120.0312 - val_loss: 67.7193\n",
            "Epoch 17/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 71.2326 - val_loss: 55.0719\n",
            "Epoch 18/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 52.7367 - val_loss: 50.1602\n",
            "Epoch 19/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 45.5039 - val_loss: 51.7004\n",
            "Epoch 20/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 42.4092 - val_loss: 65.5703\n",
            "Epoch 21/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.7407 - val_loss: 50.1057\n",
            "Epoch 22/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.6576 - val_loss: 53.1826\n",
            "Epoch 23/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.1152 - val_loss: 51.4322\n",
            "Epoch 24/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.5288 - val_loss: 45.4205\n",
            "Epoch 25/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.9084 - val_loss: 49.7471\n",
            "Epoch 26/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3488 - val_loss: 44.6143\n",
            "Epoch 27/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.0906 - val_loss: 63.5177\n",
            "Epoch 28/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.9208 - val_loss: 44.2583\n",
            "Epoch 29/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7371 - val_loss: 43.4710\n",
            "Epoch 30/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.5131 - val_loss: 40.3836\n",
            "Epoch 31/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.4085 - val_loss: 39.8500\n",
            "Epoch 32/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.3607 - val_loss: 47.5302\n",
            "Epoch 33/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.1308 - val_loss: 45.6141\n",
            "Epoch 34/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.9960 - val_loss: 62.8477\n",
            "Epoch 35/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.0788 - val_loss: 45.3093\n",
            "Epoch 36/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.8810 - val_loss: 42.3734\n",
            "Epoch 37/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.9252 - val_loss: 49.0872\n",
            "Epoch 38/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.8267 - val_loss: 46.2433\n",
            "Epoch 39/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.6471 - val_loss: 43.8816\n",
            "Epoch 40/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.6196 - val_loss: 44.2246\n",
            "Epoch 41/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.6189 - val_loss: 40.5209\n",
            "Epoch 42/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.5261 - val_loss: 50.9983\n",
            "Epoch 43/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.5122 - val_loss: 58.9242\n",
            "Epoch 44/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4277 - val_loss: 43.5490\n",
            "Epoch 45/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3201 - val_loss: 41.7665\n",
            "Epoch 46/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3937 - val_loss: 52.9713\n",
            "Epoch 47/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3548 - val_loss: 59.0217\n",
            "Epoch 48/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2585 - val_loss: 44.5452\n",
            "Epoch 49/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2921 - val_loss: 50.0425\n",
            "Epoch 50/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1905 - val_loss: 45.1631\n",
            "Epoch 51/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2664 - val_loss: 39.3818\n",
            "Epoch 52/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0607 - val_loss: 49.5118\n",
            "Epoch 53/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0220 - val_loss: 43.8293\n",
            "Epoch 54/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9727 - val_loss: 40.7184\n",
            "Epoch 55/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9976 - val_loss: 48.2658\n",
            "Epoch 56/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7599 - val_loss: 38.7018\n",
            "Epoch 57/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8352 - val_loss: 40.3395\n",
            "Epoch 58/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7625 - val_loss: 43.0727\n",
            "Epoch 59/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6803 - val_loss: 44.0634\n",
            "Epoch 60/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5569 - val_loss: 44.6114\n",
            "Epoch 61/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4580 - val_loss: 39.7705\n",
            "Epoch 62/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4657 - val_loss: 41.3724\n",
            "Epoch 63/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3825 - val_loss: 41.8694\n",
            "Epoch 64/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4130 - val_loss: 39.0097\n",
            "Epoch 65/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2470 - val_loss: 50.9600\n",
            "Epoch 66/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1455 - val_loss: 41.8102\n",
            "Epoch 67/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9815 - val_loss: 46.7465\n",
            "Epoch 68/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.0130 - val_loss: 42.3165\n",
            "Epoch 69/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.0681 - val_loss: 48.3090\n",
            "Epoch 70/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9438 - val_loss: 42.1509\n",
            "Epoch 71/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.9858 - val_loss: 45.6373\n",
            "Epoch 72/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8688 - val_loss: 41.4345\n",
            "Epoch 73/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.8004 - val_loss: 40.0286\n",
            "Epoch 74/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.7275 - val_loss: 42.6920\n",
            "Epoch 75/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.7197 - val_loss: 46.9887\n",
            "Epoch 76/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7074 - val_loss: 41.6988\n",
            "Epoch 77/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 33.6266 - val_loss: 39.1500\n",
            "Epoch 78/400\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.5338 - val_loss: 37.8117\n",
            "Epoch 79/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.5837 - val_loss: 42.3763\n",
            "Epoch 80/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.5740 - val_loss: 42.9561\n",
            "Epoch 81/400\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 33.4531 - val_loss: 41.0911\n",
            "Epoch 82/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.4407 - val_loss: 39.2960\n",
            "Epoch 83/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4411 - val_loss: 41.9566\n",
            "Epoch 84/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.3885 - val_loss: 42.1870\n",
            "Epoch 85/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.4000 - val_loss: 40.5175\n",
            "Epoch 86/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4247 - val_loss: 38.8156\n",
            "Epoch 87/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3704 - val_loss: 41.1314\n",
            "Epoch 88/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2892 - val_loss: 45.2786\n",
            "Epoch 89/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2031 - val_loss: 48.8802\n",
            "Epoch 90/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2182 - val_loss: 38.8648\n",
            "Epoch 91/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.1592 - val_loss: 43.8873\n",
            "Epoch 92/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9949 - val_loss: 42.9042\n",
            "Epoch 93/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.1800 - val_loss: 42.3019\n",
            "Epoch 94/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.0624 - val_loss: 45.2956\n",
            "Epoch 95/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.0621 - val_loss: 36.8803\n",
            "Epoch 96/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.9841 - val_loss: 43.7321\n",
            "Epoch 97/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0212 - val_loss: 42.8008\n",
            "Epoch 98/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.9816 - val_loss: 40.5131\n",
            "Epoch 99/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.8512 - val_loss: 37.2527\n",
            "Epoch 100/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9320 - val_loss: 38.5595\n",
            "Epoch 101/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9386 - val_loss: 56.9937\n",
            "Epoch 102/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8339 - val_loss: 39.0093\n",
            "Epoch 103/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.8721 - val_loss: 38.2683\n",
            "Epoch 104/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.7808 - val_loss: 37.4829\n",
            "Epoch 105/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.7700 - val_loss: 45.9360\n",
            "Epoch 106/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7518 - val_loss: 40.3227\n",
            "Epoch 107/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.7994 - val_loss: 39.7335\n",
            "Epoch 108/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.8019 - val_loss: 39.4511\n",
            "Epoch 109/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7352 - val_loss: 38.8786\n",
            "Epoch 110/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.7221 - val_loss: 36.1624\n",
            "Epoch 111/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6789 - val_loss: 43.2209\n",
            "Epoch 112/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.7019 - val_loss: 41.4337\n",
            "Epoch 113/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.6331 - val_loss: 38.3378\n",
            "Epoch 114/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.5379 - val_loss: 49.1247\n",
            "Epoch 115/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5751 - val_loss: 38.0614\n",
            "Epoch 116/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.6056 - val_loss: 39.7122\n",
            "Epoch 117/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.6688 - val_loss: 38.2465\n",
            "Epoch 118/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4902 - val_loss: 36.9689\n",
            "Epoch 119/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.5716 - val_loss: 42.4581\n",
            "Epoch 120/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4649 - val_loss: 38.0459\n",
            "Epoch 121/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.5407 - val_loss: 40.4841\n",
            "Epoch 122/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.4291 - val_loss: 41.7337\n",
            "Epoch 123/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5043 - val_loss: 36.3553\n",
            "Epoch 124/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.3981 - val_loss: 38.2076\n",
            "Epoch 125/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.4681 - val_loss: 40.1544\n",
            "Epoch 126/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4022 - val_loss: 39.2771\n",
            "Epoch 127/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4273 - val_loss: 36.9300\n",
            "Epoch 128/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.3923 - val_loss: 46.1184\n",
            "Epoch 129/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.4085 - val_loss: 47.1439\n",
            "Epoch 130/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.3921 - val_loss: 44.6574\n",
            "Epoch 131/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.3274 - val_loss: 36.5290\n",
            "Epoch 132/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3333 - val_loss: 36.6846\n",
            "Epoch 133/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3148 - val_loss: 36.6248\n",
            "Epoch 134/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.3006 - val_loss: 38.9053\n",
            "Epoch 135/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3175 - val_loss: 37.7142\n",
            "Epoch 136/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.2442 - val_loss: 37.7159\n",
            "Epoch 137/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.2832 - val_loss: 36.7144\n",
            "Epoch 138/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2653 - val_loss: 38.2797\n",
            "Epoch 139/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.2108 - val_loss: 40.0179\n",
            "Epoch 140/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2244 - val_loss: 43.1040\n",
            "Epoch 141/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1645 - val_loss: 38.0814\n",
            "Epoch 142/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.1406 - val_loss: 40.6888\n",
            "Epoch 143/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.1689 - val_loss: 38.0717\n",
            "Epoch 144/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.1394 - val_loss: 42.6362\n",
            "Epoch 145/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0648 - val_loss: 36.8923\n",
            "Epoch 146/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0915 - val_loss: 37.8647\n",
            "Epoch 147/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1108 - val_loss: 35.9083\n",
            "Epoch 148/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1126 - val_loss: 40.1691\n",
            "Epoch 149/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1217 - val_loss: 38.6925\n",
            "Epoch 150/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0515 - val_loss: 38.1318\n",
            "Epoch 151/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0246 - val_loss: 39.6180\n",
            "Epoch 152/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0132 - val_loss: 37.0519\n",
            "Epoch 153/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0115 - val_loss: 41.2953\n",
            "Epoch 154/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0220 - val_loss: 39.0835\n",
            "Epoch 155/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9990 - val_loss: 36.2888\n",
            "Epoch 156/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.9566 - val_loss: 43.9231\n",
            "Epoch 157/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.9680 - val_loss: 46.5763\n",
            "Epoch 158/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.9803 - val_loss: 38.2312\n",
            "Epoch 159/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.9055 - val_loss: 38.7034\n",
            "Epoch 160/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9172 - val_loss: 37.4238\n",
            "Epoch 161/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8902 - val_loss: 36.0768\n",
            "Epoch 162/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8714 - val_loss: 38.9368\n",
            "Epoch 163/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.8846 - val_loss: 35.5960\n",
            "Epoch 164/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9438 - val_loss: 39.8343\n",
            "Epoch 165/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.9058 - val_loss: 35.7069\n",
            "Epoch 166/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8550 - val_loss: 38.5894\n",
            "Epoch 167/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8377 - val_loss: 36.3830\n",
            "Epoch 168/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7870 - val_loss: 45.6090\n",
            "Epoch 169/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.8316 - val_loss: 39.8746\n",
            "Epoch 170/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8022 - val_loss: 42.7372\n",
            "Epoch 171/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8252 - val_loss: 42.6251\n",
            "Epoch 172/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8510 - val_loss: 38.4624\n",
            "Epoch 173/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7583 - val_loss: 50.7010\n",
            "Epoch 174/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7204 - val_loss: 36.6299\n",
            "Epoch 175/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.8076 - val_loss: 36.7093\n",
            "Epoch 176/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7821 - val_loss: 39.8888\n",
            "Epoch 177/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.6960 - val_loss: 36.8670\n",
            "Epoch 178/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7819 - val_loss: 38.0538\n",
            "Epoch 179/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6970 - val_loss: 41.5308\n",
            "Epoch 180/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6563 - val_loss: 35.7516\n",
            "Epoch 181/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7273 - val_loss: 35.7201\n",
            "Epoch 182/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.6880 - val_loss: 42.4395\n",
            "Epoch 183/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.6206 - val_loss: 36.8423\n",
            "Epoch 184/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6945 - val_loss: 48.0846\n",
            "Epoch 185/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6723 - val_loss: 35.8338\n",
            "Epoch 186/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6597 - val_loss: 53.9331\n",
            "Epoch 187/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6624 - val_loss: 36.0845\n",
            "Epoch 188/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6004 - val_loss: 36.0729\n",
            "Epoch 189/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5783 - val_loss: 36.8325\n",
            "Epoch 190/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5451 - val_loss: 36.4080\n",
            "Epoch 191/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.6093 - val_loss: 35.1542\n",
            "Epoch 192/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.5610 - val_loss: 44.2835\n",
            "Epoch 193/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6413 - val_loss: 36.0027\n",
            "Epoch 194/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.6040 - val_loss: 38.5115\n",
            "Epoch 195/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.5804 - val_loss: 39.2675\n",
            "Epoch 196/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.5434 - val_loss: 34.7486\n",
            "Epoch 197/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5804 - val_loss: 35.2329\n",
            "Epoch 198/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5595 - val_loss: 40.1725\n",
            "Epoch 199/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4968 - val_loss: 35.4350\n",
            "Epoch 200/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.5653 - val_loss: 52.4423\n",
            "Epoch 201/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5037 - val_loss: 35.8451\n",
            "Epoch 202/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4719 - val_loss: 35.6383\n",
            "Epoch 203/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5117 - val_loss: 37.1942\n",
            "Epoch 204/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.5241 - val_loss: 37.4873\n",
            "Epoch 205/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5013 - val_loss: 34.6954\n",
            "Epoch 206/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.4806 - val_loss: 36.3806\n",
            "Epoch 207/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5074 - val_loss: 35.4538\n",
            "Epoch 208/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4792 - val_loss: 37.6987\n",
            "Epoch 209/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4556 - val_loss: 41.2404\n",
            "Epoch 210/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.4566 - val_loss: 35.9146\n",
            "Epoch 211/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.4453 - val_loss: 47.7030\n",
            "Epoch 212/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.4207 - val_loss: 40.0695\n",
            "Epoch 213/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3965 - val_loss: 40.3273\n",
            "Epoch 214/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3991 - val_loss: 37.8048\n",
            "Epoch 215/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3995 - val_loss: 34.8879\n",
            "Epoch 216/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3459 - val_loss: 34.3609\n",
            "Epoch 217/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3609 - val_loss: 36.0052\n",
            "Epoch 218/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3929 - val_loss: 35.5901\n",
            "Epoch 219/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3504 - val_loss: 36.6551\n",
            "Epoch 220/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3248 - val_loss: 39.7151\n",
            "Epoch 221/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3282 - val_loss: 50.8052\n",
            "Epoch 222/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3885 - val_loss: 38.2481\n",
            "Epoch 223/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3850 - val_loss: 40.5567\n",
            "Epoch 224/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3915 - val_loss: 33.9790\n",
            "Epoch 225/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2655 - val_loss: 41.1613\n",
            "Epoch 226/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.4116 - val_loss: 36.6621\n",
            "Epoch 227/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3107 - val_loss: 36.3515\n",
            "Epoch 228/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3211 - val_loss: 34.7267\n",
            "Epoch 229/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3212 - val_loss: 38.7269\n",
            "Epoch 230/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2861 - val_loss: 36.5502\n",
            "Epoch 231/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3225 - val_loss: 36.0221\n",
            "Epoch 232/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2764 - val_loss: 38.3410\n",
            "Epoch 233/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2945 - val_loss: 37.4063\n",
            "Epoch 234/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3120 - val_loss: 42.0981\n",
            "Epoch 235/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2877 - val_loss: 36.2470\n",
            "Epoch 236/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2372 - val_loss: 33.9187\n",
            "Epoch 237/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2678 - val_loss: 36.4185\n",
            "Epoch 238/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2844 - val_loss: 35.0712\n",
            "Epoch 239/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2422 - val_loss: 36.0103\n",
            "Epoch 240/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2539 - val_loss: 37.7751\n",
            "Epoch 241/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2433 - val_loss: 35.7227\n",
            "Epoch 242/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2340 - val_loss: 36.7577\n",
            "Epoch 243/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1855 - val_loss: 37.9540\n",
            "Epoch 244/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2169 - val_loss: 37.5918\n",
            "Epoch 245/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2039 - val_loss: 35.1595\n",
            "Epoch 246/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1973 - val_loss: 46.9215\n",
            "Epoch 247/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2038 - val_loss: 36.3172\n",
            "Epoch 248/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1423 - val_loss: 35.9656\n",
            "Epoch 249/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2773 - val_loss: 35.7419\n",
            "Epoch 250/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1926 - val_loss: 38.2069\n",
            "Epoch 251/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2011 - val_loss: 36.0656\n",
            "Epoch 252/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1432 - val_loss: 36.2639\n",
            "Epoch 253/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1211 - val_loss: 38.6347\n",
            "Epoch 254/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1524 - val_loss: 39.9807\n",
            "Epoch 255/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2046 - val_loss: 39.1960\n",
            "Epoch 256/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1559 - val_loss: 37.3757\n",
            "Epoch 257/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1345 - val_loss: 37.0155\n",
            "Epoch 258/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1606 - val_loss: 54.1540\n",
            "Epoch 259/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1227 - val_loss: 41.6126\n",
            "Epoch 260/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1982 - val_loss: 37.8878\n",
            "Epoch 261/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1166 - val_loss: 36.5857\n",
            "Epoch 262/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0862 - val_loss: 36.8416\n",
            "Epoch 263/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2033 - val_loss: 38.6667\n",
            "Epoch 264/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0673 - val_loss: 37.0353\n",
            "Epoch 265/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0603 - val_loss: 38.5176\n",
            "Epoch 266/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1311 - val_loss: 36.2184\n",
            "Epoch 267/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1154 - val_loss: 40.2408\n",
            "Epoch 268/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0777 - val_loss: 37.3621\n",
            "Epoch 269/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1298 - val_loss: 42.2253\n",
            "Epoch 270/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0471 - val_loss: 35.3890\n",
            "Epoch 271/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1364 - val_loss: 36.0411\n",
            "Epoch 272/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0812 - val_loss: 36.5127\n",
            "Epoch 273/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1175 - val_loss: 34.1945\n",
            "Epoch 274/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0229 - val_loss: 34.5023\n",
            "Epoch 275/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0447 - val_loss: 34.9877\n",
            "Epoch 276/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0648 - val_loss: 38.7821\n",
            "Epoch 277/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0851 - val_loss: 43.9750\n",
            "Epoch 278/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0629 - val_loss: 36.1423\n",
            "Epoch 279/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1200 - val_loss: 41.3185\n",
            "Epoch 280/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0305 - val_loss: 37.2437\n",
            "Epoch 281/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0493 - val_loss: 35.0086\n",
            "Epoch 282/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0806 - val_loss: 36.4062\n",
            "Epoch 283/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0383 - val_loss: 45.4010\n",
            "Epoch 284/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9543 - val_loss: 36.2254\n",
            "Epoch 285/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0096 - val_loss: 35.5579\n",
            "Epoch 286/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9997 - val_loss: 42.6193\n",
            "Epoch 287/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0337 - val_loss: 36.9234\n",
            "Epoch 288/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0526 - val_loss: 34.2561\n",
            "Epoch 289/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0057 - val_loss: 36.1324\n",
            "Epoch 290/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9379 - val_loss: 39.2160\n",
            "Epoch 291/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9796 - val_loss: 38.5822\n",
            "Epoch 292/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9602 - val_loss: 37.6337\n",
            "Epoch 293/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0060 - val_loss: 36.3086\n",
            "Epoch 294/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9949 - val_loss: 34.9374\n",
            "Epoch 295/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9627 - val_loss: 36.9236\n",
            "Epoch 296/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9850 - val_loss: 37.0937\n",
            "Epoch 297/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0182 - val_loss: 34.0546\n",
            "Epoch 298/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0079 - val_loss: 43.1010\n",
            "Epoch 299/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9552 - val_loss: 36.4504\n",
            "Epoch 300/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9342 - val_loss: 36.5090\n",
            "Epoch 301/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9805 - val_loss: 41.2448\n",
            "Epoch 302/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9485 - val_loss: 35.9804\n",
            "Epoch 303/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9233 - val_loss: 35.7822\n",
            "Epoch 304/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9232 - val_loss: 33.7686\n",
            "Epoch 305/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9701 - val_loss: 39.7183\n",
            "Epoch 306/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9336 - val_loss: 41.5509\n",
            "Epoch 307/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9676 - val_loss: 37.2697\n",
            "Epoch 308/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9031 - val_loss: 33.7750\n",
            "Epoch 309/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9705 - val_loss: 34.2341\n",
            "Epoch 310/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9073 - val_loss: 37.1880\n",
            "Epoch 311/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8950 - val_loss: 34.8694\n",
            "Epoch 312/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9180 - val_loss: 38.1607\n",
            "Epoch 313/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9096 - val_loss: 37.4820\n",
            "Epoch 314/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9512 - val_loss: 34.6607\n",
            "Epoch 315/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8924 - val_loss: 34.7437\n",
            "Epoch 316/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9215 - val_loss: 35.5124\n",
            "Epoch 317/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9252 - val_loss: 35.1988\n",
            "Epoch 318/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9811 - val_loss: 33.7651\n",
            "Epoch 319/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9315 - val_loss: 34.4669\n",
            "Epoch 320/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8967 - val_loss: 35.1110\n",
            "Epoch 321/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8664 - val_loss: 38.0020\n",
            "Epoch 322/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9168 - val_loss: 42.8364\n",
            "Epoch 323/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8265 - val_loss: 35.1019\n",
            "Epoch 324/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9247 - val_loss: 39.7986\n",
            "Epoch 325/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8987 - val_loss: 35.7314\n",
            "Epoch 326/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8876 - val_loss: 34.8839\n",
            "Epoch 327/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8553 - val_loss: 33.8678\n",
            "Epoch 328/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9098 - val_loss: 36.4727\n",
            "Epoch 329/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8685 - val_loss: 40.6538\n",
            "Epoch 330/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8261 - val_loss: 40.8200\n",
            "Epoch 331/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8261 - val_loss: 37.9796\n",
            "Epoch 332/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9240 - val_loss: 34.8141\n",
            "Epoch 333/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8295 - val_loss: 35.7518\n",
            "Epoch 334/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8698 - val_loss: 34.6245\n",
            "Epoch 335/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8586 - val_loss: 33.3936\n",
            "Epoch 336/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8197 - val_loss: 33.8946\n",
            "Epoch 337/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8273 - val_loss: 35.1483\n",
            "Epoch 338/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8848 - val_loss: 36.2459\n",
            "Epoch 339/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8743 - val_loss: 35.4841\n",
            "Epoch 340/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8207 - val_loss: 35.8467\n",
            "Epoch 341/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8786 - val_loss: 35.2815\n",
            "Epoch 342/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8483 - val_loss: 36.2330\n",
            "Epoch 343/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8098 - val_loss: 37.7127\n",
            "Epoch 344/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7986 - val_loss: 34.9287\n",
            "Epoch 345/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7951 - val_loss: 36.4510\n",
            "Epoch 346/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7743 - val_loss: 38.5032\n",
            "Epoch 347/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8737 - val_loss: 34.3479\n",
            "Epoch 348/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8239 - val_loss: 35.7275\n",
            "Epoch 349/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7940 - val_loss: 34.9114\n",
            "Epoch 350/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8206 - val_loss: 35.9806\n",
            "Epoch 351/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8035 - val_loss: 36.1290\n",
            "Epoch 352/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8711 - val_loss: 36.6077\n",
            "Epoch 353/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8474 - val_loss: 34.1410\n",
            "Epoch 354/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7761 - val_loss: 35.4968\n",
            "Epoch 355/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8482 - val_loss: 36.5659\n",
            "Epoch 356/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8477 - val_loss: 35.3815\n",
            "Epoch 357/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8426 - val_loss: 36.5525\n",
            "Epoch 358/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8250 - val_loss: 38.5376\n",
            "Epoch 359/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8390 - val_loss: 35.8515\n",
            "Epoch 360/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8422 - val_loss: 38.6197\n",
            "Epoch 361/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7814 - val_loss: 35.5225\n",
            "Epoch 362/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8132 - val_loss: 34.3439\n",
            "Epoch 363/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7720 - val_loss: 34.3911\n",
            "Epoch 364/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8135 - val_loss: 35.3402\n",
            "Epoch 365/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8800 - val_loss: 35.7159\n",
            "Epoch 366/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8031 - val_loss: 35.2624\n",
            "Epoch 367/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8107 - val_loss: 34.4893\n",
            "Epoch 368/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8345 - val_loss: 36.9609\n",
            "Epoch 369/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7985 - val_loss: 41.2247\n",
            "Epoch 370/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7559 - val_loss: 34.6586\n",
            "Epoch 371/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7814 - val_loss: 42.8065\n",
            "Epoch 372/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8415 - val_loss: 35.0507\n",
            "Epoch 373/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8610 - val_loss: 36.2597\n",
            "Epoch 374/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7571 - val_loss: 37.1945\n",
            "Epoch 375/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7325 - val_loss: 36.9186\n",
            "Epoch 376/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7761 - val_loss: 38.6577\n",
            "Epoch 377/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7681 - val_loss: 35.9649\n",
            "Epoch 378/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8019 - val_loss: 43.8826\n",
            "Epoch 379/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8168 - val_loss: 38.1709\n",
            "Epoch 380/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7728 - val_loss: 35.0516\n",
            "Epoch 381/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7157 - val_loss: 37.4184\n",
            "Epoch 382/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7587 - val_loss: 33.9558\n",
            "Epoch 383/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7511 - val_loss: 35.2224\n",
            "Epoch 384/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7396 - val_loss: 34.5624\n",
            "Epoch 385/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7550 - val_loss: 35.7972\n",
            "Epoch 386/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7794 - val_loss: 35.4534\n",
            "Epoch 387/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7765 - val_loss: 37.3131\n",
            "Epoch 388/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7406 - val_loss: 38.0643\n",
            "Epoch 389/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7671 - val_loss: 34.7796\n",
            "Epoch 390/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7472 - val_loss: 33.6553\n",
            "Epoch 391/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7381 - val_loss: 34.8583\n",
            "Epoch 392/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7877 - val_loss: 34.6066\n",
            "Epoch 393/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7588 - val_loss: 36.0929\n",
            "Epoch 394/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7659 - val_loss: 34.6174\n",
            "Epoch 395/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7939 - val_loss: 35.3990\n",
            "Epoch 396/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7642 - val_loss: 36.1649\n",
            "Epoch 397/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7165 - val_loss: 35.7354\n",
            "Epoch 398/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7787 - val_loss: 37.3906\n",
            "Epoch 399/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7386 - val_loss: 34.1663\n",
            "Epoch 400/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8003 - val_loss: 36.7982\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5921e45f-d1df-4382-d280-323d5cc554fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.07748185088144129 \n",
            "MAE:  4.526589148697009 \n",
            "SD:  6.06565807348673\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cip38xZOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "59f5bfb7-32be-4193-e6d3-550d42dffcba"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4bcc04-94b6-4ac0-e7d9-712c7d6f930d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,169\n",
            "Trainable params: 1,137\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba38cdf0-9cfd-4c31-fdbf-0b47102ca0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 3691.6143 - val_loss: 3683.9248\n",
            "Epoch 2/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3504.0076 - val_loss: 3479.6858\n",
            "Epoch 3/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3304.8813 - val_loss: 3213.7500\n",
            "Epoch 4/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3059.5237 - val_loss: 2932.3572\n",
            "Epoch 5/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2753.4187 - val_loss: 2426.9580\n",
            "Epoch 6/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2387.5710 - val_loss: 1905.8925\n",
            "Epoch 7/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1955.9200 - val_loss: 1517.4434\n",
            "Epoch 8/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1506.9242 - val_loss: 1327.5958\n",
            "Epoch 9/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1103.0610 - val_loss: 1395.9597\n",
            "Epoch 10/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 748.6518 - val_loss: 1025.3317\n",
            "Epoch 11/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 468.0824 - val_loss: 421.8846\n",
            "Epoch 12/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 282.9130 - val_loss: 324.4219\n",
            "Epoch 13/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 168.3837 - val_loss: 85.8363\n",
            "Epoch 14/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.3409 - val_loss: 149.8335\n",
            "Epoch 15/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 70.0146 - val_loss: 73.7400\n",
            "Epoch 16/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 53.4577 - val_loss: 74.6267\n",
            "Epoch 17/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 46.1610 - val_loss: 45.2828\n",
            "Epoch 18/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 43.2806 - val_loss: 54.3284\n",
            "Epoch 19/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 40.8982 - val_loss: 58.6736\n",
            "Epoch 20/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.8534 - val_loss: 61.2882\n",
            "Epoch 21/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.2650 - val_loss: 48.9045\n",
            "Epoch 22/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.7615 - val_loss: 39.7319\n",
            "Epoch 23/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.3128 - val_loss: 52.9182\n",
            "Epoch 24/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.8343 - val_loss: 48.7789\n",
            "Epoch 25/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.8053 - val_loss: 44.6707\n",
            "Epoch 26/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.2474 - val_loss: 48.3829\n",
            "Epoch 27/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9979 - val_loss: 53.9816\n",
            "Epoch 28/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.7874 - val_loss: 63.4753\n",
            "Epoch 29/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6762 - val_loss: 53.7629\n",
            "Epoch 30/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6678 - val_loss: 64.8062\n",
            "Epoch 31/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4217 - val_loss: 39.5683\n",
            "Epoch 32/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.4066 - val_loss: 60.6471\n",
            "Epoch 33/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.1312 - val_loss: 51.8847\n",
            "Epoch 34/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.1211 - val_loss: 59.5490\n",
            "Epoch 35/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.9686 - val_loss: 44.4500\n",
            "Epoch 36/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7561 - val_loss: 46.1107\n",
            "Epoch 37/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.8839 - val_loss: 40.1114\n",
            "Epoch 38/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.7319 - val_loss: 40.5838\n",
            "Epoch 39/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.7231 - val_loss: 47.3303\n",
            "Epoch 40/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.6379 - val_loss: 50.2512\n",
            "Epoch 41/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.5387 - val_loss: 43.6362\n",
            "Epoch 42/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4441 - val_loss: 39.3801\n",
            "Epoch 43/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5003 - val_loss: 78.8221\n",
            "Epoch 44/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3909 - val_loss: 40.8111\n",
            "Epoch 45/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2261 - val_loss: 46.4654\n",
            "Epoch 46/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2854 - val_loss: 39.7407\n",
            "Epoch 47/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3907 - val_loss: 60.4032\n",
            "Epoch 48/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1796 - val_loss: 40.9385\n",
            "Epoch 49/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0594 - val_loss: 40.6624\n",
            "Epoch 50/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2541 - val_loss: 42.8497\n",
            "Epoch 51/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0908 - val_loss: 60.7243\n",
            "Epoch 52/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8343 - val_loss: 90.1120\n",
            "Epoch 53/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9273 - val_loss: 41.4447\n",
            "Epoch 54/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8233 - val_loss: 49.5085\n",
            "Epoch 55/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6103 - val_loss: 51.9286\n",
            "Epoch 56/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7457 - val_loss: 56.3364\n",
            "Epoch 57/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5864 - val_loss: 40.5751\n",
            "Epoch 58/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6452 - val_loss: 43.5663\n",
            "Epoch 59/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6529 - val_loss: 42.0454\n",
            "Epoch 60/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4732 - val_loss: 68.3530\n",
            "Epoch 61/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5538 - val_loss: 43.2107\n",
            "Epoch 62/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5859 - val_loss: 48.6576\n",
            "Epoch 63/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4886 - val_loss: 43.9384\n",
            "Epoch 64/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5531 - val_loss: 42.8397\n",
            "Epoch 65/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3113 - val_loss: 40.8489\n",
            "Epoch 66/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4981 - val_loss: 45.6634\n",
            "Epoch 67/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2885 - val_loss: 38.9309\n",
            "Epoch 68/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.0849 - val_loss: 37.7677\n",
            "Epoch 69/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3032 - val_loss: 40.6488\n",
            "Epoch 70/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1366 - val_loss: 50.7058\n",
            "Epoch 71/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2190 - val_loss: 40.1941\n",
            "Epoch 72/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1082 - val_loss: 42.7410\n",
            "Epoch 73/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1349 - val_loss: 45.4965\n",
            "Epoch 74/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1244 - val_loss: 41.8259\n",
            "Epoch 75/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1408 - val_loss: 64.2163\n",
            "Epoch 76/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.0972 - val_loss: 49.4404\n",
            "Epoch 77/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.0009 - val_loss: 51.4987\n",
            "Epoch 78/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0882 - val_loss: 39.2854\n",
            "Epoch 79/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0516 - val_loss: 40.7078\n",
            "Epoch 80/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0323 - val_loss: 39.2594\n",
            "Epoch 81/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.9745 - val_loss: 50.4569\n",
            "Epoch 82/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.9107 - val_loss: 44.8911\n",
            "Epoch 83/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9288 - val_loss: 43.2338\n",
            "Epoch 84/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.9289 - val_loss: 40.0751\n",
            "Epoch 85/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.8551 - val_loss: 50.0833\n",
            "Epoch 86/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.8553 - val_loss: 40.0367\n",
            "Epoch 87/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8631 - val_loss: 41.5234\n",
            "Epoch 88/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8617 - val_loss: 45.1980\n",
            "Epoch 89/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.7701 - val_loss: 39.9025\n",
            "Epoch 90/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.8764 - val_loss: 40.4231\n",
            "Epoch 91/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.7418 - val_loss: 39.8637\n",
            "Epoch 92/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.8156 - val_loss: 58.5154\n",
            "Epoch 93/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.7870 - val_loss: 49.1186\n",
            "Epoch 94/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.6894 - val_loss: 43.2147\n",
            "Epoch 95/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.7478 - val_loss: 39.2634\n",
            "Epoch 96/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.7758 - val_loss: 38.0997\n",
            "Epoch 97/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.6441 - val_loss: 66.1608\n",
            "Epoch 98/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.6162 - val_loss: 49.8448\n",
            "Epoch 99/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7183 - val_loss: 45.7728\n",
            "Epoch 100/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.6374 - val_loss: 40.7775\n",
            "Epoch 101/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5936 - val_loss: 45.1113\n",
            "Epoch 102/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.5986 - val_loss: 42.5262\n",
            "Epoch 103/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5702 - val_loss: 42.7856\n",
            "Epoch 104/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6311 - val_loss: 44.0699\n",
            "Epoch 105/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.5698 - val_loss: 54.2439\n",
            "Epoch 106/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5469 - val_loss: 41.2127\n",
            "Epoch 107/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.4802 - val_loss: 37.6452\n",
            "Epoch 108/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5010 - val_loss: 58.2777\n",
            "Epoch 109/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.4608 - val_loss: 41.7624\n",
            "Epoch 110/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5114 - val_loss: 38.3568\n",
            "Epoch 111/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.4717 - val_loss: 44.6241\n",
            "Epoch 112/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.4019 - val_loss: 39.1620\n",
            "Epoch 113/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.3935 - val_loss: 39.5714\n",
            "Epoch 114/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4387 - val_loss: 57.1833\n",
            "Epoch 115/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4034 - val_loss: 38.1273\n",
            "Epoch 116/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.4167 - val_loss: 43.0554\n",
            "Epoch 117/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3930 - val_loss: 44.2685\n",
            "Epoch 118/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.4497 - val_loss: 49.4283\n",
            "Epoch 119/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2930 - val_loss: 49.3839\n",
            "Epoch 120/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3121 - val_loss: 41.6577\n",
            "Epoch 121/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.3573 - val_loss: 63.2565\n",
            "Epoch 122/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2684 - val_loss: 42.2079\n",
            "Epoch 123/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2962 - val_loss: 37.8004\n",
            "Epoch 124/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2800 - val_loss: 42.6570\n",
            "Epoch 125/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2403 - val_loss: 42.3163\n",
            "Epoch 126/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2521 - val_loss: 43.0433\n",
            "Epoch 127/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2635 - val_loss: 46.2663\n",
            "Epoch 128/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2342 - val_loss: 41.2250\n",
            "Epoch 129/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.1886 - val_loss: 43.7360\n",
            "Epoch 130/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2454 - val_loss: 45.4266\n",
            "Epoch 131/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2157 - val_loss: 43.9306\n",
            "Epoch 132/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2239 - val_loss: 41.5507\n",
            "Epoch 133/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.1480 - val_loss: 38.8311\n",
            "Epoch 134/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.1488 - val_loss: 37.1892\n",
            "Epoch 135/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.1508 - val_loss: 47.5115\n",
            "Epoch 136/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1091 - val_loss: 39.8232\n",
            "Epoch 137/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0752 - val_loss: 41.0034\n",
            "Epoch 138/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0810 - val_loss: 45.5779\n",
            "Epoch 139/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0456 - val_loss: 54.0797\n",
            "Epoch 140/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.0739 - val_loss: 42.3206\n",
            "Epoch 141/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0533 - val_loss: 43.1914\n",
            "Epoch 142/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9987 - val_loss: 37.9721\n",
            "Epoch 143/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.0785 - val_loss: 39.2835\n",
            "Epoch 144/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.0588 - val_loss: 45.3034\n",
            "Epoch 145/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.9509 - val_loss: 42.7774\n",
            "Epoch 146/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.9131 - val_loss: 45.4207\n",
            "Epoch 147/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.0471 - val_loss: 38.6871\n",
            "Epoch 148/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.8996 - val_loss: 45.4571\n",
            "Epoch 149/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.9995 - val_loss: 37.3634\n",
            "Epoch 150/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8818 - val_loss: 42.2918\n",
            "Epoch 151/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.8870 - val_loss: 45.2316\n",
            "Epoch 152/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.8215 - val_loss: 37.4794\n",
            "Epoch 153/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.8167 - val_loss: 38.1644\n",
            "Epoch 154/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.8616 - val_loss: 42.8159\n",
            "Epoch 155/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.8030 - val_loss: 40.0977\n",
            "Epoch 156/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.8234 - val_loss: 39.2734\n",
            "Epoch 157/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7503 - val_loss: 51.6526\n",
            "Epoch 158/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.7984 - val_loss: 37.8026\n",
            "Epoch 159/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7463 - val_loss: 39.4034\n",
            "Epoch 160/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.7291 - val_loss: 42.7975\n",
            "Epoch 161/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7720 - val_loss: 37.7611\n",
            "Epoch 162/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.7387 - val_loss: 40.1912\n",
            "Epoch 163/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.6625 - val_loss: 38.9007\n",
            "Epoch 164/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7364 - val_loss: 41.7051\n",
            "Epoch 165/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6142 - val_loss: 40.0233\n",
            "Epoch 166/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7475 - val_loss: 38.3943\n",
            "Epoch 167/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6733 - val_loss: 42.6741\n",
            "Epoch 168/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.5824 - val_loss: 42.0033\n",
            "Epoch 169/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6470 - val_loss: 44.8898\n",
            "Epoch 170/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5409 - val_loss: 39.5073\n",
            "Epoch 171/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.5345 - val_loss: 44.9987\n",
            "Epoch 172/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5240 - val_loss: 48.2110\n",
            "Epoch 173/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.5207 - val_loss: 44.3862\n",
            "Epoch 174/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4561 - val_loss: 43.0448\n",
            "Epoch 175/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4880 - val_loss: 44.9950\n",
            "Epoch 176/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4541 - val_loss: 45.7336\n",
            "Epoch 177/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.4267 - val_loss: 43.5277\n",
            "Epoch 178/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3514 - val_loss: 38.8784\n",
            "Epoch 179/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.3894 - val_loss: 42.5813\n",
            "Epoch 180/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.3660 - val_loss: 39.2086\n",
            "Epoch 181/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3921 - val_loss: 42.0601\n",
            "Epoch 182/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.3330 - val_loss: 39.8846\n",
            "Epoch 183/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.3208 - val_loss: 53.9022\n",
            "Epoch 184/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.2469 - val_loss: 41.3058\n",
            "Epoch 185/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.2202 - val_loss: 36.6475\n",
            "Epoch 186/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.2897 - val_loss: 36.8049\n",
            "Epoch 187/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.2625 - val_loss: 42.9753\n",
            "Epoch 188/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.2213 - val_loss: 48.4018\n",
            "Epoch 189/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1877 - val_loss: 58.1969\n",
            "Epoch 190/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.1966 - val_loss: 39.2662\n",
            "Epoch 191/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.2316 - val_loss: 36.4244\n",
            "Epoch 192/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1948 - val_loss: 48.2825\n",
            "Epoch 193/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1534 - val_loss: 45.3592\n",
            "Epoch 194/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0699 - val_loss: 37.3144\n",
            "Epoch 195/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.1123 - val_loss: 37.7349\n",
            "Epoch 196/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.1363 - val_loss: 40.0058\n",
            "Epoch 197/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1743 - val_loss: 43.5285\n",
            "Epoch 198/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0880 - val_loss: 39.8136\n",
            "Epoch 199/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0493 - val_loss: 47.8632\n",
            "Epoch 200/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0142 - val_loss: 38.4245\n",
            "Epoch 201/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0385 - val_loss: 39.3746\n",
            "Epoch 202/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0450 - val_loss: 39.5635\n",
            "Epoch 203/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9490 - val_loss: 39.2716\n",
            "Epoch 204/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9819 - val_loss: 42.1846\n",
            "Epoch 205/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0107 - val_loss: 44.6381\n",
            "Epoch 206/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.9244 - val_loss: 37.4531\n",
            "Epoch 207/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9605 - val_loss: 41.7725\n",
            "Epoch 208/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9368 - val_loss: 38.8039\n",
            "Epoch 209/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.8708 - val_loss: 37.1644\n",
            "Epoch 210/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.9164 - val_loss: 36.6566\n",
            "Epoch 211/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.8908 - val_loss: 40.7491\n",
            "Epoch 212/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8330 - val_loss: 42.1647\n",
            "Epoch 213/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.8186 - val_loss: 39.8044\n",
            "Epoch 214/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.8384 - val_loss: 39.3217\n",
            "Epoch 215/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.8032 - val_loss: 39.1802\n",
            "Epoch 216/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.8177 - val_loss: 41.3624\n",
            "Epoch 217/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7949 - val_loss: 39.5927\n",
            "Epoch 218/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.8161 - val_loss: 44.1182\n",
            "Epoch 219/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7508 - val_loss: 38.0580\n",
            "Epoch 220/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7411 - val_loss: 40.8312\n",
            "Epoch 221/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6860 - val_loss: 38.6281\n",
            "Epoch 222/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7187 - val_loss: 43.9639\n",
            "Epoch 223/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7353 - val_loss: 47.6082\n",
            "Epoch 224/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6341 - val_loss: 40.5651\n",
            "Epoch 225/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.6851 - val_loss: 37.1610\n",
            "Epoch 226/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.6278 - val_loss: 37.0699\n",
            "Epoch 227/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6823 - val_loss: 38.0809\n",
            "Epoch 228/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.6385 - val_loss: 36.4923\n",
            "Epoch 229/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6264 - val_loss: 38.4003\n",
            "Epoch 230/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5826 - val_loss: 40.8798\n",
            "Epoch 231/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6576 - val_loss: 39.7347\n",
            "Epoch 232/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5879 - val_loss: 40.0854\n",
            "Epoch 233/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5665 - val_loss: 36.5139\n",
            "Epoch 234/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.5710 - val_loss: 39.0600\n",
            "Epoch 235/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5861 - val_loss: 37.6582\n",
            "Epoch 236/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5518 - val_loss: 47.7580\n",
            "Epoch 237/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.5641 - val_loss: 36.9279\n",
            "Epoch 238/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.5436 - val_loss: 57.1976\n",
            "Epoch 239/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4877 - val_loss: 38.8583\n",
            "Epoch 240/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.5437 - val_loss: 42.7429\n",
            "Epoch 241/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5936 - val_loss: 54.7662\n",
            "Epoch 242/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4820 - val_loss: 39.5523\n",
            "Epoch 243/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4232 - val_loss: 43.4908\n",
            "Epoch 244/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5149 - val_loss: 35.8394\n",
            "Epoch 245/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4212 - val_loss: 42.9940\n",
            "Epoch 246/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.4669 - val_loss: 40.1585\n",
            "Epoch 247/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.4165 - val_loss: 39.5615\n",
            "Epoch 248/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.4156 - val_loss: 37.7730\n",
            "Epoch 249/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4083 - val_loss: 37.3832\n",
            "Epoch 250/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3571 - val_loss: 37.9241\n",
            "Epoch 251/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.4213 - val_loss: 40.7417\n",
            "Epoch 252/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3979 - val_loss: 40.6525\n",
            "Epoch 253/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3619 - val_loss: 38.9533\n",
            "Epoch 254/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3296 - val_loss: 39.6553\n",
            "Epoch 255/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3859 - val_loss: 37.8030\n",
            "Epoch 256/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3763 - val_loss: 36.2472\n",
            "Epoch 257/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3827 - val_loss: 46.0301\n",
            "Epoch 258/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3095 - val_loss: 38.9415\n",
            "Epoch 259/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3212 - val_loss: 38.2625\n",
            "Epoch 260/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3239 - val_loss: 37.3773\n",
            "Epoch 261/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3356 - val_loss: 40.6917\n",
            "Epoch 262/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3180 - val_loss: 39.6082\n",
            "Epoch 263/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3029 - val_loss: 41.1595\n",
            "Epoch 264/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2900 - val_loss: 39.3993\n",
            "Epoch 265/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2840 - val_loss: 36.8520\n",
            "Epoch 266/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2589 - val_loss: 39.5753\n",
            "Epoch 267/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2206 - val_loss: 37.6381\n",
            "Epoch 268/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2000 - val_loss: 37.2047\n",
            "Epoch 269/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2274 - val_loss: 36.6708\n",
            "Epoch 270/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1634 - val_loss: 38.3712\n",
            "Epoch 271/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2192 - val_loss: 37.2175\n",
            "Epoch 272/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2965 - val_loss: 37.3450\n",
            "Epoch 273/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2278 - val_loss: 44.2313\n",
            "Epoch 274/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1951 - val_loss: 38.2055\n",
            "Epoch 275/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1611 - val_loss: 40.0197\n",
            "Epoch 276/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1988 - val_loss: 37.9053\n",
            "Epoch 277/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1572 - val_loss: 36.7190\n",
            "Epoch 278/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1883 - val_loss: 37.6709\n",
            "Epoch 279/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1855 - val_loss: 38.2708\n",
            "Epoch 280/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1002 - val_loss: 41.6429\n",
            "Epoch 281/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1529 - val_loss: 43.4887\n",
            "Epoch 282/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2053 - val_loss: 44.6356\n",
            "Epoch 283/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1172 - val_loss: 38.2320\n",
            "Epoch 284/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1357 - val_loss: 39.4208\n",
            "Epoch 285/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1936 - val_loss: 40.3977\n",
            "Epoch 286/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1262 - val_loss: 41.2002\n",
            "Epoch 287/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1417 - val_loss: 47.3037\n",
            "Epoch 288/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1991 - val_loss: 36.1155\n",
            "Epoch 289/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1272 - val_loss: 37.1470\n",
            "Epoch 290/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1116 - val_loss: 39.8130\n",
            "Epoch 291/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1642 - val_loss: 42.5711\n",
            "Epoch 292/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1572 - val_loss: 36.8483\n",
            "Epoch 293/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1121 - val_loss: 36.9325\n",
            "Epoch 294/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0373 - val_loss: 38.4855\n",
            "Epoch 295/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1309 - val_loss: 45.7092\n",
            "Epoch 296/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1118 - val_loss: 42.4429\n",
            "Epoch 297/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1030 - val_loss: 37.9897\n",
            "Epoch 298/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1168 - val_loss: 36.1379\n",
            "Epoch 299/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0974 - val_loss: 38.6074\n",
            "Epoch 300/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0739 - val_loss: 37.3894\n",
            "Epoch 301/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0706 - val_loss: 38.2829\n",
            "Epoch 302/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0354 - val_loss: 38.3099\n",
            "Epoch 303/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0382 - val_loss: 43.2698\n",
            "Epoch 304/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1024 - val_loss: 35.5319\n",
            "Epoch 305/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0213 - val_loss: 39.8889\n",
            "Epoch 306/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0297 - val_loss: 37.7603\n",
            "Epoch 307/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1083 - val_loss: 36.0260\n",
            "Epoch 308/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0762 - val_loss: 43.0803\n",
            "Epoch 309/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0382 - val_loss: 37.6373\n",
            "Epoch 310/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0898 - val_loss: 37.1356\n",
            "Epoch 311/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0204 - val_loss: 46.1659\n",
            "Epoch 312/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1139 - val_loss: 39.1441\n",
            "Epoch 313/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0336 - val_loss: 38.0505\n",
            "Epoch 314/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0338 - val_loss: 35.6563\n",
            "Epoch 315/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0377 - val_loss: 40.1958\n",
            "Epoch 316/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9476 - val_loss: 39.0980\n",
            "Epoch 317/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0116 - val_loss: 37.5615\n",
            "Epoch 318/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9688 - val_loss: 39.8329\n",
            "Epoch 319/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9760 - val_loss: 38.7336\n",
            "Epoch 320/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9747 - val_loss: 36.2984\n",
            "Epoch 321/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0025 - val_loss: 41.0712\n",
            "Epoch 322/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0285 - val_loss: 43.8286\n",
            "Epoch 323/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0356 - val_loss: 38.7181\n",
            "Epoch 324/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9884 - val_loss: 38.8853\n",
            "Epoch 325/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9960 - val_loss: 37.5457\n",
            "Epoch 326/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9953 - val_loss: 44.8793\n",
            "Epoch 327/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9344 - val_loss: 42.3378\n",
            "Epoch 328/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9622 - val_loss: 38.3053\n",
            "Epoch 329/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9756 - val_loss: 41.6697\n",
            "Epoch 330/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9788 - val_loss: 39.6523\n",
            "Epoch 331/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0073 - val_loss: 36.6210\n",
            "Epoch 332/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9429 - val_loss: 37.0045\n",
            "Epoch 333/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9015 - val_loss: 41.6625\n",
            "Epoch 334/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9850 - val_loss: 37.0815\n",
            "Epoch 335/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9311 - val_loss: 37.2075\n",
            "Epoch 336/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9225 - val_loss: 36.0870\n",
            "Epoch 337/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9739 - val_loss: 37.2382\n",
            "Epoch 338/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9323 - val_loss: 36.7316\n",
            "Epoch 339/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9358 - val_loss: 38.3637\n",
            "Epoch 340/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8887 - val_loss: 35.6881\n",
            "Epoch 341/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9617 - val_loss: 39.9855\n",
            "Epoch 342/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9166 - val_loss: 37.1109\n",
            "Epoch 343/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9414 - val_loss: 36.9246\n",
            "Epoch 344/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8958 - val_loss: 37.2653\n",
            "Epoch 345/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0062 - val_loss: 38.3880\n",
            "Epoch 346/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9752 - val_loss: 36.5562\n",
            "Epoch 347/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9364 - val_loss: 36.4325\n",
            "Epoch 348/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9061 - val_loss: 48.8046\n",
            "Epoch 349/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8545 - val_loss: 47.9438\n",
            "Epoch 350/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9261 - val_loss: 36.5727\n",
            "Epoch 351/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9109 - val_loss: 44.7621\n",
            "Epoch 352/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9027 - val_loss: 37.6751\n",
            "Epoch 353/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9432 - val_loss: 36.1731\n",
            "Epoch 354/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8620 - val_loss: 35.4455\n",
            "Epoch 355/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9475 - val_loss: 36.2839\n",
            "Epoch 356/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9264 - val_loss: 39.4802\n",
            "Epoch 357/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8925 - val_loss: 36.8407\n",
            "Epoch 358/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8488 - val_loss: 43.3894\n",
            "Epoch 359/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8993 - val_loss: 37.0485\n",
            "Epoch 360/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8967 - val_loss: 36.2717\n",
            "Epoch 361/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9253 - val_loss: 38.7062\n",
            "Epoch 362/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8913 - val_loss: 38.6946\n",
            "Epoch 363/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8368 - val_loss: 36.7549\n",
            "Epoch 364/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9125 - val_loss: 42.5946\n",
            "Epoch 365/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8972 - val_loss: 36.9669\n",
            "Epoch 366/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9203 - val_loss: 36.5615\n",
            "Epoch 367/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8837 - val_loss: 36.1436\n",
            "Epoch 368/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8800 - val_loss: 44.6692\n",
            "Epoch 369/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9176 - val_loss: 42.8744\n",
            "Epoch 370/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8397 - val_loss: 46.0946\n",
            "Epoch 371/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8181 - val_loss: 35.5851\n",
            "Epoch 372/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8622 - val_loss: 48.2031\n",
            "Epoch 373/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8264 - val_loss: 41.8047\n",
            "Epoch 374/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8742 - val_loss: 38.8430\n",
            "Epoch 375/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8444 - val_loss: 42.1075\n",
            "Epoch 376/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8500 - val_loss: 36.4831\n",
            "Epoch 377/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8554 - val_loss: 36.3619\n",
            "Epoch 378/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8995 - val_loss: 38.3588\n",
            "Epoch 379/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7938 - val_loss: 35.4275\n",
            "Epoch 380/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8660 - val_loss: 35.0893\n",
            "Epoch 381/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8424 - val_loss: 40.0231\n",
            "Epoch 382/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.9196 - val_loss: 37.4331\n",
            "Epoch 383/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8556 - val_loss: 35.6440\n",
            "Epoch 384/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8372 - val_loss: 39.9934\n",
            "Epoch 385/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8449 - val_loss: 41.1651\n",
            "Epoch 386/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8199 - val_loss: 35.7402\n",
            "Epoch 387/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7943 - val_loss: 36.2333\n",
            "Epoch 388/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8227 - val_loss: 38.2333\n",
            "Epoch 389/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8098 - val_loss: 38.2655\n",
            "Epoch 390/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8235 - val_loss: 36.5830\n",
            "Epoch 391/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8277 - val_loss: 36.8507\n",
            "Epoch 392/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7756 - val_loss: 35.4753\n",
            "Epoch 393/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7576 - val_loss: 44.9879\n",
            "Epoch 394/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8357 - val_loss: 39.8765\n",
            "Epoch 395/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7457 - val_loss: 39.3116\n",
            "Epoch 396/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8318 - val_loss: 37.7779\n",
            "Epoch 397/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7692 - val_loss: 41.5596\n",
            "Epoch 398/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8401 - val_loss: 43.1869\n",
            "Epoch 399/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8193 - val_loss: 40.9201\n",
            "Epoch 400/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8001 - val_loss: 36.1307\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3acd05b0-f9c2-44d1-dcce-0636307a9b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.5262053799892994 \n",
            "MAE:  4.425382320633294 \n",
            "SD:  5.987803647374198\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "67c1b902-1d65-44b9-9228-4075b6ab6fd8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ce065f3-afbf-4f59-ba74-16a448240db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_15 (Dense)            (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,169\n",
            "Trainable params: 1,137\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d29fcb74-8937-4908-92dd-bfb0993380eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 3652.0029 - val_loss: 3672.5361\n",
            "Epoch 2/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3433.6799 - val_loss: 3431.1431\n",
            "Epoch 3/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3201.9685 - val_loss: 3082.7434\n",
            "Epoch 4/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2931.9604 - val_loss: 2768.4304\n",
            "Epoch 5/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2629.0208 - val_loss: 2448.3438\n",
            "Epoch 6/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2306.1101 - val_loss: 2059.7903\n",
            "Epoch 7/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1975.6962 - val_loss: 1665.4358\n",
            "Epoch 8/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1618.9585 - val_loss: 1374.6226\n",
            "Epoch 9/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1282.5912 - val_loss: 1099.1946\n",
            "Epoch 10/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 951.1874 - val_loss: 597.9728\n",
            "Epoch 11/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 688.5928 - val_loss: 632.1835\n",
            "Epoch 12/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 481.3680 - val_loss: 499.0529\n",
            "Epoch 13/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 324.4924 - val_loss: 265.8543\n",
            "Epoch 14/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 212.7667 - val_loss: 222.7704\n",
            "Epoch 15/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 138.9916 - val_loss: 116.1924\n",
            "Epoch 16/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 93.3569 - val_loss: 57.7070\n",
            "Epoch 17/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.9600 - val_loss: 100.3451\n",
            "Epoch 18/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 52.8841 - val_loss: 57.8369\n",
            "Epoch 19/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 45.7335 - val_loss: 66.5420\n",
            "Epoch 20/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 41.9395 - val_loss: 43.0080\n",
            "Epoch 21/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 40.0101 - val_loss: 47.4410\n",
            "Epoch 22/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.0387 - val_loss: 50.7473\n",
            "Epoch 23/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.1809 - val_loss: 57.8917\n",
            "Epoch 24/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.7847 - val_loss: 45.3134\n",
            "Epoch 25/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3871 - val_loss: 47.8162\n",
            "Epoch 26/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.1714 - val_loss: 40.3938\n",
            "Epoch 27/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6940 - val_loss: 46.1976\n",
            "Epoch 28/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.5038 - val_loss: 52.4153\n",
            "Epoch 29/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.2780 - val_loss: 55.2913\n",
            "Epoch 30/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.1122 - val_loss: 43.8089\n",
            "Epoch 31/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.0601 - val_loss: 42.6636\n",
            "Epoch 32/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.0901 - val_loss: 49.4791\n",
            "Epoch 33/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7263 - val_loss: 43.3287\n",
            "Epoch 34/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.6690 - val_loss: 37.9481\n",
            "Epoch 35/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3496 - val_loss: 42.1272\n",
            "Epoch 36/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3585 - val_loss: 46.2343\n",
            "Epoch 37/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2371 - val_loss: 43.2478\n",
            "Epoch 38/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1171 - val_loss: 38.4478\n",
            "Epoch 39/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0251 - val_loss: 42.9917\n",
            "Epoch 40/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8727 - val_loss: 44.6866\n",
            "Epoch 41/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7560 - val_loss: 43.8781\n",
            "Epoch 42/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6328 - val_loss: 38.0662\n",
            "Epoch 43/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5539 - val_loss: 43.2157\n",
            "Epoch 44/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3309 - val_loss: 38.3380\n",
            "Epoch 45/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2215 - val_loss: 51.7144\n",
            "Epoch 46/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3107 - val_loss: 41.3651\n",
            "Epoch 47/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2091 - val_loss: 51.6432\n",
            "Epoch 48/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0834 - val_loss: 38.9516\n",
            "Epoch 49/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.0115 - val_loss: 39.5130\n",
            "Epoch 50/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.8648 - val_loss: 39.5133\n",
            "Epoch 51/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.8683 - val_loss: 36.7666\n",
            "Epoch 52/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.7501 - val_loss: 38.8484\n",
            "Epoch 53/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.7231 - val_loss: 37.6327\n",
            "Epoch 54/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.6621 - val_loss: 38.7156\n",
            "Epoch 55/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4513 - val_loss: 57.7645\n",
            "Epoch 56/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.3683 - val_loss: 40.0948\n",
            "Epoch 57/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.3690 - val_loss: 35.7950\n",
            "Epoch 58/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.1803 - val_loss: 36.4920\n",
            "Epoch 59/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.2824 - val_loss: 38.2148\n",
            "Epoch 60/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.1504 - val_loss: 36.9543\n",
            "Epoch 61/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9870 - val_loss: 37.1653\n",
            "Epoch 62/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.9584 - val_loss: 36.5112\n",
            "Epoch 63/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8976 - val_loss: 40.6026\n",
            "Epoch 64/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.8981 - val_loss: 40.4971\n",
            "Epoch 65/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8209 - val_loss: 35.5266\n",
            "Epoch 66/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8160 - val_loss: 40.1941\n",
            "Epoch 67/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7130 - val_loss: 40.8113\n",
            "Epoch 68/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.6991 - val_loss: 39.4557\n",
            "Epoch 69/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6304 - val_loss: 40.9425\n",
            "Epoch 70/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5803 - val_loss: 48.3448\n",
            "Epoch 71/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.6500 - val_loss: 38.2888\n",
            "Epoch 72/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.4440 - val_loss: 55.9370\n",
            "Epoch 73/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.4729 - val_loss: 38.4533\n",
            "Epoch 74/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.5089 - val_loss: 36.4739\n",
            "Epoch 75/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.2978 - val_loss: 45.0883\n",
            "Epoch 76/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3972 - val_loss: 43.0985\n",
            "Epoch 77/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.3834 - val_loss: 36.6086\n",
            "Epoch 78/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2276 - val_loss: 39.8062\n",
            "Epoch 79/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.2662 - val_loss: 38.4783\n",
            "Epoch 80/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.3042 - val_loss: 38.3038\n",
            "Epoch 81/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.1504 - val_loss: 48.8159\n",
            "Epoch 82/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.2007 - val_loss: 42.7257\n",
            "Epoch 83/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0842 - val_loss: 38.6250\n",
            "Epoch 84/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0687 - val_loss: 41.1967\n",
            "Epoch 85/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0982 - val_loss: 46.3427\n",
            "Epoch 86/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0118 - val_loss: 40.3935\n",
            "Epoch 87/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0107 - val_loss: 37.9033\n",
            "Epoch 88/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 32.0014 - val_loss: 37.4876\n",
            "Epoch 89/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8589 - val_loss: 36.6888\n",
            "Epoch 90/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9653 - val_loss: 39.5761\n",
            "Epoch 91/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8555 - val_loss: 42.4146\n",
            "Epoch 92/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8560 - val_loss: 40.9652\n",
            "Epoch 93/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8205 - val_loss: 35.7561\n",
            "Epoch 94/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7512 - val_loss: 40.4933\n",
            "Epoch 95/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7598 - val_loss: 38.3117\n",
            "Epoch 96/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7071 - val_loss: 72.9776\n",
            "Epoch 97/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8036 - val_loss: 36.4667\n",
            "Epoch 98/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.6869 - val_loss: 41.4367\n",
            "Epoch 99/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7114 - val_loss: 49.4330\n",
            "Epoch 100/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.6877 - val_loss: 38.6925\n",
            "Epoch 101/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.7123 - val_loss: 41.0729\n",
            "Epoch 102/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5718 - val_loss: 54.0728\n",
            "Epoch 103/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6284 - val_loss: 36.8949\n",
            "Epoch 104/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6496 - val_loss: 35.8442\n",
            "Epoch 105/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.6370 - val_loss: 41.1034\n",
            "Epoch 106/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5503 - val_loss: 39.5193\n",
            "Epoch 107/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.5053 - val_loss: 62.1557\n",
            "Epoch 108/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.6010 - val_loss: 54.5859\n",
            "Epoch 109/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5610 - val_loss: 56.4556\n",
            "Epoch 110/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5461 - val_loss: 39.5335\n",
            "Epoch 111/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5586 - val_loss: 40.1772\n",
            "Epoch 112/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.4656 - val_loss: 36.5608\n",
            "Epoch 113/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4671 - val_loss: 36.9323\n",
            "Epoch 114/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4680 - val_loss: 40.1031\n",
            "Epoch 115/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.5154 - val_loss: 37.5345\n",
            "Epoch 116/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4268 - val_loss: 41.5349\n",
            "Epoch 117/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3692 - val_loss: 37.2457\n",
            "Epoch 118/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4051 - val_loss: 35.5140\n",
            "Epoch 119/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4154 - val_loss: 51.4231\n",
            "Epoch 120/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2920 - val_loss: 49.2436\n",
            "Epoch 121/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4267 - val_loss: 40.4369\n",
            "Epoch 122/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3511 - val_loss: 36.7643\n",
            "Epoch 123/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3521 - val_loss: 58.4550\n",
            "Epoch 124/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3702 - val_loss: 40.8685\n",
            "Epoch 125/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3417 - val_loss: 35.9437\n",
            "Epoch 126/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.3378 - val_loss: 42.3183\n",
            "Epoch 127/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2970 - val_loss: 43.8039\n",
            "Epoch 128/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3308 - val_loss: 40.8306\n",
            "Epoch 129/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2582 - val_loss: 46.8108\n",
            "Epoch 130/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2889 - val_loss: 40.0630\n",
            "Epoch 131/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2235 - val_loss: 56.8445\n",
            "Epoch 132/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3041 - val_loss: 36.1044\n",
            "Epoch 133/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1960 - val_loss: 35.9785\n",
            "Epoch 134/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2530 - val_loss: 47.2208\n",
            "Epoch 135/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2492 - val_loss: 40.1912\n",
            "Epoch 136/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2301 - val_loss: 35.8196\n",
            "Epoch 137/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2431 - val_loss: 38.8694\n",
            "Epoch 138/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2288 - val_loss: 37.7532\n",
            "Epoch 139/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1700 - val_loss: 46.0838\n",
            "Epoch 140/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1704 - val_loss: 45.2098\n",
            "Epoch 141/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1870 - val_loss: 38.7825\n",
            "Epoch 142/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1045 - val_loss: 41.0142\n",
            "Epoch 143/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2341 - val_loss: 36.4851\n",
            "Epoch 144/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1550 - val_loss: 39.4951\n",
            "Epoch 145/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0857 - val_loss: 41.1624\n",
            "Epoch 146/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1293 - val_loss: 35.0212\n",
            "Epoch 147/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1693 - val_loss: 62.6709\n",
            "Epoch 148/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1099 - val_loss: 35.7922\n",
            "Epoch 149/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.2473 - val_loss: 48.0519\n",
            "Epoch 150/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1536 - val_loss: 38.8587\n",
            "Epoch 151/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0775 - val_loss: 52.5784\n",
            "Epoch 152/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0994 - val_loss: 38.9566\n",
            "Epoch 153/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1744 - val_loss: 35.5744\n",
            "Epoch 154/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0324 - val_loss: 40.5875\n",
            "Epoch 155/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0814 - val_loss: 38.2743\n",
            "Epoch 156/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0833 - val_loss: 58.7784\n",
            "Epoch 157/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0880 - val_loss: 44.0889\n",
            "Epoch 158/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0223 - val_loss: 38.5339\n",
            "Epoch 159/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0621 - val_loss: 48.5588\n",
            "Epoch 160/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1126 - val_loss: 37.9573\n",
            "Epoch 161/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9691 - val_loss: 49.5140\n",
            "Epoch 162/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9995 - val_loss: 38.0566\n",
            "Epoch 163/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0601 - val_loss: 42.3452\n",
            "Epoch 164/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0285 - val_loss: 36.7197\n",
            "Epoch 165/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0400 - val_loss: 36.3446\n",
            "Epoch 166/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0134 - val_loss: 37.0795\n",
            "Epoch 167/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0034 - val_loss: 36.7572\n",
            "Epoch 168/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0592 - val_loss: 43.9924\n",
            "Epoch 169/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9452 - val_loss: 52.6509\n",
            "Epoch 170/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0366 - val_loss: 48.7892\n",
            "Epoch 171/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.1112 - val_loss: 35.5959\n",
            "Epoch 172/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0953 - val_loss: 36.7025\n",
            "Epoch 173/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0362 - val_loss: 36.5206\n",
            "Epoch 174/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9599 - val_loss: 46.6553\n",
            "Epoch 175/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0158 - val_loss: 35.0650\n",
            "Epoch 176/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0273 - val_loss: 37.2440\n",
            "Epoch 177/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0194 - val_loss: 37.4555\n",
            "Epoch 178/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9970 - val_loss: 35.6619\n",
            "Epoch 179/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0552 - val_loss: 50.4021\n",
            "Epoch 180/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9380 - val_loss: 41.6247\n",
            "Epoch 181/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0023 - val_loss: 38.1534\n",
            "Epoch 182/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9819 - val_loss: 52.8968\n",
            "Epoch 183/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9810 - val_loss: 40.7645\n",
            "Epoch 184/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8966 - val_loss: 42.4468\n",
            "Epoch 185/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9272 - val_loss: 39.0930\n",
            "Epoch 186/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9727 - val_loss: 49.0137\n",
            "Epoch 187/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0052 - val_loss: 37.0566\n",
            "Epoch 188/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8777 - val_loss: 38.1816\n",
            "Epoch 189/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9687 - val_loss: 35.9450\n",
            "Epoch 190/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9667 - val_loss: 43.0231\n",
            "Epoch 191/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9478 - val_loss: 37.3917\n",
            "Epoch 192/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0692 - val_loss: 35.9987\n",
            "Epoch 193/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9422 - val_loss: 43.9878\n",
            "Epoch 194/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8854 - val_loss: 39.3950\n",
            "Epoch 195/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0085 - val_loss: 35.1877\n",
            "Epoch 196/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9272 - val_loss: 35.1236\n",
            "Epoch 197/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9934 - val_loss: 39.7510\n",
            "Epoch 198/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8969 - val_loss: 37.2308\n",
            "Epoch 199/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9589 - val_loss: 47.8265\n",
            "Epoch 200/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9472 - val_loss: 36.3446\n",
            "Epoch 201/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8703 - val_loss: 41.7505\n",
            "Epoch 202/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8964 - val_loss: 41.5722\n",
            "Epoch 203/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9412 - val_loss: 39.7093\n",
            "Epoch 204/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0094 - val_loss: 38.9814\n",
            "Epoch 205/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8957 - val_loss: 37.4870\n",
            "Epoch 206/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9070 - val_loss: 44.6657\n",
            "Epoch 207/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 31.0001 - val_loss: 36.7370\n",
            "Epoch 208/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9472 - val_loss: 35.0027\n",
            "Epoch 209/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8946 - val_loss: 40.2080\n",
            "Epoch 210/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8848 - val_loss: 37.6632\n",
            "Epoch 211/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8719 - val_loss: 50.0882\n",
            "Epoch 212/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9300 - val_loss: 36.5694\n",
            "Epoch 213/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9528 - val_loss: 38.4156\n",
            "Epoch 214/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9023 - val_loss: 40.9536\n",
            "Epoch 215/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8978 - val_loss: 36.3280\n",
            "Epoch 216/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9506 - val_loss: 36.8462\n",
            "Epoch 217/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9262 - val_loss: 36.0090\n",
            "Epoch 218/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9121 - val_loss: 36.5506\n",
            "Epoch 219/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8536 - val_loss: 37.1585\n",
            "Epoch 220/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8619 - val_loss: 35.6061\n",
            "Epoch 221/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8670 - val_loss: 36.2492\n",
            "Epoch 222/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8278 - val_loss: 38.2562\n",
            "Epoch 223/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9188 - val_loss: 37.8570\n",
            "Epoch 224/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8810 - val_loss: 36.9105\n",
            "Epoch 225/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8632 - val_loss: 36.5252\n",
            "Epoch 226/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8315 - val_loss: 35.8602\n",
            "Epoch 227/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8584 - val_loss: 36.5284\n",
            "Epoch 228/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8183 - val_loss: 56.2855\n",
            "Epoch 229/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8453 - val_loss: 42.4518\n",
            "Epoch 230/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8396 - val_loss: 37.3548\n",
            "Epoch 231/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8774 - val_loss: 44.4096\n",
            "Epoch 232/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8609 - val_loss: 49.6358\n",
            "Epoch 233/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8497 - val_loss: 37.4101\n",
            "Epoch 234/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8424 - val_loss: 37.6718\n",
            "Epoch 235/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8399 - val_loss: 35.0639\n",
            "Epoch 236/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8039 - val_loss: 39.0405\n",
            "Epoch 237/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7930 - val_loss: 36.6506\n",
            "Epoch 238/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.9003 - val_loss: 38.5639\n",
            "Epoch 239/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8038 - val_loss: 35.0625\n",
            "Epoch 240/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8445 - val_loss: 37.2748\n",
            "Epoch 241/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8591 - val_loss: 34.5222\n",
            "Epoch 242/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8372 - val_loss: 35.7115\n",
            "Epoch 243/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8706 - val_loss: 39.7194\n",
            "Epoch 244/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8602 - val_loss: 38.6272\n",
            "Epoch 245/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8632 - val_loss: 34.4207\n",
            "Epoch 246/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8406 - val_loss: 42.1748\n",
            "Epoch 247/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7793 - val_loss: 36.3878\n",
            "Epoch 248/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8625 - val_loss: 40.7095\n",
            "Epoch 249/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8216 - val_loss: 39.8451\n",
            "Epoch 250/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8039 - val_loss: 35.5025\n",
            "Epoch 251/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7514 - val_loss: 39.0517\n",
            "Epoch 252/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8260 - val_loss: 41.6645\n",
            "Epoch 253/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8729 - val_loss: 34.8682\n",
            "Epoch 254/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8071 - val_loss: 41.8586\n",
            "Epoch 255/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8291 - val_loss: 35.8012\n",
            "Epoch 256/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8369 - val_loss: 40.3578\n",
            "Epoch 257/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7910 - val_loss: 43.2066\n",
            "Epoch 258/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7721 - val_loss: 35.2058\n",
            "Epoch 259/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7913 - val_loss: 36.5560\n",
            "Epoch 260/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7857 - val_loss: 37.0600\n",
            "Epoch 261/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7764 - val_loss: 35.5240\n",
            "Epoch 262/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7845 - val_loss: 42.0004\n",
            "Epoch 263/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.8121 - val_loss: 37.3842\n",
            "Epoch 264/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7452 - val_loss: 38.5192\n",
            "Epoch 265/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7251 - val_loss: 43.1789\n",
            "Epoch 266/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7368 - val_loss: 38.9717\n",
            "Epoch 267/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7405 - val_loss: 39.1165\n",
            "Epoch 268/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7770 - val_loss: 35.4903\n",
            "Epoch 269/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7048 - val_loss: 35.0693\n",
            "Epoch 270/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7565 - val_loss: 36.1123\n",
            "Epoch 271/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7837 - val_loss: 38.9129\n",
            "Epoch 272/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7963 - val_loss: 39.5554\n",
            "Epoch 273/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7413 - val_loss: 38.3628\n",
            "Epoch 274/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6730 - val_loss: 42.3374\n",
            "Epoch 275/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7981 - val_loss: 36.4644\n",
            "Epoch 276/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7017 - val_loss: 36.1006\n",
            "Epoch 277/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7973 - val_loss: 36.8695\n",
            "Epoch 278/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7020 - val_loss: 35.2187\n",
            "Epoch 279/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7329 - val_loss: 35.8065\n",
            "Epoch 280/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7381 - val_loss: 35.2743\n",
            "Epoch 281/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7202 - val_loss: 36.2303\n",
            "Epoch 282/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7437 - val_loss: 38.1526\n",
            "Epoch 283/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6877 - val_loss: 37.0366\n",
            "Epoch 284/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7245 - val_loss: 36.1982\n",
            "Epoch 285/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7444 - val_loss: 35.4622\n",
            "Epoch 286/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6877 - val_loss: 37.4653\n",
            "Epoch 287/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7343 - val_loss: 35.1745\n",
            "Epoch 288/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7724 - val_loss: 48.0271\n",
            "Epoch 289/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7605 - val_loss: 52.3951\n",
            "Epoch 290/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7335 - val_loss: 38.1104\n",
            "Epoch 291/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6964 - val_loss: 41.0181\n",
            "Epoch 292/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7096 - val_loss: 36.8594\n",
            "Epoch 293/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7207 - val_loss: 37.1071\n",
            "Epoch 294/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7531 - val_loss: 35.3469\n",
            "Epoch 295/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7628 - val_loss: 38.0237\n",
            "Epoch 296/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6755 - val_loss: 36.8113\n",
            "Epoch 297/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7160 - val_loss: 39.5826\n",
            "Epoch 298/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6860 - val_loss: 37.2739\n",
            "Epoch 299/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7385 - val_loss: 37.7306\n",
            "Epoch 300/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6702 - val_loss: 36.6570\n",
            "Epoch 301/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7433 - val_loss: 42.2334\n",
            "Epoch 302/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7420 - val_loss: 34.4921\n",
            "Epoch 303/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6764 - val_loss: 36.6912\n",
            "Epoch 304/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6731 - val_loss: 40.4159\n",
            "Epoch 305/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7308 - val_loss: 37.3718\n",
            "Epoch 306/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6830 - val_loss: 36.6849\n",
            "Epoch 307/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6674 - val_loss: 38.0301\n",
            "Epoch 308/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6833 - val_loss: 39.6614\n",
            "Epoch 309/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6454 - val_loss: 36.4134\n",
            "Epoch 310/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6553 - val_loss: 46.1891\n",
            "Epoch 311/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6764 - val_loss: 35.3962\n",
            "Epoch 312/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6445 - val_loss: 34.7211\n",
            "Epoch 313/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6666 - val_loss: 36.2605\n",
            "Epoch 314/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6238 - val_loss: 37.8251\n",
            "Epoch 315/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6658 - val_loss: 44.6144\n",
            "Epoch 316/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6722 - val_loss: 38.2009\n",
            "Epoch 317/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6801 - val_loss: 36.8623\n",
            "Epoch 318/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6704 - val_loss: 35.1695\n",
            "Epoch 319/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7031 - val_loss: 37.3527\n",
            "Epoch 320/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7136 - val_loss: 42.6561\n",
            "Epoch 321/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6965 - val_loss: 38.3461\n",
            "Epoch 322/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6690 - val_loss: 35.8756\n",
            "Epoch 323/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6855 - val_loss: 36.3342\n",
            "Epoch 324/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6176 - val_loss: 36.8590\n",
            "Epoch 325/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6525 - val_loss: 38.6705\n",
            "Epoch 326/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6347 - val_loss: 38.7627\n",
            "Epoch 327/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6966 - val_loss: 45.6809\n",
            "Epoch 328/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6798 - val_loss: 36.4386\n",
            "Epoch 329/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.5879 - val_loss: 39.5881\n",
            "Epoch 330/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5994 - val_loss: 39.4246\n",
            "Epoch 331/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6615 - val_loss: 37.6445\n",
            "Epoch 332/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6419 - val_loss: 42.4085\n",
            "Epoch 333/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.7197 - val_loss: 45.1274\n",
            "Epoch 334/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6152 - val_loss: 35.2837\n",
            "Epoch 335/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6758 - val_loss: 38.6976\n",
            "Epoch 336/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6727 - val_loss: 41.3336\n",
            "Epoch 337/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6534 - val_loss: 34.8921\n",
            "Epoch 338/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7288 - val_loss: 35.8610\n",
            "Epoch 339/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6190 - val_loss: 42.0967\n",
            "Epoch 340/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6888 - val_loss: 43.0698\n",
            "Epoch 341/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6094 - val_loss: 37.5666\n",
            "Epoch 342/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6532 - val_loss: 38.3454\n",
            "Epoch 343/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6343 - val_loss: 53.8949\n",
            "Epoch 344/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6162 - val_loss: 38.6076\n",
            "Epoch 345/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7008 - val_loss: 40.8629\n",
            "Epoch 346/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6432 - val_loss: 34.9829\n",
            "Epoch 347/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6323 - val_loss: 36.8276\n",
            "Epoch 348/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6088 - val_loss: 42.6400\n",
            "Epoch 349/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6377 - val_loss: 49.3108\n",
            "Epoch 350/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6419 - val_loss: 36.8346\n",
            "Epoch 351/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.6323 - val_loss: 36.1595\n",
            "Epoch 352/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.5728 - val_loss: 34.9383\n",
            "Epoch 353/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5955 - val_loss: 36.8666\n",
            "Epoch 354/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7182 - val_loss: 36.4235\n",
            "Epoch 355/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6396 - val_loss: 39.1577\n",
            "Epoch 356/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6542 - val_loss: 41.5095\n",
            "Epoch 357/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6225 - val_loss: 36.3324\n",
            "Epoch 358/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.5994 - val_loss: 46.1573\n",
            "Epoch 359/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.5910 - val_loss: 41.7619\n",
            "Epoch 360/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5859 - val_loss: 36.9577\n",
            "Epoch 361/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5917 - val_loss: 36.6481\n",
            "Epoch 362/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.5945 - val_loss: 39.5594\n",
            "Epoch 363/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6122 - val_loss: 36.1102\n",
            "Epoch 364/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6232 - val_loss: 35.0226\n",
            "Epoch 365/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5726 - val_loss: 34.7065\n",
            "Epoch 366/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5769 - val_loss: 37.6551\n",
            "Epoch 367/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6403 - val_loss: 46.6161\n",
            "Epoch 368/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6115 - val_loss: 37.5745\n",
            "Epoch 369/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6331 - val_loss: 37.7384\n",
            "Epoch 370/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5644 - val_loss: 36.7727\n",
            "Epoch 371/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5813 - val_loss: 41.4021\n",
            "Epoch 372/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6201 - val_loss: 37.7206\n",
            "Epoch 373/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6098 - val_loss: 37.7879\n",
            "Epoch 374/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6167 - val_loss: 39.9446\n",
            "Epoch 375/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5561 - val_loss: 37.7459\n",
            "Epoch 376/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6376 - val_loss: 37.9028\n",
            "Epoch 377/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5621 - val_loss: 38.1986\n",
            "Epoch 378/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.5791 - val_loss: 39.4031\n",
            "Epoch 379/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6171 - val_loss: 34.7426\n",
            "Epoch 380/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6491 - val_loss: 41.4902\n",
            "Epoch 381/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5569 - val_loss: 35.3271\n",
            "Epoch 382/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.5298 - val_loss: 40.7476\n",
            "Epoch 383/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5547 - val_loss: 35.4468\n",
            "Epoch 384/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.5570 - val_loss: 36.4907\n",
            "Epoch 385/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6216 - val_loss: 35.6792\n",
            "Epoch 386/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.5878 - val_loss: 48.5949\n",
            "Epoch 387/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5661 - val_loss: 35.7756\n",
            "Epoch 388/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6235 - val_loss: 37.3646\n",
            "Epoch 389/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.5520 - val_loss: 39.4133\n",
            "Epoch 390/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5868 - val_loss: 39.9607\n",
            "Epoch 391/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5805 - val_loss: 41.1944\n",
            "Epoch 392/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.5756 - val_loss: 43.9254\n",
            "Epoch 393/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5947 - val_loss: 43.6581\n",
            "Epoch 394/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.5196 - val_loss: 40.3332\n",
            "Epoch 395/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.6187 - val_loss: 36.4718\n",
            "Epoch 396/400\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 30.5903 - val_loss: 36.6562\n",
            "Epoch 397/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6054 - val_loss: 36.5130\n",
            "Epoch 398/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6243 - val_loss: 37.2832\n",
            "Epoch 399/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6231 - val_loss: 35.1549\n",
            "Epoch 400/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5924 - val_loss: 44.4059\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e141bf3c-f788-4445-8bdf-9471bdc55329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  2.870981182856782 \n",
            "MAE:  4.994783837103851 \n",
            "SD:  6.013599785351958\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "9896ed0a-47e4-4733-9ad3-0c190d822fe4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a938f4-00f4-47ef-eabb-4a51a1665610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  0.7557646506620138 \n",
            "Ensemble_std:  6.022353835404295\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "\bBP_hv3_2(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}