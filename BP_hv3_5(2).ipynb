{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/%08BP_hv3_5(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2YTF6cMiY1Hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# batch_size"
      ],
      "metadata": {
        "id": "XiiiBla2-j1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsCoux5AOZnK",
        "outputId": "f992d066-cc41-4fea-9ab7-f2e33b594bc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "TensorFlow version :  2.8.2\n",
            "Keras version :  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlHICkovd809",
        "outputId": "3f91d400-8d1d-483d-f894-939a1fbdc2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러z오기\n",
        "train_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ],
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "85063051-6247-457d-9cb6-adf71644dc8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "041cf765-0cd7-4015-88d7-b9a4cb58e7bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c315d59c-6803-438c-ae80-dbd281e09d2f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c315d59c-6803-438c-ae80-dbd281e09d2f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c315d59c-6803-438c-ae80-dbd281e09d2f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c315d59c-6803-438c-ae80-dbd281e09d2f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "960178ea-5cd0-4c5b-9db9-9b9c3b5993c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21ae9dca-7839-4c24-a56a-a11d2a6c1167\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21ae9dca-7839-4c24-a56a-a11d2a6c1167')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-21ae9dca-7839-4c24-a56a-a11d2a6c1167 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-21ae9dca-7839-4c24-a56a-a11d2a6c1167');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ],
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8SHBwBOZnO",
        "outputId": "63c8ed7e-b0cc-4a5a-c07a-e36f9b619dbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 64)                8192      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 64)               256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,745\n",
            "Trainable params: 12,361\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dGT6-7NcOZnO",
        "outputId": "c771006b-9192-4fa7-d29e-ae4d3b844436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 6s 11ms/step - loss: 12196.3223 - val_loss: 12128.5459\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 11757.2598 - val_loss: 11692.3340\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 11224.4043 - val_loss: 10341.6309\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 10476.7861 - val_loss: 9901.2900\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 9615.9062 - val_loss: 10339.5938\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 8682.6230 - val_loss: 7640.6787\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 7675.4971 - val_loss: 6220.8086\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 6672.0298 - val_loss: 5865.3398\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 5696.1313 - val_loss: 5087.9263\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 4769.2129 - val_loss: 3124.5024\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 3903.2539 - val_loss: 1725.1733\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 3126.0508 - val_loss: 969.8645\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 2449.8408 - val_loss: 3896.4131\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 1873.3507 - val_loss: 806.3427\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 1390.2769 - val_loss: 587.7069\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 1005.7602 - val_loss: 1069.6443\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 712.1979 - val_loss: 543.3297\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 494.8975 - val_loss: 245.7253\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 342.3760 - val_loss: 943.3370\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 239.8672 - val_loss: 369.3717\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 174.5594 - val_loss: 109.9888\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 137.3315 - val_loss: 182.9674\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 119.1617 - val_loss: 142.8593\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.7211 - val_loss: 123.7995\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 104.9012 - val_loss: 129.4921\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 100.2608 - val_loss: 150.3587\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 98.2697 - val_loss: 157.1834\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 97.5648 - val_loss: 126.4040\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 95.1753 - val_loss: 109.6634\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 94.2996 - val_loss: 109.9549\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 93.4572 - val_loss: 127.5027\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 93.7634 - val_loss: 116.3384\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 91.7236 - val_loss: 136.7124\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 90.2704 - val_loss: 114.7953\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 89.6316 - val_loss: 103.9265\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 88.4235 - val_loss: 115.8707\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 88.0297 - val_loss: 104.3574\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 87.6160 - val_loss: 129.8706\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 88.4525 - val_loss: 136.7251\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 88.0967 - val_loss: 167.0464\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 86.3001 - val_loss: 101.2236\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 84.8769 - val_loss: 170.1631\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 84.3413 - val_loss: 102.7743\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 83.8199 - val_loss: 122.1996\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 83.9305 - val_loss: 111.6021\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 82.8909 - val_loss: 119.5199\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 83.2863 - val_loss: 111.0321\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 82.1823 - val_loss: 117.1599\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 81.5418 - val_loss: 117.6616\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 81.2819 - val_loss: 117.8132\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 80.9402 - val_loss: 121.1022\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 79.8783 - val_loss: 99.2394\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 79.0663 - val_loss: 114.3403\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 78.2890 - val_loss: 108.6899\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 78.4574 - val_loss: 114.7950\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.3094 - val_loss: 102.0885\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.3165 - val_loss: 130.5462\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 77.2144 - val_loss: 105.1771\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 77.1858 - val_loss: 112.5749\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 76.3885 - val_loss: 99.9385\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 75.6520 - val_loss: 131.9729\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 75.1541 - val_loss: 100.9110\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 74.5629 - val_loss: 113.2625\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 74.4186 - val_loss: 100.2124\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 74.2008 - val_loss: 100.4884\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 73.4800 - val_loss: 110.0138\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 73.0472 - val_loss: 135.4386\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 73.1206 - val_loss: 103.4146\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 72.9603 - val_loss: 140.6642\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 72.4020 - val_loss: 97.3525\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.6857 - val_loss: 97.5754\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.6786 - val_loss: 105.5539\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.4167 - val_loss: 123.0588\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.2222 - val_loss: 119.4790\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.5424 - val_loss: 103.2016\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.2848 - val_loss: 107.3271\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.1897 - val_loss: 112.9561\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.8951 - val_loss: 102.1697\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.6896 - val_loss: 116.7308\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.2380 - val_loss: 89.0887\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.7703 - val_loss: 170.6102\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.6411 - val_loss: 131.6290\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 68.5691 - val_loss: 231.7843\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.3617 - val_loss: 97.4641\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 68.9645 - val_loss: 105.2369\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.1582 - val_loss: 106.2657\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 68.0587 - val_loss: 114.7148\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.3576 - val_loss: 110.8888\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 67.1307 - val_loss: 97.8120\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 67.2521 - val_loss: 97.2589\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.3688 - val_loss: 111.7984\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.8398 - val_loss: 115.6126\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.6687 - val_loss: 100.3795\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.9561 - val_loss: 104.8830\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.1936 - val_loss: 124.1003\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 65.9834 - val_loss: 96.6722\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.7528 - val_loss: 94.1551\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.5590 - val_loss: 144.5138\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 65.5995 - val_loss: 105.2235\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.1067 - val_loss: 96.1563\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 64.6972 - val_loss: 109.5210\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 65.1512 - val_loss: 109.9788\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.5654 - val_loss: 104.2989\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 64.6315 - val_loss: 115.9054\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.4353 - val_loss: 95.3292\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.8761 - val_loss: 131.1653\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.7958 - val_loss: 101.0303\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.8085 - val_loss: 112.4302\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.6171 - val_loss: 88.6333\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.3241 - val_loss: 124.0270\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.2923 - val_loss: 114.0391\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.2233 - val_loss: 100.8893\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.9825 - val_loss: 235.8463\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.2668 - val_loss: 89.5870\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.0024 - val_loss: 141.7708\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.5686 - val_loss: 104.0619\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.2752 - val_loss: 115.4989\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.1696 - val_loss: 101.7579\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.0260 - val_loss: 131.9576\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 62.4846 - val_loss: 99.6662\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 62.7945 - val_loss: 111.2675\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.4789 - val_loss: 97.8978\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.1700 - val_loss: 104.4543\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.5356 - val_loss: 109.7672\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.5109 - val_loss: 97.6245\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.2875 - val_loss: 110.6234\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.9260 - val_loss: 106.8216\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.4063 - val_loss: 94.2413\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.1814 - val_loss: 95.1680\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.7492 - val_loss: 105.6676\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.5500 - val_loss: 99.8517\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.8761 - val_loss: 101.1002\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.7662 - val_loss: 124.0102\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.7785 - val_loss: 102.9441\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.7258 - val_loss: 109.2132\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 60.4066 - val_loss: 113.5890\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 60.3366 - val_loss: 97.8886\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 60.4485 - val_loss: 100.4083\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.2061 - val_loss: 101.5943\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.8820 - val_loss: 121.6015\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.0309 - val_loss: 170.7375\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.8912 - val_loss: 108.0768\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.8582 - val_loss: 94.3182\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.5318 - val_loss: 112.7486\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.6168 - val_loss: 93.2231\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.3867 - val_loss: 104.9518\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.4952 - val_loss: 98.8413\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.4414 - val_loss: 91.6539\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.2337 - val_loss: 105.0989\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.6354 - val_loss: 162.2501\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.2668 - val_loss: 116.8376\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.1120 - val_loss: 90.7982\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.6303 - val_loss: 102.5036\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.7623 - val_loss: 98.4176\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.7744 - val_loss: 107.1904\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.4861 - val_loss: 96.5795\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.6670 - val_loss: 92.0621\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.5055 - val_loss: 117.9157\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.1779 - val_loss: 120.0306\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.8930 - val_loss: 95.6681\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.2284 - val_loss: 106.6849\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.9062 - val_loss: 114.5862\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.0414 - val_loss: 114.9974\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.9181 - val_loss: 95.9715\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.3378 - val_loss: 107.5848\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.7308 - val_loss: 106.9495\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.7850 - val_loss: 102.2370\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.9758 - val_loss: 104.2882\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.7617 - val_loss: 99.2291\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.7354 - val_loss: 98.2724\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.8559 - val_loss: 89.5251\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.5673 - val_loss: 97.9262\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.7316 - val_loss: 93.0733\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.4319 - val_loss: 127.6263\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.3167 - val_loss: 90.2575\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.9472 - val_loss: 107.9493\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.8509 - val_loss: 97.5235\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.9595 - val_loss: 144.5711\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.2034 - val_loss: 91.6805\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 56.8947 - val_loss: 125.4211\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 56.7791 - val_loss: 92.7023\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.6113 - val_loss: 93.8760\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.5322 - val_loss: 94.0911\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.6306 - val_loss: 97.6856\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.4057 - val_loss: 97.2797\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.6617 - val_loss: 101.3158\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.5560 - val_loss: 134.3246\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.6209 - val_loss: 117.4188\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.4596 - val_loss: 99.2797\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.6298 - val_loss: 114.2746\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.2823 - val_loss: 100.1198\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.3416 - val_loss: 108.4074\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.2782 - val_loss: 120.2364\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.0665 - val_loss: 108.4863\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.2671 - val_loss: 142.9613\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.4774 - val_loss: 101.8488\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.0434 - val_loss: 96.4952\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.0068 - val_loss: 115.2027\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.1037 - val_loss: 99.1864\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.7585 - val_loss: 106.1998\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.7338 - val_loss: 96.2819\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.7348 - val_loss: 95.3369\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.8324 - val_loss: 90.6693\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.6012 - val_loss: 98.1060\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.7473 - val_loss: 107.8021\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.7657 - val_loss: 99.5078\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.8069 - val_loss: 95.3073\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.5809 - val_loss: 107.0195\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.8215 - val_loss: 112.2625\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.6282 - val_loss: 140.5879\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.1847 - val_loss: 100.6230\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.4993 - val_loss: 92.2784\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.3636 - val_loss: 109.3964\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.3999 - val_loss: 119.1710\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.4883 - val_loss: 104.4654\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.4007 - val_loss: 132.0150\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.1988 - val_loss: 105.2815\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.8350 - val_loss: 114.9419\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.0743 - val_loss: 107.2497\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.0072 - val_loss: 104.0025\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.8858 - val_loss: 92.7050\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.9364 - val_loss: 125.3134\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.8788 - val_loss: 98.0594\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.1355 - val_loss: 111.2961\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.3309 - val_loss: 110.0504\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.0428 - val_loss: 101.6811\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.7004 - val_loss: 95.8293\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.5769 - val_loss: 106.2573\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.6146 - val_loss: 104.9861\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.7655 - val_loss: 107.6328\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.6468 - val_loss: 96.2735\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.7928 - val_loss: 98.0348\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.7006 - val_loss: 103.7029\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.4986 - val_loss: 97.5471\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.4659 - val_loss: 135.9471\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.3498 - val_loss: 113.2823\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.1854 - val_loss: 99.7810\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.3335 - val_loss: 94.1865\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.1458 - val_loss: 102.1085\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.2891 - val_loss: 94.2949\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.1710 - val_loss: 92.6312\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.0175 - val_loss: 94.5173\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.0982 - val_loss: 100.0060\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.1385 - val_loss: 119.8729\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.9856 - val_loss: 98.4185\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.8557 - val_loss: 111.5254\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.0043 - val_loss: 100.3059\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.0197 - val_loss: 95.9227\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.0926 - val_loss: 95.3112\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.7494 - val_loss: 107.1855\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.8529 - val_loss: 93.5529\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.7655 - val_loss: 107.3087\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.8486 - val_loss: 95.1302\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.5965 - val_loss: 93.6786\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.8111 - val_loss: 131.8799\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.9209 - val_loss: 100.7360\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.6062 - val_loss: 106.8662\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.7032 - val_loss: 95.0131\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.5714 - val_loss: 102.0178\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.6908 - val_loss: 107.6549\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.6087 - val_loss: 93.9995\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.6454 - val_loss: 91.9460\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.7030 - val_loss: 103.3220\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.6313 - val_loss: 106.2412\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.5560 - val_loss: 93.1971\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.3928 - val_loss: 97.6324\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.4566 - val_loss: 110.6769\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.3809 - val_loss: 101.7419\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.4930 - val_loss: 105.6036\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.3497 - val_loss: 107.1947\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.4695 - val_loss: 102.3501\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.5193 - val_loss: 95.2034\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.2433 - val_loss: 103.5498\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.5645 - val_loss: 109.9366\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.1936 - val_loss: 101.6310\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.2224 - val_loss: 119.1026\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.2090 - val_loss: 93.8776\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.2172 - val_loss: 96.4874\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.9827 - val_loss: 127.5991\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.9204 - val_loss: 117.8473\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.0031 - val_loss: 100.5417\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.0935 - val_loss: 104.1029\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.8926 - val_loss: 120.4918\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.9377 - val_loss: 92.7825\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.9998 - val_loss: 98.3531\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.1100 - val_loss: 117.4736\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.9625 - val_loss: 97.5888\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.8185 - val_loss: 105.7634\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.8352 - val_loss: 116.8880\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.0530 - val_loss: 96.0653\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.8899 - val_loss: 100.8104\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.5418 - val_loss: 109.4796\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.9648 - val_loss: 93.7026\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.7753 - val_loss: 93.7863\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.8149 - val_loss: 122.3350\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.7688 - val_loss: 105.0385\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.9317 - val_loss: 91.8064\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.7057 - val_loss: 107.4818\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.8159 - val_loss: 106.4473\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.9530 - val_loss: 102.3040\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.8044 - val_loss: 104.1160\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.5150 - val_loss: 98.8152\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 52.5765 - val_loss: 96.7506\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 52.7767 - val_loss: 106.6568\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.5817 - val_loss: 131.7932\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.5776 - val_loss: 96.2218\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.4308 - val_loss: 103.1622\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.4737 - val_loss: 108.3404\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.6551 - val_loss: 97.0708\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.5189 - val_loss: 111.3522\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.2401 - val_loss: 109.1051\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.2904 - val_loss: 94.3708\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.3420 - val_loss: 107.1607\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.4449 - val_loss: 104.4655\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.2218 - val_loss: 103.1174\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.2913 - val_loss: 118.3272\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.3972 - val_loss: 113.5534\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.3885 - val_loss: 102.7694\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.1887 - val_loss: 99.6940\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.2881 - val_loss: 107.3063\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.1712 - val_loss: 102.0674\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.1910 - val_loss: 101.3386\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.2022 - val_loss: 97.1929\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.2305 - val_loss: 96.3587\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.3552 - val_loss: 122.2974\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.3099 - val_loss: 116.8070\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.2629 - val_loss: 99.8444\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.2886 - val_loss: 108.4338\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.0208 - val_loss: 98.6051\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.1915 - val_loss: 111.6518\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.3886 - val_loss: 114.1745\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.4152 - val_loss: 104.9739\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.9120 - val_loss: 97.5177\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.8819 - val_loss: 106.4460\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.9150 - val_loss: 92.2259\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.3309 - val_loss: 112.1316\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.8885 - val_loss: 99.5393\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 51.8876 - val_loss: 97.6992\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.8968 - val_loss: 104.9707\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.7560 - val_loss: 111.7908\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 51.5775 - val_loss: 100.9267\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.8866 - val_loss: 113.2057\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.8041 - val_loss: 113.2761\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.8927 - val_loss: 102.1143\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.7081 - val_loss: 98.4370\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.8737 - val_loss: 109.0110\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.9085 - val_loss: 98.3692\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.6008 - val_loss: 93.2479\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.8330 - val_loss: 100.7387\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 51.5303 - val_loss: 96.0521\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 51.6650 - val_loss: 107.5425\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.6351 - val_loss: 103.4950\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 51.7034 - val_loss: 102.7956\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.6496 - val_loss: 99.3054\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.5193 - val_loss: 100.8279\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.5510 - val_loss: 96.9374\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 51.4998 - val_loss: 99.7410\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.5657 - val_loss: 102.6162\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.5075 - val_loss: 107.8774\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.3456 - val_loss: 100.5336\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.5718 - val_loss: 106.6748\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.6660 - val_loss: 110.0957\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 51.7014 - val_loss: 100.8785\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.4295 - val_loss: 100.5253\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.5090 - val_loss: 113.9697\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.4872 - val_loss: 92.8966\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.3548 - val_loss: 97.4505\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.4937 - val_loss: 94.6568\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.5796 - val_loss: 112.0328\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.4105 - val_loss: 99.3033\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.4849 - val_loss: 104.0232\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.7134 - val_loss: 92.3860\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.4397 - val_loss: 115.7261\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.1944 - val_loss: 105.3444\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.3996 - val_loss: 97.5110\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.4294 - val_loss: 130.6852\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 51.2800 - val_loss: 106.4576\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.1976 - val_loss: 100.2125\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.2863 - val_loss: 100.8333\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.4473 - val_loss: 99.4974\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.3153 - val_loss: 97.6077\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.3239 - val_loss: 107.8627\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.3882 - val_loss: 102.9964\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.2226 - val_loss: 116.6357\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.1942 - val_loss: 106.9602\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.2716 - val_loss: 102.0265\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.2315 - val_loss: 111.2491\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.0839 - val_loss: 118.7569\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.2006 - val_loss: 103.5446\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.2161 - val_loss: 105.4843\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.1844 - val_loss: 102.6188\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.1648 - val_loss: 107.8445\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.0340 - val_loss: 99.7447\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.8314 - val_loss: 93.4401\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.0434 - val_loss: 111.9393\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.0388 - val_loss: 124.9580\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.1114 - val_loss: 93.8812\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.2855 - val_loss: 106.1013\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.1767 - val_loss: 100.5298\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.1303 - val_loss: 102.5763\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 51.0670 - val_loss: 112.4347\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.8635 - val_loss: 107.4414\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.0413 - val_loss: 103.4498\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.0198 - val_loss: 106.3396\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.0693 - val_loss: 117.6426\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.0798 - val_loss: 110.6745\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.9821 - val_loss: 98.3411\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.7067 - val_loss: 121.2009\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.9109 - val_loss: 104.0957\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.9515 - val_loss: 95.4019\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.9956 - val_loss: 109.1040\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 50.6715 - val_loss: 99.9183\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 50.9724 - val_loss: 118.7059\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.8269 - val_loss: 107.1134\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.8624 - val_loss: 103.7626\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.8396 - val_loss: 99.3426\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.7903 - val_loss: 96.1080\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.7817 - val_loss: 91.3707\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.8788 - val_loss: 107.9351\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.8488 - val_loss: 103.0100\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.7083 - val_loss: 95.0592\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.7507 - val_loss: 98.6269\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.8346 - val_loss: 123.6114\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 50.6629 - val_loss: 96.9614\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 50.5443 - val_loss: 95.7560\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.5519 - val_loss: 96.6473\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.7433 - val_loss: 100.6416\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.8351 - val_loss: 97.5821\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.7500 - val_loss: 100.6951\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.7000 - val_loss: 116.9777\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.7136 - val_loss: 101.7632\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.7503 - val_loss: 101.0621\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.5146 - val_loss: 101.1195\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5679 - val_loss: 100.2405\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.7294 - val_loss: 100.0494\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5798 - val_loss: 94.1727\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.6007 - val_loss: 123.2300\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5909 - val_loss: 100.0315\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.7703 - val_loss: 112.4052\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5921 - val_loss: 95.8238\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5767 - val_loss: 100.3225\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.6919 - val_loss: 97.2851\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.6258 - val_loss: 120.5265\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.3931 - val_loss: 97.0511\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.3147 - val_loss: 133.5391\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.2952 - val_loss: 94.8608\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.6800 - val_loss: 99.9878\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.7780 - val_loss: 110.7238\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5309 - val_loss: 93.2169\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.3614 - val_loss: 101.1085\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5484 - val_loss: 96.2390\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.3413 - val_loss: 119.8375\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5103 - val_loss: 101.8607\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.4197 - val_loss: 108.0687\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.5779 - val_loss: 121.0785\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.1328 - val_loss: 98.5647\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.2950 - val_loss: 106.0319\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.6817 - val_loss: 102.5186\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.4758 - val_loss: 113.3408\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.2505 - val_loss: 96.8484\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.5538 - val_loss: 103.2465\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.3116 - val_loss: 97.9907\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.2673 - val_loss: 91.8853\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.1089 - val_loss: 100.8081\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.4506 - val_loss: 104.5302\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.4521 - val_loss: 96.3765\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5327 - val_loss: 105.7063\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.1944 - val_loss: 97.8344\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.2699 - val_loss: 103.8767\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.3317 - val_loss: 110.5193\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.0699 - val_loss: 95.8000\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.1645 - val_loss: 93.8211\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.1766 - val_loss: 98.4337\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.0850 - val_loss: 102.2143\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.0078 - val_loss: 99.2001\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.0929 - val_loss: 110.2154\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.2262 - val_loss: 124.3824\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.0297 - val_loss: 95.2099\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.2543 - val_loss: 100.1888\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.4501 - val_loss: 92.7720\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.0467 - val_loss: 101.7935\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.0208 - val_loss: 101.4749\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.3555 - val_loss: 100.8615\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.3622 - val_loss: 102.1183\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.2780 - val_loss: 104.6290\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.1105 - val_loss: 104.2550\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.1012 - val_loss: 99.2269\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.4388 - val_loss: 100.5374\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 49.9512 - val_loss: 95.8577\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 49.9728 - val_loss: 95.5963\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.1746 - val_loss: 113.1692\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.0029 - val_loss: 100.5953\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.1008 - val_loss: 98.0772\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.2013 - val_loss: 103.6849\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.0542 - val_loss: 102.3267\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.1278 - val_loss: 95.0712\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.2287 - val_loss: 104.1496\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.0713 - val_loss: 103.2447\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 49.9079 - val_loss: 102.7976\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.1066 - val_loss: 111.5062\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "c7ed9503-199a-4f00-c055-f64af000016c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -4.093482315494199 \n",
            "MAE:  8.136088614800926 \n",
            "SD:  9.733941566565136\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "dbb2bec3-bf5e-45f9-d941-5a1d93b14f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "d6910fce-2780-4f13-8ad0-c8b9ae588436",
        "id": "su2Sj5jZCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_11 (Dense)            (None, 64)                8192      \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 64)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 32)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,745\n",
            "Trainable params: 12,361\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "outputId": "bce06bc6-fe9f-4cf4-8339-494219271e4a",
        "id": "kPRh6v-mCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 4s 10ms/step - loss: 12387.7168 - val_loss: 12425.0371\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 12087.4609 - val_loss: 11720.8203\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 11694.0264 - val_loss: 11101.0205\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 11131.7920 - val_loss: 10390.3057\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 10501.2852 - val_loss: 9269.8975\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 9803.1504 - val_loss: 10308.2568\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 9050.5557 - val_loss: 8035.0488\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 8262.5830 - val_loss: 7577.6816\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 7462.2183 - val_loss: 7151.0356\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 6647.0029 - val_loss: 5048.1675\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 5844.6899 - val_loss: 4875.4512\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 5073.8794 - val_loss: 6081.5938\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 4353.3208 - val_loss: 1758.8473\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 3687.4143 - val_loss: 4065.9658\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 3069.6006 - val_loss: 2564.0989\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 2495.6628 - val_loss: 2979.0239\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 2005.5085 - val_loss: 2232.0588\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 1583.0406 - val_loss: 1060.7223\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1227.2050 - val_loss: 766.6187\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 926.9426 - val_loss: 1185.5774\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 696.1142 - val_loss: 960.9585\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 514.4419 - val_loss: 159.0066\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 378.3176 - val_loss: 438.2570\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 275.9240 - val_loss: 258.7011\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 207.5125 - val_loss: 196.2787\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 164.4345 - val_loss: 567.1598\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 143.7084 - val_loss: 250.1859\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 122.9779 - val_loss: 124.1321\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 110.0435 - val_loss: 112.6710\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 105.0920 - val_loss: 122.0809\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 101.5678 - val_loss: 237.4200\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 99.2063 - val_loss: 113.2450\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 97.2041 - val_loss: 115.3576\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 96.4739 - val_loss: 108.4491\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 95.0710 - val_loss: 106.8282\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 94.1203 - val_loss: 106.8657\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 92.8816 - val_loss: 103.5852\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 92.6770 - val_loss: 108.0904\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 91.2616 - val_loss: 103.9559\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 90.3167 - val_loss: 109.2809\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 89.5635 - val_loss: 115.0814\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 88.7679 - val_loss: 121.7629\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 88.2917 - val_loss: 102.7139\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 87.8258 - val_loss: 104.8980\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 87.5802 - val_loss: 97.0941\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 87.4436 - val_loss: 100.9197\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 86.1252 - val_loss: 108.1367\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 85.4476 - val_loss: 144.6873\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 85.2169 - val_loss: 126.8183\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 84.5617 - val_loss: 104.2231\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 84.1608 - val_loss: 95.3745\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 84.0456 - val_loss: 119.8785\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 82.9317 - val_loss: 99.1953\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 82.2496 - val_loss: 103.6978\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 82.9343 - val_loss: 103.1251\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 82.1084 - val_loss: 108.5227\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 82.0739 - val_loss: 105.6455\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 80.9918 - val_loss: 170.2553\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 80.5268 - val_loss: 113.9219\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 80.0746 - val_loss: 105.1482\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 80.4154 - val_loss: 98.9707\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 81.3744 - val_loss: 161.5995\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 80.9737 - val_loss: 242.6696\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 79.7886 - val_loss: 115.1607\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 79.9480 - val_loss: 120.4857\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 79.1679 - val_loss: 96.2273\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 79.9014 - val_loss: 175.0978\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 78.7340 - val_loss: 138.7431\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 79.4482 - val_loss: 101.6814\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 77.5447 - val_loss: 134.5908\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 78.5531 - val_loss: 106.8696\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 76.7371 - val_loss: 109.6049\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 76.4740 - val_loss: 110.9934\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 76.4840 - val_loss: 97.8163\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 75.9173 - val_loss: 96.3274\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 77.2427 - val_loss: 109.3366\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 78.1634 - val_loss: 95.3969\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 75.9053 - val_loss: 215.4460\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 75.2417 - val_loss: 107.9285\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 75.4901 - val_loss: 99.5239\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 75.0346 - val_loss: 105.7152\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 75.7831 - val_loss: 110.0032\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 74.4741 - val_loss: 98.7670\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 73.7211 - val_loss: 134.0085\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 74.5857 - val_loss: 95.8108\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 74.2410 - val_loss: 102.7167\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 74.0082 - val_loss: 93.2829\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 73.3451 - val_loss: 113.2095\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 73.2128 - val_loss: 99.0835\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 73.6334 - val_loss: 106.0233\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 74.0984 - val_loss: 126.3557\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 73.8028 - val_loss: 98.4332\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 74.3715 - val_loss: 112.1921\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 73.2734 - val_loss: 112.3278\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 72.0892 - val_loss: 118.3627\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.8062 - val_loss: 109.3374\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.3328 - val_loss: 97.5269\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.6119 - val_loss: 122.7830\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.2052 - val_loss: 131.1308\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 71.4099 - val_loss: 143.8351\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.5891 - val_loss: 131.1413\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.6210 - val_loss: 111.3899\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.2700 - val_loss: 160.0621\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.3461 - val_loss: 111.6064\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.7226 - val_loss: 95.5960\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.2611 - val_loss: 111.9579\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.9540 - val_loss: 123.3286\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.0562 - val_loss: 207.9033\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.5316 - val_loss: 107.6227\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.1191 - val_loss: 168.7764\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.7535 - val_loss: 102.7494\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.8930 - val_loss: 109.3883\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.0291 - val_loss: 109.4381\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.9698 - val_loss: 155.0966\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.7687 - val_loss: 88.2421\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.5093 - val_loss: 115.5891\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.6185 - val_loss: 103.2615\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.4417 - val_loss: 96.8568\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.0318 - val_loss: 109.1335\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.0643 - val_loss: 147.2761\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.1245 - val_loss: 111.9555\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.8373 - val_loss: 93.2213\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 68.4149 - val_loss: 108.5701\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.8772 - val_loss: 97.2300\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 68.0615 - val_loss: 109.8132\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.3861 - val_loss: 109.2855\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.0238 - val_loss: 96.9167\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 68.5501 - val_loss: 124.2599\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.3549 - val_loss: 93.8306\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.3069 - val_loss: 119.1641\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.8716 - val_loss: 107.6113\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.4672 - val_loss: 91.2409\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.6439 - val_loss: 112.8790\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.6277 - val_loss: 106.3359\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.4069 - val_loss: 94.1321\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.2433 - val_loss: 157.0291\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.9881 - val_loss: 95.2450\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.0013 - val_loss: 105.9918\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.6248 - val_loss: 101.3610\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 67.1043 - val_loss: 133.7838\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.6343 - val_loss: 124.1901\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.3437 - val_loss: 113.2809\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.1623 - val_loss: 99.5404\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.5955 - val_loss: 109.0492\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 68.3210 - val_loss: 169.2717\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.3789 - val_loss: 108.1179\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.8530 - val_loss: 89.5863\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 66.1078 - val_loss: 103.4967\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 66.3282 - val_loss: 133.7477\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 68.9109 - val_loss: 126.1809\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.8033 - val_loss: 87.7172\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.4837 - val_loss: 98.2180\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 66.6350 - val_loss: 163.4750\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.7012 - val_loss: 90.1963\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.6558 - val_loss: 104.4784\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.4289 - val_loss: 126.2369\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.1185 - val_loss: 112.3223\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 65.5924 - val_loss: 95.9952\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.1200 - val_loss: 109.6620\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.2394 - val_loss: 147.3815\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 68.3429 - val_loss: 94.7742\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 66.4836 - val_loss: 105.2978\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 65.2146 - val_loss: 168.5761\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 65.2691 - val_loss: 94.7602\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 64.6166 - val_loss: 94.3679\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.3133 - val_loss: 93.4056\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.1538 - val_loss: 96.0365\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.3641 - val_loss: 186.5092\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 64.8003 - val_loss: 110.3251\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.7696 - val_loss: 97.5244\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.9107 - val_loss: 109.4194\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.9976 - val_loss: 95.0527\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.8296 - val_loss: 86.1578\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.2758 - val_loss: 120.7974\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.6207 - val_loss: 95.2504\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.6037 - val_loss: 94.7363\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 64.0362 - val_loss: 185.7201\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 64.0462 - val_loss: 100.9796\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.0662 - val_loss: 121.2411\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.3498 - val_loss: 101.3179\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.3334 - val_loss: 96.8851\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.9545 - val_loss: 109.9423\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.1843 - val_loss: 100.9324\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.7979 - val_loss: 164.2461\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.2096 - val_loss: 113.5693\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.7303 - val_loss: 113.6186\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.6945 - val_loss: 94.4850\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.0640 - val_loss: 91.1010\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.0630 - val_loss: 127.5611\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.6846 - val_loss: 357.8636\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.5385 - val_loss: 91.0335\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.7457 - val_loss: 96.3100\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.4650 - val_loss: 128.9943\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.0576 - val_loss: 122.1046\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.5161 - val_loss: 94.6992\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 62.9460 - val_loss: 94.5037\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.2697 - val_loss: 98.4445\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.7220 - val_loss: 93.2733\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.2348 - val_loss: 87.5782\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.0141 - val_loss: 96.1821\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.4957 - val_loss: 96.1553\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.0002 - val_loss: 97.0572\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.9534 - val_loss: 91.7452\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.0016 - val_loss: 104.6809\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.3719 - val_loss: 134.1811\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.9605 - val_loss: 95.0041\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.4627 - val_loss: 192.1786\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 61.9032 - val_loss: 89.4384\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.8403 - val_loss: 95.3473\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.1033 - val_loss: 100.7508\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 61.5931 - val_loss: 103.4369\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 61.6732 - val_loss: 104.0278\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.2400 - val_loss: 101.5985\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.1531 - val_loss: 120.0196\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.0227 - val_loss: 122.6241\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.0794 - val_loss: 132.1601\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.3323 - val_loss: 96.5252\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.7762 - val_loss: 101.6508\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.5840 - val_loss: 100.9601\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.8168 - val_loss: 116.7467\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.6454 - val_loss: 90.3381\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 60.9327 - val_loss: 115.4135\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 60.6703 - val_loss: 100.6403\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.7344 - val_loss: 119.2233\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.5043 - val_loss: 117.2497\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 60.7127 - val_loss: 111.0686\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.7264 - val_loss: 95.1608\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.8336 - val_loss: 109.9088\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 60.4850 - val_loss: 100.1781\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 61.5229 - val_loss: 103.9404\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.3426 - val_loss: 102.6228\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.5720 - val_loss: 132.3065\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.6626 - val_loss: 108.4331\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.9556 - val_loss: 141.0674\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.8338 - val_loss: 105.7692\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 60.7430 - val_loss: 98.2575\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 61.1440 - val_loss: 122.2029\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 60.8949 - val_loss: 145.2424\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.1977 - val_loss: 107.0988\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 60.1573 - val_loss: 106.9474\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 60.5384 - val_loss: 111.8273\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 60.3755 - val_loss: 93.0550\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.5971 - val_loss: 92.1100\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.7615 - val_loss: 545.3071\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.7140 - val_loss: 92.4846\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.8852 - val_loss: 96.0527\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.3800 - val_loss: 98.2018\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.0880 - val_loss: 92.2811\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.0270 - val_loss: 131.4336\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.5481 - val_loss: 106.3790\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.8556 - val_loss: 99.8486\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.9766 - val_loss: 91.6373\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.7385 - val_loss: 104.2157\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.4603 - val_loss: 101.5090\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.9667 - val_loss: 96.9833\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.0527 - val_loss: 103.3989\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.3645 - val_loss: 92.3097\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.5730 - val_loss: 108.4297\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.2831 - val_loss: 96.2834\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.5921 - val_loss: 263.9204\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.4549 - val_loss: 97.9144\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.6639 - val_loss: 130.1244\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 60.0424 - val_loss: 116.2575\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.2825 - val_loss: 103.8422\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.0310 - val_loss: 104.2488\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.7398 - val_loss: 101.1629\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 59.8043 - val_loss: 88.7867\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 60.4703 - val_loss: 112.0800\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.0396 - val_loss: 117.7624\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.9720 - val_loss: 106.0618\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.0708 - val_loss: 102.6716\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.9682 - val_loss: 108.9668\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 58.8422 - val_loss: 118.0786\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 58.5154 - val_loss: 110.3648\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 58.9520 - val_loss: 103.8132\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.3305 - val_loss: 89.9418\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.2519 - val_loss: 98.7129\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.5237 - val_loss: 101.3870\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.7944 - val_loss: 111.5884\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.8131 - val_loss: 96.8098\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.8400 - val_loss: 107.8858\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.7768 - val_loss: 96.2130\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.3684 - val_loss: 104.8065\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.3206 - val_loss: 136.4637\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 61.4665 - val_loss: 93.0010\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.8339 - val_loss: 117.3263\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.4224 - val_loss: 99.7188\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.7190 - val_loss: 152.2610\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.2964 - val_loss: 95.0931\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.0412 - val_loss: 133.2597\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.8581 - val_loss: 89.9845\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.3824 - val_loss: 123.0757\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.7731 - val_loss: 93.9904\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.0614 - val_loss: 93.6487\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.1271 - val_loss: 98.9458\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.7134 - val_loss: 95.8842\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.4814 - val_loss: 115.1744\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.9906 - val_loss: 105.4932\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.7372 - val_loss: 115.1821\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.5321 - val_loss: 110.0636\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.3907 - val_loss: 93.5504\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.8642 - val_loss: 95.7955\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.0799 - val_loss: 101.2494\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.0304 - val_loss: 145.4819\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.3581 - val_loss: 101.6686\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.8341 - val_loss: 98.6455\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 57.4641 - val_loss: 95.2495\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 57.4289 - val_loss: 100.7454\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.3875 - val_loss: 97.4353\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.4732 - val_loss: 106.3425\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.1122 - val_loss: 103.9147\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.5374 - val_loss: 89.9982\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.2424 - val_loss: 95.0102\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.3272 - val_loss: 145.8553\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.3690 - val_loss: 95.4062\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.3274 - val_loss: 114.2499\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.6536 - val_loss: 147.1722\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.8712 - val_loss: 93.6059\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.1668 - val_loss: 178.9684\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.6588 - val_loss: 98.4323\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.1619 - val_loss: 98.1223\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.2272 - val_loss: 151.1836\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.2385 - val_loss: 109.6460\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.5098 - val_loss: 105.5480\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.4166 - val_loss: 98.1820\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.8201 - val_loss: 115.8444\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.3061 - val_loss: 97.5107\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.2899 - val_loss: 107.4974\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.6888 - val_loss: 96.9376\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.3015 - val_loss: 108.1564\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.2419 - val_loss: 116.4604\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.0447 - val_loss: 97.7620\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.8932 - val_loss: 179.8935\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.0690 - val_loss: 109.0758\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.9622 - val_loss: 92.0016\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.9871 - val_loss: 97.6593\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.5702 - val_loss: 88.6720\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.8714 - val_loss: 126.3331\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.4287 - val_loss: 92.4839\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.6322 - val_loss: 126.4593\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.6264 - val_loss: 138.8719\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.5332 - val_loss: 107.9733\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.0045 - val_loss: 104.4975\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.8347 - val_loss: 93.2297\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.4790 - val_loss: 149.1105\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.8390 - val_loss: 109.4445\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.9145 - val_loss: 99.3319\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.3430 - val_loss: 146.2634\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.3070 - val_loss: 94.8728\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.8855 - val_loss: 286.2585\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.3290 - val_loss: 99.1184\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.1812 - val_loss: 91.4627\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.7166 - val_loss: 124.4749\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.6859 - val_loss: 118.7240\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.2768 - val_loss: 90.4294\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.4880 - val_loss: 103.5232\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.8120 - val_loss: 110.7883\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.1246 - val_loss: 106.0581\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 57.1950 - val_loss: 113.2484\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.9678 - val_loss: 106.2329\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.2431 - val_loss: 106.6303\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.7631 - val_loss: 94.4567\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.3863 - val_loss: 109.3793\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.5433 - val_loss: 92.4428\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.0760 - val_loss: 94.5845\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.5151 - val_loss: 102.1909\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.6914 - val_loss: 101.2652\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 57.0960 - val_loss: 126.1643\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.3656 - val_loss: 119.3807\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.0825 - val_loss: 111.0769\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 56.7076 - val_loss: 96.0699\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.4880 - val_loss: 102.6374\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.2394 - val_loss: 109.6413\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.6290 - val_loss: 97.8046\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 56.3420 - val_loss: 99.1717\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 57.0773 - val_loss: 119.0297\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.1891 - val_loss: 100.4090\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.2059 - val_loss: 112.9336\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.1172 - val_loss: 95.9486\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 56.1400 - val_loss: 125.7313\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.6610 - val_loss: 90.9333\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 56.3191 - val_loss: 110.3694\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 56.4522 - val_loss: 99.0048\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.9683 - val_loss: 95.2148\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 56.6034 - val_loss: 101.4228\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.9309 - val_loss: 96.9884\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 56.0486 - val_loss: 103.7222\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.6262 - val_loss: 107.4675\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 56.0339 - val_loss: 113.2947\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.7767 - val_loss: 113.1573\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.5861 - val_loss: 93.4771\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.8911 - val_loss: 112.3477\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.6554 - val_loss: 91.7286\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 56.4089 - val_loss: 101.9337\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.1883 - val_loss: 101.6720\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.7258 - val_loss: 99.7339\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.3528 - val_loss: 102.4349\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.7152 - val_loss: 95.8823\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.2649 - val_loss: 100.9619\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 55.3561 - val_loss: 100.1464\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 55.2461 - val_loss: 111.7470\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.7367 - val_loss: 120.7484\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.3128 - val_loss: 118.9393\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.3899 - val_loss: 119.3515\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.3793 - val_loss: 99.3981\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.8128 - val_loss: 122.6998\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 56.0625 - val_loss: 96.8491\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.4579 - val_loss: 106.8238\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.8696 - val_loss: 147.5029\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.6828 - val_loss: 96.6237\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.4655 - val_loss: 97.6210\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.8095 - val_loss: 97.7935\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.5532 - val_loss: 123.0577\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.5640 - val_loss: 115.1400\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.7132 - val_loss: 106.7468\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.5902 - val_loss: 137.9309\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.7289 - val_loss: 100.2651\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.3569 - val_loss: 119.4655\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.2531 - val_loss: 118.8464\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.6588 - val_loss: 112.6671\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.4926 - val_loss: 100.8925\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.1681 - val_loss: 89.6477\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.4940 - val_loss: 99.0594\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.2476 - val_loss: 104.3360\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.5132 - val_loss: 112.3736\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.7157 - val_loss: 98.3349\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.6247 - val_loss: 109.2260\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.3616 - val_loss: 137.9143\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.5931 - val_loss: 101.3622\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.2026 - val_loss: 145.7872\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.1785 - val_loss: 106.9317\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.9155 - val_loss: 116.8472\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.9271 - val_loss: 97.4143\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.1382 - val_loss: 115.3789\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.6224 - val_loss: 106.3928\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.4374 - val_loss: 104.6933\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.3173 - val_loss: 108.9201\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.4704 - val_loss: 103.4593\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.3347 - val_loss: 100.0624\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.2044 - val_loss: 88.8264\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.5116 - val_loss: 99.1064\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.3258 - val_loss: 104.8368\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.3271 - val_loss: 99.0454\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.4621 - val_loss: 99.9281\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.3146 - val_loss: 95.4293\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.8642 - val_loss: 162.3936\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.4094 - val_loss: 109.3635\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.7539 - val_loss: 96.4541\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.0902 - val_loss: 90.2137\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.5917 - val_loss: 94.9826\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.9400 - val_loss: 95.0334\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.6697 - val_loss: 117.4152\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.3960 - val_loss: 97.5111\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.6645 - val_loss: 101.5731\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.3059 - val_loss: 100.2781\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.9572 - val_loss: 99.9874\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.0648 - val_loss: 125.6103\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.3129 - val_loss: 93.6491\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.3191 - val_loss: 97.7271\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.7834 - val_loss: 102.3818\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.5801 - val_loss: 97.4516\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.7517 - val_loss: 113.4051\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.7454 - val_loss: 107.4510\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.2845 - val_loss: 103.3964\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.0516 - val_loss: 122.8412\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.0025 - val_loss: 97.4584\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.0956 - val_loss: 96.3082\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.1415 - val_loss: 101.1762\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.1381 - val_loss: 138.0419\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 55.0175 - val_loss: 93.3581\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.6099 - val_loss: 116.7493\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.5566 - val_loss: 110.3914\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.6377 - val_loss: 112.8810\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.6791 - val_loss: 103.2097\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.6976 - val_loss: 100.6230\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.6764 - val_loss: 178.3155\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.3198 - val_loss: 103.7723\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.5623 - val_loss: 95.8442\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.8589 - val_loss: 95.7631\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.0102 - val_loss: 115.6727\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.3024 - val_loss: 117.8640\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.7474 - val_loss: 99.4764\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.3182 - val_loss: 90.4404\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.0962 - val_loss: 103.6317\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.6961 - val_loss: 103.1878\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.6495 - val_loss: 104.2931\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.1532 - val_loss: 93.8049\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.6045 - val_loss: 97.3437\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.8532 - val_loss: 100.3433\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.1297 - val_loss: 96.2734\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.2559 - val_loss: 101.2003\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.9961 - val_loss: 94.4036\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.0351 - val_loss: 92.0357\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.8188 - val_loss: 97.8693\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.8460 - val_loss: 92.9008\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.8981 - val_loss: 103.5138\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.2114 - val_loss: 111.3097\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.5384 - val_loss: 101.7344\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.8162 - val_loss: 105.9145\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.0665 - val_loss: 110.3854\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d699e46e-29ea-4b2f-c530-dd77083fead1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -2.134633858017672 \n",
            "MAE:  7.850791065257864 \n",
            "SD:  10.287310522961862\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "63f166e9-3e55-4254-b150-e8c0a37a6afe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAFBCAYAAAAsfIegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2debwUxbXHf+feC/eioCBBdtlEUSSigAEV4xLFLe4KSlxwexrjnij6NKhPjdFEjXmKKwqKuxiJK4QQeLiyCMommyyX7QKyw4W71PvjdDE1Pd09PTPds/X5fj7zmemaXqq6u359+tSpKlJKQRAEQQiOklxnQBAEodgQYRUEQQgYEVZBEISAEWEVBEEIGBFWQRCEgBFhFQRBCJjQhJWIKojoGyKaRURziOh+K70TEX1NRIuI6C0iamill1vLi6z/O4aVN0EQhDAJ02LdBeBEpdThAHoCOJWI+gL4M4AnlFIHAtgI4Cpr/asAbLTSn7DWEwRBKDhCE1bFbLMWG1gfBeBEAO9a6SMBnGP9PttahvX/SUREYeVPEAQhLEL1sRJRKRHNBFAFYDyAxQA2KaVqrVUqAbS1frcFsAIArP83A2geZv4EQRDCoCzMnSul6gD0JKKmAN4H0C3TfRLRtQCuBYC99967V7duPnc5fTpmozv2wg50xo9Ax45Ac9HtUNm2DfjhB6BRI+DQQ7N33K1bgQUL+HevXtk7rlA0TJ8+fb1SqkW624cqrBql1CYimgigH4CmRFRmWaXtAKy0VlsJoD2ASiIqA7AvgA0O+3oewPMA0Lt3bzVt2jR/mSDCIXgHP8d3eAuDgPvuAy6/PMOSCZ5MmQL07w8ccgjg9zoFwYQJwK9+xb+zeVyhaCCiZZlsH2ZUQAvLUgURNQJwMoB5ACYCuMBa7XIAH1i/x1rLsP7/twp4hBiCQr0usgw+Ez65OsdybYUcE6bF2hrASCIqBQv420qpD4loLoA3iehBAN8CeMla/yUArxLRIgA/ARgUdIZKUA8FaQ8TBCFcQhNWpdR3AI5wSF8C4CiH9GoAF4aVH0As1qwj51iIKJHqeSUWa5bRwprtqDkRdCHHREpYxWLNEXKuhYgRKWEVizXLiKAKESVywioWaxYRV4AQUSIlrHGuAEEQhJCIlMrEuQLEqgkfOcdCRImUsIrFmmWkg0B2WLUKWL8+17kQDLLSpTVfEItVKEraWuMYyT2dN0TKfBOLNctIRRciSqRURizWLCOuACGiREpYxWIVBCEbREplxGLNMnKOhYgSOWEVizWLiCtAiCiRUhkZKyDL5KrnlSDkmEgJq4wVEBHkoSnkmEgJq1isWUbOsRBRIiWs4mPNMiKsQkSJlMpIVEBEkGsr5JjICWsdSnOdjeggAidElEgJaynqxMeaTeQcCxElUsIqPtaIIIIu5JhIqYzMIJBl5BwLESVywio+1iwiHQSEiBIpYRUfa47I9rmWayvkmEgJq/hYs4wInBBRIqUyca4AqfThI64AIaJESljjXAFC8SIPTSHHREplJCogy+TzOa6pYUv6oYdynROhCImcsEpUQBbJZ2HdsYO/H300t/kQipJICatEBUQEubZCjomUsEpUQJYRgRMiSqRURqICsoycYyGiREpYJSogy+Qq3CoVQRfxF0IgUiojUQE5Qs61EDGiK6xC+MgsrUJEiZTKSLhVlimEnlf5nDehYAlNWImoPRFNJKK5RDSHiG620u8jopVENNP6nG5scxcRLSKiH4hoQNB5knArYQ9y/YUQKQtx37UAbldKzSCiJgCmE9F4678nlFJ/MVcmokMBDALQHUAbAP8iooOUUnVBZUhcAVkmn10BIqxCiISmMkqp1UqpGdbvrQDmAWjrscnZAN5USu1SSv0IYBGAo4LMk4RbZZl8Psf5nDeh4MmK+UZEHQEcAeBrK+l3RPQdEY0gomZWWlsAK4zNKuEtxCkj4VbCHkRYhRAJXWWIqDGA9wDcopTaAmA4gC4AegJYDeCvKe7vWiKaRkTT1q1bl1Je2BVgWaz19SltK6SBuAKEiBKqsBJRA7CojlZKjQEApdRapVSdUqoewAuIve6vBNDe2LydlRaHUup5pVRvpVTvFi1apJSfErCY1oOkYmWDfD7H+Zw3oeAJMyqAALwEYJ5S6nEjvbWx2rkAZlu/xwIYRETlRNQJQFcA3wSZp1JwO1g9SqRiRR25/kKIhBkVcAyASwF8T0QzrbS7AVxMRD0BKABLAfwXACil5hDR2wDmgiMKbggyIgAwLVYR1qwgrgAhooQmrEqpKQCcoq8/9tjmIQChjTyshbUOpVKxskE+dxCQ6y+ESKSayONcAdJ4lT3yUcTyMU9C0RApYQ3NFXD11cCHHwa3v2JBXAFCRBFhDYKXXgJ+/evg9lcsiCtAiCiRFFbxsQpy/YUwiZSwio81y4grQIgokRJWCbfKMoVwjgshj0LBEUlh3eMKaNsWOPHEHOeqiMln0crnvAkFT5gdBPKOhJ5Xq1bxRyguxBUg5JhIWqyB+lilgrqTz+cmn/MmFDyRFNZAowKkEcydfJ6lVYRVCJFICWsog7CIsCYnH0Usn2NshYInUsIaSlSACKs7+SiomnzOm1DwRFJY61AanCCKsLojrgAhokRKWMUVIOxBhFUIkUgJq7gCskw+i1c+500oeERYM0WE1R3p0ipElEgKq/hYBRFWIUwiJaziY80y+Sxe+Zw3oeCJlLCKKyDLiCtAiCiRFFbpeSWIsAphEilhDWU8VhFWd/JZvPI5b0LBEylhFVdAlhFXQPgUSzmKjEgKq7gChD2IMAkhEClhlaiALJPPopXPeUuFYilHkREpYZXxWLOMjBUQPsVSjiIjksKadVdAXR2wa1cwxysk8rnS53PeUqFYylFkREpYc+YK+PWvgYqKYI4nBEOxCFKxlKPIiJSw5iwq4JNPgjlWoSFRAeFTLOUoMqIrrBLHGj75XOnzOW9CwRNJYZVwqyJHLFYhx0RKWCXcKsvkc6Uvljmv8vkcR5hICav0vMoy+SxexSJIxVKOIiOSwlqQ47GuWAFs2JCdYwVNtiu/uAKEHFOW6wxkE+0KKEgf6wEHcMjWzp3ZOV4Q5HOlz+e8pUKxlKPIiJTFWvA+1urq7B0rCMQVIESUSAprLcoKU1gFf4grQMgxkRLWMtQCsIS10HyshUg+V/p8zlsqFEs5iozQhJWI2hPRRCKaS0RziOhmK30/IhpPRAut72ZWOhHRU0S0iIi+I6Ijg86TFtZQfKz5+Lqba/K50udz3lKhWMpRZIRpsdYCuF0pdSiAvgBuIKJDAQwFMEEp1RXABGsZAE4D0NX6XAtgeNAZCtUVIMKaP4grQMgxoQmrUmq1UmqG9XsrgHkA2gI4G8BIa7WRAM6xfp8NYJRivgLQlIhaB5knsVizTCFU+kLIo1BwZMXHSkQdARwB4GsALZVSq62/1gBoaf1uC2CFsVmllRYY4mPNMvksWvmct1QolnIUGaELKxE1BvAegFuUUlvM/5RSCkBKdwYRXUtE04ho2rp161LKi7gCsowMdB0+xVKOIiNUYSWiBmBRHa2UGmMlr9Wv+NZ3lZW+EkB7Y/N2VlocSqnnlVK9lVK9W7RokVJ+QnUF+CGqlSAfy52PeUqHYilHkRFmVAABeAnAPKXU48ZfYwFcbv2+HMAHRvplVnRAXwCbDZdBIMS5AurqgtlpKlZZ1NwG+Vzp8zlvqVAs5SgywuzSegyASwF8T0QzrbS7ATwC4G0iugrAMgAXWf99DOB0AIsA7AAwJOgM7XEFNG4WnLCKxeqOuALCp1jKUWSEJqxKqSkA3GrUSQ7rKwA3hJUfACiBAhFQ13jf4BuvxGItLESQhBCJVM8rACgrA2pRGrzF6kdYo1aZ87m8+Zy3VCiWchQZkRPW0lKgFg1EWLOB3/Jeckmw7gJxBQg5JnLCWlYG1OVqzitxBTjzxhvZP2axCFKxlKPIiJywlpYCtSrAqACxWN2RWVrDp1jKUWRETljZxxpCzytpvEoknyt9PuctFYqlHEVGJIW1ThqvhHwehFsoeCInrOwKyJGwFrrFOmkS0LgxsHFjLE0pYM0a5/XFFRA+xVKOIiNywrrHYs1F41WhV4IHHwS2bwemTYulPf000Lo1MGdO4vq6vPlY7nzMUzoUSzmKjEgKa87iWNMV83yrPGZ+xo/n70WL/K2fL+RjntKhWMpRZEROWAsyKiBfKk+q/khxBYRPsZSjyIicsIbmCgjTYi0E36xTBc9nV4Amn/MmFCyRFNZQGq/8UOgWqxOF2qqez+c0FcxyFEuZioDICSt3aRWLNSP8VuBcWazF4gqYNYstgRUr3NcRYc1LIiesZWVAXa7CrdK98fNFWNP1seZjhc+HPFVV8Tl99lnn/599lu/TDz9034cIqzurVgFjx+bk0JET1j0Way5cAcVisfolnyt6PuTtxx/5++WX09+HCKs7xx0HnH12Ts5L5IR1j481KLFK5aIVusWqSbUc4grIDK+8FlI5ss3ixfwtwho+oboCJk+OxXU6UejCql0BqfpY85F8zpumUMLb8p0c1J8wp2bJS0pLgV1hdRB46CFgyxbg5JO91033GLlGfKy5wa/FWkhlyiY5qD8RtVhDGI+ViMW6psZ93UK3WL3Ip9fVQnMFuOUl1QbRfCpTPhGUEZUCkRTW0OJY6+u9hbUYu7R6Vf588MW67TMfzqnfNwCxWDNDXAHhE9joVtu387dpsdbXA7W17tsUs8XqRKqugPp6vkDZoFhESIQ1OeIKCJ89jVeZnux99uEh9FIR1nzzsc6alVplDLuXVVDCUGiuADfEFRAMIqzhE5grQF8su7CG7WO9/fb09mFnwgSgZ09g+HDv9WprgTvuANatS/0Y6VisQZPPrgAhO4iwhs8eV0AY47Fmw2J9/PH09mFHD/M3a5b3ep9+Cjz2GHDDDbG0sMKtxGJNHbFYkyPCGj57Zmk1LdZMbshi97Hq8uzalf4+cmmxupFPIpQsL9J4lRkirOHj2HiVyYlPxRWQbz7WVEk1AiBVV0AuogJyOTpXsmOLjzUYRFjDp6wMqK23uQLy3WLNhwpjVnKvcKs5c4Df/Y7PRar5zmY343w4p0EgwpociWMNn4YNgd31tiizIIRV/y6kqAA7RxwBdOsWn+ZliTpx+uk8D9by5f7W97vfsMilGAV9bBFWZ8RiDZ+KCqC6vkF8YhDCqlRh+Vid8jJzJvDDD87rE8WsU6/8mOMJ5MpiNcnnqIAgHjj5UI4wmDqV76XPP898XyKs4VNRAeyyC2sQPlYtrMXmY3WquF6vVk4ug2xbrIXiCkh2XaPsYx03jr8//jjzfYmwhk95OVBTX4Z6uPgMU0WLjBbWurrgrKRbbuFY0zBujFQbbcz1nfJjL1s6FT5qUQF+yxvlqIAgypSvXVqJaG8AO5VS9UR0EIBuAD5RSnmYZ/lJRQV/70I5GqGaF4J0BQAsrmUOpzbVC/y3v6W3XZA4NVR5CatbI5cfouYKEIvVnVSHqPQijy3WyQAqiKgtgHEALgXwSliZCoXPPwfmzkV5OS9WoyL2XyYnXlus9fWx/bi5A3LpY129OvN9eAmrPc30sYorILM8RNFiDTIMLo+FlZRSOwCcB+AZpdSFALqHl60QOPpo4JBD9lisccIatMXq1oCVK2H9z3+ANm2Ad97JPC/2/NjFNt8ar9zIBxEKurz5UKagKXKLlYioH4DBAD6y0rI0DFGwmK6APQTpYwXchTWoYQOrqmLOfT98+y1/f/FFescHkvtY7Wnp9GzLRQeBXCKuAHcK3BXgd9jAWwDcBeB9pdQcIuoMYGJ42QoPR1dA0BZr2K6AE04A5s5l8SoJuf3RKc9ewqorhNmIl+3Gq0JxBQRR3nwoRxgE6QrI1w4CSqlJSqmzlFJ/JqISAOuVUjd5bUNEI4ioiohmG2n3EdFKIpppfU43/ruLiBYR0Q9ENCDtEiXB0RUQtI81aIvVvt3cuZntLx2SxbHahbW2NneDsOTbsdwIWljzoUxBU6AWqy9hJaLXiWgfKzpgNoC5RPSHJJu9AuBUh/QnlFI9rc/H1v4PBTAI7Lc9FcAzRBSKq0FbrIG5AnLpY83khslk9Ho/UQHpuAIKKSpgwgSgY0dgx4709yHhVu4UuCvA73vkoUqpLQDOAfAJgE7gyABXlFKTAfzkc/9nA3hTKbVLKfUjgEUAjvK5bUoE3niVDR9rGMKaapnT8bHmymLNhivg978Hli1z76nmB33Opk0DWrVKHEEsyj7WIMljYW1ARA3AwjrWil9N9yr+joi+s1wFzay0tgBWGOtUWmmBE3jjVS58rJps+I4K0cdqEpbFGoSImftYuxZYuTK4/RU6EbFYnwOwFMDeACYTUQcAW9I43nAAXQD0BLAawF9T3QERXUtE04ho2ro0RrXPShxrvlqsmd6kTj5WN79rLqMCstl4lUkji1PsrxNO6VdeyQ2XxWqxRiGOVSn1lFKqrVLqdMUsA3BCqgdTSq1VStUppeoBvIDY6/5KAO2NVdtZaU77eF4p1Vsp1btFixapZqG44lh1+qRJ3mMUeOF1A3/9NTB/vr/8eFmsfolaHOvs2fHLbnlyejN5+eX0YoXDYswYHsQnaIrZYiWifYnocW0pEtFfwdZrShBRa2PxXHBDGACMBTCIiMqJqBOArgC+SXX/fshKHKubyKV7gd32V1cHfPMNcPzxwD33pLdvr7L37QsMG8a/M/GxShxrIh9/DNx9t/c6fkYTmzQp9juXZTr/fB52Migi4goYAWArgIuszxYAL3ttQERvAPgSwMFEVElEVwF4lIi+J6LvwBbvrQCglJoD4G0AcwF8CuAGpVQoDsSsxLEGbbF6CfXatfx7zpzU9pmJf9ZpWyeLNVUyqQDV1cAzz/gfYDsbPtZvv+XJH+3rEgFnnOF/n17n5c47U8tTPrBsGZ+DTz5xX6fA41j9dhDoopQ631i+n4g87X6l1MUOyS95rP8QgId85idtHC1Wrxt33DhgwABgyRKgU6fE/7WIBuljVSo22R/gLazp3oB2IUxGsjhWp3CrbDZe/fGPPOlhGu6hjPA6f8cey+FY99/PU6Unw15+PxarSaEI61df8ffLLwOnnea9bpFbrDuJ6Fi9QETHANgZTpbCJWWLddQo/p4yxfn/3btj+wgqKuCFF4CDDko8hp1MJkRM5ymebgeBbLgCtOW+fbu/feajCLldk2ITVj8P8yBcAak+mALEr8V6HYBRRLSvtbwRwOXhZClcUm68amANiu0mljo9yDhWe5/+oH226WybyVgBYeXJaVt7S7kbQYmQnzz7PRdu943f7QtFWDVe+S3wqABfwqqUmgXgcCLax1reQkS3APguzMyFQVkZUEL12KV8Nl7pcVXdbvpUhDVoH2vQflK/+BndKluNV1OmAJ9+GitPqc8Oe0H5WMMU1lSOUUikIppF7goAwIJq9cACgNtCyE/oEAHlpbX+41i1sJriZl7sbAirlwWc7pM9LFeAuf9shFv17w889JC7sIbtCvBzHr3mQfPalz4fxeYK0PixWAtUWP26ApzI4YTsmVFRWovq2hRdAWblMC9UKj7WoMOtzP2l+loVtBvBKyogGz7WXLkCghRW+3pOwvrEE0C7ds7b50pYM+kenQ3y3WK1UWCPxxjlZbX+41iTCasperqSBW2xejVeZcti9etjzVUHAX3O/Q6jmE1hTdcVoM+Huf1ttwEXXeS8fa6E1al83bsDI0Ykps+fD1x4If/2yq8ZxuiX117j+2/zZud9ZRHPu5CIthLRFofPVgBtspTHwKmwuwL8+FidBNSeHlaXVi+LNZ0b0MxLOpXRK9xKk+0OAnoAE7uwBu0KmD4d+OCD1PyfQVqsXuRKWJ18w3PnAlddlbju//6vv32m46p69FH+XrYsPj3fXAFKqSbZykg2qSir8R/H6hQV4GaxeqUB/m98+yhHXsKabldWs8eYE05B7eZxnfJirpftOFZ9zsLuINC7N38fdhh/p2qxepWxWITV65z4HdsgnXshlS7BIRO56a8BoLysLnWL1bx5zAvl9Jp+xRXAhx8mputKP2WK9zG3bYtf9ooK8GsNabZv5+10GerqeP9+wqW8KrpTHKsfZsyI/Q7CYvV73Fz5WL3WT9Z4lexY+SKsbq4rIPWHbCplcjMG8s0VUKyk5ArQF8fNYnW7ia6+OjFNKZ7Qr39/7nXihl1Y3Y5RX5+6sDZuzHkzK2vDhjxakol9v0SpCatfi7VXr8R9pIObsIYVFaC3v/nmxDcMO36FNZmP1X5f5Av2Mnm9Rfm9xplYmfZ2BxHW7JDQeOV14vUFNsXNzcdq4lRx6+tjXVUXLnQ/5tat8ctePls/wmr3B77ySvxwhwAwcmTyY/oRVk2mA11/8QXw9797r/+S0UO6upq//U4JE5R1N3eu90MSiL9f0hFW/Z1MWPPFYvUS1lRdAelYrCKsuaEiFVeAvkl2Gj14k/lY3fZppnm15qfjCvAqg/7Pyep22rebYDs12Ngrf1BTsxxzDHCT57Rq8W8Fbhbr8uWcp08/jU8PUoSSWVfpWqz2821/4NqJurC67VeENTtUlKXgCtA3jSmsmVisfm6UTZvil70ar/xYrDq/TpENTq+xbvt18vW5WbHZjgrQbxT2c/X11/xtWrfmsbZu5bFEMyFZyFtQFmuUhNX+RuUHvT83X3UWiaSwlpfV+Y9jdRJWPz7WdC1WpYANG+LTvOJY/VisTtZpMmF1s2TNb/O49tcwU1grK4Gzz/Zu1LDvN1VMi9U8Fzr8yr5vc52PPkr9eH7fPnSeNJk0XhWbsHrht8HOad8irLkhwWL1OvFBugL8WKxbt/q/Uf2GWzlZrLoM2jdp36/Ta6mTsHpZrCZjxyYfYT6MxivdxdVLWMNuFMpW41WhCasfYyAdYXU7j1kkmsLaIEOL1bzYqbTKOx2npibeYrJbq3odJ/y6ArwsVidh/f77xEpsPhT8+ljtN7RT2UyCCrdysibtFTSbwhqEK+Cnn4Dx4zPLx08/ARs3ZrYPJ1IR1lSjAoKwWCWONTukFMeqbxrzldnPzeHkX3Tabtgw4Mwzgf/8h5e1+OyzT2ydTONYvXysTsJ61FHAkCHxaWYvL7/Cas/bT0lmQ3c6P34rhZvF6uYKMMlUWINyBXgJ68knJ++1lOzB1Lw5sN9+iek1NcAvfgFMnOi9vRvZtFjr6oDrrosfCN6+P7FYc0NKjVf6JjGF1U9ld2rVdLKkFizgby2o+rtVq9i6fuJYzX1/+238XEg6L04hY07CCsQafcx9ePlYnToI2CtYMmF1sohTHRkqW66AVKwgM09e5fESVrMjhRu//S1HQaTKsmU8d5pT7LUfsuljnT4deO454JJL3LcVH2tuKG+osAvlsVFk/HQzTNVi1diF1S0URFtWTsKaahzrkUfyBIMap4aqLdboj27C6nQsPxar+Tpmz1syV4DehzmIRqpddt3iWIN2BZhlGzPG+eHn5IbIxMeajMmTgcGD/a1r4hb/6Xdbs+xLliQ+lJ2OlQw3V4D9AQ7w9Vu/XizWXNNo7xLUoxQ1sMYBsF/sTZtiU304CaufG92vK8AurNoHtv/+sXXs042Y+dD588qTPoZZBm09+hVWs5XfS1jNCmG/wauqvI+h979+fSwt1Z5ldiF2C9sJUljHj+c5t+w4VfRMogL8sNJx1vh47OcoE2H9zW+Ao4+OLXfpwr3R3PaXqSvAKa+HHcZznUlUQG5pvC+/Hm6DNcGb/QK3bx+zGJ1cAan4WO2vzfYbQ/+vX1m10Jk+VrcGB9Ni9RIgp9d+LazJumOax7JX9K+/jsXc/v3v/DAyK4S9AidrONHbTp0aS0vHYjXR24ftCli61N+6mcSx+iHZw8tpnXQaDZcs4TC61193XyeZsHqRirDq0azEYs0tjZvywCquwmpWNNNifeEFtqaCtFj1vvSNol+r9torto6XsGrh8BIgJ1dAqharXVh37wb69gWmTYut89hj3hZrMpHU58qcuDFVi9XuCjBn0XU6FsDXO1Vx8TsmgX3dsIXV7e3GRL+N2fOUisXapQsbIF7U1yc+uFPtIOBHWN22NfORZaIprM3YBbBHWP34WCsrgWuvZR9WulEBXhZrXR37ILXQ7b13bDs3YTXFy0mA9CuaU8xqpsKabMxZJ2G1+yHd/M0//hhL0/vYvh1o3Rr4xz+882k/ppurxP7A83se3I4TprAGHS6khXXVKuDcc2O+7zBG9t9rr/hz69Tw6YRb2Z18rPb9icWaGxrv1xCAh8VqYrey1qxJ3xXg5WNduhT42c+Ahx/mMWAbNoyt48cV4GQNfvklfzu5ArRV7rfS2uNYk01w6OQKsAurmziZ+dT7WLGCz/2553rnMx1XABA7H088waPcX3EFt7K7YfqB3Qiq8SpoYdBCOmwYP6hefZWXwxDW+vp4147fc2G3WBcvZmNA30NOM0V4Cetrr/EbZ5aIpLA2acGhVo7Cave3OVX+dIKW9W9767lenj8/dryGDWMDbJvr2PHjY92927nxKlXCsFjty3pbszPG8uXADTcAO3b4y2c6rgCAr/uWLTz1yYABPNrX8OH+jum0P3ueNH4br7ZujQ0cE5awapJ1NbaTqgW9erXzscaNc48gsFusBx7I073o7b0eAlddFe8SqasDLr2U3zizRCSFtfH+7L/cI6zffBP7c82a2G+lnJ9+mXQQsL+a6n2ZDQoNG8YG2PZi6FD3wUc0W7d6jwvgF7PcTtaouZ5eJ5mwurVOmxbrNdcAzzzDYw1ovEQ2HVcAwMJaWZl4fK9t7P/V1vIwgvauu36ttBUruNF0zhwWAk1Ywmr36/u1WO2DBCXDFFb7Ne/b1zn21skVsGaNd171utu3A88+G0ufOze1/AZANIW1RSMAhrDeemvsT/tNkK7F6hYVYLcw9b7MeXrsFqsbCxbEemy5WZBbt2bPYjXPTW1t8sYr+7JpsZaXx/IPxEQPiL9GdtyO4cdi1aFKZkSGSbKGtGHD2Ko64giObU013Oq999j/+dRT8eKcqrA6PQDMPLhZrH6FVfvnf/azxP+aOMzmtGpV4rFMDjkkMc2t8corr+b9vWFDbPOWl+0AACAASURBVB2nXlohE01htSzWrXC4Ccw4wOpqZ6vKfqPfeCOw777xaU49r0zfZG0tVz4tGGaDjV9hBWLC41bply6NvRZlIqz2ONZkFuuLLyaG9aTiCtAV1HQLaFatcrceM3EF6GtRYfTKM18pk/UoMl9rzd/p9Lyyv8qm4v+sq+PuqdrHDsSfa7vFOmdO/LLmr39l36Qd7fNv3TrxPydh9bJYAec3ELfGK1NYN2yIxcwC8ff3d9/FrrFfN1KA+HjfLD6a7MvPkz0WK8A3dVlZvGVUXe3PYi0rA9q2TZx2V69v/tb7q64Gzj8/9p9pRZSX+xdWXQHdKv0JJ/jbTzL8+ljNc5PMl+flCmjcmBuInF7LV692P/5PP8W7c9JxBZj5qqoCOnVyzq+dcmNgn82bM2u8MsUgVYu1pgY48UT+rcvpJKwa7d83hXXtWuD3v+ffkyaxv1m7p7TF2ro1D9hj4uTC0sJ6993xXa29MIXVLL8WTyLggQfYuteYZTQHNnJ6OIdMJC1WHSK67dZ7gaef5gXdyrtiRWzFnTv9+VhLSxOFcPdunlfdHhWgK6dXUHrDhiwsftAVMJUGCCerIhl2YfWyWDt2dP4vmbA6WaxO5Vq1yr2848bFNzql4gpYt45/m8JcVcX+OiLvDg5KxVu6ussw4N8VYOYxE2F18inaX5OBRAvVXDbf3F58MT622MtidXor2rSJy/2nP3nn28RsgzD3aVqsZuSM27EB/xbrk09yWVONnXYgksJaWsriuq20KdCyJSfq2D67xepkVdkrR0mJ85P6qqs4TESzfn3Mqe4krAceyN8NG/IgE7ff7r9Qu3bxPt96K/m6TZv636/G3njldPPNncvWidm5wSRZ+JVpsXqJ/+uvJ1pKbqTiCtDWsfnmsXYt8Mgj/Nuru6hdWM1wLL8Wq8ZpzqZUXAF6im4T81xv2sSuJ7slN3s2h5sB3mXVDw1dd0ycrMMtW1IfrtC815yEtaQEaNbMeRs7prB6nf9bb+XGUrf7NwUiKawAj6BWVYXkwqpdBBql4tcBnC1Wza9+Ffv95z/HfjsJq+7JoqMCzPWTsXUrWwSDBiVf135D+sGMY/3nP4HDD09cR4cHud2YfixW3ejl1oAEcFxkv37+8u3mCrC/Dm/b5mwFV1XF8unVicAurNr6NfPglA8/pOpjNbnxRt5Wl61RIy7rkUcCo0Ylrn/bbfztR1hbtEj8z24dNm7M92ayuN9773UeTL621r/F6oaZJz/tDKl2o3YgssLasaPVEN+mDScsWABcfjmHXulXHG2xmr2glOLgcRM3ixWIfyU0cRJWnRct0nr8AMBfxXr44eTrAJlbrMlwCt4G/PlYtXi5Wax+K5P9GKagLVyYOLPqtm3Olc4UVvt1t2P6WE2RSFVY7UMEphpnaqLHcNVla9qURS5ZyJSXsG7dyvdj8+aJ/9nfZFq04DqQbGSzBx+Mn5dMn6edO92F1ct3aj6YzYZAt20CDmmLtLAuXQpumOjenZ/s+gnetSt/79zJN6A50pRZMbSAeFmsbjgJqxZ0pyfm5Zentn8vwhZWNyHwExWgb3w3H7N9oGZtYbnh1IHCKfzGSVgbNowXVvubih2zPDt3JrZsz57tzxoyeyrpfWXaK0qf12bN/I2LYI8tNe/7LVv4wdeoUfL97L8/C5t9fAInzPOnz93Wrc7CCnj7Tp0G9Abc3zrMB42bYZACkRXWDh24ntTWET8tTbSvc+NGvrDmYBNm6Ii+QUtLEy3WU0/1zoCXsDpZTgMGJFa4dAlbWN1et5LFsSoVEwA3i9XuZjjiiNjvu+9OXN8ca0Bjvglo7MJaVgYccAALq5/GDDPvAFd6U9SXLgV69Ej+IHAiiFZt7Tf26wayN4CZgrRlC1uEpuvDpFev2G9tlHiN/qUx/bD6XrNfFy2sdXXewurmSnI7l6ar4oADkuc1CZEV1o4d+dosWwbgnHPi/9ThNeedx9+msJoX+fTT+bukJNFi1dEGbjgJq/b3Oj1VGzWKWS1uT2O/pCOsZhxrMtyE1Y/FqkN5/EYumK6BAQO4ZddEi/f69fyKvWGDs0Xyzjvx+a6tZVEwLVYvTDcGwBXYDHLXPlcz6sTEa6Qouxj4qfjDhgF9+sSWUxHW+npg3jzg0ENjaea52brVW1gHDeIeVUDMD2vGaWvsdWbFCr4OBx8cuze2b48/rzofNTX+XQEmO3dypxr7dTCFVb+xZkBowkpEI4ioiohmG2n7EdF4IlpofTez0omIniKiRUT0HREdGVa+NPqe2fNgNkfs18Kq6dAhcQfffstDpwHOPtZk4VJOwqo7GTi9Su+1V6wzgJlXwNnX5YW22Px0m9UEYbHW18e/UtofIIsXxyxQN2G1i7tZOcvLEyu7GRXQqxdf2z/8IXG/GzcCEybEC1w6wtqrFzfEbN8e3wCTrFXcKXRJs2NHvCvAfn868dvfAsccE1vWr7p+HqqVlZz/I41qaLdYmzRx93eXl8fOuzYCFi50P56eX23FCnZ5LVgQ662lVPy5M7twe1ms9g47ml/8gmO7r7wyPt0UVv3GmgFhWqyvALC/Dw8FMEEp1RXABGsZAE4D0NX6XAsghdEv0uOww/h7T9TO+PFc4SZNir8hAX6C2uncOWZhbt6c+PRNJqxOT1v9lHUSpkaNYpaWPZzGqXXWC33Tu918TgQhrED8Q0M/KDRmLx+/IS9mhW/Y0F1YzWPa+/KbrePduvF3SQlfjy1b/JW7tpbz0qhRou+xtjb5fF9ewmr3sfqJcW7WLP7hNHIkf/t529HWnD4XAMexfvABW8LaYnXz+zZsGHsY6ePp3l0m+iF56608E4Ge/83MAxAvelpMZ88GRo927lYLxLeLmOh6Z39Am9cnn4VVKTUZgP1uOhuAdYUxEsA5RvooxXwFoCkRedxpmdOkCbsDZs2yEg47DHj0UeC44/iPM8+MrWzeYADw5pt8Y5mhWnbrz49j3ylTgLMrYK+9gP79+VXpr3+N/y9VYdU3vVdIk51UhNVrsGXT+rNb7U7jsNoxK0RFBXDWWbHl8vLE8+7H2jz+eG7ABHj7jz7iitu4sf/ZBWpq+LpVVCQ+FHbt4i6WXtjfQkyqq+PL7eeh06BBvLDqHk9+LNZjj+Vv801t+HB2mT3wAHeVbdLEW1j19dOuB6+Gv4oK4Oc/53W08JkPXTOiYMSI+P9ra41KbPCLX7gfT+exrg64/36uv2bssukCSZNs+1hbKqV0688aADrCuC0A0+lRaaWFyrHHcpdqR70w+yDbfS7a3DWFVVusDz/MjUzptOLqiuBk8e21F+/zggsSxcPt6ezENdckCqufVlAzjjUZXuOYmhark2hdcQX3frnoIuftzTw88QSHwulXUidXgB9hLS2NPTzLy9l3fsghvO9kYUKa3btZFCoqEq/PSy8l73WU7CFnlsMM//PCyZ3Ss6e/bQFnF5imstJbWLW7yclCvuUWHgNWX8tGjVhY3fC6Bps2xYe5aZJ15Z43j42h++4DbropJqyffsq++gzJWeOVUkoB8FlTYxDRtUQ0jYimrTODsNPg1FO5TcFxZmHzFcN+g2ofl5PF2rKlc88XPyRzBWjsQpiKj/XPf46JiB5VyE9sqJfFetllsRjam28G/vKXWKWzx+Sa/jInYT34YO6x5tQwYr4qtm3Lr49A7Lw1bOj8Gp7s/JSVxfZhVlK/AgZwRf3220SLtU0bf1EFTpEKbvjt7my/b594IrG3lFPvKY1Xg9odd7g/DMrLeaSuO+4Ajjoq8f+LL45dO4CvmVeDUbLOBU7370EHAZ9/HmtgNvPaoEF8lMKOHezyadSIRTWAAb+zLaxr9Su+9a2HP1oJwLyK7ay0BJRSzyuleiulerdI9RXYximn8DnUHYbicPPdPPporOL06MGNLY8/HhNWt0o0aBDw1VcsCCbmOKO6Ijjtw6ysdmFN5UZo0oR9WpMnx9wd9ie+k6B4CStRrHODfthpQdXnRef/oIOA99/n307C2q5d/HYmXbvGrJwPP4wJjH6DIEoU5E8+SR6uZL42m5XUS8Dsx9GVf8uW+Gtluiq8SCV2Ul+f66/3Xs8urC1aJLoRvAZ/tpffHIPh/PP53jeD+jUNG7Kf8s9/dhZf7ds3LVYvf78prH/5S/xYtfp4Thx9dGzcCvMBYq+Da9awey2VNockZFtYxwLQke6XA/jASL/Mig7oC2Cz4TIIjRYt2LgcPdrBLaiF1W7tmC3KFRVs7h5/fExIzFc2059VWsp+H7uvadQobg1ftixWWZ3mhjcrhN26SaWbZFkZb9+/f+x49hvT6fVNC6uTY58oduNqi1TnUZ8XU6w//ph9xX/5S+K+tLC6dbjQldE8H+++y+exXTtn37ZX6/GAAVyhtAg5WaxOeXF78M6ZE58HP8J69908NsS4cfHphx3mLEw6X82a8XV0w/6AbNEiltavH78d3HST+/b2h4f2vZpcdllimnmPmOI+YgQ3gOnG4DPOiB3HXM/ehmC6Ao4/Pv6B3L279wNQXyfTXabvMY2eENPPRIw+CTPc6g0AXwI4mIgqiegqAI8AOJmIFgL4lbUMAB8DWAJgEYAXAHg46YLl/vt51DR7+CMqKvhp7DZ1hB3t8DYv2rBhsd9ulbuigiMMDjggNoLSK68krufmCjj33HjLZfZs3p8f9D7tQu1kseqBVwYM4AgKk/p6Hqbuiitig3hoi1N/mxXn5Zfdfaj6/GmBu+uuxGOZ/wP88NNWjFtspea66+KXP/2Uz6eunKZVrtPKy7lnnonbq/vmzfGif/zxvO4BB8TmlrLz0EMsECefzPfJuHHs+/v++1hjjYm+Ptu3e1u6dqu/RYvYdRg0iG98twfEG28knkv9VmJGMJSWAldfzQ/JRo24Ycts/DEfSu3bxz9o3nqLjYqSkviHu310NN1r67vvOJxNDwr/3HN8vH33TWxg1ujymddLl8OOPUolE5RSBfvp1auXCoJTTlFq//2V2rjRY6XevXmOVTfq65WaNCkx/cMPebuRI2Np776rlJ6ztb7eO3PPPqtUp06Jx9LbK6XU6tX8u3FjXp44Mfa//WMyYQKntW0bv07Pnkq98YZSF1+cuP3f/qbUggXxaWedlZjvZs34PyL+/tWv3PNkfnbsiO2jri6xrJdcwr/Xr3c+XwsXeu9/9uzY78WLY9v97/9y2hVXxNLGjOG05s15efFipcaN47TOnZ33/9VXSn39dXyef/ELpQYOVOrzz5NfEzsff5y4/rPP8vfVVyt13nnu+5w8OT5t+XJOHz06/mb/6qv49dq2jf1nptfXK/XAA3wOU0FvP22av/X+9S+l9t2Xz5t5/K1beb3mzWPXUrN9u1Lz5iWe0x9+UGrAAKVGjYr9d+65/H3bbUodc4zjtQAwTWWgTRmLWy4/QQnrtGlKlZQodcstHivV1Ci1a1d6B9i9OzHNT6Xywtx+wwb+vc8+vGyvKPqzdm38Pr74gtMPOCB+vX79+P8tW/ik/POfsf+mT1eqqiq2fM01Sn36aWL+OnSI3+fTTyvVp09see+9Y7+fecb7fJj/7dih1KxZ7uelstK57PqzdKnzsUaO5LRLLomlffYZp7VuHUubP5/TDjwwXqQBpYYN43WmTuXlrl15ed06FrKZM2Pr1tT4uwf+/e/EMrz6Kn9ffDFf+7/+NXEdpfiYZprTfagpLeV1Zs1S6qefYul+HwBe6O0XLfK33tSpMYNDp51zTmy9igpOMx/C9n147f83v+Hvf/5TqaFDHcsowhoQl1/OZ+OppwLbpTe33srWXLqYN8KuXfz7vvt4ub5eqRdeiFWW2bPjK4tmxgz+v2PH+JvrxBMT1x05kp/uNTXxouB0cyul1Kmnxu/z2285Xd/UvXrF/vvpJ38Vwg9btiSKjPnRDyH7/t58k9MuvDCWpi3Mjh1jaWvWcNp118Xn7be/ZYtJKaV27lTqggvYWjJZtCj+2P/4B1vKXug3HvOzeDF/v/de4jmyl233bn/nT6+jrULN+eendv699r1hg7/1Fi5MTBs3Lpb21VdKPfyw8z769OFr4bX/qiql/v53fiP65JNYeps2xqoirIGwaRPX9ebNldq2LbDdhoefm/1nP+N17JaqZu5c/r9Ll/hKecYZ/o/vZgXddluioCml1F138fLJJ8f+27QpOGFVKiYG11yj1ODB/DnwQE7btYsfaoMHx2+jX/tNt4a2MA86KH7dH36IlTuVvGmXTSpl+f77+PP4n/84r+cmrEop9V//xZavF3q7mhr3/9JFb19b62+9qqrEtAUL0j++fV8m27bFrvuaNcaqIqyB8cUXbERefXWguw0HPzf7Rx+xv9SpsigVE9aDDoqvlBdc4P/4bj7iOXPiBVSvp1/7f/lLtmIXL1Zq8+ZghbW+PtFtU1mp1DvvuG+jLcNTT42laWHt0cN9u3St6VR4/33eRrtonPjwQ6VefJHXM10XfhkyxD1f336bmbD5LbNez7x2Oq26Ov3jJ8vH5s0Jop+psEZyMkE3+vUDhg7lTjLHH+8c9VRQnH56LEDaCd3q3adPLPi+f3//A2YD7jG0hx7Kt7EdHXS+fXusF5AOn9HdSu28+qr/oHidJ3sIWdu23GvNDb2+2TNM5/WWW9y3++Uv/Xd71a35jz/ub32NjvLwarXWoUvV1cmHrHTihReAv//d+b9Uems5sd9+ycdKMDGv3T//yVEoTr2rUmXGDOf5wFLp2u2XTFQ515+gLVal+A2vf39uzBo6NL7hOK/I9PVMM2UKt6imur90j6/9uoccEp/+5pvuLotsoBu9Xnwxd3lwQ/uF77gj1zlJj3Xr/FWkK64I5p4OAGRosZJysioKhN69e6tpOrg3QLZt49DQ117j0Lx587y7TeeE115jS0L3OsoUPcjLZ5/5W19bqqnePxs2cGzh3Xdz/GY+UVfHMZUBdGkMnKoqjtdNpeurkDZENF0plWbfdIiwejF3Ls+Zd9ZZHLOfzqzRRUtpaWoDs5isW8evhyISQp6SqbBGdgYBPxx6KI+SNmYMuyGdhpSMLIsXJ/bA8kuLFiKqQlEjwpqEu+7iru2rVvFb9wsvBDI7buHTsWP81N6CIOxBhNUHp53G3ap79uTBgBo25DFEBEEQnBBh9UmbNjzEpB7T4aKLeJyKL7/kiTdTiSYRBKG4kTjWFOjcmRtnt2zhEQAvvTQ2Yt9773FDfYcOgcyeKwhCASMWa4oQ8ShlkyfzCHqdO/NY1cuX83RZHTrwcoBDOwqCUGCIsKbJ/vvznHOLF/M4yy+/HOtYM3ZsbB66DRuAgQOBH37IbX4FQcgeIqwBUFbGYzxv2xYbjBzgrrFDhgBvvw08/XTOsicIQpYRH2vA9OrFs/FeeGF8l/s33+SOTUuWcNfzILo+C4KQn4iwhsDPfw7MnMnjR/zjHzzn2h13xGYjmT+fx+4YNIhnv6iv5xmz+/RJbU45QRDyE+nSmiXGjwe++ILn2NKnvFMnnsxQx8T+7W/ec7sJgpAdpEtrgXDyyTy34KJFPKX68OEc+/ruuxxNAAD/8z/xE1IKglCYiLBmmc6deXjR667jySaXLgUmTWJXwE8/Aa1asZvg+ut5wtbHH+fBYFasYB/tAQcAf/wj8O236Y1/IghC+IgrII+YMoVH0/vii1jHA0379iyudgYO5C63F14YP+uyIAjpI66AIuLYY7njQW0tcO+9nNaxI49RoEX1+OOBf/0LOOooXn7rLQ71GjKEp6T/4x856iCVKdJ37mSrWRCEYJCogDzl3nuBbt2AM8/kzgbLl7MbQEcN/OMf3NgFAOvXAy+9xB0Tqqs57csvgTvv5G33358jFcyIg7VrgYsvBp57jsPCXnmF/b9VVdyo1qoV8OOPwGWX8ZgI7dpltfiCUNhkMv1Arj9hTM1SqEyapFSnTkodfLBSjzwSmzlFf9q14zkCjz6aJ0zU6VdcoVSrVvz7pJP4u3lznvfuppt4+fe/z3XpBCG7QKZmKR4fa6boS1lfDzzzDFufhx7K48e++y7wf/8HbNqU+n47deLpabZt415m++7L6Zs2caPbyScHVwZByAdkahYRVt/s2sWRBps28et9//7Agw9ypMKyZUCXLsBf/gJ88gnwX//FroijjoqfvJMIOOUU3k9lJQ8A/tlnnAawiC9dyr7fsjJg4kTgxRd5Jpb/+z8euMacUmrdOm6oa9UqMb8LFvCoYUOH5uc0VELxIsIqwhoKO3bwRIpKsS/3iSd4BoWaGm5IW7qUhVqz774swhMmsMVscs457K+dNQu45x7gvvt4ZpaaGqBlS2DzZm6QGzmSfcV6Oq0uXfg4P/7IjXiCkC1EWEVYs4JS8VbjZ5+xZXv99TwOwgcfcGzt0UezCC5bBnz+OXDggdwoZnLyyUDbtsCoUYkiPH48jwR2333cKAfwnGPnnhtm6QQhHhFWEda8oKaGIxc6d2YBrq1lkTzpJOCGG4CvvgIeeYRF+I03+PX/8ss5lGzSJI5y8GLYMP58+inw5JPACSewkHfpwm6LzZt5YJuDDw63nPX13EPussvY9ywUJyKsIqwFx6JF3MtMx+LW17NVWlLC7oCjj+bBwnfsYMEdNw74+msesEaHkzlBxOFp9fU8oM2YMewz3rwZ6N6dBx/v1StmeX/xBft/Bw3ieRFff52t5f/5n8R9V1fzXGdffsnxxieeyG4PoTgRYRVhLXrq6ngg8alTWRyPPJLHvX3iCfbJ7r03C9/HH3MDGeDe3feww7jL8Pjx3DjmxO9+x8LcpQvQpAlbpkccwbNDHHQQ8OyzvN6IEdyI99VXPJtv587sl27YMPhzIGQXEVYRVsFCKXZBLFgAvP8+d4p44w2en2zHDqB5c+CBB4A1a9gF0aYNW59vvMHjMdx4I89bVlUVv9/99ks+WWTDhmxxN23K4zv06gWMHg3ssw/no39/4JtvgBkzeAQzImD1al7/kUeAOXOACy4AWrfmbTdt4oa9iRPZmi4t5eNs2cJul/324156paUcNVFezkLfoUM45zZqiLCKsAopsHs3W7f77BOfXlvL4WE6JK2qil0D06fzujfeyGL47rvA7bezuA0fzr3aTjmFuxXX1XH34B9/9M5Dy5Z8HK+Y4pKSWMPer37FFvOBB3IvucpKbvxbvjx+m5NOYpFesIAfELt2sbU9YACL8L77srD37s3TuM+fz26Wu+7isi9ZwiF2q1bF/McbN/J5OO00bpDs04d782kWLuR8nHSSr9NfMIiwirAKeURdHftep07lLsPjxrFPuWlT9hGXlrLl2qQJi3STJhxZsWwZR1MsWMCivXs3C+jnn/O2pggfdBD/t99+QIMG7Cf+7juehl1X5w4dWFjXrEme59at2aLfvDmW1qMHD/yzZQsPDqTp3Jn93rNm8cNG5+uXv4xFbrRpw9vttRfQtSv/7tCB9z9jBneP7tKFBbumhofNbN+e19+9m90py5eztT5kCO/PPL8lJTwWhv3hGCQirCKsQoFiD2HzYu5ctjDbt2er0WnbZcs47K1zZ+D001mA5s1jH/Ahh/Dy/PlslX72GYuY7tBRV8eukmXLOPKiSZPYQD4DBwKHH86iP2YMCyUA9OvHluwbb/BxwuLAA4G+fVmQn3ySHzRr1gBnnMGiO2YMPxy2buWH1ymncHTITz/x8saNLPDz5vHD7Iwz2BXUrx/vs0kTPpf6DYHFXYQ119kQhKKjpoat2OpqoFmzWIPc4sXAhx+yYF1xBft2d+5k4a+oYIHevZutyR9+YLdIkyZsYbdsyWI2axaL9MEHsxVbVsaWfHU1N07uvTdbrw0acIeSZcti+SotZWFs2ZLjoJVi67dLFxbcDRs4vjoVWrTgt4BFi7jbdqNGwPr1BSisRLQUwFYAdQBqlVK9iWg/AG8B6AhgKYCLlFIbvfYjwioIxY/u4Td/Prso9Cht1dUstA0axK+/cSOLe/PmwPffs/i+9Ra7Kw44gF0oTz3F7hM9JOeSJbzdvHnAWWcBY8cWrrD2VkqtN9IeBfCTUuoRIhoKoJlS6k6v/YiwCoIQJPX1LNzFNND12QBGWr9HAjgnh3kRBCGCBDVLcq6EVQEYR0TTiehaK62lUmq19XsNgJa5yZogCEJm5GoGgWOVUiuJaH8A44lovvmnUkoRkaOPwhLiawHggAMOCD+ngiAIKZITi1UptdL6rgLwPoCjAKwlotYAYH1XuWz7vFKqt1Kqd4sWLbKVZUEQBN9kXViJaG8iaqJ/AzgFwGwAYwFcbq12OYAPsp03QRCEIMiFK6AlgPeJo5vLALyulPqUiKYCeJuIrgKwDMBFOcibIAhCxmRdWJVSSwAc7pC+AUCR9TgWBCGKFN301zU1NaisrES118CdQtaoqKhAu3bt0MAexS0IRUzRCWtlZSWaNGmCjh07gmQGupyilMKGDRtQWVmJTjLcvhAh8qmDQCBUV1ejefPmIqp5ABGhefPm8vYgRI6iE1YAIqp5hFwLIYoUpbAKgiDkEhHWCNHYHPrdxtKlS3HYYYdlMTeCULyIsAqCIARM0UUFxHHLLcDMmcHus2dPHsbcg6VLl+LUU09F37598cUXX6BPnz4YMmQIhg0bhqqqKowePRo7d+7EzTffDID9kJMnT0aTJk3w2GOP4e2338auXbtw7rnn4v7773c8xtChQ9G+fXvccMMNAID77rsPjRs3xnXXXYezzz4bGzduRE1NDR588EGcffbZKRWxuroa119/PaZNm4aysjI8/vjjOOGEEzBnzhwMGTIEu3fvRn19Pd577z20adMGF110ESorK1FXV4d7770XAwcOTOl4glBsFLew5pBFixbhnXfewYgRI9CnTx+8/vrrmDJlCsaOHYuHH34YdXV1ePrpp3HMMcdg27ZtqKiowLhx47Bw4UJ88803UErhrLPOwuTJk3Hccccl7H/gwIG45ZZb9gjr22+/X0A4VgAAC+tJREFUjc8++wwVFRV4//33sc8++2D9+vXo27cvzjrrrJQakZ5++mkQEb7//nvMnz8fp5xyChYsWIBnn30WN998MwYPHozdu3ejrq4OH3/8Mdq0aYOPPvoIALDZnDhJECJKcQtrEssyTDp16oQePXoAALp3746TTjoJRIQePXpg6dKlGDRoEG677TYMHjwY5513Htq1a4dx48Zh3LhxOOKIIwAA27Ztw8KFCx2F9YgjjkBVVRVWrVqFdevWoVmzZmjfvj1qampw9913Y/LkySgpKcHKlSuxdu1atGrVynfep0yZghtvvBEA0K1bN3To0AELFixAv3798NBDD6GyshLnnXceunbtih49euD222/HnXfeiTPPPBP9+/cP4OwJQmEjPtaQKC8v3/O7pKRkz3JJSQlqa2sxdOhQvPjii9i5cyeOOeYYzJ8/H0op3HXXXZg5cyZmzpyJRYsW4aqrrnI9xoUXXoh3330Xb7311p7X79GjR2PdunWYPn06Zs6ciZYtWwYWR3rJJZdg7NixaNSoEU4//XT8+9//xkEHHYQZM2agR48euOeee/DAAw8EcixBKGSK22LNYxYvXowePXqgR48emDp1KubPn48BAwbg3nvvxeDBg9G4cWOsXLkSDRo0wP777++4j4EDB+Kaa67B+vXrMWnSJAD8Kr7//vujQYMGmDhxIpaZM7H5pH///hg9ejROPPFELFiwAMuXL8fBBx+MJUuWoHPnzrjpppuwfPlyfPfdd+jWrRv2228//OY3v0HTpk3x4osvZnReBKEYEGHNEU8++SQmTpyIkpISdO/eHaeddhrKy8sxb9489OvXDwCHR7322muuwtq9e3ds3boVbdu2RevWrQEAgwcPxq9//Wv06NEDvXv3Rrdu3VLO229/+1tcf/316NGjB8rKyvDKK6+gvLwcb7/9Nl599VU0aNAArVq1wt13342pU6fiD3/4A0pKStCgQQMMHz48/ZMiCEVC0U1/PW/ePBxyyCE5ypHghFwTodAopskEBUEQigJxBeQ5GzZswEknJQ5TO2HCBDRv3jzl/X3//fe49NJL49LKy8vx9ddfp51HQRDiEWHNc5o3b46ZAXZy6NGjR6D7EwQhEXEFCIIgBIwIqyAIQsCIsAqCIASMCKsgCELAiLAWMF7jqwqCkDtEWAVBEAKmqMOtcjQca1bGYzVRSuGOO+7AJ598AiLCPffcg4EDB2L16tUYOHAgtmzZgtraWgwfPhxHH300rrrqKkybNg1EhCuvvBK33nprEKdGEASLohbWXBL2eKwmY8aMwcyZMzFr1iysX78effr0wXHHHYfXX38dAwYMwH//93+jrq4OO3bswMyZM7Fy5UrMnj0bALBp06ZsnA5BiBRFLaw5HI419PFYTaZMmYKLL74YpaWlaNmyJX75y19i6tSp6NOnD6688krU1NTgnHPOQc+ePdG5c2csWbIEN954I8444wyccsopoZ8LQYga4mMNiWyMx5qM4447DpMnT0bbtm1xxRVXYNSoUWjWrBlmzZqF448/Hs8++yyuvvrqjMsqCEI8Iqw5Qo/Heuedd6JPnz57xmMdMWIEtm3bBgBYuXIlqqqqku6rf//+eOutt1BXV4d169Zh8uTJOOqoo7Bs2TK0bNkS11xzDa6++mrMmDED69evR319Pc4//3w8+OCDmDFjRthFFYTIUdSugHwmiPFYNeeeey6+/PJLHH744SAiPProo2jVqhVGjhyJxx57DA0aNEDjxo0xatQorFy5EkOGDEF9fT0A4E9/+lPoZRWEqCHjsQqhI9dEKDRkPFZBEIQ8Q1wBeU7Q47EKghA+Iqx5TtDjsQqCED5F6QooZL9xsSHXQogiRSesFRUV2LBhg1ToPEAphQ0bNqCioiLXWRGErFJ0roB27dqhsrIS69aty3VWBPCDrl27drnOhiBklbwTViI6FcDfAJQCeFEp9Ugq2zdo0ACdOnUKJW+CIAh+yCtXABGVAngawGkADgVwMREdmttcCYIgpEZeCSuAowAsUkotUUrtBvAmgLNznCdBEISUyDdhbQtghbFcaaUJgiAUDHnnY00GEV0L4FprcRcRzc5lfkLmZwDW5zoTISLlK1yKuWwAcHAmG+ebsK4E0N5Ybmel7UEp9TyA5wGAiKZl0p8335HyFTbFXL5iLhvA5ctk+3xzBUwF0JWIOhFRQwCDAIzNcZ4EQRBSIq8sVqVULRH9DsBn4HCrEUqpOTnOliAIQkrklbACgFLqYwAf+1z9+TDzkgdI+QqbYi5fMZcNyLB8BT0eqyAIQj6Sbz5WQRCEgqdghZWITiWiH4hoERENzXV+0oGIRhBRlRkyRkT7EdF4IlpofTez0omInrLK+x0RHZm7nCeHiNoT0UQimktEc4joZiu9WMpXQUTfENEsq3z3W+mdiOhrqxxvWY2wIKJya3mR9X/HXObfD0RUSkTfEtGH1nLRlA0AiGgpEX1PRDN1FEBQ92dBCmsRdX19BcCptrShACYopboCmGAtA1zWrtbnWgDDs5THdKkFcLtS6lAAfQHcYF2jYinfLgAnKqUOB9ATwKlE1BfAnwE8oZQ6EMBGAHqa3asAbLTSn7DWy3duBjDPWC6msmlOUEr1NELHgrk/lVIF9wHQD8BnxvJdAO7Kdb7SLEtHALON5R8AtLZ+twbwg/X7OQAXO61XCB8AHwA4uRjLB2AvADMA/AIcNF9mpe+5T8GRLv2s32XWepTrvHuUqZ0lLCcC+BAAFUvZjDIuBfAzW1og92dBWqwo7q6vLZVSq63fawC0tH4XbJmtV8MjAHyNIiqf9ao8E0AVgPEAFgPYpJSqtVYxy7CnfNb/mwHk89w6TwK4A0C9tdwcxVM2jQIwjoimWz06gYDuz7wLtxJiKKUUERV02AYRNQbwHoBblFJbiGjPf4VePqVUHYCeRNQUwPsAuuU4S4FARGcCqFJKTSei43OdnxA5Vim1koj2BzCeiOabf2ZyfxaqxZq062sBs5aIWgOA9V1lpRdcmYmoAVhURyulxljJRVM+jVJqE4CJ4NfjpkSkDRazDHvKZ/2/L4ANWc6qX44BcBYRLQWPMHcieIzkYijbHpRSK63vKvCD8SgEdH8WqrAWc9fXsQAut35fDvZN6vTLrNbJvgA2G68seQexafoSgHlKqceNv4qlfC0sSxVE1AjsP54HFtgLrNXs5dPlvgDAv5XlrMs3lFJ3KaXaKaU6guvWv5VSg1EEZdMQ0d5E1ET/BnAKgNkI6v7MtQM5A8fz6QAWgP1a/53r/KRZhjcArAZQA/bZXAX2TU0AsBDAvwDsZ61L4EiIxQC+B9A71/lPUrZjwT6s7wDMtD6nF1H5fg7gW6t8swH80UrvDOAbAIsAvAOg3EqvsJYXWf93znUZfJbzeAAfFlvZrLLMsj5ztIYEdX9KzytBEISAKVRXgCAIQt4iwioIghAwIqyCIAgBI8IqCIIQMCKsgiAIASPCKggWRHS8HslJEDJBhFUQBCFgRFiFgoOIfmONhTqTiJ6zBkPZRkRPWGOjTiCiFta6PYnoK2sMzfeN8TUPJKJ/WeOpziCiLtbuGxPRu0Q0n4hGkzm4gSD4RIRVKCiI6BAAAwEco5TqCaAOwGAAewOYppTqDmASgGHWJqMA3KmU+jm4x4xOHw3gacXjqR4N7gEH8Chct4DH+e0M7jcvCCkho1sJhcZJAHoBmGoZk43AA2XUA3jLWuc1AGOIaF8ATZVSk6z0kQDesfqIt1VKvQ8ASqlqALD2941SqtJangkeL3dK+MUSigkRVqHQIAAjlVJ3xSUS3WtbL92+2ruM33WQOiKkgbgChEJjAoALrDE09RxFHcD3sh556RIAU5RSmwFsJKL+VvqlACYppbYCqCSic6x9lBPRXlkthVDUyNNYKCiUUnOJ6B7wyO8l4JHBbgCwHcBR1n9VYD8swEO/PWsJ5xIAQ6z0SwE8R0QPWPu4MIvFEIocGd1KKAqIaJtSqnGu8yEIgLgCBEEQAkcsVkEQhIARi1UQBCFgRFgFQRACRoRVEAQhYERYBUEQAkaEVRAEIWBEWAVBEALm/wHAo2sxk3f3zAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d7a4951-86d2-4b81-fe34-68d0dcd54085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_22 (Dense)            (None, 64)                8192      \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 64)                0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 32)                0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_24 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_24 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_25 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_25 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_26 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_26 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_27 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_27 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_28 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_28 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_29 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_29 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,745\n",
            "Trainable params: 12,361\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dcXAOqd2CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d11ec24-e7e8-4f6a-8b92-1eedb8a7ea43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 4s 10ms/step - loss: 12500.3311 - val_loss: 12328.8770\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 12273.5732 - val_loss: 12146.4297\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 12069.8506 - val_loss: 12089.1455\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 11796.1025 - val_loss: 11648.6416\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 11441.1680 - val_loss: 11656.5068\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 11028.2119 - val_loss: 11338.6953\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 10524.0947 - val_loss: 9125.1357\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 9927.5703 - val_loss: 9606.6445\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 9269.9932 - val_loss: 8829.9131\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 8579.5977 - val_loss: 8044.8188\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 7796.4717 - val_loss: 5272.3687\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 7029.8740 - val_loss: 5838.2500\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 6294.3389 - val_loss: 5270.0239\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 5585.8203 - val_loss: 6727.8008\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 4915.0415 - val_loss: 2070.6709\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 4271.7222 - val_loss: 4561.0684\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 3675.9417 - val_loss: 2685.4700\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 3123.4670 - val_loss: 2043.1129\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 2620.9226 - val_loss: 1444.7280\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 2172.8137 - val_loss: 2872.6160\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 1783.9395 - val_loss: 1981.2988\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1436.1647 - val_loss: 1693.3505\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 1139.1453 - val_loss: 372.1455\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 885.2391 - val_loss: 774.2008\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 679.3672 - val_loss: 825.8210\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 519.3921 - val_loss: 160.0498\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 391.6396 - val_loss: 133.8597\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 300.0872 - val_loss: 684.0901\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 231.2282 - val_loss: 264.8239\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 180.4519 - val_loss: 349.5848\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 151.0285 - val_loss: 152.7849\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 130.2610 - val_loss: 250.5427\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 117.2466 - val_loss: 180.1113\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 109.7942 - val_loss: 262.6691\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 105.6747 - val_loss: 128.0721\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 103.0181 - val_loss: 138.7655\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 100.4591 - val_loss: 121.9041\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 98.9248 - val_loss: 226.2586\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 97.7917 - val_loss: 104.5747\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 95.9109 - val_loss: 159.2466\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 95.4113 - val_loss: 145.6394\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 94.0463 - val_loss: 119.5310\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 94.2992 - val_loss: 112.5803\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 93.0366 - val_loss: 112.7901\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 92.0267 - val_loss: 111.8511\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 91.5323 - val_loss: 184.0051\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 90.5578 - val_loss: 143.3934\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 90.0067 - val_loss: 123.4057\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 88.9901 - val_loss: 111.7165\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 88.4018 - val_loss: 131.5039\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 88.0672 - val_loss: 104.6178\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 87.7697 - val_loss: 104.0290\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 86.9389 - val_loss: 101.2801\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 86.3139 - val_loss: 108.1024\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 86.9455 - val_loss: 113.1457\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 85.5602 - val_loss: 95.7279\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 85.1930 - val_loss: 116.0405\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 84.6725 - val_loss: 104.8125\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 83.9449 - val_loss: 104.1833\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 83.9510 - val_loss: 109.3872\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 83.7633 - val_loss: 129.9557\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 83.2736 - val_loss: 125.1472\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 82.7770 - val_loss: 101.9225\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 82.0109 - val_loss: 131.3434\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 81.9485 - val_loss: 94.5408\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 81.6904 - val_loss: 235.4285\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 81.5550 - val_loss: 131.4673\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 81.2276 - val_loss: 106.2141\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 80.5180 - val_loss: 155.7159\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 80.4545 - val_loss: 101.6262\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 80.3119 - val_loss: 95.7723\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 79.2799 - val_loss: 99.2898\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 78.7049 - val_loss: 112.1587\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 78.5286 - val_loss: 100.7269\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 77.9444 - val_loss: 109.0452\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 77.8770 - val_loss: 100.6212\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 76.9673 - val_loss: 146.1424\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 77.5227 - val_loss: 136.0220\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 77.6238 - val_loss: 114.6875\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 77.6564 - val_loss: 118.1853\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 77.5937 - val_loss: 103.7350\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 77.6497 - val_loss: 123.1968\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 77.2825 - val_loss: 100.2447\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 76.5847 - val_loss: 97.9548\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 76.0885 - val_loss: 98.4869\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 75.3028 - val_loss: 151.9549\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 75.6499 - val_loss: 98.5245\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 75.3329 - val_loss: 126.1256\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 75.1826 - val_loss: 103.6504\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.6226 - val_loss: 102.5644\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 74.9562 - val_loss: 105.0312\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 74.6741 - val_loss: 118.9413\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 73.9644 - val_loss: 122.2531\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 73.5247 - val_loss: 100.6168\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 73.1149 - val_loss: 93.7942\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 73.5805 - val_loss: 113.3174\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 73.1150 - val_loss: 99.4632\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 72.6786 - val_loss: 101.2011\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 72.1508 - val_loss: 97.6288\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 72.1370 - val_loss: 103.2163\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.9125 - val_loss: 87.7116\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 72.3206 - val_loss: 97.4462\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.7813 - val_loss: 114.4647\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.2100 - val_loss: 117.7125\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.9012 - val_loss: 145.0007\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.2496 - val_loss: 117.4194\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.4399 - val_loss: 100.5869\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 71.0132 - val_loss: 96.9342\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.5372 - val_loss: 101.3925\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.9831 - val_loss: 107.2777\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 70.0154 - val_loss: 101.3349\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.4233 - val_loss: 103.0028\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.4343 - val_loss: 88.2890\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 69.2249 - val_loss: 92.1310\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.2747 - val_loss: 105.4268\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.0815 - val_loss: 104.3152\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.3858 - val_loss: 112.6213\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.2374 - val_loss: 112.3907\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 69.0187 - val_loss: 106.4049\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 68.3238 - val_loss: 255.6592\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.9427 - val_loss: 100.0569\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.6657 - val_loss: 94.4736\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 67.4114 - val_loss: 91.6884\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.9729 - val_loss: 95.3460\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 66.8444 - val_loss: 100.9358\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 67.2282 - val_loss: 116.6237\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.6934 - val_loss: 99.0497\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.9609 - val_loss: 109.5085\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.7965 - val_loss: 93.2961\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.8232 - val_loss: 97.5731\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.7517 - val_loss: 90.3472\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 66.0909 - val_loss: 95.5448\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 66.2214 - val_loss: 96.1102\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 65.8701 - val_loss: 98.0306\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.8703 - val_loss: 119.0917\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 65.6449 - val_loss: 94.8760\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.2785 - val_loss: 90.5505\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.3543 - val_loss: 94.5068\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.7020 - val_loss: 92.2283\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 65.0952 - val_loss: 94.6488\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.9685 - val_loss: 120.1625\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.2629 - val_loss: 107.4527\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.1100 - val_loss: 108.2772\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.2522 - val_loss: 235.4918\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.8025 - val_loss: 89.2233\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.8639 - val_loss: 87.1442\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.4469 - val_loss: 90.3824\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.3651 - val_loss: 92.4616\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.1479 - val_loss: 90.1880\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 62.9050 - val_loss: 122.7273\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.2023 - val_loss: 93.3891\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.0894 - val_loss: 106.1855\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 63.1249 - val_loss: 114.0920\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 63.3333 - val_loss: 102.2576\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.6513 - val_loss: 110.2107\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.6149 - val_loss: 122.2449\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.3132 - val_loss: 94.6282\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.3022 - val_loss: 95.8473\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.4557 - val_loss: 121.7482\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 62.3150 - val_loss: 97.9086\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 61.8887 - val_loss: 93.4061\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 62.2809 - val_loss: 103.7471\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 62.1019 - val_loss: 93.7954\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 61.6933 - val_loss: 127.8011\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 61.5359 - val_loss: 92.8045\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.4213 - val_loss: 95.3545\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 61.3239 - val_loss: 108.3663\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.9169 - val_loss: 96.4477\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.3456 - val_loss: 109.3026\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.9999 - val_loss: 97.7666\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.0727 - val_loss: 100.9403\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 61.1457 - val_loss: 103.9934\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.5452 - val_loss: 97.1949\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.8151 - val_loss: 94.5953\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.5076 - val_loss: 109.3835\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.2164 - val_loss: 103.2044\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.3842 - val_loss: 207.2104\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.9171 - val_loss: 102.3650\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 60.0539 - val_loss: 112.6268\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.9166 - val_loss: 91.8730\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.7681 - val_loss: 91.2291\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.8640 - val_loss: 91.2459\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.6963 - val_loss: 91.7379\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.4625 - val_loss: 103.8214\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.2315 - val_loss: 101.4256\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.2322 - val_loss: 108.9588\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.4149 - val_loss: 97.2675\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.1436 - val_loss: 101.9697\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.0667 - val_loss: 100.4235\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.3685 - val_loss: 100.8850\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.9474 - val_loss: 110.1009\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.4970 - val_loss: 91.9075\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 59.1552 - val_loss: 102.2337\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.0307 - val_loss: 105.1731\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.8096 - val_loss: 97.3422\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.7604 - val_loss: 128.5498\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.3371 - val_loss: 95.2584\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.9111 - val_loss: 116.8259\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.6911 - val_loss: 95.2759\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.6667 - val_loss: 100.2611\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.9297 - val_loss: 91.1999\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.3642 - val_loss: 104.4742\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.2714 - val_loss: 90.2599\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.0719 - val_loss: 91.8943\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 58.1556 - val_loss: 99.4852\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.7601 - val_loss: 92.8299\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 58.0388 - val_loss: 177.7154\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.9694 - val_loss: 109.0213\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.7349 - val_loss: 89.2331\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.5527 - val_loss: 107.2536\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.5586 - val_loss: 88.8156\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.3422 - val_loss: 94.6856\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.4426 - val_loss: 100.6254\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.4307 - val_loss: 94.1826\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.0762 - val_loss: 139.2312\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.6824 - val_loss: 108.5329\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.1930 - val_loss: 89.0726\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 57.0719 - val_loss: 97.1058\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.9322 - val_loss: 105.9905\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.7638 - val_loss: 97.3942\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 57.2007 - val_loss: 99.8675\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.8124 - val_loss: 116.0875\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.8427 - val_loss: 94.4224\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.5878 - val_loss: 100.5914\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.8111 - val_loss: 94.9746\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.9058 - val_loss: 91.9644\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.5720 - val_loss: 98.4146\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.6589 - val_loss: 94.3956\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.7496 - val_loss: 107.2152\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.6395 - val_loss: 104.1095\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.5745 - val_loss: 99.2200\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 56.3885 - val_loss: 118.7715\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.3610 - val_loss: 104.5578\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.9136 - val_loss: 110.4875\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.0376 - val_loss: 96.9334\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 56.1653 - val_loss: 92.6553\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.8061 - val_loss: 131.0695\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.8760 - val_loss: 102.8699\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.9928 - val_loss: 91.4251\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.7330 - val_loss: 116.7390\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.7285 - val_loss: 128.4601\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.7321 - val_loss: 117.4835\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.8760 - val_loss: 102.0689\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.6830 - val_loss: 102.0740\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.7477 - val_loss: 103.4416\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.6642 - val_loss: 114.1498\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.4421 - val_loss: 91.5596\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.4690 - val_loss: 125.6507\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.5636 - val_loss: 88.1234\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.4726 - val_loss: 100.4687\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.4976 - val_loss: 107.2172\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.3884 - val_loss: 102.7660\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.0395 - val_loss: 129.1818\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.1122 - val_loss: 98.7622\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.1253 - val_loss: 150.8527\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.1207 - val_loss: 96.1624\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.0620 - val_loss: 93.5899\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.0315 - val_loss: 107.0409\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.0926 - val_loss: 99.5368\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.2509 - val_loss: 91.1367\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.1338 - val_loss: 101.1452\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 55.4609 - val_loss: 120.4866\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.3417 - val_loss: 106.7039\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.9658 - val_loss: 97.3842\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 55.1057 - val_loss: 97.3851\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.7452 - val_loss: 107.9903\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.7657 - val_loss: 101.6050\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.9491 - val_loss: 87.4802\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.4826 - val_loss: 100.0221\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.7461 - val_loss: 110.8051\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.7351 - val_loss: 104.7973\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.5881 - val_loss: 102.0679\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.4798 - val_loss: 88.7842\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.3693 - val_loss: 88.8630\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.6221 - val_loss: 99.5354\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.5442 - val_loss: 97.7812\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.7953 - val_loss: 229.5099\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.4794 - val_loss: 100.1095\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.4377 - val_loss: 142.9366\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.3698 - val_loss: 104.9647\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.5092 - val_loss: 101.3440\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.5938 - val_loss: 94.6016\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.4576 - val_loss: 99.6403\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.1886 - val_loss: 95.6916\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.2827 - val_loss: 119.3419\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.0988 - val_loss: 101.6969\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.0752 - val_loss: 98.6201\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.2407 - val_loss: 96.0154\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.0451 - val_loss: 92.3994\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.8264 - val_loss: 107.9316\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.1566 - val_loss: 97.8198\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.8378 - val_loss: 93.8974\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.0195 - val_loss: 98.5980\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 54.0835 - val_loss: 96.5191\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.8660 - val_loss: 95.6545\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.9867 - val_loss: 117.2457\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.7703 - val_loss: 100.2880\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 53.4587 - val_loss: 102.0518\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 53.6116 - val_loss: 94.8537\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.8024 - val_loss: 149.1827\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.7351 - val_loss: 105.4500\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.6423 - val_loss: 140.0071\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.5780 - val_loss: 105.9381\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.6104 - val_loss: 105.3242\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.5276 - val_loss: 109.2156\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.8389 - val_loss: 169.4760\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.5490 - val_loss: 99.5954\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.6204 - val_loss: 176.6702\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.6925 - val_loss: 94.2296\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.6545 - val_loss: 100.8392\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.8337 - val_loss: 98.8919\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.3349 - val_loss: 91.3865\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.4068 - val_loss: 135.0381\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.2200 - val_loss: 117.1926\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.3232 - val_loss: 103.4899\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.2022 - val_loss: 90.1178\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.2994 - val_loss: 97.8500\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.1170 - val_loss: 94.3904\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.1797 - val_loss: 113.4005\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.2681 - val_loss: 101.6373\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.2481 - val_loss: 115.7333\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.0992 - val_loss: 91.4435\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.0483 - val_loss: 97.4549\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.1793 - val_loss: 108.4892\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.9842 - val_loss: 137.8264\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.1453 - val_loss: 108.9053\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.0891 - val_loss: 91.4806\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.3778 - val_loss: 104.6818\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.0963 - val_loss: 94.9272\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.9230 - val_loss: 100.8989\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 53.1105 - val_loss: 103.6075\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.7583 - val_loss: 90.9510\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.9609 - val_loss: 95.5744\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 53.0127 - val_loss: 98.6206\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.7714 - val_loss: 101.8849\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.0146 - val_loss: 110.8793\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.8265 - val_loss: 101.5571\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.3019 - val_loss: 96.1478\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.6898 - val_loss: 106.6828\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.9065 - val_loss: 97.1817\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.7273 - val_loss: 108.0791\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.7759 - val_loss: 101.3354\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.7347 - val_loss: 99.9098\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.6207 - val_loss: 98.0779\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.7161 - val_loss: 156.4541\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.4903 - val_loss: 93.6573\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.6993 - val_loss: 118.2014\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.6458 - val_loss: 90.6329\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.4489 - val_loss: 95.8379\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.6893 - val_loss: 103.0317\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.5339 - val_loss: 104.7011\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.6697 - val_loss: 94.9014\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.4638 - val_loss: 98.8933\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.6476 - val_loss: 100.7473\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.4824 - val_loss: 98.9459\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.5397 - val_loss: 114.1332\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.3305 - val_loss: 95.1488\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.5651 - val_loss: 124.4250\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.6075 - val_loss: 127.7379\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.4688 - val_loss: 104.1250\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.4488 - val_loss: 118.2520\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.3818 - val_loss: 105.5397\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.4454 - val_loss: 100.7440\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.3326 - val_loss: 94.4002\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.3516 - val_loss: 109.5399\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.2037 - val_loss: 120.7242\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.3459 - val_loss: 124.9451\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.2432 - val_loss: 110.0140\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.3692 - val_loss: 107.4149\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.4208 - val_loss: 92.5374\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.2409 - val_loss: 103.3006\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.9928 - val_loss: 95.0402\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.1230 - val_loss: 102.3330\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.2633 - val_loss: 95.0294\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.0311 - val_loss: 159.0061\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.1749 - val_loss: 131.1075\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.0328 - val_loss: 103.6549\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.7451 - val_loss: 97.8865\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.9355 - val_loss: 102.0990\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.7841 - val_loss: 93.4527\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.9061 - val_loss: 95.5597\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.8587 - val_loss: 105.4523\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.8097 - val_loss: 99.4141\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.9319 - val_loss: 107.9953\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.0763 - val_loss: 102.8823\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 52.1039 - val_loss: 96.3650\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.2589 - val_loss: 102.9041\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.7728 - val_loss: 120.7315\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.8081 - val_loss: 109.8039\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.9150 - val_loss: 107.4243\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 52.0028 - val_loss: 88.7871\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.8109 - val_loss: 90.6807\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.8362 - val_loss: 106.5905\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.8237 - val_loss: 116.1922\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.9377 - val_loss: 100.3760\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.0066 - val_loss: 100.7663\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.6406 - val_loss: 104.7768\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.7060 - val_loss: 98.3850\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.5306 - val_loss: 112.0650\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.5321 - val_loss: 195.8737\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.5059 - val_loss: 121.1068\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.7307 - val_loss: 100.5021\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.6682 - val_loss: 98.6714\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.6249 - val_loss: 99.0869\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.4560 - val_loss: 95.7197\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.4811 - val_loss: 109.4867\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.4755 - val_loss: 107.2094\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.5140 - val_loss: 99.2765\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.2929 - val_loss: 100.4744\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.3023 - val_loss: 103.0696\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.3357 - val_loss: 137.9847\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.1060 - val_loss: 94.3736\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.6038 - val_loss: 93.7362\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.1658 - val_loss: 92.6173\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 51.4116 - val_loss: 95.3795\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.3451 - val_loss: 109.8451\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.2859 - val_loss: 100.4143\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 51.2697 - val_loss: 99.0149\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.5133 - val_loss: 103.4925\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.2758 - val_loss: 107.3069\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.0131 - val_loss: 97.5685\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.0847 - val_loss: 95.7026\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.4779 - val_loss: 104.1025\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.4512 - val_loss: 94.2583\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 51.1571 - val_loss: 105.4473\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.2403 - val_loss: 93.9256\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.0609 - val_loss: 95.9751\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 51.3062 - val_loss: 104.6855\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 51.1971 - val_loss: 96.1478\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 51.0693 - val_loss: 98.4802\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.2754 - val_loss: 127.5439\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.0578 - val_loss: 121.8818\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.1684 - val_loss: 111.5223\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.1198 - val_loss: 108.9424\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.1997 - val_loss: 107.3263\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.9411 - val_loss: 91.1385\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.9546 - val_loss: 99.4417\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.8722 - val_loss: 92.3667\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.1277 - val_loss: 100.3298\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.9749 - val_loss: 97.2429\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.1643 - val_loss: 98.3058\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 51.0627 - val_loss: 140.9057\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.0652 - val_loss: 93.7202\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.9299 - val_loss: 96.7242\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.7550 - val_loss: 109.5380\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.8311 - val_loss: 105.1879\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.7632 - val_loss: 95.0981\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.8884 - val_loss: 108.7616\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.9021 - val_loss: 103.2593\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.8533 - val_loss: 99.7769\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.0086 - val_loss: 96.3618\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.8840 - val_loss: 102.7405\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.7852 - val_loss: 128.7666\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.8111 - val_loss: 94.7168\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.8994 - val_loss: 99.1755\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.8650 - val_loss: 99.7099\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.7928 - val_loss: 99.0088\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.8871 - val_loss: 93.1902\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.7645 - val_loss: 102.0857\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.8313 - val_loss: 96.7195\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.8480 - val_loss: 109.0440\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.8420 - val_loss: 98.8865\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.8843 - val_loss: 97.7185\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.7058 - val_loss: 98.0535\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5309 - val_loss: 105.9230\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.6903 - val_loss: 99.7544\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.7824 - val_loss: 92.2917\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.6230 - val_loss: 103.6525\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.6914 - val_loss: 106.1792\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.6116 - val_loss: 95.7205\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.6758 - val_loss: 109.0740\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.6353 - val_loss: 97.0607\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5298 - val_loss: 91.7221\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.6160 - val_loss: 101.0122\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.3916 - val_loss: 117.4950\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.6708 - val_loss: 110.4139\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.6750 - val_loss: 92.2271\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.4838 - val_loss: 101.7941\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.6515 - val_loss: 93.1759\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.5909 - val_loss: 105.8074\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.3764 - val_loss: 108.3677\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5990 - val_loss: 96.0031\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.5803 - val_loss: 106.9153\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.7066 - val_loss: 100.6219\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.6496 - val_loss: 105.2881\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.6171 - val_loss: 112.1945\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.4218 - val_loss: 94.1644\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.5266 - val_loss: 94.6543\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.4477 - val_loss: 105.1260\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.4398 - val_loss: 94.7389\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.4525 - val_loss: 100.3121\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.6061 - val_loss: 96.3902\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.5361 - val_loss: 104.8124\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 50.5345 - val_loss: 97.3269\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.3617 - val_loss: 110.9236\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.7267 - val_loss: 110.3580\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.4481 - val_loss: 102.6378\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 50.3598 - val_loss: 98.0677\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.4479 - val_loss: 96.6244\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.5355 - val_loss: 107.9401\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696v_fuFCTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d56e55-3b53-48bb-ce4b-cf7dc092b1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  1.8871369887371072 \n",
            "MAE:  7.7521725069462875 \n",
            "SD:  10.216593860207892\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "fe061054-6ce5-47cf-d413-edeaa6fbea9d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "mdZF2osWCUQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ee8f6f-1729-4026-dcf8-8417f3dc58aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -1.4469930615915878 \n",
            "Ensemble_std:  10.079281983244963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf183cdc-b291-4290-afba-062add0c64c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_33 (Dense)            (None, 64)                8192      \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_30 (Activation)  (None, 64)                0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_31 (Activation)  (None, 32)                0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_32 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_33 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_33 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_34 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_35 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_36 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_37 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_38 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_38 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_39 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_39 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,745\n",
            "Trainable params: 12,361\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a2e41d-f10a-47b7-fa41-323704a73a68"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 4s 11ms/step - loss: 3751.5754 - val_loss: 3714.9958\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 3562.4924 - val_loss: 3321.5205\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 3335.2166 - val_loss: 2935.7771\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 3035.1787 - val_loss: 2566.8989\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 2684.6409 - val_loss: 2100.3992\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 2307.1721 - val_loss: 1804.6462\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 1923.0443 - val_loss: 1659.3524\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 1557.5509 - val_loss: 1421.0928\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 1225.3591 - val_loss: 1025.1772\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 936.0826 - val_loss: 1007.6223\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 692.1222 - val_loss: 786.2979\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 499.8927 - val_loss: 1041.8330\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 352.0489 - val_loss: 327.0001\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 238.7828 - val_loss: 409.0703\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 162.9056 - val_loss: 73.6802\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 109.2448 - val_loss: 227.6618\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 78.3460 - val_loss: 359.4362\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 59.0648 - val_loss: 103.9315\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 49.1532 - val_loss: 56.2537\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 43.6882 - val_loss: 68.9838\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 41.9459 - val_loss: 54.4480\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 39.4027 - val_loss: 43.2701\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 37.8289 - val_loss: 41.5333\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 36.9902 - val_loss: 46.7250\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 36.0570 - val_loss: 51.5712\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.3825 - val_loss: 80.3069\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 34.7181 - val_loss: 39.9762\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 34.2947 - val_loss: 44.7017\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 33.9246 - val_loss: 39.6751\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.5707 - val_loss: 52.4262\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.4996 - val_loss: 40.8727\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.2289 - val_loss: 83.1531\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.0686 - val_loss: 70.3888\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 32.7369 - val_loss: 41.6782\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 32.5158 - val_loss: 40.8212\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 32.2321 - val_loss: 46.7186\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 32.0982 - val_loss: 37.3362\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 31.8521 - val_loss: 40.9677\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.4787 - val_loss: 40.8062\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3304 - val_loss: 45.0001\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 31.2535 - val_loss: 47.7383\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 30.9974 - val_loss: 41.0466\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 30.7657 - val_loss: 39.0080\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 30.4569 - val_loss: 49.1298\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 30.3175 - val_loss: 37.0732\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 30.2055 - val_loss: 43.5324\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.8974 - val_loss: 43.7528\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.8018 - val_loss: 47.5795\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.6917 - val_loss: 44.3045\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.3669 - val_loss: 43.8851\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.2688 - val_loss: 46.0256\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.9424 - val_loss: 40.1460\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.7387 - val_loss: 51.6651\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.5313 - val_loss: 40.1867\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.3896 - val_loss: 41.1576\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.1434 - val_loss: 45.4744\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.2354 - val_loss: 37.6297\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.8469 - val_loss: 38.9872\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.9257 - val_loss: 60.4367\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 27.6631 - val_loss: 49.9257\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.4807 - val_loss: 48.3927\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.2843 - val_loss: 52.4161\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.1030 - val_loss: 38.3520\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.2150 - val_loss: 41.4158\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.9312 - val_loss: 43.1277\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.8835 - val_loss: 36.2493\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.7217 - val_loss: 41.8021\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.6440 - val_loss: 42.3538\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.5049 - val_loss: 35.7859\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.4849 - val_loss: 39.1930\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.3117 - val_loss: 39.5163\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.0727 - val_loss: 40.8501\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.0040 - val_loss: 38.3430\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.1334 - val_loss: 40.0865\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 25.8566 - val_loss: 38.1271\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.8782 - val_loss: 47.5931\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.6840 - val_loss: 39.1129\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.6564 - val_loss: 43.2951\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.6760 - val_loss: 45.9753\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.5179 - val_loss: 37.7725\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.7178 - val_loss: 65.3389\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.4853 - val_loss: 36.4311\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.3197 - val_loss: 49.8325\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.1639 - val_loss: 40.3996\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 24.9974 - val_loss: 45.0389\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.0203 - val_loss: 40.5106\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.8650 - val_loss: 41.5327\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.8602 - val_loss: 35.8031\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.8320 - val_loss: 37.5076\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.8207 - val_loss: 38.9590\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.7553 - val_loss: 35.9182\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.6296 - val_loss: 40.3060\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.6936 - val_loss: 43.1440\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 24.4721 - val_loss: 37.2205\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.3607 - val_loss: 37.8537\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.3862 - val_loss: 40.9127\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.3134 - val_loss: 37.3252\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.3351 - val_loss: 35.2278\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.2324 - val_loss: 40.9634\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 24.2931 - val_loss: 36.2925\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.2050 - val_loss: 37.6847\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.1453 - val_loss: 44.3244\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.0182 - val_loss: 37.2137\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 23.9812 - val_loss: 35.1831\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.9674 - val_loss: 40.3102\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.8381 - val_loss: 46.3428\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.9024 - val_loss: 35.4753\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.7689 - val_loss: 39.0403\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.8499 - val_loss: 43.1912\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 23.7309 - val_loss: 37.5306\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.7276 - val_loss: 49.2827\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.5037 - val_loss: 38.1191\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.5544 - val_loss: 43.2585\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.4875 - val_loss: 52.8288\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.5428 - val_loss: 40.0453\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.4398 - val_loss: 39.7944\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.4333 - val_loss: 37.7295\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.2782 - val_loss: 41.9929\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.1919 - val_loss: 37.9804\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.2706 - val_loss: 39.7534\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.1732 - val_loss: 40.2466\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.1574 - val_loss: 37.6245\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.1060 - val_loss: 38.1925\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.0517 - val_loss: 45.3017\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.0288 - val_loss: 38.6666\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.0901 - val_loss: 39.8260\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.9640 - val_loss: 49.6875\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.9150 - val_loss: 60.1430\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.8986 - val_loss: 43.4039\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.9392 - val_loss: 37.0468\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.7777 - val_loss: 35.3154\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.7688 - val_loss: 47.2576\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.7946 - val_loss: 37.6969\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.7175 - val_loss: 38.4555\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.7250 - val_loss: 44.3160\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 22.8017 - val_loss: 41.1698\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.6737 - val_loss: 41.6520\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6223 - val_loss: 35.9186\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6598 - val_loss: 45.6213\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6313 - val_loss: 33.8923\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.5010 - val_loss: 42.5490\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6419 - val_loss: 42.3802\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.5032 - val_loss: 39.8378\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.3422 - val_loss: 36.2410\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.4461 - val_loss: 44.7431\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.2749 - val_loss: 42.6825\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.3606 - val_loss: 40.9766\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.4062 - val_loss: 46.7244\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.4096 - val_loss: 41.3529\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.2525 - val_loss: 38.0031\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.2727 - val_loss: 37.5611\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.2483 - val_loss: 35.5709\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.1400 - val_loss: 59.3060\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.1447 - val_loss: 42.7731\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.1704 - val_loss: 35.7077\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.1365 - val_loss: 39.0254\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.0911 - val_loss: 34.7096\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.0114 - val_loss: 36.6034\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.1362 - val_loss: 36.5737\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.0278 - val_loss: 45.8826\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.0524 - val_loss: 34.3554\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9222 - val_loss: 50.2244\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9777 - val_loss: 71.8597\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.2500 - val_loss: 37.1543\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9501 - val_loss: 51.9770\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9641 - val_loss: 45.8499\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.9636 - val_loss: 40.6867\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.8976 - val_loss: 47.1544\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9017 - val_loss: 36.3555\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.8932 - val_loss: 36.9740\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.9083 - val_loss: 44.7199\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.7809 - val_loss: 39.6696\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.7186 - val_loss: 37.2947\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.8044 - val_loss: 34.4864\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.8571 - val_loss: 36.5327\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.7905 - val_loss: 44.4681\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.7035 - val_loss: 35.9903\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.7494 - val_loss: 36.2640\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 21.6724 - val_loss: 39.5661\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 21.6862 - val_loss: 42.0085\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.7480 - val_loss: 42.5877\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.6109 - val_loss: 36.2378\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.5967 - val_loss: 43.1599\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.7108 - val_loss: 38.1146\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.5859 - val_loss: 38.5160\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.5641 - val_loss: 39.9732\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.5513 - val_loss: 37.5675\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4872 - val_loss: 35.2313\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4082 - val_loss: 38.6590\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4880 - val_loss: 37.1734\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4816 - val_loss: 55.1904\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.5201 - val_loss: 36.1565\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4397 - val_loss: 39.0580\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.5048 - val_loss: 37.8770\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.3872 - val_loss: 37.5281\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.3898 - val_loss: 51.7106\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4507 - val_loss: 41.3375\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4316 - val_loss: 37.4807\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.3507 - val_loss: 34.1773\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.3494 - val_loss: 45.5534\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4618 - val_loss: 37.5549\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.3516 - val_loss: 36.1625\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.3389 - val_loss: 38.5368\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.3707 - val_loss: 37.2959\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2819 - val_loss: 36.9173\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.3157 - val_loss: 38.4640\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.1868 - val_loss: 34.8098\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2447 - val_loss: 39.6200\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2549 - val_loss: 41.3834\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.1639 - val_loss: 40.3461\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2545 - val_loss: 39.3470\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2448 - val_loss: 42.3563\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.2122 - val_loss: 35.7929\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2054 - val_loss: 37.7303\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.1108 - val_loss: 38.9962\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.1636 - val_loss: 50.6432\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.1041 - val_loss: 42.3275\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.1182 - val_loss: 42.5284\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.0777 - val_loss: 53.1946\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.1093 - val_loss: 37.5902\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.1696 - val_loss: 41.4608\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.0289 - val_loss: 42.9116\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.1763 - val_loss: 37.7543\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9580 - val_loss: 38.7413\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.9717 - val_loss: 44.5988\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.0716 - val_loss: 42.4149\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9435 - val_loss: 34.9358\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.9368 - val_loss: 37.2249\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.8883 - val_loss: 41.2896\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9485 - val_loss: 39.4629\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.9290 - val_loss: 50.4495\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.9582 - val_loss: 34.5740\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9387 - val_loss: 40.7432\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.8902 - val_loss: 42.2013\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.9519 - val_loss: 40.4572\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.8614 - val_loss: 44.4673\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9540 - val_loss: 36.6018\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.9109 - val_loss: 42.7025\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.8790 - val_loss: 41.8864\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9091 - val_loss: 37.6920\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.8517 - val_loss: 59.8973\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.8355 - val_loss: 37.0439\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.8253 - val_loss: 47.5267\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.8283 - val_loss: 42.2437\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.9177 - val_loss: 40.4766\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.8576 - val_loss: 40.8232\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.8178 - val_loss: 39.1266\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7593 - val_loss: 36.3084\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7993 - val_loss: 41.6494\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7466 - val_loss: 46.0879\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7125 - val_loss: 40.6306\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7207 - val_loss: 42.4575\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.6987 - val_loss: 41.1261\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.6702 - val_loss: 39.7239\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7036 - val_loss: 37.4204\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7313 - val_loss: 41.8279\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7557 - val_loss: 35.1731\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.7169 - val_loss: 47.5087\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7113 - val_loss: 49.3778\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.6276 - val_loss: 34.9574\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.6502 - val_loss: 36.1597\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.6325 - val_loss: 47.5102\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.6925 - val_loss: 41.6928\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.7027 - val_loss: 39.2344\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.6577 - val_loss: 34.3305\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5837 - val_loss: 37.3118\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5141 - val_loss: 43.1792\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.6273 - val_loss: 35.9981\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5503 - val_loss: 37.4142\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5337 - val_loss: 36.2400\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.5437 - val_loss: 38.0705\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5845 - val_loss: 37.4245\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.5094 - val_loss: 69.0770\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.5730 - val_loss: 39.0571\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5703 - val_loss: 36.4404\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.5096 - val_loss: 38.3437\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5546 - val_loss: 34.9942\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5421 - val_loss: 35.7443\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.4353 - val_loss: 36.4505\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.5339 - val_loss: 41.1313\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.5135 - val_loss: 45.9262\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.5102 - val_loss: 37.5302\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.4611 - val_loss: 36.4923\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5046 - val_loss: 35.8442\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.4526 - val_loss: 48.6387\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.4865 - val_loss: 35.2821\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4088 - val_loss: 35.8090\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.4428 - val_loss: 42.2781\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.4049 - val_loss: 36.3001\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4386 - val_loss: 41.4284\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.3355 - val_loss: 37.7702\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3666 - val_loss: 37.1856\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4259 - val_loss: 38.7535\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3878 - val_loss: 36.0249\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.3874 - val_loss: 35.6022\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.3671 - val_loss: 40.7360\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4017 - val_loss: 42.9212\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.4161 - val_loss: 50.4587\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3737 - val_loss: 36.2040\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.3850 - val_loss: 44.0037\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3431 - val_loss: 42.3852\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.2711 - val_loss: 38.6141\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3266 - val_loss: 41.5750\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4030 - val_loss: 37.8526\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3679 - val_loss: 41.6305\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2902 - val_loss: 43.4804\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3130 - val_loss: 44.1205\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3042 - val_loss: 40.1350\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3725 - val_loss: 45.7256\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.2752 - val_loss: 36.2291\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.2030 - val_loss: 43.7264\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.2569 - val_loss: 39.0378\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2932 - val_loss: 37.0211\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.2177 - val_loss: 41.0268\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.2648 - val_loss: 38.4015\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3320 - val_loss: 36.1643\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.2959 - val_loss: 38.9556\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2513 - val_loss: 40.0987\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.2818 - val_loss: 37.4662\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 20.2456 - val_loss: 40.6070\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 20.3136 - val_loss: 36.6288\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1487 - val_loss: 36.3160\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.1620 - val_loss: 50.0774\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1954 - val_loss: 45.2266\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2055 - val_loss: 35.6860\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1578 - val_loss: 36.6848\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.1675 - val_loss: 37.1466\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2448 - val_loss: 38.0224\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1747 - val_loss: 35.7986\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0635 - val_loss: 40.0793\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.1629 - val_loss: 43.4992\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1311 - val_loss: 36.5156\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1219 - val_loss: 36.9789\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1365 - val_loss: 39.0066\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.1500 - val_loss: 38.1857\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0493 - val_loss: 42.3362\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.1118 - val_loss: 36.6703\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.0459 - val_loss: 41.2723\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1077 - val_loss: 48.4432\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0312 - val_loss: 38.5798\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1612 - val_loss: 37.0401\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1254 - val_loss: 37.2717\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0988 - val_loss: 35.7263\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1450 - val_loss: 42.5872\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0280 - val_loss: 40.4251\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.0386 - val_loss: 36.8859\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0249 - val_loss: 37.5562\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9878 - val_loss: 35.2122\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1226 - val_loss: 43.1739\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9629 - val_loss: 37.8438\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0145 - val_loss: 38.7369\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0173 - val_loss: 38.4853\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0484 - val_loss: 40.7823\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0880 - val_loss: 36.7524\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0511 - val_loss: 44.2026\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0104 - val_loss: 39.7263\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9639 - val_loss: 38.4021\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9960 - val_loss: 40.2627\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9222 - val_loss: 35.2588\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9480 - val_loss: 53.6271\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9452 - val_loss: 41.6122\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9545 - val_loss: 42.8426\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9674 - val_loss: 40.6585\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9769 - val_loss: 40.2438\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9600 - val_loss: 42.4080\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9801 - val_loss: 36.9073\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9114 - val_loss: 37.9458\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9209 - val_loss: 36.2557\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8342 - val_loss: 41.4470\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9653 - val_loss: 56.7521\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9106 - val_loss: 37.4283\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9246 - val_loss: 38.1256\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9249 - val_loss: 35.6157\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9066 - val_loss: 38.5432\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8978 - val_loss: 39.3052\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9568 - val_loss: 39.5053\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8669 - val_loss: 43.0048\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8274 - val_loss: 43.0763\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8403 - val_loss: 38.2066\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0039 - val_loss: 36.6087\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9016 - val_loss: 41.3953\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8534 - val_loss: 35.5003\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8710 - val_loss: 37.7667\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8702 - val_loss: 40.7223\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8599 - val_loss: 38.8670\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8130 - val_loss: 43.3529\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8550 - val_loss: 34.6293\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8115 - val_loss: 37.3587\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8112 - val_loss: 37.4080\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8036 - val_loss: 37.9159\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.7993 - val_loss: 39.2943\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8062 - val_loss: 39.5752\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8189 - val_loss: 41.7056\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8372 - val_loss: 40.9278\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8291 - val_loss: 40.8214\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.7825 - val_loss: 44.8207\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7716 - val_loss: 35.8157\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7578 - val_loss: 39.1940\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8277 - val_loss: 35.7541\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8664 - val_loss: 38.9035\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.7877 - val_loss: 40.3060\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8266 - val_loss: 40.8287\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8536 - val_loss: 44.7967\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7495 - val_loss: 39.7319\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8038 - val_loss: 38.5053\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7486 - val_loss: 36.0506\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7434 - val_loss: 49.2699\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6825 - val_loss: 35.6820\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7214 - val_loss: 42.8415\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.7713 - val_loss: 39.6603\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7759 - val_loss: 35.4936\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7291 - val_loss: 39.9127\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6837 - val_loss: 38.1003\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7064 - val_loss: 37.1600\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7425 - val_loss: 37.4355\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.7213 - val_loss: 58.5616\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6969 - val_loss: 44.4405\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7268 - val_loss: 36.4682\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7014 - val_loss: 36.1654\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7330 - val_loss: 34.6168\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6910 - val_loss: 34.2259\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.7196 - val_loss: 38.4907\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6513 - val_loss: 36.4397\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6976 - val_loss: 37.3107\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.7006 - val_loss: 41.5199\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6396 - val_loss: 40.2442\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6935 - val_loss: 35.7070\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.7226 - val_loss: 40.5016\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6844 - val_loss: 43.5319\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6827 - val_loss: 36.6030\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6656 - val_loss: 35.6517\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.5879 - val_loss: 39.2080\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7250 - val_loss: 43.1738\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6319 - val_loss: 38.7687\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6373 - val_loss: 38.2912\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6685 - val_loss: 34.7615\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6370 - val_loss: 38.4568\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5726 - val_loss: 36.8231\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6131 - val_loss: 36.7917\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6351 - val_loss: 48.0312\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6194 - val_loss: 36.8917\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6422 - val_loss: 37.0582\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5852 - val_loss: 36.5312\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6501 - val_loss: 36.5452\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5971 - val_loss: 37.7106\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6157 - val_loss: 53.0364\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6419 - val_loss: 36.7693\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6316 - val_loss: 35.8776\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6240 - val_loss: 37.1721\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.5366 - val_loss: 41.3872\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6149 - val_loss: 49.0259\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5056 - val_loss: 35.8976\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6612 - val_loss: 37.1303\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.6157 - val_loss: 36.9679\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5569 - val_loss: 37.8162\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6115 - val_loss: 39.4223\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5986 - val_loss: 38.4239\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 19.5695 - val_loss: 38.6772\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 19.5305 - val_loss: 35.8862\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6108 - val_loss: 38.1310\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5482 - val_loss: 38.5541\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5468 - val_loss: 39.1575\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5338 - val_loss: 35.7048\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5296 - val_loss: 35.7726\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5423 - val_loss: 42.3876\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4930 - val_loss: 37.1355\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5775 - val_loss: 39.4459\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4733 - val_loss: 42.4773\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5204 - val_loss: 37.2073\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6134 - val_loss: 50.7032\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5441 - val_loss: 35.7325\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.5567 - val_loss: 36.2510\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5210 - val_loss: 33.0689\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5831 - val_loss: 37.9004\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.6056 - val_loss: 37.2617\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.5460 - val_loss: 37.2345\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4558 - val_loss: 37.7268\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5535 - val_loss: 41.8079\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.5397 - val_loss: 35.7503\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4212 - val_loss: 37.7163\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4789 - val_loss: 37.0991\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5361 - val_loss: 35.5966\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4535 - val_loss: 50.9506\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5365 - val_loss: 41.9810\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4918 - val_loss: 42.1135\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4664 - val_loss: 36.3502\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.4829 - val_loss: 36.5380\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4313 - val_loss: 41.0913\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.4055 - val_loss: 36.2028\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.4086 - val_loss: 37.8007\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4045 - val_loss: 38.6933\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.4689 - val_loss: 38.7679\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.4464 - val_loss: 34.8021\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4216 - val_loss: 36.8866\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4389 - val_loss: 35.4598\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4772 - val_loss: 42.0058\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.3712 - val_loss: 37.5153\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4420 - val_loss: 37.7050\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.4041 - val_loss: 36.6985\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.5296 - val_loss: 38.9070\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dbf70f0-6e81-4bef-e782-732d8ae0059c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.3622258055319973 \n",
            "MAE:  4.640890229466306 \n",
            "SD:  6.2270223052128255\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cip38xZOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "f91d6fe0-26d2-4cb6-b665-d521b12e77ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfcb4bfb-0eb2-441c-e1c3-44bd708a4f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_44 (Dense)            (None, 64)                8192      \n",
            "                                                                 \n",
            " batch_normalization_40 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_40 (Activation)  (None, 64)                0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " batch_normalization_41 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_41 (Activation)  (None, 32)                0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_42 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_42 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_43 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_43 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_44 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_44 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_45 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_45 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_46 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_46 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_47 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_47 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_48 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_48 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_49 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_49 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,745\n",
            "Trainable params: 12,361\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f15505da-3b79-42e6-d767-360da2284ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 4s 10ms/step - loss: 3767.8447 - val_loss: 3717.7939\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 3595.1685 - val_loss: 3492.3108\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 3375.9431 - val_loss: 3340.0813\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 3095.5967 - val_loss: 3072.1995\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 2717.1350 - val_loss: 1952.2787\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 2325.8914 - val_loss: 2283.1528\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 1919.0317 - val_loss: 1377.6621\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 1537.6375 - val_loss: 2254.8997\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 1200.4653 - val_loss: 804.3814\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 902.2610 - val_loss: 586.5408\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 656.1354 - val_loss: 1130.6149\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 456.6417 - val_loss: 439.9322\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 305.0633 - val_loss: 345.5658\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 204.0474 - val_loss: 315.6021\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 136.2751 - val_loss: 85.8905\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 95.3797 - val_loss: 48.9341\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 64.8934 - val_loss: 175.8833\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 54.6738 - val_loss: 46.8545\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 45.4455 - val_loss: 68.0178\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 41.2284 - val_loss: 41.8016\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 39.4438 - val_loss: 51.2326\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 39.1181 - val_loss: 47.8798\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 38.3302 - val_loss: 50.9063\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 37.7088 - val_loss: 48.9736\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 37.9594 - val_loss: 56.8914\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 36.8057 - val_loss: 44.4230\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 37.0167 - val_loss: 48.4572\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 36.7848 - val_loss: 82.4810\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 36.3134 - val_loss: 52.4582\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.6079 - val_loss: 53.7519\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 35.4108 - val_loss: 43.9987\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 34.7430 - val_loss: 58.8727\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 34.1377 - val_loss: 43.8382\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.9504 - val_loss: 41.3229\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7598 - val_loss: 56.0358\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7558 - val_loss: 45.2539\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.7526 - val_loss: 52.5549\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 32.9786 - val_loss: 56.9029\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 32.9198 - val_loss: 46.0816\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 32.4133 - val_loss: 46.3733\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 32.3517 - val_loss: 49.0217\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 32.8432 - val_loss: 49.4867\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 32.7923 - val_loss: 65.1376\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 33.3320 - val_loss: 71.6505\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.4732 - val_loss: 110.7488\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.8022 - val_loss: 44.1741\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.1838 - val_loss: 60.1716\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 32.8885 - val_loss: 62.6624\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 33.0899 - val_loss: 37.9647\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 32.4122 - val_loss: 43.8075\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 32.6358 - val_loss: 77.8053\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 32.1424 - val_loss: 44.6324\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 31.7820 - val_loss: 40.6791\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 31.6433 - val_loss: 47.8310\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 31.8494 - val_loss: 44.4925\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 31.4860 - val_loss: 43.8012\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 31.4611 - val_loss: 45.4605\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 31.1369 - val_loss: 40.9675\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 30.8724 - val_loss: 55.6237\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 30.9076 - val_loss: 41.0306\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 30.6732 - val_loss: 37.5081\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 30.4081 - val_loss: 35.7811\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 30.3180 - val_loss: 41.1670\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 30.3156 - val_loss: 39.2546\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 30.0418 - val_loss: 42.8822\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.8915 - val_loss: 39.8303\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.8628 - val_loss: 42.4694\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.7036 - val_loss: 49.4501\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.7992 - val_loss: 45.4957\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.6770 - val_loss: 46.3358\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.4472 - val_loss: 36.5736\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.1919 - val_loss: 38.0829\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.3368 - val_loss: 42.2085\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.1545 - val_loss: 38.0861\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 29.0993 - val_loss: 38.7194\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.9155 - val_loss: 43.7096\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.8714 - val_loss: 45.2843\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.7295 - val_loss: 36.7398\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.6460 - val_loss: 42.6706\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.3348 - val_loss: 35.0201\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.1992 - val_loss: 42.5228\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 28.3449 - val_loss: 40.6590\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.3833 - val_loss: 41.3815\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.2724 - val_loss: 39.6050\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 28.1251 - val_loss: 35.5869\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.9832 - val_loss: 40.2415\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.8971 - val_loss: 39.0030\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.7422 - val_loss: 37.3713\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.9097 - val_loss: 38.7006\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.8069 - val_loss: 38.5765\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.5376 - val_loss: 48.6592\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.5161 - val_loss: 36.9767\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.3235 - val_loss: 36.4473\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.3117 - val_loss: 51.7679\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.2181 - val_loss: 34.9804\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.0834 - val_loss: 43.7513\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.9956 - val_loss: 39.6821\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 27.0059 - val_loss: 43.0771\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.7862 - val_loss: 37.2991\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.6430 - val_loss: 37.0720\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.7135 - val_loss: 39.8634\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.7124 - val_loss: 37.9000\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.6306 - val_loss: 53.8108\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.3938 - val_loss: 36.6962\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.4714 - val_loss: 35.2809\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.4285 - val_loss: 38.4716\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.2814 - val_loss: 36.7341\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.2227 - val_loss: 35.7089\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.1142 - val_loss: 45.5475\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.3000 - val_loss: 42.3596\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.1440 - val_loss: 43.0774\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.2212 - val_loss: 36.9796\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 26.0797 - val_loss: 38.7856\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.0335 - val_loss: 34.6146\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.7956 - val_loss: 42.2031\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.7920 - val_loss: 40.6333\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.7321 - val_loss: 41.2266\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.6387 - val_loss: 36.8602\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.6515 - val_loss: 34.2138\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.6634 - val_loss: 45.1589\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.5863 - val_loss: 37.5306\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.3838 - val_loss: 36.5291\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.5271 - val_loss: 36.1009\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.4291 - val_loss: 43.8587\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.3918 - val_loss: 48.4764\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.3916 - val_loss: 37.4984\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.3885 - val_loss: 34.6564\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.3399 - val_loss: 34.2401\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.2577 - val_loss: 38.4864\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.1939 - val_loss: 38.6141\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.0673 - val_loss: 34.2260\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.1275 - val_loss: 38.5948\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.1441 - val_loss: 46.8824\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 25.1986 - val_loss: 37.5549\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.9904 - val_loss: 35.9954\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.8803 - val_loss: 36.7034\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.9501 - val_loss: 35.1415\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.7434 - val_loss: 51.7378\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.7464 - val_loss: 45.5431\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.6703 - val_loss: 36.2720\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.7732 - val_loss: 38.8119\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.6846 - val_loss: 36.4472\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.5638 - val_loss: 38.5393\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.4998 - val_loss: 33.7017\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 24.5285 - val_loss: 34.1505\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.5419 - val_loss: 35.8737\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.5714 - val_loss: 35.4175\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.5207 - val_loss: 37.3864\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.5182 - val_loss: 55.0371\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.5103 - val_loss: 42.7806\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.5189 - val_loss: 37.8317\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.3576 - val_loss: 51.0792\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.3216 - val_loss: 37.8644\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.3991 - val_loss: 32.1681\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.2250 - val_loss: 43.4214\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 24.3012 - val_loss: 40.1416\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.2308 - val_loss: 33.8683\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.1668 - val_loss: 33.1613\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.0844 - val_loss: 79.1642\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.2089 - val_loss: 39.9232\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.1117 - val_loss: 35.9980\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.2797 - val_loss: 36.2197\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.0826 - val_loss: 33.7375\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.0368 - val_loss: 44.7122\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.9840 - val_loss: 37.6798\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.0131 - val_loss: 42.6682\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.8756 - val_loss: 40.1961\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.7709 - val_loss: 38.0516\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.7477 - val_loss: 37.3446\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.9476 - val_loss: 37.2251\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 24.3065 - val_loss: 47.2136\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 24.0909 - val_loss: 37.4309\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.7465 - val_loss: 32.7587\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.7250 - val_loss: 38.5198\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.8338 - val_loss: 38.3535\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.7700 - val_loss: 37.3554\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.7868 - val_loss: 35.1028\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.6689 - val_loss: 36.8047\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.6984 - val_loss: 36.9559\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.5570 - val_loss: 32.6672\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.6286 - val_loss: 37.3418\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.5821 - val_loss: 36.2789\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.6315 - val_loss: 47.3008\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.6164 - val_loss: 50.3381\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.5071 - val_loss: 41.2866\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.3426 - val_loss: 33.0328\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 23.2936 - val_loss: 35.4760\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.3680 - val_loss: 41.5265\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.3393 - val_loss: 44.9480\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.3386 - val_loss: 47.2687\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 23.2559 - val_loss: 39.0491\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.2989 - val_loss: 32.2072\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.1911 - val_loss: 58.1004\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 23.0569 - val_loss: 38.4194\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.1978 - val_loss: 36.6831\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.1374 - val_loss: 36.5900\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.1445 - val_loss: 38.0788\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.1486 - val_loss: 41.4835\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.0846 - val_loss: 33.8546\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.9723 - val_loss: 39.8213\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.9782 - val_loss: 42.3388\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.0497 - val_loss: 39.4539\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 23.2189 - val_loss: 46.0499\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.9926 - val_loss: 37.6424\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.8972 - val_loss: 41.2843\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.0915 - val_loss: 51.6915\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.2001 - val_loss: 39.9262\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.2140 - val_loss: 52.9569\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.0748 - val_loss: 48.4153\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.0586 - val_loss: 38.5649\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.8464 - val_loss: 33.3049\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.0082 - val_loss: 35.5284\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.8105 - val_loss: 31.9920\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.0191 - val_loss: 37.9491\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.0200 - val_loss: 41.3234\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 23.0468 - val_loss: 37.7308\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 23.1586 - val_loss: 37.9133\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.9723 - val_loss: 60.3129\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.9167 - val_loss: 34.3676\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7832 - val_loss: 34.4143\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 23.0046 - val_loss: 40.2020\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.8087 - val_loss: 38.0571\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.8990 - val_loss: 38.4797\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6671 - val_loss: 48.8203\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.8325 - val_loss: 35.0470\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.9346 - val_loss: 34.6624\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6514 - val_loss: 44.5440\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6234 - val_loss: 33.6441\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6366 - val_loss: 34.7538\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6419 - val_loss: 38.7236\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6385 - val_loss: 40.9551\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.5672 - val_loss: 37.7357\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6098 - val_loss: 45.1770\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6619 - val_loss: 45.2940\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6873 - val_loss: 47.7211\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.7438 - val_loss: 42.4110\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.8571 - val_loss: 39.9977\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.6990 - val_loss: 33.3887\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.5009 - val_loss: 36.7059\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.4902 - val_loss: 45.4388\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 22.4513 - val_loss: 35.9265\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.4604 - val_loss: 44.7992\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.4573 - val_loss: 31.4832\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.5866 - val_loss: 37.8847\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.3920 - val_loss: 35.9055\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.3594 - val_loss: 35.4730\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.3438 - val_loss: 31.1473\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.3665 - val_loss: 41.6694\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.3194 - val_loss: 37.3821\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.3326 - val_loss: 36.1804\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.3141 - val_loss: 37.0726\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.2467 - val_loss: 34.3761\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.1969 - val_loss: 36.3226\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.1984 - val_loss: 41.7089\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.3554 - val_loss: 39.6679\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.1925 - val_loss: 40.2971\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.2203 - val_loss: 37.2479\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 22.3904 - val_loss: 33.7666\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.2328 - val_loss: 66.8981\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.2022 - val_loss: 35.7164\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.1507 - val_loss: 40.2169\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.1811 - val_loss: 40.6500\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.2214 - val_loss: 32.4869\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.1916 - val_loss: 34.0482\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.1275 - val_loss: 33.0941\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.0184 - val_loss: 37.7029\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.9955 - val_loss: 35.9287\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9933 - val_loss: 34.5457\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.1134 - val_loss: 35.7645\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.2661 - val_loss: 38.1041\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 22.1628 - val_loss: 37.7464\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9839 - val_loss: 38.1726\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.8458 - val_loss: 36.5913\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9629 - val_loss: 36.1097\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9272 - val_loss: 41.5841\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9909 - val_loss: 41.0977\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9294 - val_loss: 38.3832\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.7495 - val_loss: 33.0378\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.7574 - val_loss: 42.3812\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.8344 - val_loss: 36.3623\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.7725 - val_loss: 36.3806\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9075 - val_loss: 37.5430\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.9195 - val_loss: 35.9690\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.8354 - val_loss: 40.1473\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.8730 - val_loss: 36.9565\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.8673 - val_loss: 35.8134\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.6730 - val_loss: 38.7511\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.6711 - val_loss: 40.6438\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.7084 - val_loss: 33.9078\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.7061 - val_loss: 36.8540\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.6186 - val_loss: 35.4487\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.6764 - val_loss: 39.7939\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.6916 - val_loss: 35.1562\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.6609 - val_loss: 34.2882\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.7441 - val_loss: 46.6819\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.7554 - val_loss: 34.9846\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.6834 - val_loss: 35.0296\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.6000 - val_loss: 35.1428\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.6529 - val_loss: 41.2986\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.6245 - val_loss: 32.4763\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.6247 - val_loss: 35.9407\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.5613 - val_loss: 36.7315\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.5103 - val_loss: 35.0974\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.5004 - val_loss: 34.2360\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.5390 - val_loss: 38.0133\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4913 - val_loss: 38.6432\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.5117 - val_loss: 43.2949\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.5133 - val_loss: 36.9765\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.5125 - val_loss: 37.3183\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4255 - val_loss: 35.2448\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4366 - val_loss: 44.5675\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4165 - val_loss: 37.6352\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.3846 - val_loss: 36.5357\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.4364 - val_loss: 38.4388\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4874 - val_loss: 33.9770\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 21.4452 - val_loss: 35.1767\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.4290 - val_loss: 32.9792\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.3560 - val_loss: 38.1495\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.3432 - val_loss: 34.2842\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2766 - val_loss: 33.2424\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.3414 - val_loss: 36.7286\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.2738 - val_loss: 35.8281\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.3652 - val_loss: 34.4637\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.3979 - val_loss: 33.3397\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2618 - val_loss: 33.3976\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.5360 - val_loss: 37.9020\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.4574 - val_loss: 34.7258\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.4212 - val_loss: 39.7288\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.1943 - val_loss: 36.5374\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.1804 - val_loss: 36.8212\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.3645 - val_loss: 32.4270\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2279 - val_loss: 33.9090\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.2185 - val_loss: 45.2856\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2706 - val_loss: 37.6841\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.3393 - val_loss: 42.3929\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.1509 - val_loss: 35.9697\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.1600 - val_loss: 33.7608\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.1504 - val_loss: 40.0058\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.2878 - val_loss: 37.7420\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2512 - val_loss: 37.4290\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2137 - val_loss: 41.1459\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.2191 - val_loss: 37.4623\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.1098 - val_loss: 34.5297\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.1047 - val_loss: 38.2519\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9756 - val_loss: 38.4391\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.1270 - val_loss: 44.7358\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.0042 - val_loss: 33.6496\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9408 - val_loss: 35.8446\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.0066 - val_loss: 33.8483\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.0287 - val_loss: 38.8039\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.2829 - val_loss: 47.1156\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.1370 - val_loss: 42.4645\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 21.1169 - val_loss: 35.2476\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 21.0154 - val_loss: 33.7369\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9014 - val_loss: 34.7843\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.8685 - val_loss: 45.8795\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9075 - val_loss: 40.4890\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.8712 - val_loss: 37.6378\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.8722 - val_loss: 35.6052\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9100 - val_loss: 41.7030\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9052 - val_loss: 33.3440\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.7754 - val_loss: 35.0161\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.8120 - val_loss: 53.2198\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9309 - val_loss: 43.2988\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.9320 - val_loss: 34.8816\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.8058 - val_loss: 35.4991\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.8138 - val_loss: 32.8584\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 20.7816 - val_loss: 50.9560\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.8734 - val_loss: 38.4461\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7632 - val_loss: 42.3638\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.8355 - val_loss: 35.4417\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.8282 - val_loss: 33.3756\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.7287 - val_loss: 34.8849\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7618 - val_loss: 35.5213\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.6695 - val_loss: 36.4871\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7617 - val_loss: 35.2924\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.8079 - val_loss: 38.0835\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7605 - val_loss: 41.5892\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.8508 - val_loss: 34.0512\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7911 - val_loss: 49.7594\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.7275 - val_loss: 39.0413\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5879 - val_loss: 33.4730\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.6352 - val_loss: 34.7963\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5975 - val_loss: 37.0009\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5567 - val_loss: 34.5735\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.6122 - val_loss: 36.5704\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.6118 - val_loss: 37.8458\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.6073 - val_loss: 34.5722\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5161 - val_loss: 34.8231\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5737 - val_loss: 39.3827\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.5311 - val_loss: 38.7254\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5727 - val_loss: 35.6203\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5246 - val_loss: 35.7341\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.6066 - val_loss: 34.8250\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4539 - val_loss: 40.2099\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.6029 - val_loss: 36.9844\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.5668 - val_loss: 37.3363\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.4683 - val_loss: 36.9403\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5534 - val_loss: 33.4090\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4631 - val_loss: 33.6741\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5046 - val_loss: 40.6298\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4804 - val_loss: 37.3315\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.6303 - val_loss: 35.7858\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.4273 - val_loss: 35.3932\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4629 - val_loss: 38.4273\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4838 - val_loss: 32.3200\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4137 - val_loss: 37.2941\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.5044 - val_loss: 37.9013\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4098 - val_loss: 32.1659\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3984 - val_loss: 31.9667\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3588 - val_loss: 36.4965\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4274 - val_loss: 39.6788\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.3568 - val_loss: 36.9047\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.4164 - val_loss: 35.0480\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.3347 - val_loss: 40.7245\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3126 - val_loss: 32.8078\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.3534 - val_loss: 38.2343\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.3452 - val_loss: 36.5315\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3930 - val_loss: 32.3688\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3491 - val_loss: 33.7798\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3391 - val_loss: 32.5467\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.3479 - val_loss: 41.8171\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2920 - val_loss: 36.1189\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3021 - val_loss: 35.0753\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2932 - val_loss: 34.4225\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.2681 - val_loss: 44.3415\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2244 - val_loss: 38.2249\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2574 - val_loss: 36.4775\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2971 - val_loss: 39.5140\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.2184 - val_loss: 32.6208\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.3448 - val_loss: 33.3852\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.1771 - val_loss: 38.4334\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.2608 - val_loss: 34.9847\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2186 - val_loss: 41.2898\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2135 - val_loss: 36.3067\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2057 - val_loss: 37.1461\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2747 - val_loss: 48.1095\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2712 - val_loss: 37.4978\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1917 - val_loss: 36.2431\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1776 - val_loss: 35.2176\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2242 - val_loss: 37.4713\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2233 - val_loss: 48.6644\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.2251 - val_loss: 34.4623\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.1274 - val_loss: 43.0347\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1219 - val_loss: 39.6729\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1836 - val_loss: 34.0189\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0549 - val_loss: 33.7088\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0731 - val_loss: 44.2493\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1316 - val_loss: 32.4968\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0779 - val_loss: 34.5938\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0463 - val_loss: 33.6205\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.0705 - val_loss: 32.8072\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.0944 - val_loss: 38.4944\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.1383 - val_loss: 37.0206\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0848 - val_loss: 35.8274\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.0523 - val_loss: 35.0274\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.0783 - val_loss: 40.8123\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.0569 - val_loss: 37.0600\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.1361 - val_loss: 34.8962\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.0576 - val_loss: 39.6615\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9933 - val_loss: 38.4400\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.0394 - val_loss: 55.4528\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0528 - val_loss: 35.7588\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9817 - val_loss: 39.7809\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 20.0492 - val_loss: 35.1561\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0094 - val_loss: 33.3914\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9464 - val_loss: 39.0064\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 20.0388 - val_loss: 35.9176\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9450 - val_loss: 41.2218\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9463 - val_loss: 37.6416\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9746 - val_loss: 33.6517\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9207 - val_loss: 42.4634\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9684 - val_loss: 33.6904\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 20.0019 - val_loss: 43.6308\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9135 - val_loss: 36.7597\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9556 - val_loss: 34.6190\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9479 - val_loss: 54.7300\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8591 - val_loss: 34.6946\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9330 - val_loss: 34.7332\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8590 - val_loss: 33.6436\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9573 - val_loss: 35.7245\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8263 - val_loss: 39.8873\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9892 - val_loss: 33.8737\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8844 - val_loss: 35.7183\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8914 - val_loss: 39.3044\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9122 - val_loss: 35.5020\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8526 - val_loss: 39.3758\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9448 - val_loss: 34.4595\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8263 - val_loss: 39.5869\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8823 - val_loss: 35.2732\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7951 - val_loss: 39.5889\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.9345 - val_loss: 33.7236\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8316 - val_loss: 42.5019\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.8597 - val_loss: 33.2235\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8044 - val_loss: 33.1985\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8329 - val_loss: 34.5269\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.8591 - val_loss: 37.8716\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.7818 - val_loss: 33.8382\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 19.9129 - val_loss: 34.6985\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 19.7757 - val_loss: 36.5926\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroUKm9cD3wf"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS--HwX9D3wf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfHNI3w7D4JT"
      },
      "outputs": [],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT"
      },
      "outputs": [],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd86928c-2a4b-4846-fc73-8d3f6ff2fea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -2.8481154211553297 \n",
            "MAE:  5.19202535105797 \n",
            "SD:  6.028239517059023\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTKbd7D4JU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a45c165-a6d7-4249-9f92-6ba015418afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -1.5570994055335483 \n",
            "Ensemble_std:  6.041791431733746\n",
            "\n",
            "Ensemble_me:  -1.5570994055335483 \n",
            "Ensemble_std:  6.041791431733746\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "\bBP_hv3_5(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}