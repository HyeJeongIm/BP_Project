{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/_BP_hv3_4(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiiiBla2-j1S"
      },
      "source": [
        "# batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCoux5AOZnK",
        "outputId": "05ec15c5-4d35-4fbf-df2c-7869c9eda218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version :  3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]\n",
            "TensorFlow version :  2.3.0\n",
            "Keras version :  2.4.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러오기\n",
        "# train_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/train_shuffled_raw_v1.mat')\n",
        "# test_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/test_not_shuffled_raw_v1.mat')\n",
        "\n",
        "train_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/new/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/new/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "b570a293-9e53-473a-f3ed-a9b19951a83d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "8f731ca9-0203-46e7-87ab-30015e694029"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "8abc45d0-bd5c-49cd-8d12-1cde358bdca2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "outputs": [],
      "source": [
        "#parameter\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN",
        "outputId": "568fccf2-ddd3-4d12-c665-8ae40eeb79c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8SHBwBOZnO"
      },
      "outputs": [],
      "source": [
        "# model = model1()\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGT6-7NcOZnO",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 12323.2998 - val_loss: 12471.4033\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 12001.0820 - val_loss: 11357.3232\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 11654.6348 - val_loss: 11199.0537\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 11178.9561 - val_loss: 9854.3965\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 10582.4160 - val_loss: 8661.6279\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 9889.9453 - val_loss: 8138.6240\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 9097.0488 - val_loss: 6830.6670\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 8222.8877 - val_loss: 6772.3867\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 7254.0742 - val_loss: 9802.7275\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 6275.6924 - val_loss: 4889.2832\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 4s 26ms/step - loss: 5282.2954 - val_loss: 6515.3125\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 4318.6655 - val_loss: 2133.7983\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 3431.2480 - val_loss: 4881.6865\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 2669.0068 - val_loss: 4795.4858\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 2027.3557 - val_loss: 1096.7604\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 1503.4636 - val_loss: 879.4736\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 1093.6852 - val_loss: 414.4372\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 773.0219 - val_loss: 1224.0492\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 549.2385 - val_loss: 412.9704\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 372.7332 - val_loss: 1025.7153\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 272.4932 - val_loss: 253.7714\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 202.6628 - val_loss: 212.7502\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 160.4766 - val_loss: 345.6348\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 4s 26ms/step - loss: 140.8427 - val_loss: 159.0605\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 130.2790 - val_loss: 152.0345\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 122.3822 - val_loss: 209.5017\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 116.1837 - val_loss: 160.2338\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 113.4850 - val_loss: 163.5751\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 112.9285 - val_loss: 145.6424\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 111.2643 - val_loss: 334.8317\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 110.5725 - val_loss: 144.1883\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 108.3164 - val_loss: 155.2912\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 106.4482 - val_loss: 401.1765\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 107.6122 - val_loss: 133.2672\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 106.0079 - val_loss: 131.4882\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 5s 29ms/step - loss: 103.9775 - val_loss: 601.1456\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 103.4087 - val_loss: 157.7692\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 101.7496 - val_loss: 129.5362\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 101.0061 - val_loss: 119.5060\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 100.2068 - val_loss: 118.0044\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 99.0918 - val_loss: 176.1275\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 8s 48ms/step - loss: 98.1611 - val_loss: 120.2954\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 96.8589 - val_loss: 106.6184\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 96.3812 - val_loss: 111.4874\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 95.4431 - val_loss: 107.2466\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 94.8581 - val_loss: 120.2397\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 94.2222 - val_loss: 115.3680\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 94.2087 - val_loss: 112.8814\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 93.9606 - val_loss: 114.7167\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 92.7615 - val_loss: 117.9276\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 92.5723 - val_loss: 123.8720\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 91.1241 - val_loss: 136.0019\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 90.4798 - val_loss: 147.8774\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 90.4233 - val_loss: 100.6656\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 89.7202 - val_loss: 117.7468\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 6s 37ms/step - loss: 89.3120 - val_loss: 111.7001\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 88.2200 - val_loss: 103.6556\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 88.6837 - val_loss: 136.0662\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 87.7189 - val_loss: 113.9006\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 6s 37ms/step - loss: 88.1303 - val_loss: 164.6059\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 88.1147 - val_loss: 119.5280\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 86.4006 - val_loss: 137.6873\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 6s 37ms/step - loss: 85.1831 - val_loss: 123.7072\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 85.6037 - val_loss: 115.2358\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 84.7208 - val_loss: 135.2913\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 84.2478 - val_loss: 124.2185\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 8s 47ms/step - loss: 84.7014 - val_loss: 97.1457\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 83.6805 - val_loss: 107.1087\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 6s 37ms/step - loss: 83.1959 - val_loss: 115.6831\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 82.9808 - val_loss: 123.7589\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 81.9715 - val_loss: 99.5452\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 81.3279 - val_loss: 106.8125\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 81.2849 - val_loss: 128.3556\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 6s 37ms/step - loss: 81.4045 - val_loss: 108.4920\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 80.5174 - val_loss: 142.4097\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 80.0910 - val_loss: 109.1588\n",
            "Epoch 77/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 3s 16ms/step - loss: 80.2691 - val_loss: 118.7589\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 79.7410 - val_loss: 110.0854\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 79.2271 - val_loss: 115.4147\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 78.7615 - val_loss: 125.3992\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 79.1235 - val_loss: 127.0990\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 77.6955 - val_loss: 117.9695\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 77.3079 - val_loss: 99.8918\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 76.8722 - val_loss: 105.2529\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 8s 46ms/step - loss: 76.6785 - val_loss: 104.4362\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 76.6518 - val_loss: 94.4749\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 76.2925 - val_loss: 96.3577\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 75.4444 - val_loss: 226.8235\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 74.9453 - val_loss: 118.7739\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 74.8783 - val_loss: 95.4241\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 75.0146 - val_loss: 144.7258\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 74.6478 - val_loss: 116.7964\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5762 - val_loss: 101.7591\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.0183 - val_loss: 90.4577\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 73.8611 - val_loss: 90.7673\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 74.1310 - val_loss: 105.9784\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 73.3189 - val_loss: 96.3644\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 72.8210 - val_loss: 124.6508\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 73.7840 - val_loss: 118.2112\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 73.4079 - val_loss: 95.8318\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 72.2305 - val_loss: 110.3373\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 73.5852 - val_loss: 92.9555\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 71.7013 - val_loss: 115.7887\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 72.3436 - val_loss: 109.8712\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 71.3177 - val_loss: 157.3794\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 71.5329 - val_loss: 106.5746\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 71.3097 - val_loss: 89.9429\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 70.8979 - val_loss: 97.9604\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 71.5310 - val_loss: 119.1503\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 71.2006 - val_loss: 98.7076\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.5102 - val_loss: 98.3888\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 70.2148 - val_loss: 110.5340\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 69.7048 - val_loss: 107.6698\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 70.1898 - val_loss: 101.7001\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 69.7362 - val_loss: 129.5565\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.5197 - val_loss: 122.1645\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 69.3613 - val_loss: 106.3461\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 70.8258 - val_loss: 127.2484\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.4776 - val_loss: 101.0756\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.5306 - val_loss: 110.3549\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.0308 - val_loss: 92.0030\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 68.7395 - val_loss: 106.3506\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.6998 - val_loss: 103.8233\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.2817 - val_loss: 142.5760\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.3578 - val_loss: 91.5621\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.4455 - val_loss: 99.9542\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 6s 38ms/step - loss: 68.3319 - val_loss: 147.8121\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.2512 - val_loss: 104.9815\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.7310 - val_loss: 162.8170\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 70.0135 - val_loss: 103.6208\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.0050 - val_loss: 108.6510\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.7419 - val_loss: 103.1471\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.1836 - val_loss: 92.6452\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.4314 - val_loss: 110.0022\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 6s 37ms/step - loss: 68.2006 - val_loss: 111.2575\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 68.2067 - val_loss: 103.0458\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.6087 - val_loss: 123.1341\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 6s 37ms/step - loss: 67.9044 - val_loss: 102.1151\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 68.6592 - val_loss: 149.3795\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 67.8346 - val_loss: 103.9712\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.4762 - val_loss: 95.8269\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 67.7108 - val_loss: 93.8093\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.3254 - val_loss: 104.4494\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 67.5168 - val_loss: 98.3170\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.2743 - val_loss: 96.7446\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.3212 - val_loss: 97.9862\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.8222 - val_loss: 99.9381\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.1849 - val_loss: 105.3906\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.6555 - val_loss: 100.6762\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.3714 - val_loss: 118.6611\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.4148 - val_loss: 112.5340\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.0975 - val_loss: 105.9506\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.2704 - val_loss: 97.2352\n",
            "Epoch 154/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 3s 16ms/step - loss: 66.0384 - val_loss: 146.3671\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.9457 - val_loss: 103.2531\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 65.9695 - val_loss: 121.6368\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.1001 - val_loss: 100.6689\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.1767 - val_loss: 106.3042\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 65.7822 - val_loss: 128.9893\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.1577 - val_loss: 93.4422\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 65.2717 - val_loss: 90.1595\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 65.0841 - val_loss: 97.5599\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.2451 - val_loss: 94.6186\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.0546 - val_loss: 94.3579\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 64.9952 - val_loss: 110.4702\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 65.2751 - val_loss: 266.2114\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 65.2925 - val_loss: 94.5230\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 64.7043 - val_loss: 109.0623\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 64.9217 - val_loss: 173.5676\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 65.0099 - val_loss: 94.1383\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 64.4507 - val_loss: 104.6233\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 64.4184 - val_loss: 96.1002\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 64.5146 - val_loss: 90.6511\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 64.4794 - val_loss: 92.9536\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 64.1004 - val_loss: 100.0120\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 64.2729 - val_loss: 113.2930\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 63.8391 - val_loss: 91.5992\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 64.3410 - val_loss: 95.8490\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 64.8125 - val_loss: 111.8604\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 64.0087 - val_loss: 110.9520\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 64.0737 - val_loss: 105.8249\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 63.8593 - val_loss: 120.3465\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 63.8440 - val_loss: 99.0649\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 64.0099 - val_loss: 107.1275\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 63.6559 - val_loss: 100.0814\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 63.4335 - val_loss: 171.6258\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 63.4710 - val_loss: 97.1561\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 63.5332 - val_loss: 108.9262\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 63.2533 - val_loss: 89.8613\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 63.3088 - val_loss: 129.7182\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 63.1961 - val_loss: 96.6122\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 63.4014 - val_loss: 93.9817\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 62.9910 - val_loss: 92.5897\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 63.0330 - val_loss: 95.1209\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 63.2259 - val_loss: 89.0760\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 62.8129 - val_loss: 103.4268\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 62.9999 - val_loss: 92.7701\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 63.0558 - val_loss: 92.6159\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 62.5310 - val_loss: 90.0004\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 62.5892 - val_loss: 93.4908\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 62.5357 - val_loss: 94.2836\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 62.4688 - val_loss: 118.8334\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 62.3534 - val_loss: 95.1494\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 62.3693 - val_loss: 120.3107\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 62.4150 - val_loss: 120.7995\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 62.3369 - val_loss: 102.0571\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 62.5565 - val_loss: 111.2009\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 62.4865 - val_loss: 94.8471\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 62.2070 - val_loss: 95.4377\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.9245 - val_loss: 100.5776\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 62.0906 - val_loss: 89.5579\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.9644 - val_loss: 100.1423\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.7833 - val_loss: 94.6127\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.8211 - val_loss: 111.1277\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.9187 - val_loss: 109.3252\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 61.9306 - val_loss: 101.1224\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.5775 - val_loss: 106.1370\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.6565 - val_loss: 86.8811\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 62.1819 - val_loss: 95.9853\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.5990 - val_loss: 90.4635\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.5760 - val_loss: 95.0328\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 61.7421 - val_loss: 98.5189\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.7439 - val_loss: 95.1296\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.2447 - val_loss: 90.8677\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.2825 - val_loss: 90.4206\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.3857 - val_loss: 102.9692\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 61.3410 - val_loss: 106.6338\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.0748 - val_loss: 89.6391\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.3367 - val_loss: 86.6886\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.0084 - val_loss: 121.1899\n",
            "Epoch 231/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 3s 16ms/step - loss: 60.9989 - val_loss: 97.4280\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 61.0898 - val_loss: 98.2435\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.9129 - val_loss: 101.8883\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 60.8967 - val_loss: 90.7465\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.9372 - val_loss: 94.2479\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.9302 - val_loss: 105.8427\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.8824 - val_loss: 91.0163\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.7619 - val_loss: 126.7822\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.7380 - val_loss: 100.6451\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 61.0261 - val_loss: 100.7418\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.6451 - val_loss: 102.7699\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.5126 - val_loss: 120.3656\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.8177 - val_loss: 93.5747\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.6329 - val_loss: 90.5368\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.6728 - val_loss: 95.5544\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.8601 - val_loss: 161.7692\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 60.7881 - val_loss: 92.2878\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.5005 - val_loss: 103.5385\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.8418 - val_loss: 92.6634\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 60.5312 - val_loss: 90.0871\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.3801 - val_loss: 92.9979\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.6666 - val_loss: 86.2601\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.4756 - val_loss: 123.3266\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.2652 - val_loss: 98.5372\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.0440 - val_loss: 92.9789\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 60.1267 - val_loss: 111.9043\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.9882 - val_loss: 95.5147\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.4490 - val_loss: 101.4075\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.3145 - val_loss: 92.1450\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.0071 - val_loss: 108.5735\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.0732 - val_loss: 99.6010\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.1764 - val_loss: 108.8721\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.2917 - val_loss: 86.1784\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.6798 - val_loss: 92.8210\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 59.9235 - val_loss: 102.0716\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 60.0457 - val_loss: 131.1846\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.6932 - val_loss: 101.4391\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 59.8089 - val_loss: 90.5502\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.7050 - val_loss: 106.7690\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.5501 - val_loss: 90.7982\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.7718 - val_loss: 99.8108\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.9138 - val_loss: 109.6328\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.5249 - val_loss: 95.6807\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.5804 - val_loss: 88.8708\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 59.5742 - val_loss: 101.6515\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 59.5545 - val_loss: 91.0877\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.4405 - val_loss: 102.0273\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.5171 - val_loss: 94.1774\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.6456 - val_loss: 96.5517\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.5830 - val_loss: 86.8726\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.5570 - val_loss: 93.5762\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.4610 - val_loss: 88.2022\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.3558 - val_loss: 99.9479\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.1717 - val_loss: 95.8212\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.2689 - val_loss: 100.3013\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.2807 - val_loss: 91.7995\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 59.2169 - val_loss: 95.1622\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.3664 - val_loss: 90.9329\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.2772 - val_loss: 160.0638\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.3707 - val_loss: 94.4643\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.2683 - val_loss: 94.2731\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.2245 - val_loss: 94.9393\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.1566 - val_loss: 98.0076\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.9289 - val_loss: 101.6848\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.9060 - val_loss: 90.0062\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.2247 - val_loss: 116.1662\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.3620 - val_loss: 112.2288\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 59.1515 - val_loss: 104.1001\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.1846 - val_loss: 93.2602\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 58.9252 - val_loss: 95.0577\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 58.9158 - val_loss: 107.6458\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.0204 - val_loss: 93.9589\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.1948 - val_loss: 142.0345\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.7366 - val_loss: 89.7174\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.8149 - val_loss: 120.5742\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.8046 - val_loss: 107.7302\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.8622 - val_loss: 95.7953\n",
            "Epoch 308/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 3s 16ms/step - loss: 58.8692 - val_loss: 88.1711\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.5690 - val_loss: 101.3821\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.7146 - val_loss: 101.5520\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.8648 - val_loss: 122.9411\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.8005 - val_loss: 92.6662\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 59.1330 - val_loss: 98.3038\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 58.9715 - val_loss: 92.0148\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 58.7927 - val_loss: 88.7038\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 58.8834 - val_loss: 92.8334\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.9969 - val_loss: 90.0503\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.0490 - val_loss: 86.2877\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.6174 - val_loss: 95.3891\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.8820 - val_loss: 84.5457\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 58.7171 - val_loss: 94.0601\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 58.6270 - val_loss: 97.6615\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 59.1197 - val_loss: 104.4640\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.6128 - val_loss: 110.1958\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.6402 - val_loss: 86.3755\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 58.5289 - val_loss: 89.6388\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.3400 - val_loss: 86.0572\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.4474 - val_loss: 146.3748\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.5773 - val_loss: 108.3981\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.3686 - val_loss: 181.4992\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.3195 - val_loss: 92.3041\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.2340 - val_loss: 87.0076\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.1857 - val_loss: 91.2149\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.4577 - val_loss: 98.9897\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.4444 - val_loss: 97.8815\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.2872 - val_loss: 86.7221\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.3959 - val_loss: 99.1011\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.5866 - val_loss: 87.9767\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.0240 - val_loss: 85.4731\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 57.9888 - val_loss: 91.5407\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.4010 - val_loss: 94.2842\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.1882 - val_loss: 107.0789\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.3123 - val_loss: 95.0100\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.0937 - val_loss: 89.3903\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.1383 - val_loss: 95.6190\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.1686 - val_loss: 97.7885\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 58.2550 - val_loss: 91.5425\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 58.0279 - val_loss: 98.0036\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.0079 - val_loss: 103.2201\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.2070 - val_loss: 92.9034\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.1247 - val_loss: 107.3467\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.0129 - val_loss: 87.3589\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.2008 - val_loss: 95.5100\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.9273 - val_loss: 96.2853\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 58.3866 - val_loss: 129.2560\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.0600 - val_loss: 102.8128\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.0074 - val_loss: 97.6908\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.8606 - val_loss: 84.5253\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.0712 - val_loss: 108.0004\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.0120 - val_loss: 105.9947\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.8065 - val_loss: 88.8315\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.9306 - val_loss: 129.2043\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 58.0469 - val_loss: 124.3949\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.8942 - val_loss: 96.1720\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.8730 - val_loss: 91.5369\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.8358 - val_loss: 97.2788\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.9258 - val_loss: 92.7514\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.9345 - val_loss: 91.9098\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.5669 - val_loss: 107.6607\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 57.7023 - val_loss: 95.3844\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.8729 - val_loss: 102.7545\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.8772 - val_loss: 100.2932\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 58.1272 - val_loss: 112.7691\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.9938 - val_loss: 88.7837\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.6872 - val_loss: 121.6038\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 58.0293 - val_loss: 120.7002\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.8814 - val_loss: 86.0303\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.9893 - val_loss: 101.6975\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.6900 - val_loss: 96.8721\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.8177 - val_loss: 92.0977\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.8235 - val_loss: 108.8555\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.6238 - val_loss: 129.5555\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.8777 - val_loss: 88.7761\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.4463 - val_loss: 183.9088\n",
            "Epoch 385/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 3s 16ms/step - loss: 57.7269 - val_loss: 90.2761\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.7429 - val_loss: 87.0868\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.6714 - val_loss: 116.4084\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.8737 - val_loss: 92.2541\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.6429 - val_loss: 85.4188\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.6734 - val_loss: 94.3191\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.4551 - val_loss: 89.9189\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.6277 - val_loss: 103.8506\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.7012 - val_loss: 91.6551\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.4055 - val_loss: 90.8150\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.4115 - val_loss: 86.4737\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.3107 - val_loss: 98.8241\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.4248 - val_loss: 96.6851\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.6226 - val_loss: 113.1757\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.3041 - val_loss: 94.6279\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.4497 - val_loss: 98.0290\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.5851 - val_loss: 95.2112\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.7510 - val_loss: 122.3239\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.5727 - val_loss: 88.3207\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.7001 - val_loss: 94.2761\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.4553 - val_loss: 93.9797\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.1544 - val_loss: 94.8115\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.3622 - val_loss: 94.2114\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.4984 - val_loss: 95.9690\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.2726 - val_loss: 86.7911\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 57.3154 - val_loss: 90.6505\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.8030 - val_loss: 116.3400\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.3919 - val_loss: 107.8295\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.3869 - val_loss: 92.1665\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.3889 - val_loss: 91.9119\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.4363 - val_loss: 98.9194\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.4128 - val_loss: 101.4050\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.2497 - val_loss: 115.4159\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.2841 - val_loss: 113.3478\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.2696 - val_loss: 198.0794\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.3808 - val_loss: 99.4232\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.5527 - val_loss: 228.0392\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.4423 - val_loss: 87.8042\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.1903 - val_loss: 85.9388\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.3316 - val_loss: 100.0050\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 56.9694 - val_loss: 91.6942\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.2335 - val_loss: 85.1953\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.2782 - val_loss: 98.2569\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.1491 - val_loss: 100.3919\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.0640 - val_loss: 118.9248\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.2199 - val_loss: 84.7346\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.0193 - val_loss: 100.1142\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.2260 - val_loss: 95.9122\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.9965 - val_loss: 95.2134\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.9786 - val_loss: 89.2527\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.8953 - val_loss: 86.2709\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.0009 - val_loss: 94.9873\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 56.9543 - val_loss: 103.0973\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 57.1162 - val_loss: 93.0128\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 56.9619 - val_loss: 118.6617\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.8544 - val_loss: 91.6338\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.9806 - val_loss: 108.5485\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.9118 - val_loss: 105.6823\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.8721 - val_loss: 90.2348\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.8473 - val_loss: 95.3928\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.6507 - val_loss: 96.5370\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 57.0198 - val_loss: 87.1400\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.0727 - val_loss: 97.9080\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.0144 - val_loss: 95.9785\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.9115 - val_loss: 90.2560\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.8703 - val_loss: 114.7752\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.9689 - val_loss: 90.5946\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.0469 - val_loss: 101.5917\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.8522 - val_loss: 116.5312\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7984 - val_loss: 90.2560\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7852 - val_loss: 87.6870\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 57.2106 - val_loss: 97.4259\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.9983 - val_loss: 89.0435\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.9126 - val_loss: 95.8031\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 56.7096 - val_loss: 109.0873\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7547 - val_loss: 85.5195\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7845 - val_loss: 97.6569\n",
            "Epoch 462/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7749 - val_loss: 93.7875\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7555 - val_loss: 85.7370\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.9343 - val_loss: 84.4424\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.8267 - val_loss: 94.1934\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.9416 - val_loss: 87.0864\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.6903 - val_loss: 92.2273\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7985 - val_loss: 85.8529\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.6579 - val_loss: 85.0431\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7469 - val_loss: 109.1010\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7550 - val_loss: 126.5796\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7501 - val_loss: 88.1165\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7976 - val_loss: 87.4840\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.6168 - val_loss: 110.7849\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.6608 - val_loss: 88.1112\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7008 - val_loss: 91.5618\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.6816 - val_loss: 88.5003\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.6695 - val_loss: 92.9051\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.4680 - val_loss: 88.6885\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.6179 - val_loss: 95.8484\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.5437 - val_loss: 92.1722\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.6303 - val_loss: 100.1862\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 56.7141 - val_loss: 95.2838\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7413 - val_loss: 90.3597\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.7320 - val_loss: 102.3220\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.4242 - val_loss: 100.1847\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.5746 - val_loss: 95.3461\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.6452 - val_loss: 109.0442\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.6242 - val_loss: 94.9993\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.5249 - val_loss: 98.3210\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.5715 - val_loss: 101.1032\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.4450 - val_loss: 91.4882\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.5990 - val_loss: 95.8720\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.8649 - val_loss: 93.7679\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.5816 - val_loss: 84.8824\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.3587 - val_loss: 96.4331\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.5162 - val_loss: 86.0541\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 56.5198 - val_loss: 101.3358\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.6895 - val_loss: 101.5459\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 56.4754 - val_loss: 88.4640\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  1.174429792406134 \n",
            "MAE:  7.014545628758179 \n",
            "SD:  9.331917127731725\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su2Sj5jZCSwH",
        "outputId": "4a85f233-7730-4d19-a30a-66ad769ace4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPRh6v-mCSwH",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 12388.9111 - val_loss: 12249.5742\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 12015.1318 - val_loss: 11996.3223\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 11569.1172 - val_loss: 11389.5537\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 10985.5684 - val_loss: 10794.4844\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 10251.5234 - val_loss: 10437.8301\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 9414.0869 - val_loss: 8328.3350\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 8536.7451 - val_loss: 7881.2451\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 7632.5840 - val_loss: 7180.1484\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 6729.9297 - val_loss: 6835.3618\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 5845.3428 - val_loss: 4426.6841\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 5009.3105 - val_loss: 5319.6025\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 4222.0820 - val_loss: 2593.1748\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 3495.3821 - val_loss: 5814.6831\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 2861.5034 - val_loss: 1438.6377\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 2271.2463 - val_loss: 935.3517\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 1780.2751 - val_loss: 1504.7794\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 1374.0813 - val_loss: 1056.3511\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 1040.1599 - val_loss: 340.9102\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 755.4523 - val_loss: 467.5170\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 547.1320 - val_loss: 1168.7452\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 393.6385 - val_loss: 391.0013\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 286.2115 - val_loss: 147.8686\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 219.3649 - val_loss: 238.9062\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 174.3437 - val_loss: 130.3913\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 139.3601 - val_loss: 233.3253\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 120.5864 - val_loss: 293.2098\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 116.3547 - val_loss: 126.9609\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 109.3695 - val_loss: 158.8181\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 107.9261 - val_loss: 118.1602\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 106.9269 - val_loss: 116.2440\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 104.6347 - val_loss: 111.7263\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 102.9602 - val_loss: 113.8165\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 102.1949 - val_loss: 117.7284\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 101.3362 - val_loss: 172.3829\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 99.4788 - val_loss: 106.6288\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 99.4815 - val_loss: 107.0625\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 98.3984 - val_loss: 145.0741\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 97.0528 - val_loss: 114.4324\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 96.0286 - val_loss: 219.1860\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 96.6858 - val_loss: 110.3345\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 95.4040 - val_loss: 116.6877\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 94.5807 - val_loss: 120.7722\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 94.3342 - val_loss: 113.1898\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 94.1823 - val_loss: 126.2517\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 93.8371 - val_loss: 131.5816\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 94.1687 - val_loss: 121.3977\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 92.0630 - val_loss: 117.3388\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 91.8613 - val_loss: 167.9100\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 91.1348 - val_loss: 143.2468\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 90.6717 - val_loss: 117.3266\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 90.3527 - val_loss: 153.2072\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 89.5183 - val_loss: 116.7657\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 89.5716 - val_loss: 113.5314\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 88.7964 - val_loss: 99.2954\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 87.8995 - val_loss: 118.7800\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 87.5510 - val_loss: 102.9576\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 86.6985 - val_loss: 142.5403\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 86.2015 - val_loss: 237.2394\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 87.6534 - val_loss: 166.4907\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 86.9751 - val_loss: 124.6830\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 85.8877 - val_loss: 105.3662\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 85.1267 - val_loss: 98.7262\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 84.3715 - val_loss: 110.7643\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 84.1506 - val_loss: 113.4783\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 84.1687 - val_loss: 113.6275\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 83.6877 - val_loss: 112.7497\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 82.5705 - val_loss: 147.2671\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 82.5563 - val_loss: 125.5399\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 81.8924 - val_loss: 124.7690\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 82.0017 - val_loss: 114.1744\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 81.9072 - val_loss: 126.8269\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 81.9042 - val_loss: 101.9444\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 81.8413 - val_loss: 121.9775\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 81.5510 - val_loss: 124.7501\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 81.1244 - val_loss: 110.2035\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 80.3308 - val_loss: 122.0159\n",
            "Epoch 77/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 3s 16ms/step - loss: 80.1180 - val_loss: 102.4326\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 80.2365 - val_loss: 97.6207\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 79.2455 - val_loss: 116.7721\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 78.8732 - val_loss: 149.2778\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 78.5903 - val_loss: 113.4991\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 78.0301 - val_loss: 109.9105\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 78.2270 - val_loss: 106.4147\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 77.2974 - val_loss: 135.7794\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 77.4419 - val_loss: 104.7140\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 77.2843 - val_loss: 97.2321\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 76.3546 - val_loss: 91.5514\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 76.6813 - val_loss: 105.9771\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 76.0384 - val_loss: 151.4186\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 76.0034 - val_loss: 110.1939\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 76.4979 - val_loss: 153.9900\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 76.0501 - val_loss: 102.1112\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 75.2479 - val_loss: 117.2401\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 74.9608 - val_loss: 120.4239\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 74.8561 - val_loss: 107.4720\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 75.1803 - val_loss: 117.1408\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 74.2199 - val_loss: 97.2104\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 75.2426 - val_loss: 117.0261\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 73.9157 - val_loss: 109.5192\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 74.0881 - val_loss: 113.0950\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 73.7337 - val_loss: 100.3418\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 74.0226 - val_loss: 130.6472\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 73.5363 - val_loss: 206.3465\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 73.0098 - val_loss: 102.5257\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 73.0091 - val_loss: 97.3563\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 73.0335 - val_loss: 113.7782\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 72.4855 - val_loss: 111.4920\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 71.9238 - val_loss: 109.5728\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 71.7853 - val_loss: 104.0157\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 71.5448 - val_loss: 98.2678\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 71.4348 - val_loss: 95.6322\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 72.4226 - val_loss: 147.3423\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 71.4879 - val_loss: 119.3415\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 71.2508 - val_loss: 110.4696\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 71.0141 - val_loss: 92.8970\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 70.4918 - val_loss: 97.2745\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 70.9667 - val_loss: 115.2887\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 70.5630 - val_loss: 98.2759\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 70.4697 - val_loss: 93.1655\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 70.5042 - val_loss: 149.7076\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 70.0707 - val_loss: 98.7481\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.8168 - val_loss: 94.9396\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.8012 - val_loss: 120.0005\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.6335 - val_loss: 132.6609\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.5852 - val_loss: 109.5359\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.4179 - val_loss: 126.6598\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.2440 - val_loss: 93.6179\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.0882 - val_loss: 94.0009\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.5274 - val_loss: 118.4390\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.2514 - val_loss: 132.4442\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.8558 - val_loss: 91.5216\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.7826 - val_loss: 92.0273\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.5786 - val_loss: 96.9851\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.8874 - val_loss: 139.1789\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 68.4858 - val_loss: 114.1719\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.3755 - val_loss: 105.7942\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.5184 - val_loss: 101.1309\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.2085 - val_loss: 101.8910\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.8491 - val_loss: 98.6246\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.4417 - val_loss: 122.9823\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.5715 - val_loss: 96.9358\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.0435 - val_loss: 95.2176\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.4340 - val_loss: 100.4635\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.1289 - val_loss: 109.9472\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.9526 - val_loss: 100.7978\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.0426 - val_loss: 99.9324\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.7608 - val_loss: 124.0735\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 67.6671 - val_loss: 96.5473\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.0880 - val_loss: 125.6125\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 68.3769 - val_loss: 95.2272\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.6686 - val_loss: 102.7508\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.5955 - val_loss: 93.6361\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.9018 - val_loss: 105.8241\n",
            "Epoch 154/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 3s 15ms/step - loss: 67.8867 - val_loss: 135.6662\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.3331 - val_loss: 133.4301\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.2223 - val_loss: 104.6858\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.2600 - val_loss: 104.8898\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.2689 - val_loss: 133.8067\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.9851 - val_loss: 101.2411\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.1567 - val_loss: 126.5960\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.0195 - val_loss: 122.2258\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.2371 - val_loss: 99.7831\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.2954 - val_loss: 111.6055\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.2467 - val_loss: 107.4240\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.4870 - val_loss: 95.9806\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.0742 - val_loss: 105.0804\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.9440 - val_loss: 102.4296\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.8410 - val_loss: 135.5883\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.7455 - val_loss: 117.8638\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.2408 - val_loss: 100.7413\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.6317 - val_loss: 93.6283\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.2017 - val_loss: 94.1545\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.3245 - val_loss: 96.1439\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.6236 - val_loss: 99.4002\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.4677 - val_loss: 94.7208\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 65.8700 - val_loss: 92.4392\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.9113 - val_loss: 107.9446\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.5775 - val_loss: 91.4296\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.0724 - val_loss: 103.2782\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.0256 - val_loss: 144.7842\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.2281 - val_loss: 104.8632\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.3143 - val_loss: 211.1884\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.4542 - val_loss: 97.6387\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.3526 - val_loss: 105.1592\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.1058 - val_loss: 94.9160\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.0172 - val_loss: 106.9765\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.6247 - val_loss: 97.5514\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 4s 25ms/step - loss: 65.8469 - val_loss: 105.2267\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.1894 - val_loss: 89.6862\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.6463 - val_loss: 112.8957\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.6779 - val_loss: 93.7986\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.7287 - val_loss: 107.3330\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.3862 - val_loss: 225.0880\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.1489 - val_loss: 98.1494\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.3196 - val_loss: 117.4845\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.9648 - val_loss: 94.8154\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.6742 - val_loss: 168.1761\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.7189 - val_loss: 93.8004\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.7050 - val_loss: 117.8500\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.0281 - val_loss: 92.5111\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 4s 26ms/step - loss: 65.8244 - val_loss: 98.9218\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 66.0656 - val_loss: 99.3585\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.1785 - val_loss: 92.5867\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.4663 - val_loss: 103.4275\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.6214 - val_loss: 105.1488\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.1577 - val_loss: 94.9594\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.0648 - val_loss: 143.2266\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.5639 - val_loss: 109.8894\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.2522 - val_loss: 109.9768\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.4322 - val_loss: 92.8479\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 65.5783 - val_loss: 110.4836\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 65.1128 - val_loss: 98.1054\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.7853 - val_loss: 105.1386\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.1613 - val_loss: 95.7683\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.7554 - val_loss: 87.1675\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.4886 - val_loss: 174.7213\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.9203 - val_loss: 96.7758\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.6406 - val_loss: 99.1970\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.9589 - val_loss: 92.8571\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.9659 - val_loss: 95.4422\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.3969 - val_loss: 164.1644\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.5466 - val_loss: 92.2948\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.5310 - val_loss: 91.8708\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.5980 - val_loss: 100.7843\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.6678 - val_loss: 93.0023\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.2639 - val_loss: 92.0143\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.6260 - val_loss: 100.6752\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.4828 - val_loss: 102.8213\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.4704 - val_loss: 166.9516\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.4018 - val_loss: 131.6915\n",
            "Epoch 231/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 7ms/step - loss: 64.6583 - val_loss: 109.9106\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.6592 - val_loss: 101.8430\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.2137 - val_loss: 100.7139\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 64.8786 - val_loss: 89.3247\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 64.2069 - val_loss: 141.4962\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.0269 - val_loss: 93.4809\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.8026 - val_loss: 110.0055\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.5526 - val_loss: 106.3287\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.1786 - val_loss: 109.9915\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.8301 - val_loss: 86.9876\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.8927 - val_loss: 96.3167\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.0697 - val_loss: 107.2878\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.6375 - val_loss: 106.8356\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.9950 - val_loss: 89.3904\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.4601 - val_loss: 113.1520\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.6011 - val_loss: 105.7529\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.7917 - val_loss: 97.4080\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.1488 - val_loss: 95.7881\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.5801 - val_loss: 106.4849\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.0588 - val_loss: 97.0896\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.6099 - val_loss: 92.8874\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.8045 - val_loss: 108.5306\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.4623 - val_loss: 115.4362\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.6763 - val_loss: 113.7580\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.4650 - val_loss: 116.3650\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 63.3661 - val_loss: 96.2573\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.1708 - val_loss: 119.9663\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.0747 - val_loss: 90.3392\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.9247 - val_loss: 98.2946\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.1254 - val_loss: 124.0138\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.4404 - val_loss: 125.7244\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.5598 - val_loss: 88.6301\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.2602 - val_loss: 92.2603\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.2775 - val_loss: 93.2152\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.0417 - val_loss: 91.0840\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.8509 - val_loss: 91.0974\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.3258 - val_loss: 101.5792\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.7058 - val_loss: 91.1902\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.4833 - val_loss: 123.7272\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.3210 - val_loss: 111.0304\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.1123 - val_loss: 100.2367\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.8146 - val_loss: 92.6300\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.6287 - val_loss: 93.8726\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.7918 - val_loss: 96.2776\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.6746 - val_loss: 94.8465\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.1040 - val_loss: 102.0229\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.7952 - val_loss: 120.0242\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.8876 - val_loss: 90.7271\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.8429 - val_loss: 101.8567\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.5562 - val_loss: 87.4637\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.5428 - val_loss: 97.9616\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.7735 - val_loss: 92.9227\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 62.5644 - val_loss: 85.9481\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.5676 - val_loss: 95.4627\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.0071 - val_loss: 101.8234\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.4957 - val_loss: 94.3508\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.4134 - val_loss: 99.8021\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.3069 - val_loss: 103.5036\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.6759 - val_loss: 154.4840\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.4467 - val_loss: 108.4786\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.2499 - val_loss: 110.3210\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.5085 - val_loss: 168.1339\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.0956 - val_loss: 98.9114\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.9393 - val_loss: 98.3788\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.0949 - val_loss: 103.5919\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.5516 - val_loss: 104.4828\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.0969 - val_loss: 87.8330\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.3707 - val_loss: 123.1671\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.2403 - val_loss: 91.6911\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.7523 - val_loss: 99.8939\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.8052 - val_loss: 87.5716\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.1023 - val_loss: 124.0829\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.6866 - val_loss: 111.5630\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.5505 - val_loss: 119.5031\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.8988 - val_loss: 86.0295\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.9211 - val_loss: 102.8822\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.9953 - val_loss: 162.8665\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.9304 - val_loss: 109.1811\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.1868 - val_loss: 93.3416\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.0451 - val_loss: 96.1700\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 62.0871 - val_loss: 95.3689\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 61.9939 - val_loss: 102.3984\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 61.6306 - val_loss: 120.8068\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.4439 - val_loss: 88.0775\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.4609 - val_loss: 102.0199\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.5622 - val_loss: 109.1994\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.3854 - val_loss: 94.7426\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.8292 - val_loss: 101.9633\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.7803 - val_loss: 164.8344\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.6474 - val_loss: 170.4461\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.6294 - val_loss: 112.1014\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.8668 - val_loss: 104.1885\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.5141 - val_loss: 132.1044\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.4329 - val_loss: 82.5156\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.1363 - val_loss: 98.3927\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.3028 - val_loss: 109.1362\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.1311 - val_loss: 97.0962\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.4578 - val_loss: 102.1652\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.8241 - val_loss: 95.1871\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.2906 - val_loss: 94.7727\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.0721 - val_loss: 88.8131\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.2347 - val_loss: 106.3879\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.0354 - val_loss: 102.5184\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.2196 - val_loss: 98.6428\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.2243 - val_loss: 90.5989\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.2851 - val_loss: 94.8907\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.4291 - val_loss: 98.3750\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.8898 - val_loss: 110.8030\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.5656 - val_loss: 107.0538\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.1261 - val_loss: 101.4876\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.1097 - val_loss: 103.3404\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.1211 - val_loss: 204.2042\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.3692 - val_loss: 85.3340\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.4833 - val_loss: 95.3477\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.4185 - val_loss: 96.5036\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.3087 - val_loss: 117.8753\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.2983 - val_loss: 145.1403\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.3868 - val_loss: 97.0911\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.9044 - val_loss: 112.5391\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.7413 - val_loss: 109.8035\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.1837 - val_loss: 159.8084\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.8753 - val_loss: 93.7457\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.6919 - val_loss: 103.8838\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 60.7517 - val_loss: 111.9441\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.0643 - val_loss: 87.9148\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.8571 - val_loss: 89.4363\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.6484 - val_loss: 110.1521\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.5582 - val_loss: 91.5533\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.6587 - val_loss: 87.6534\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.5711 - val_loss: 91.5703\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.6494 - val_loss: 112.6750\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.5905 - val_loss: 86.3425\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.7237 - val_loss: 91.0418\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.5148 - val_loss: 89.9002\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.9040 - val_loss: 90.9117\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.7084 - val_loss: 125.7994\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.5936 - val_loss: 90.0454\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.5221 - val_loss: 87.8592\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.3216 - val_loss: 92.7602\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.2784 - val_loss: 89.7856\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.6473 - val_loss: 87.1477\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.5018 - val_loss: 103.9819\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.7999 - val_loss: 114.2372\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.2076 - val_loss: 89.5663\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.2552 - val_loss: 97.7626\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.0152 - val_loss: 91.5231\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.0445 - val_loss: 103.2389\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.0220 - val_loss: 103.8334\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.3150 - val_loss: 99.5015\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.9395 - val_loss: 86.3763\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.2391 - val_loss: 108.0129\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.4017 - val_loss: 103.9264\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.1360 - val_loss: 95.4339\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.9184 - val_loss: 95.6757\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.2712 - val_loss: 96.3617\n",
            "Epoch 386/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 7ms/step - loss: 60.1388 - val_loss: 89.3470\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.2194 - val_loss: 105.9075\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.2770 - val_loss: 94.4362\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.0752 - val_loss: 93.8839\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.5140 - val_loss: 96.0828\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.0655 - val_loss: 93.5880\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.1030 - val_loss: 136.4619\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.9811 - val_loss: 89.6295\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.9213 - val_loss: 89.6718\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.9748 - val_loss: 101.2627\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.6823 - val_loss: 89.9499\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.0805 - val_loss: 96.1950\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.1235 - val_loss: 92.5326\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.8179 - val_loss: 90.7545\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.7447 - val_loss: 130.4881\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.7106 - val_loss: 93.1767\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.8495 - val_loss: 95.8032\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.6410 - val_loss: 92.0020\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.0979 - val_loss: 111.0529\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.8031 - val_loss: 93.3922\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.8437 - val_loss: 88.8006\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 60.1668 - val_loss: 97.1891\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 59.5795 - val_loss: 100.7898\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 59.8214 - val_loss: 98.8468\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.7662 - val_loss: 117.5257\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.8880 - val_loss: 85.8837\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.5002 - val_loss: 91.5277\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3725 - val_loss: 93.2141\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.7091 - val_loss: 90.0536\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.9626 - val_loss: 110.4520\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.8170 - val_loss: 102.9659\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.8695 - val_loss: 124.4103\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.8847 - val_loss: 101.5091\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.8345 - val_loss: 146.2235\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2178 - val_loss: 90.3253\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2759 - val_loss: 99.8456\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3484 - val_loss: 89.9167\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.1197 - val_loss: 91.6555\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3339 - val_loss: 128.8645\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3346 - val_loss: 98.2049\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.4765 - val_loss: 88.1970\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.8362 - val_loss: 96.1718\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.5523 - val_loss: 93.9978\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.4177 - val_loss: 88.5068\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3457 - val_loss: 94.7583\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.5166 - val_loss: 96.7462\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3309 - val_loss: 94.9381\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2967 - val_loss: 85.5342\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2658 - val_loss: 87.3720\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3213 - val_loss: 94.7398\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3686 - val_loss: 124.3511\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.4815 - val_loss: 90.3102\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.4055 - val_loss: 95.8973\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.4711 - val_loss: 96.3983\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.4498 - val_loss: 99.5095\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.5615 - val_loss: 84.3574\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2553 - val_loss: 104.0608\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3479 - val_loss: 102.2443\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.0959 - val_loss: 88.0065\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2278 - val_loss: 100.4123\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.1042 - val_loss: 103.5519\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2828 - val_loss: 98.8363\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2560 - val_loss: 103.8927\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.0693 - val_loss: 113.0024\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2195 - val_loss: 90.3570\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.6638 - val_loss: 94.4301\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.0016 - val_loss: 88.8237\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 59.66 - 1s 7ms/step - loss: 59.5313 - val_loss: 97.1435\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.9213 - val_loss: 87.6784\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.7052 - val_loss: 85.2537\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.0326 - val_loss: 101.0149\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.1281 - val_loss: 93.8422\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.0976 - val_loss: 94.0832\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8672 - val_loss: 107.9905\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.1242 - val_loss: 102.0986\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.9897 - val_loss: 87.9854\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8584 - val_loss: 90.5069\n",
            "Epoch 463/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 7ms/step - loss: 58.6688 - val_loss: 94.7733\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.4991 - val_loss: 126.1152\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.6858 - val_loss: 95.9417\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.9137 - val_loss: 97.4205\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.1564 - val_loss: 104.3757\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3105 - val_loss: 104.5915\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3712 - val_loss: 96.1642\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.9905 - val_loss: 135.9800\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.0824 - val_loss: 89.6384\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.6313 - val_loss: 166.9788\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.6672 - val_loss: 105.3840\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.5980 - val_loss: 105.0455\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.9167 - val_loss: 130.7059\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.7256 - val_loss: 95.5815\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.6677 - val_loss: 94.8724\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8437 - val_loss: 90.8149\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.7623 - val_loss: 90.0306\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8930 - val_loss: 125.9248\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.9396 - val_loss: 88.5222\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8296 - val_loss: 92.7259\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.0117 - val_loss: 108.7865\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8832 - val_loss: 123.0848\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.6048 - val_loss: 90.8008\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.5706 - val_loss: 97.7235\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.5763 - val_loss: 90.9680\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.4296 - val_loss: 94.7180\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.5265 - val_loss: 102.6925\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.4064 - val_loss: 100.6105\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8493 - val_loss: 96.7700\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.7198 - val_loss: 100.9139\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.6835 - val_loss: 95.6642\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.4203 - val_loss: 87.7503\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.4584 - val_loss: 92.0864\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.5012 - val_loss: 109.8705\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.6009 - val_loss: 99.7745\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.6713 - val_loss: 95.9817\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.7221 - val_loss: 107.8254\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8281 - val_loss: 98.8592\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -2.7105438260699484 \n",
            "MAE:  7.527559899548526 \n",
            "SD:  9.566198059625961\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "outputId": "4a85f233-7730-4d19-a30a-66ad769ace4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_24 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXAOqd2CTsa",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 12123.9844 - val_loss: 12135.0049\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 11633.0674 - val_loss: 11092.9980\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 11029.5332 - val_loss: 10867.0547\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 10253.8184 - val_loss: 10243.3008\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 9264.1592 - val_loss: 7088.0093\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 8213.6455 - val_loss: 10094.0908\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 7139.6909 - val_loss: 8278.6348\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 6077.7539 - val_loss: 3799.7434\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 5058.2178 - val_loss: 3401.3420\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 4098.4077 - val_loss: 2808.1340\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 3246.7991 - val_loss: 1100.0491\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 2492.0891 - val_loss: 959.9736\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 1890.4514 - val_loss: 727.8723\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 1394.9647 - val_loss: 466.7764\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 981.5717 - val_loss: 1359.3951\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 684.2509 - val_loss: 511.7067\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 480.6955 - val_loss: 249.6592\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 333.1678 - val_loss: 522.3969\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 242.5820 - val_loss: 181.0274\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 178.7056 - val_loss: 436.8556\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 143.5137 - val_loss: 193.1177\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 125.2322 - val_loss: 242.7908\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 118.0652 - val_loss: 148.0601\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 113.0126 - val_loss: 152.7311\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 109.0936 - val_loss: 293.3332\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 107.6421 - val_loss: 116.4727\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 105.0177 - val_loss: 127.9008\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 102.7510 - val_loss: 111.8192\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 101.8804 - val_loss: 120.1805\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 100.8766 - val_loss: 110.2095\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 100.4817 - val_loss: 115.9580\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 99.5537 - val_loss: 142.0879\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 98.5853 - val_loss: 121.4516\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 97.5232 - val_loss: 130.0916\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 95.9222 - val_loss: 130.2135\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.9272 - val_loss: 108.0183\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 94.9814 - val_loss: 107.2234\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 94.1490 - val_loss: 105.9600\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 93.8890 - val_loss: 161.9571\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 93.3318 - val_loss: 101.9311\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 92.7216 - val_loss: 139.5841\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 91.7975 - val_loss: 293.6137\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 91.2286 - val_loss: 162.7978\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 91.1026 - val_loss: 129.7529\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 90.2855 - val_loss: 101.6319\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 89.13 - 1s 7ms/step - loss: 89.1310 - val_loss: 123.3113\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 87.9666 - val_loss: 105.3297\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 87.7211 - val_loss: 150.9127\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 86.9761 - val_loss: 105.9530\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 86.1032 - val_loss: 106.2062\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 86.3906 - val_loss: 245.8653\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 85.5081 - val_loss: 109.6578\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 85.1529 - val_loss: 114.7826\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 84.1916 - val_loss: 157.0123\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 84.9904 - val_loss: 136.3782\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 83.7591 - val_loss: 164.4604\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 83.2952 - val_loss: 94.4981\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 82.4081 - val_loss: 109.0529\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 81.9799 - val_loss: 96.0387\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 82.1719 - val_loss: 116.9753\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 82.1467 - val_loss: 111.1866\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 81.4049 - val_loss: 116.7349\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 81.0528 - val_loss: 119.0322\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 80.6992 - val_loss: 132.9915\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 80.0481 - val_loss: 99.2456\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 79.8619 - val_loss: 96.0289\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 79.4143 - val_loss: 95.7739\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 79.2273 - val_loss: 112.8317\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 79.5145 - val_loss: 128.5854\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 78.4384 - val_loss: 151.2768\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 77.9969 - val_loss: 110.4987\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 77.7517 - val_loss: 105.9532\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 77.5022 - val_loss: 105.3601\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 77.3594 - val_loss: 123.6112\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 77.0504 - val_loss: 134.4962\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 77.4350 - val_loss: 97.0509\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 76.8539 - val_loss: 102.5734\n",
            "Epoch 78/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 7ms/step - loss: 76.3101 - val_loss: 105.6451\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.2475 - val_loss: 95.8003\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.6565 - val_loss: 105.8187\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.3719 - val_loss: 114.8302\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.6054 - val_loss: 101.5883\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.3099 - val_loss: 164.1155\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.4343 - val_loss: 91.2662\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 74.6482 - val_loss: 99.3103\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 74.1672 - val_loss: 185.6378\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 74.1534 - val_loss: 98.1326\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 74.2556 - val_loss: 122.7576\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 73.4099 - val_loss: 102.1580\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 73.1155 - val_loss: 103.1998\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 73.8331 - val_loss: 99.8866\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 73.1454 - val_loss: 96.7803\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 72.6390 - val_loss: 129.4755\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 72.9268 - val_loss: 106.6023\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 72.6520 - val_loss: 114.4288\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 72.6406 - val_loss: 106.5208\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 72.1561 - val_loss: 111.4469\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 72.5593 - val_loss: 113.2806\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 72.1431 - val_loss: 101.8315\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6057 - val_loss: 103.6636\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 71.2383 - val_loss: 129.0658\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 71.8273 - val_loss: 199.2121\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 71.3705 - val_loss: 94.6571\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 71.4004 - val_loss: 96.0350\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 71.1886 - val_loss: 130.3512\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 70.6605 - val_loss: 97.9547\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 70.2823 - val_loss: 104.7336\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 70.9473 - val_loss: 95.9831\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 70.3480 - val_loss: 134.9400\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0224 - val_loss: 89.8077\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 69.6314 - val_loss: 89.8477\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 69.3270 - val_loss: 100.0686\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 69.0950 - val_loss: 120.7646\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 69.6831 - val_loss: 90.8136\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 69.6649 - val_loss: 99.0984\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 69.2066 - val_loss: 98.2350\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 69.1169 - val_loss: 110.0838\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 69.1440 - val_loss: 95.3665\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 69.2532 - val_loss: 131.6328\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 68.8274 - val_loss: 101.7572\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 68.9033 - val_loss: 141.2974\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 68.5140 - val_loss: 104.9683\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 69.2692 - val_loss: 98.4246\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 68.4406 - val_loss: 105.0021\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 68.1853 - val_loss: 88.2934\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 67.5917 - val_loss: 102.7912\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 67.6166 - val_loss: 99.8504\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 67.5430 - val_loss: 125.2519\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 67.4828 - val_loss: 99.2030\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 67.7627 - val_loss: 91.1621\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 67.7144 - val_loss: 96.9297\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 67.4204 - val_loss: 223.2071\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 67.6004 - val_loss: 104.8048\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 67.2471 - val_loss: 134.4523\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 67.1186 - val_loss: 85.4611\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 66.9443 - val_loss: 91.4642\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 66.6346 - val_loss: 89.5964\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 66.9317 - val_loss: 147.8830\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 66.8881 - val_loss: 93.8348\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 66.7379 - val_loss: 102.3264\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 66.7422 - val_loss: 138.4441\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 66.2339 - val_loss: 104.6578\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.9983 - val_loss: 93.5157\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.9579 - val_loss: 90.0671\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 66.0982 - val_loss: 97.3279\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 66.0163 - val_loss: 88.4858\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 66.5423 - val_loss: 93.4417\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 65.6688 - val_loss: 122.8901\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.4682 - val_loss: 91.8821\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.6413 - val_loss: 101.9423\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.4441 - val_loss: 104.4012\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.1801 - val_loss: 105.6225\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.4223 - val_loss: 94.6430\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.1230 - val_loss: 126.9989\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.2578 - val_loss: 119.1071\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.4459 - val_loss: 92.5829\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.3485 - val_loss: 126.4165\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.4828 - val_loss: 121.9715\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 65.3658 - val_loss: 124.2958\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 64.8708 - val_loss: 98.4632\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.9791 - val_loss: 113.6211\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 65.1252 - val_loss: 199.4585\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.7773 - val_loss: 104.1344\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.0570 - val_loss: 95.4896\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.4263 - val_loss: 99.3890\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 64.0955 - val_loss: 96.5505\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.3288 - val_loss: 136.1107\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.6470 - val_loss: 110.7830\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.6519 - val_loss: 100.1791\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.2038 - val_loss: 98.3807\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.9886 - val_loss: 113.3256\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.0593 - val_loss: 116.7911\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.6915 - val_loss: 97.2642\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.7638 - val_loss: 109.2913\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 64.0081 - val_loss: 94.3414\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 63.5708 - val_loss: 100.5443\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.6361 - val_loss: 94.5529\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.4661 - val_loss: 121.4146\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.5942 - val_loss: 162.2145\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.1145 - val_loss: 118.8753\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.5152 - val_loss: 97.9320\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.3936 - val_loss: 106.6845\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.2893 - val_loss: 109.7632\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.0320 - val_loss: 107.6253\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.8513 - val_loss: 126.7697\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.1200 - val_loss: 95.6313\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.2388 - val_loss: 147.4256\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.9184 - val_loss: 92.1969\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.9859 - val_loss: 94.9550\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.2041 - val_loss: 136.2394\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.9776 - val_loss: 108.8061\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.0094 - val_loss: 125.5962\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 62.8639 - val_loss: 100.1339\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.6673 - val_loss: 121.3341\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.4594 - val_loss: 93.5338\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.6491 - val_loss: 91.0719\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.4638 - val_loss: 111.9687\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.4983 - val_loss: 86.0641\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.4515 - val_loss: 89.2790\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.4174 - val_loss: 89.4801\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.9199 - val_loss: 90.7112\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 63.1788 - val_loss: 130.4690\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.5816 - val_loss: 101.0596\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.6963 - val_loss: 92.4033\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.3362 - val_loss: 86.8251\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.2571 - val_loss: 91.9201\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.5795 - val_loss: 88.3788\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.0443 - val_loss: 94.1903\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.0187 - val_loss: 100.4709\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.0208 - val_loss: 99.2683\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.0265 - val_loss: 121.5901\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.8970 - val_loss: 88.6124\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.7397 - val_loss: 87.1315\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.1720 - val_loss: 103.3235\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.0816 - val_loss: 92.8007\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.9251 - val_loss: 107.8583\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.1093 - val_loss: 99.6469\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.8370 - val_loss: 101.1294\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.0774 - val_loss: 147.0773\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.8262 - val_loss: 97.8121\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.8435 - val_loss: 93.3676\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.7516 - val_loss: 92.0618\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.1019 - val_loss: 88.7117\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.8242 - val_loss: 100.2217\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 62.1479 - val_loss: 94.2952\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.2531 - val_loss: 89.3067\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.6147 - val_loss: 92.2232\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.6217 - val_loss: 106.4894\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.4752 - val_loss: 92.8540\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.0274 - val_loss: 96.5062\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.6378 - val_loss: 126.2144\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.1347 - val_loss: 122.2924\n",
            "Epoch 233/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 7ms/step - loss: 61.1643 - val_loss: 88.2505\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.4713 - val_loss: 91.8024\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.2848 - val_loss: 133.4335\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.9970 - val_loss: 150.2687\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.8342 - val_loss: 95.8832\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.9373 - val_loss: 105.2481\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.0687 - val_loss: 91.7281\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 61.5743 - val_loss: 150.0017\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.7898 - val_loss: 105.3815\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.8521 - val_loss: 95.8796\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.8338 - val_loss: 127.1225\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.6341 - val_loss: 102.9526\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.9326 - val_loss: 97.7156\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.6346 - val_loss: 110.8626\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.5285 - val_loss: 85.9894\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.9656 - val_loss: 107.3360\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.4785 - val_loss: 83.9242\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.4697 - val_loss: 86.7095\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.7542 - val_loss: 104.9147\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.4103 - val_loss: 173.5347\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.7389 - val_loss: 177.2896\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.6103 - val_loss: 90.7921\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.1425 - val_loss: 145.7993\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.8431 - val_loss: 112.4964\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.4598 - val_loss: 92.3112\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.4729 - val_loss: 90.2346\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.3603 - val_loss: 100.6437\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.7727 - val_loss: 94.9827\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.4940 - val_loss: 96.2328\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.4599 - val_loss: 90.7004\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.3015 - val_loss: 97.6792\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.9217 - val_loss: 105.3637\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.9482 - val_loss: 95.1415\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.3728 - val_loss: 85.4399\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.1601 - val_loss: 88.7588\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.7941 - val_loss: 103.7210\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.7188 - val_loss: 97.3454\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.9286 - val_loss: 90.0869\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.8314 - val_loss: 89.0458\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.8417 - val_loss: 236.5004\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.9490 - val_loss: 121.4142\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.9238 - val_loss: 94.2514\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.4894 - val_loss: 89.8905\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 60.0125 - val_loss: 87.8585\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.7864 - val_loss: 87.2560\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.7228 - val_loss: 87.5287\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.9490 - val_loss: 100.9994\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.6620 - val_loss: 105.3225\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.5545 - val_loss: 87.9732\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.4367 - val_loss: 125.6872\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.5429 - val_loss: 92.5202\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.5393 - val_loss: 89.2202\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.5861 - val_loss: 93.7861\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.5310 - val_loss: 88.6895\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.5878 - val_loss: 96.8906\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2905 - val_loss: 119.7170\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.1890 - val_loss: 110.6131\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.1894 - val_loss: 108.9363\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.4880 - val_loss: 95.0558\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3277 - val_loss: 102.6798\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2772 - val_loss: 88.0383\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2662 - val_loss: 94.8818\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.3003 - val_loss: 106.8280\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.4502 - val_loss: 112.1636\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.0194 - val_loss: 100.5700\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2108 - val_loss: 116.7970\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.2089 - val_loss: 92.4420\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.9902 - val_loss: 98.5277\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8253 - val_loss: 85.1683\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8127 - val_loss: 104.2519\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8508 - val_loss: 89.7078\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.7965 - val_loss: 90.2255\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8537 - val_loss: 98.0778\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 59.1822 - val_loss: 92.1460\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.9841 - val_loss: 113.0400\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.7044 - val_loss: 98.6467\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.7446 - val_loss: 134.9069\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.8400 - val_loss: 89.6318\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.4620 - val_loss: 96.2048\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.7232 - val_loss: 126.9647\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.4120 - val_loss: 102.1439\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 58.4480 - val_loss: 88.6147\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.6728 - val_loss: 87.6286\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.9992 - val_loss: 128.5385\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.5468 - val_loss: 95.8086\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.5704 - val_loss: 91.9887\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.5835 - val_loss: 114.1848\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.4744 - val_loss: 97.5875\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.7012 - val_loss: 91.9618\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.3773 - val_loss: 112.3560\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.5073 - val_loss: 87.4104\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.3657 - val_loss: 92.0708\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.2790 - val_loss: 107.1808\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.5090 - val_loss: 167.7322\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.6367 - val_loss: 96.5823\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.4070 - val_loss: 101.1724\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.4198 - val_loss: 108.8939\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.0797 - val_loss: 91.6534\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.4673 - val_loss: 88.5153\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.3710 - val_loss: 158.4416\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.2269 - val_loss: 82.5102\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.0563 - val_loss: 95.0045\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.1696 - val_loss: 99.4525\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.0245 - val_loss: 97.9423\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.2117 - val_loss: 98.1485\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.2295 - val_loss: 93.8031\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.2799 - val_loss: 96.8333\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.1635 - val_loss: 95.9097\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 57.9718 - val_loss: 95.6586\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.0829 - val_loss: 93.8914\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.0556 - val_loss: 104.8128\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.0949 - val_loss: 106.6534\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.2472 - val_loss: 132.9231\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 57.9094 - val_loss: 101.9950\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 57.9460 - val_loss: 99.8605\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.1333 - val_loss: 86.5973\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 58.0022 - val_loss: 107.5296\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 57.7311 - val_loss: 100.1415\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 57.7801 - val_loss: 88.2102\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 57.7673 - val_loss: 87.0689\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 57.6855 - val_loss: 85.1928\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 57.8570 - val_loss: 140.2902\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 58.2267 - val_loss: 120.7973\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 57.9964 - val_loss: 94.5490\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 57.7005 - val_loss: 100.0020\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 57.5778 - val_loss: 121.4082\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 57.6428 - val_loss: 90.2473\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 58.1223 - val_loss: 87.8905\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.5948 - val_loss: 97.7171\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 57.6396 - val_loss: 115.2391\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 57.6240 - val_loss: 157.2339\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.6930 - val_loss: 91.2784\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 57.7539 - val_loss: 105.9405\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.6088 - val_loss: 103.0596\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.8437 - val_loss: 92.6871\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.6957 - val_loss: 100.4673\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.3271 - val_loss: 126.0441\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.6496 - val_loss: 87.2215\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.5923 - val_loss: 88.3534\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.3437 - val_loss: 103.2338\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.6888 - val_loss: 97.9804\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 57.4300 - val_loss: 112.8010\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.3704 - val_loss: 96.2604\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.6045 - val_loss: 99.8379\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.4723 - val_loss: 111.1603\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 57.5118 - val_loss: 90.4015\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.4864 - val_loss: 89.3527\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 57.4802 - val_loss: 92.4936\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.5199 - val_loss: 89.4042\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.5032 - val_loss: 98.9375\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 57.4085 - val_loss: 90.5888\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.4530 - val_loss: 118.0394\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.1990 - val_loss: 89.9691\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.1088 - val_loss: 100.0002\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.4952 - val_loss: 104.6126\n",
            "Epoch 388/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 14ms/step - loss: 57.5897 - val_loss: 110.3590\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.0247 - val_loss: 93.2692\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.0421 - val_loss: 90.6512\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.1075 - val_loss: 108.2897\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.2592 - val_loss: 92.1239\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.1357 - val_loss: 89.5887\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.1087 - val_loss: 106.5385\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.9008 - val_loss: 95.0540\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 57.1968 - val_loss: 100.8473\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.0478 - val_loss: 113.2897\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.0394 - val_loss: 131.2553\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.1867 - val_loss: 91.5703\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.0640 - val_loss: 96.4072\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.0268 - val_loss: 105.2936\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 56.9834 - val_loss: 102.1606\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.9834 - val_loss: 97.7983\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.9015 - val_loss: 102.9391\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.3322 - val_loss: 93.7384\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 57.0117 - val_loss: 87.6282\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.9739 - val_loss: 93.6891\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 57.0424 - val_loss: 88.3677\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.9839 - val_loss: 90.1053\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.8169 - val_loss: 86.4090\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.6544 - val_loss: 112.5866\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.9475 - val_loss: 93.0019\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.9378 - val_loss: 94.4149\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.8262 - val_loss: 87.5628\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 56.9704 - val_loss: 94.3051\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.9734 - val_loss: 91.8023\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.4151 - val_loss: 127.3181\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.0651 - val_loss: 90.2158\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.6980 - val_loss: 99.3727\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.1720 - val_loss: 92.3356\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.7377 - val_loss: 103.7031\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.3801 - val_loss: 94.0378\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.7022 - val_loss: 113.4446\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.6165 - val_loss: 98.3087\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.7063 - val_loss: 93.3400\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.8933 - val_loss: 112.9353\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.6409 - val_loss: 93.5034\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.5532 - val_loss: 95.6364\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.8244 - val_loss: 92.9048\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.9418 - val_loss: 85.6614\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.8149 - val_loss: 91.6078\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.7308 - val_loss: 94.3754\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.6458 - val_loss: 94.0505\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.9580 - val_loss: 102.2302\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.7942 - val_loss: 92.0755\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.6008 - val_loss: 85.3606\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.7365 - val_loss: 88.8768\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 56.8095 - val_loss: 103.0415\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.7450 - val_loss: 99.6166\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.5777 - val_loss: 86.5023\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.7888 - val_loss: 104.6882\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.5774 - val_loss: 94.5987\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.4304 - val_loss: 88.2071\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 56.49 - 2s 14ms/step - loss: 56.4870 - val_loss: 90.8732\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.3627 - val_loss: 94.2674\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.6946 - val_loss: 90.5500\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.7890 - val_loss: 195.8871\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.7367 - val_loss: 92.3893\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.6021 - val_loss: 93.9387\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.5161 - val_loss: 111.1607\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.7528 - val_loss: 96.5332\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 56.3439 - val_loss: 99.1184\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.3639 - val_loss: 89.5530\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.2294 - val_loss: 90.3740\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.3969 - val_loss: 100.0582\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.4351 - val_loss: 105.6226\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.5852 - val_loss: 94.7564\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.0672 - val_loss: 102.9624\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.3534 - val_loss: 89.4386\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.1971 - val_loss: 86.7598\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.1781 - val_loss: 102.9466\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.5191 - val_loss: 87.9895\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.4140 - val_loss: 107.5994\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.5390 - val_loss: 99.3126\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.4149 - val_loss: 91.3474\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.2681 - val_loss: 94.1295\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.3160 - val_loss: 91.8763\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 56.4371 - val_loss: 91.9095\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.3542 - val_loss: 89.5867\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.3173 - val_loss: 90.5787\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.3829 - val_loss: 89.3251\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.5150 - val_loss: 98.7631\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.3209 - val_loss: 87.0221\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.2877 - val_loss: 85.2687\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.4373 - val_loss: 97.9773\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.2435 - val_loss: 92.6804\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.1456 - val_loss: 103.8286\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.2446 - val_loss: 94.2251\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.1743 - val_loss: 96.8176\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.1441 - val_loss: 91.5928\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 56.4430 - val_loss: 95.3705\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 56.3719 - val_loss: 88.5643\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 56.2069 - val_loss: 113.9722\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 56.4500 - val_loss: 95.4350\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 57.4557 - val_loss: 88.6980\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 55.9429 - val_loss: 98.3810\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 56.0334 - val_loss: 96.8299\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.0862 - val_loss: 82.9370\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.1191 - val_loss: 89.4137\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.3893 - val_loss: 101.4993\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 57.0922 - val_loss: 116.0486\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.5768 - val_loss: 104.7538\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.1048 - val_loss: 97.9498\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.2706 - val_loss: 108.9930\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 55.9885 - val_loss: 89.3889\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.1153 - val_loss: 108.2844\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.2865 - val_loss: 107.2871\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.1467 - val_loss: 84.6899\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.3564 - val_loss: 120.6094\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 56.2200 - val_loss: 97.8005\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696v_fuFCTsa",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -3.246382805285643 \n",
            "MAE:  7.607974872071703 \n",
            "SD:  9.341384543331005\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdZF2osWCUQS",
        "outputId": "342d6fab-0403-405e-f3b7-c42f0d451c9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  -1.5941656129831525 \n",
            "Ensemble_std:  9.41316657689623\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_36 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 3681.2466 - val_loss: 3693.5564\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 3472.0801 - val_loss: 3219.0527\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 3199.6089 - val_loss: 2873.2378\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2863.7527 - val_loss: 2611.9648\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2473.2131 - val_loss: 2022.1865\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2015.3867 - val_loss: 1176.2686\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 1592.1071 - val_loss: 1612.4009\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 1210.7032 - val_loss: 1578.8312\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 887.2531 - val_loss: 351.3552\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 618.3522 - val_loss: 196.3984\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 413.3080 - val_loss: 116.3371\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 264.5070 - val_loss: 307.0477\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 164.5240 - val_loss: 165.3767\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 103.8018 - val_loss: 184.4109\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.1954 - val_loss: 46.7679\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 52.1300 - val_loss: 50.9324\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 43.8689 - val_loss: 51.7690\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 40.2430 - val_loss: 48.4487\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 38.9360 - val_loss: 43.6685\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.7301 - val_loss: 49.4880\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.2133 - val_loss: 48.0948\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 36.9514 - val_loss: 56.0650\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 36.2111 - val_loss: 43.2732\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 36.1999 - val_loss: 40.3813\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 35.8424 - val_loss: 49.3844\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 35.2686 - val_loss: 55.1104\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.8892 - val_loss: 41.5660\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 35.0355 - val_loss: 46.6166\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.7064 - val_loss: 45.2432\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.8502 - val_loss: 73.6511\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.5207 - val_loss: 45.3935\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 33.8669 - val_loss: 61.7951\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.6082 - val_loss: 52.9305\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 33.6554 - val_loss: 51.3038\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.4141 - val_loss: 45.4694\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.5417 - val_loss: 102.3825\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.1175 - val_loss: 45.8481\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 32.5693 - val_loss: 40.9251\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.5016 - val_loss: 39.8300\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.2915 - val_loss: 39.4916\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.8811 - val_loss: 36.8906\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.8798 - val_loss: 37.3380\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.8454 - val_loss: 37.1231\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.9733 - val_loss: 54.5582\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 31.6435 - val_loss: 52.8671\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.4340 - val_loss: 58.6693\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 31.3924 - val_loss: 74.1196\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 31.4426 - val_loss: 58.9249\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 31.4955 - val_loss: 45.5016\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.3208 - val_loss: 55.5660\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.9450 - val_loss: 39.6501\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.7538 - val_loss: 39.1588\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.5991 - val_loss: 41.3021\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.5816 - val_loss: 48.2934\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.5225 - val_loss: 52.7492\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.4565 - val_loss: 52.6601\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.4516 - val_loss: 45.8660\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.3886 - val_loss: 47.3277\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.1811 - val_loss: 44.4419\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.3033 - val_loss: 36.3361\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2103 - val_loss: 45.5537\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 29.9232 - val_loss: 36.5204\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.0126 - val_loss: 41.0423\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 29.8546 - val_loss: 39.0786\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 29.9377 - val_loss: 37.8334\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 29.6754 - val_loss: 50.3871\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 29.4648 - val_loss: 59.2507\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 29.6125 - val_loss: 46.3738\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 29.7438 - val_loss: 39.1594\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2584 - val_loss: 35.2569\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.0502 - val_loss: 43.8857\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 29.3506 - val_loss: 41.8614\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9367 - val_loss: 50.4276\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 29.0643 - val_loss: 41.5215\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.9317 - val_loss: 40.0245\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.7964 - val_loss: 36.2231\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.6033 - val_loss: 55.0442\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.9567 - val_loss: 39.0141\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.9614 - val_loss: 38.5533\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.7727 - val_loss: 38.5466\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5703 - val_loss: 34.5101\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7002 - val_loss: 41.5471\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5354 - val_loss: 48.9513\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4111 - val_loss: 37.3595\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.3250 - val_loss: 48.8520\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3920 - val_loss: 53.5645\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.3852 - val_loss: 49.8508\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.1905 - val_loss: 37.0547\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.0093 - val_loss: 42.5975\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1266 - val_loss: 47.0889\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.0443 - val_loss: 34.1965\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.9680 - val_loss: 37.6177\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2170 - val_loss: 39.9900\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2688 - val_loss: 48.5460\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.3195 - val_loss: 47.4391\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.1320 - val_loss: 41.6868\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.9165 - val_loss: 37.1642\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.6846 - val_loss: 41.7247\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5924 - val_loss: 36.7012\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.6253 - val_loss: 39.4440\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.5587 - val_loss: 41.3025\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.6406 - val_loss: 37.5834\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.5150 - val_loss: 38.8979\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.3888 - val_loss: 37.4124\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2973 - val_loss: 36.7216\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2971 - val_loss: 48.3460\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4339 - val_loss: 49.7616\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.2485 - val_loss: 38.4550\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.1116 - val_loss: 38.8875\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.1494 - val_loss: 49.0635\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1899 - val_loss: 37.4928\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0177 - val_loss: 36.8917\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.0951 - val_loss: 41.7178\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9755 - val_loss: 42.9088\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.0719 - val_loss: 51.0182\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8513 - val_loss: 39.5460\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.0037 - val_loss: 43.0369\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9415 - val_loss: 44.6687\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.8549 - val_loss: 34.5550\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9399 - val_loss: 58.0337\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.7859 - val_loss: 37.5294\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.7565 - val_loss: 37.3516\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8325 - val_loss: 44.8354\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6657 - val_loss: 40.1925\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.6856 - val_loss: 42.0934\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.7504 - val_loss: 42.4775\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6977 - val_loss: 41.5719\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7567 - val_loss: 37.7279\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7347 - val_loss: 53.2751\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.6670 - val_loss: 36.2910\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7034 - val_loss: 42.3148\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.7135 - val_loss: 38.0326\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.4505 - val_loss: 35.9926\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.3467 - val_loss: 61.4193\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4025 - val_loss: 36.9263\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.5483 - val_loss: 45.4390\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.2386 - val_loss: 35.5110\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.0892 - val_loss: 36.0006\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.0607 - val_loss: 39.5758\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.1675 - val_loss: 44.1153\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1503 - val_loss: 62.4648\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.0943 - val_loss: 39.3342\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.1710 - val_loss: 37.3358\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.9877 - val_loss: 35.4735\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.9961 - val_loss: 36.7756\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.9890 - val_loss: 35.7591\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.0875 - val_loss: 37.5515\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.9646 - val_loss: 39.4552\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.8449 - val_loss: 34.4767\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.0876 - val_loss: 38.7017\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.8018 - val_loss: 36.3371\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.9093 - val_loss: 35.3416\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.9392 - val_loss: 36.0845\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.8305 - val_loss: 38.8610\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.7485 - val_loss: 34.6891\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.7109 - val_loss: 56.1686\n",
            "Epoch 157/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 7ms/step - loss: 25.6031 - val_loss: 47.5869\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.5512 - val_loss: 42.1523\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.7244 - val_loss: 35.8786\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.5267 - val_loss: 39.8407\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.4812 - val_loss: 38.4758\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.5073 - val_loss: 39.5098\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.5169 - val_loss: 56.2843\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.5442 - val_loss: 40.8617\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.7777 - val_loss: 39.7390\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.4020 - val_loss: 35.2579\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.4058 - val_loss: 35.8182\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.4741 - val_loss: 37.1886\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.3251 - val_loss: 32.5434\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.3736 - val_loss: 39.7474\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.2601 - val_loss: 33.0743\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.2650 - val_loss: 40.5646\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.2685 - val_loss: 46.3708\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.2050 - val_loss: 36.1829\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.1098 - val_loss: 43.8454\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.1595 - val_loss: 36.8020\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.2323 - val_loss: 34.8093\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.1201 - val_loss: 34.5597\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.1090 - val_loss: 38.8726\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.1206 - val_loss: 42.3481\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 25.1602 - val_loss: 36.4274\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 24.9537 - val_loss: 41.9876\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 24.9677 - val_loss: 35.4966\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.0066 - val_loss: 47.7116\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.0755 - val_loss: 37.9534\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.0479 - val_loss: 47.6109\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 25.0646 - val_loss: 40.9576\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 24.9349 - val_loss: 62.4008\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.9417 - val_loss: 34.4268- los\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.8781 - val_loss: 35.4049\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.9846 - val_loss: 39.1037\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.9170 - val_loss: 38.1961\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.0420 - val_loss: 45.8314\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.8717 - val_loss: 56.6797\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.8634 - val_loss: 45.9049\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.7929 - val_loss: 37.5937\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.7203 - val_loss: 39.6428\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.8035 - val_loss: 37.8912\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 24.7258 - val_loss: 45.8796\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.6227 - val_loss: 36.8821\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.7241 - val_loss: 41.8680\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.7091 - val_loss: 36.0008\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.6564 - val_loss: 35.0300\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.6125 - val_loss: 41.6642\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.7004 - val_loss: 42.6715\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.6341 - val_loss: 33.1962\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.8430 - val_loss: 35.7990\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.5787 - val_loss: 34.0921\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.6491 - val_loss: 38.8544\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.5506 - val_loss: 34.1169\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.7086 - val_loss: 40.4460\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 24.5180 - val_loss: 58.7264\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.5646 - val_loss: 39.3885\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.3768 - val_loss: 67.7423\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.4386 - val_loss: 38.0479\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.6488 - val_loss: 49.7050\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.4372 - val_loss: 35.2648\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.3997 - val_loss: 34.1519\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.3341 - val_loss: 55.8216\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.5557 - val_loss: 43.3223\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.4892 - val_loss: 32.5827\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.4197 - val_loss: 34.6360\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 24.1706 - val_loss: 42.8504\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.2484 - val_loss: 44.5859\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.2940 - val_loss: 36.3648\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.3960 - val_loss: 39.4861\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.3152 - val_loss: 42.8974\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.2434 - val_loss: 40.3412\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.4146 - val_loss: 41.5645\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.2907 - val_loss: 41.2959\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.1959 - val_loss: 35.3396\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.0848 - val_loss: 38.1183\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.2133 - val_loss: 37.1136\n",
            "Epoch 234/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 14ms/step - loss: 24.0779 - val_loss: 34.5139\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.0635 - val_loss: 38.4695\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.0450 - val_loss: 35.6348\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 24.0728 - val_loss: 36.0495\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.0302 - val_loss: 36.7833\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.1023 - val_loss: 38.7186\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.1309 - val_loss: 38.3584\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.0836 - val_loss: 35.7527\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.1123 - val_loss: 40.1254\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.0371 - val_loss: 33.8063\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.0116 - val_loss: 35.4277\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.9464 - val_loss: 44.8161\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.9292 - val_loss: 32.7647\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.8195 - val_loss: 35.2360\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 23.9033 - val_loss: 34.5200\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.8525 - val_loss: 35.5149\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.8751 - val_loss: 34.6452\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.9660 - val_loss: 37.4226\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.8817 - val_loss: 34.3015\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.8944 - val_loss: 38.2434\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.8904 - val_loss: 35.3295\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.7915 - val_loss: 33.0220\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.8055 - val_loss: 34.7339\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.7858 - val_loss: 37.1812\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.8012 - val_loss: 36.2864\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.7583 - val_loss: 42.1478\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 23.8359 - val_loss: 33.1141\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.8908 - val_loss: 36.7951\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.8347 - val_loss: 35.5329\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.7636 - val_loss: 36.7383\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.7903 - val_loss: 33.3480\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.7263 - val_loss: 41.1292\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.7491 - val_loss: 36.0881\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.7474 - val_loss: 34.5977\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6259 - val_loss: 33.0040\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.6046 - val_loss: 36.0517\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.6698 - val_loss: 32.7879\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6320 - val_loss: 37.7334\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.7581 - val_loss: 40.1820\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.6578 - val_loss: 39.6575\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6094 - val_loss: 39.3385\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.6276 - val_loss: 35.0473\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.5030 - val_loss: 34.4845\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5448 - val_loss: 34.4062\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5716 - val_loss: 40.8975\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5497 - val_loss: 32.9154\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.5587 - val_loss: 41.9063\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5245 - val_loss: 43.3650\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5595 - val_loss: 36.4717\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.4888 - val_loss: 34.3206\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.4262 - val_loss: 37.2661\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3775 - val_loss: 38.6548\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5028 - val_loss: 35.7985\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.4469 - val_loss: 38.7343\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.4598 - val_loss: 34.3270\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.4531 - val_loss: 35.0122\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.4640 - val_loss: 34.1605\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.4072 - val_loss: 32.5644\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3076 - val_loss: 40.1860\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.4233 - val_loss: 36.1822\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3962 - val_loss: 33.3701\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5490 - val_loss: 34.5436\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.4020 - val_loss: 35.7586\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3071 - val_loss: 37.1516\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3834 - val_loss: 37.6678\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2584 - val_loss: 34.6910\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.3083 - val_loss: 34.0682\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2026 - val_loss: 34.0037\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3120 - val_loss: 36.2095\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.3214 - val_loss: 35.5728\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.3256 - val_loss: 39.6300\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2096 - val_loss: 37.2474\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.2091 - val_loss: 33.8605\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.2730 - val_loss: 37.4843\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.2337 - val_loss: 33.4807\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2242 - val_loss: 35.1485\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.1683 - val_loss: 43.2547\n",
            "Epoch 311/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 14ms/step - loss: 23.3078 - val_loss: 35.3908\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2251 - val_loss: 34.1397\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.1198 - val_loss: 34.9805\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.0887 - val_loss: 40.2933\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3009 - val_loss: 36.8257\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.1557 - val_loss: 32.9516\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.0967 - val_loss: 32.7967\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.1371 - val_loss: 35.9978\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.0931 - val_loss: 34.9356\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0585 - val_loss: 33.5847\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0920 - val_loss: 38.0658\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.1763 - val_loss: 39.7469\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.1178 - val_loss: 33.3412\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.0679 - val_loss: 34.9703\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0381 - val_loss: 43.1144\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0914 - val_loss: 32.5499\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9759 - val_loss: 39.6880\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.0096 - val_loss: 32.3863\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0004 - val_loss: 35.8980\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.0196 - val_loss: 39.1363\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9589 - val_loss: 34.6181\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9999 - val_loss: 33.6546\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.0110 - val_loss: 33.2117\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9629 - val_loss: 38.0358\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9807 - val_loss: 35.8057\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.0536 - val_loss: 33.8769\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0644 - val_loss: 36.2947\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8924 - val_loss: 36.0035\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9181 - val_loss: 36.2881\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.9114 - val_loss: 39.7635\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9587 - val_loss: 34.9906\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9098 - val_loss: 33.9139\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9062 - val_loss: 35.5870\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8818 - val_loss: 35.5272\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9179 - val_loss: 34.0082\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8785 - val_loss: 33.8996\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8465 - val_loss: 38.4312\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8370 - val_loss: 33.3206\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8870 - val_loss: 38.3354\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8501 - val_loss: 32.3791\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8395 - val_loss: 32.8058\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.8507 - val_loss: 34.6681\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8512 - val_loss: 40.2031\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7504 - val_loss: 33.4070\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8610 - val_loss: 39.4140\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8815 - val_loss: 36.3128\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7750 - val_loss: 36.1231\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7295 - val_loss: 34.4001\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6833 - val_loss: 32.5247\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7329 - val_loss: 32.8365\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7636 - val_loss: 36.5499\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7625 - val_loss: 34.5669\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7842 - val_loss: 46.8108\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 22.7935 - val_loss: 33.2903\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7305 - val_loss: 32.3897\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8204 - val_loss: 34.9963\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6924 - val_loss: 34.1024\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6181 - val_loss: 32.9743\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6688 - val_loss: 32.4590\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8331 - val_loss: 38.2635\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7744 - val_loss: 34.7156\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6209 - val_loss: 35.8779\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6348 - val_loss: 33.6785\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7037 - val_loss: 36.9578\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 22.7921 - val_loss: 33.9128\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6109 - val_loss: 41.8687\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6143 - val_loss: 41.0880\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6555 - val_loss: 35.7851\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5768 - val_loss: 35.9212\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6497 - val_loss: 35.0693\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6518 - val_loss: 33.4577\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5208 - val_loss: 51.7688\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5586 - val_loss: 39.0864\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5618 - val_loss: 32.8972\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5481 - val_loss: 33.3251\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 22.6017 - val_loss: 34.3001\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5889 - val_loss: 34.5792\n",
            "Epoch 388/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5156 - val_loss: 36.6531\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5923 - val_loss: 36.8715\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5152 - val_loss: 35.1103\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5032 - val_loss: 33.7034\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5336 - val_loss: 36.5572\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5902 - val_loss: 34.2458\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6806 - val_loss: 32.5133\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5144 - val_loss: 35.8946\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.4845 - val_loss: 34.5957\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.4331 - val_loss: 32.4029\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5239 - val_loss: 34.5734\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5346 - val_loss: 34.3864\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5527 - val_loss: 36.2720\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.4330 - val_loss: 41.4195\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4364 - val_loss: 34.2569\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5004 - val_loss: 37.3879\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4953 - val_loss: 34.9681\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4384 - val_loss: 35.8039\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.4244 - val_loss: 36.6640\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5124 - val_loss: 40.7871\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 22.4470 - val_loss: 34.1377\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4405 - val_loss: 33.7530\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.4285 - val_loss: 32.6739\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.4187 - val_loss: 34.2563\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.3810 - val_loss: 40.4835\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3920 - val_loss: 43.6196\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.3915 - val_loss: 33.6068\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 22.42 - 2s 15ms/step - loss: 22.3869 - val_loss: 37.4949\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.3966 - val_loss: 34.7382\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4203 - val_loss: 31.7852\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.3193 - val_loss: 33.1180\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.3732 - val_loss: 36.0392\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.4244 - val_loss: 35.1272\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.3783 - val_loss: 33.6176\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.3448 - val_loss: 35.4520\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 22.3708 - val_loss: 33.6086\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3285 - val_loss: 41.7653\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.3272 - val_loss: 34.3739\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3127 - val_loss: 44.3882\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4349 - val_loss: 38.6146\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3377 - val_loss: 36.2222\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3126 - val_loss: 35.5204\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2848 - val_loss: 35.4957\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2857 - val_loss: 37.2631\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.3748 - val_loss: 35.4076\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2194 - val_loss: 38.8900\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 22.2238 - val_loss: 40.6775\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3326 - val_loss: 35.6497\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2422 - val_loss: 33.6658\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2467 - val_loss: 43.7221\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.3125 - val_loss: 37.8089\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2007 - val_loss: 34.6059\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2190 - val_loss: 32.8561\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2274 - val_loss: 36.9121\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2220 - val_loss: 32.7146\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2215 - val_loss: 35.7281\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2906 - val_loss: 33.7539\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 22.2366 - val_loss: 49.3569\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1703 - val_loss: 36.7457\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2916 - val_loss: 37.3599\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2920 - val_loss: 32.6235\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2326 - val_loss: 36.2257\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2570 - val_loss: 41.9803\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2125 - val_loss: 33.6729\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1535 - val_loss: 36.1799\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2706 - val_loss: 34.9650\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1348 - val_loss: 35.0428\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2272 - val_loss: 33.5265\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2269 - val_loss: 35.6650\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 22.1582 - val_loss: 35.2106\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1635 - val_loss: 34.8831\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0731 - val_loss: 35.1311\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1829 - val_loss: 34.3986\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1695 - val_loss: 32.7459\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.0954 - val_loss: 33.1519\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1731 - val_loss: 39.0720\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2280 - val_loss: 34.7802\n",
            "Epoch 465/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1436 - val_loss: 34.9933\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2410 - val_loss: 33.5839\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1237 - val_loss: 38.0438\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1793 - val_loss: 35.9864\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 22.1469 - val_loss: 35.6772\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.0899 - val_loss: 36.0878\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1556 - val_loss: 34.9077\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1098 - val_loss: 35.7291\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0851 - val_loss: 33.2050\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1051 - val_loss: 36.5429\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1112 - val_loss: 34.4521\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1477 - val_loss: 37.0261\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.0787 - val_loss: 34.6535\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0573 - val_loss: 34.3439\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.0375 - val_loss: 36.3727\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.0823 - val_loss: 36.6481\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0315 - val_loss: 38.8994\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0533 - val_loss: 36.8948\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.0118 - val_loss: 34.9615\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1204 - val_loss: 41.9044\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9874 - val_loss: 33.7364\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.0541 - val_loss: 35.1102\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.0538 - val_loss: 34.4579\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0496 - val_loss: 34.5949\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1055 - val_loss: 33.7648\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0253 - val_loss: 36.8130\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0548 - val_loss: 35.2631\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 22.0969 - val_loss: 36.0958\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0064 - val_loss: 37.3067\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.0075 - val_loss: 33.8687\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0221 - val_loss: 33.3657\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0461 - val_loss: 32.9582\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0446 - val_loss: 33.0159\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9859 - val_loss: 39.2865\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.0410 - val_loss: 34.2771\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9813 - val_loss: 38.1100\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -1.8176357876063023 \n",
            "MAE:  4.804397115484172 \n",
            "SD:  5.899681379434153\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cip38xZOZnV",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_48 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_54 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 3742.4314 - val_loss: 3712.7642\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 3546.8508 - val_loss: 3325.5256\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 3309.67 - 2s 15ms/step - loss: 3302.1125 - val_loss: 3073.5881\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 2999.9358 - val_loss: 3132.0745\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2655.6113 - val_loss: 3149.4297\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2295.0862 - val_loss: 2916.9954\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 1902.1168 - val_loss: 1458.4504\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 1487.4017 - val_loss: 375.7902\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 1079.1620 - val_loss: 2251.1001\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 730.3501 - val_loss: 547.9125\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 472.2382 - val_loss: 149.1990\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 286.4946 - val_loss: 618.3155\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 171.7881 - val_loss: 652.3789\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 102.3704 - val_loss: 212.1013\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.9824 - val_loss: 55.9159\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 52.2414 - val_loss: 54.0931\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 46.1942 - val_loss: 72.7474\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 43.7732 - val_loss: 154.4905\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 42.2562 - val_loss: 50.8867\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 40.0176 - val_loss: 53.3988\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 39.0978 - val_loss: 62.7666\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 37.9559 - val_loss: 52.9290\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 37.6804 - val_loss: 45.1323\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.0083 - val_loss: 45.1660\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 37.0915 - val_loss: 100.8549\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 37.2827 - val_loss: 52.0701\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 36.2078 - val_loss: 41.4390\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 35.5426 - val_loss: 39.5042\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 34.8962 - val_loss: 41.8470\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.4790 - val_loss: 47.0742\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.0447 - val_loss: 41.4881\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 33.7705 - val_loss: 45.2150\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 33.5198 - val_loss: 48.7214\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 33.3828 - val_loss: 56.6625\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 33.1867 - val_loss: 77.9384\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.5937 - val_loss: 45.1976\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.5880 - val_loss: 46.9666\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.2991 - val_loss: 41.6361\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 31.9647 - val_loss: 38.6820\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.7763 - val_loss: 40.5254\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 31.5392 - val_loss: 47.0192\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.4933 - val_loss: 42.1598\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.3817 - val_loss: 42.8513\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.0409 - val_loss: 36.9110\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.9006 - val_loss: 57.1297\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 31.0249 - val_loss: 50.7052\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.7853 - val_loss: 47.9940\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 30.4489 - val_loss: 36.8161\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 30.3232 - val_loss: 43.0054\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 30.1578 - val_loss: 40.3040\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 30.00 - 2s 14ms/step - loss: 29.9906 - val_loss: 40.1871\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 29.8492 - val_loss: 47.9094\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 29.8669 - val_loss: 43.5712\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 29.5741 - val_loss: 40.0835\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.4115 - val_loss: 36.7861\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.1714 - val_loss: 36.9332\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 29.0724 - val_loss: 35.9780\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 29.0045 - val_loss: 45.2139\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.8496 - val_loss: 41.4037\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.7414 - val_loss: 37.5224\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.5988 - val_loss: 36.4920\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 28.6030 - val_loss: 38.4611\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 28.3886 - val_loss: 46.8815\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 28.1600 - val_loss: 39.4553\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 28.0953 - val_loss: 37.0223\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1169 - val_loss: 46.9887\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 28.0498 - val_loss: 36.8679\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8729 - val_loss: 40.5349\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 27.8377 - val_loss: 41.1505\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.7809 - val_loss: 53.3729\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.7607 - val_loss: 37.9263\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6537 - val_loss: 35.5547\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.5188 - val_loss: 44.8036\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5631 - val_loss: 37.5473\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4435 - val_loss: 38.8326\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.2471 - val_loss: 40.6908\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2220 - val_loss: 59.3273\n",
            "Epoch 78/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1440 - val_loss: 37.8653\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.0755 - val_loss: 63.1418\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 26.8657 - val_loss: 36.7644\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.8806 - val_loss: 34.6551\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.7362 - val_loss: 43.6967\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5784 - val_loss: 39.5708\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.6993 - val_loss: 52.0078\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5464 - val_loss: 49.9012\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5507 - val_loss: 39.5442\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4711 - val_loss: 36.1325\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4041 - val_loss: 35.8658\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3823 - val_loss: 39.2289\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2457 - val_loss: 55.6467\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1944 - val_loss: 53.2134\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3115 - val_loss: 39.4918\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0205 - val_loss: 37.9059\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 26.1013 - val_loss: 34.4589\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.0942 - val_loss: 38.6027\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9207 - val_loss: 36.7751\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8961 - val_loss: 36.1413\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8318 - val_loss: 37.3052\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8642 - val_loss: 36.9563\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7323 - val_loss: 47.6600\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5875 - val_loss: 34.0709\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5287 - val_loss: 47.6741\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5895 - val_loss: 35.3776\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6537 - val_loss: 34.5228\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 25.5187 - val_loss: 37.8127\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.4024 - val_loss: 34.1185\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5751 - val_loss: 47.8748\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.3757 - val_loss: 35.8004\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.2469 - val_loss: 44.0634\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.3786 - val_loss: 46.2918\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.1822 - val_loss: 34.9891\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.4111 - val_loss: 32.9320\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.2399 - val_loss: 38.2699\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.1051 - val_loss: 34.0393\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.1355 - val_loss: 33.0359\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.1413 - val_loss: 35.6040\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.1154 - val_loss: 37.5444\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.0563 - val_loss: 34.4524\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.9895 - val_loss: 41.5158\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.9433 - val_loss: 35.8422\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.9289 - val_loss: 44.5221\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.8414 - val_loss: 34.6491\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.8731 - val_loss: 32.6878\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.9819 - val_loss: 33.4237\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.7203 - val_loss: 34.5952\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.6923 - val_loss: 36.6659\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.6776 - val_loss: 37.8115\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.6078 - val_loss: 44.0027\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.6808 - val_loss: 65.0076\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.8328 - val_loss: 37.3849\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.6190 - val_loss: 36.6377\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 24.5094 - val_loss: 40.1406\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.5690 - val_loss: 39.2825\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.5139 - val_loss: 33.9308\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.5922 - val_loss: 36.3447\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.5938 - val_loss: 38.6719\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.5474 - val_loss: 36.6943\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.3741 - val_loss: 35.9346\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.4804 - val_loss: 36.7166\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.4167 - val_loss: 34.0512\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.3662 - val_loss: 36.2207\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.2871 - val_loss: 34.6504\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.1956 - val_loss: 44.1524\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.3134 - val_loss: 36.9473\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 24.2999 - val_loss: 34.2088\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.1979 - val_loss: 38.6699\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.2059 - val_loss: 38.6528\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.1900 - val_loss: 39.9416\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.2414 - val_loss: 39.2037\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.3652 - val_loss: 37.0434\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.2507 - val_loss: 46.0334\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.1381 - val_loss: 49.1141\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.0371 - val_loss: 37.0788\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.1651 - val_loss: 48.6272\n",
            "Epoch 155/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 4s 23ms/step - loss: 24.1786 - val_loss: 36.1422\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.1138 - val_loss: 38.5306\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.0454 - val_loss: 39.6195\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.0710 - val_loss: 42.5529\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.0281 - val_loss: 35.5736\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.9441 - val_loss: 33.1732\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.9861 - val_loss: 42.5229\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.0420 - val_loss: 34.1468\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.0075 - val_loss: 38.6366\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.8504 - val_loss: 34.0344\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.8405 - val_loss: 33.5487\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.8011 - val_loss: 37.2976\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.7934 - val_loss: 43.3775\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.0025 - val_loss: 38.5761\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.8069 - val_loss: 35.5231\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.8272 - val_loss: 42.1680\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.9384 - val_loss: 35.7884\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.8667 - val_loss: 32.5134\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.9834 - val_loss: 37.9643\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.0800 - val_loss: 39.2497\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.1769 - val_loss: 38.2819\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.9501 - val_loss: 36.2129\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.7813 - val_loss: 33.3132\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 23.6476 - val_loss: 33.9338\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.6857 - val_loss: 35.4446\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6551 - val_loss: 35.7409\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.6646 - val_loss: 36.4539\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.6939 - val_loss: 36.6165\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.6504 - val_loss: 39.8705\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6974 - val_loss: 34.7270\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5898 - val_loss: 34.0898\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5069 - val_loss: 45.7223\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.6245 - val_loss: 35.8507\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.6851 - val_loss: 37.3696\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.4561 - val_loss: 37.3730\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5627 - val_loss: 35.5194\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.5487 - val_loss: 38.9257\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5854 - val_loss: 35.7535\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.4630 - val_loss: 34.1500\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.4457 - val_loss: 41.5781\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.7123 - val_loss: 45.6041\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.4031 - val_loss: 33.8299\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.4870 - val_loss: 33.3849\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.4426 - val_loss: 36.9417\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3267 - val_loss: 37.9601\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2907 - val_loss: 36.7127\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 23.3601 - val_loss: 39.0873\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.4207 - val_loss: 38.2054\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3587 - val_loss: 41.6492\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.3070 - val_loss: 48.0814\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.3147 - val_loss: 39.0316\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.4025 - val_loss: 32.7135\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3275 - val_loss: 34.1467\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2697 - val_loss: 38.5698\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5707 - val_loss: 34.6092\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2905 - val_loss: 36.4669\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.3109 - val_loss: 37.5400\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.3018 - val_loss: 36.4365\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.2191 - val_loss: 34.2384\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.3121 - val_loss: 32.4564\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.2107 - val_loss: 35.4119\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.1995 - val_loss: 47.7066\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2646 - val_loss: 36.4862\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.1286 - val_loss: 43.0984\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.2033 - val_loss: 39.2542\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.1093 - val_loss: 34.1971\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.0859 - val_loss: 44.2252\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.1483 - val_loss: 37.3639\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.1647 - val_loss: 39.3337\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.2112 - val_loss: 43.8417\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.1931 - val_loss: 34.8347\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0794 - val_loss: 48.4081\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.1055 - val_loss: 51.0681\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.0411 - val_loss: 35.9297\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0255 - val_loss: 39.4256\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3534 - val_loss: 35.5524\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.1554 - val_loss: 33.9541\n",
            "Epoch 232/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0619 - val_loss: 39.5069\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.1559 - val_loss: 40.6591\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0644 - val_loss: 39.0232\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.0633 - val_loss: 33.2024\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9529 - val_loss: 38.8156\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9854 - val_loss: 39.0246\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9823 - val_loss: 33.9337\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9556 - val_loss: 35.8681\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0873 - val_loss: 33.5844\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9350 - val_loss: 34.1425\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9625 - val_loss: 39.2335\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9498 - val_loss: 35.9825\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0893 - val_loss: 58.1414\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0799 - val_loss: 36.2627\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9771 - val_loss: 36.5553\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8691 - val_loss: 34.5562\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9043 - val_loss: 41.4977\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0242 - val_loss: 41.7416\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8893 - val_loss: 34.4544\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8896 - val_loss: 40.5456\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0253 - val_loss: 40.8765\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9664 - val_loss: 36.7115\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8869 - val_loss: 37.2097\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7989 - val_loss: 36.6396\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7471 - val_loss: 39.6735\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9195 - val_loss: 43.9419\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8299 - val_loss: 33.4048\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7518 - val_loss: 41.7247\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8558 - val_loss: 36.3940\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 22.7861 - val_loss: 33.3033\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7635 - val_loss: 34.5068\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7128 - val_loss: 37.0793\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8908 - val_loss: 34.2741\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6184 - val_loss: 36.6799\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7489 - val_loss: 36.7082\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7965 - val_loss: 35.4314\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6954 - val_loss: 34.8515\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6373 - val_loss: 34.3196\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6746 - val_loss: 35.6386\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6198 - val_loss: 43.4879\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8201 - val_loss: 33.5835\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7188 - val_loss: 34.7452\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6419 - val_loss: 36.9714\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.7177 - val_loss: 34.7909\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6114 - val_loss: 34.3943\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7029 - val_loss: 34.9711\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7110 - val_loss: 37.7454\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6846 - val_loss: 35.4222\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6627 - val_loss: 36.1832\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5763 - val_loss: 34.8797\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7604 - val_loss: 32.8990\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6490 - val_loss: 37.8066\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6663 - val_loss: 38.0166\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6409 - val_loss: 35.5308\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5853 - val_loss: 39.4337\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5731 - val_loss: 35.5547\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5803 - val_loss: 34.2953\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5275 - val_loss: 38.8788\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6679 - val_loss: 35.4160\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5602 - val_loss: 39.9099\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5803 - val_loss: 34.7835\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6353 - val_loss: 36.3891\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6057 - val_loss: 37.4464\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.5758 - val_loss: 49.3442\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4643 - val_loss: 37.4565\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5004 - val_loss: 36.2265\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5592 - val_loss: 36.0492\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6271 - val_loss: 47.3970\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5668 - val_loss: 47.8558\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4934 - val_loss: 33.4428\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5691 - val_loss: 39.9837\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5557 - val_loss: 38.6021\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.4835 - val_loss: 37.2872\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.5411 - val_loss: 42.4503\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5075 - val_loss: 37.5964\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4300 - val_loss: 36.4493\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5377 - val_loss: 34.1022\n",
            "Epoch 309/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5252 - val_loss: 43.4030\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5512 - val_loss: 37.1084\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4742 - val_loss: 37.1208\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3574 - val_loss: 34.9583\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3588 - val_loss: 40.7572\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4156 - val_loss: 43.7787\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.3866 - val_loss: 43.9302\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4481 - val_loss: 35.8066\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4186 - val_loss: 32.8860\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3887 - val_loss: 34.6432\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.4071 - val_loss: 33.3114\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4949 - val_loss: 40.7503\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3898 - val_loss: 35.8991\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3903 - val_loss: 41.5864\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3173 - val_loss: 37.5235\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4326 - val_loss: 38.6045\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.4527 - val_loss: 34.6434\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.3042 - val_loss: 38.4954\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2909 - val_loss: 37.9510\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3522 - val_loss: 34.7293\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3764 - val_loss: 35.7136\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3655 - val_loss: 38.7271\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2904 - val_loss: 33.3345\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2654 - val_loss: 37.4523\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2278 - val_loss: 38.0038\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3828 - val_loss: 36.1749\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.3448 - val_loss: 34.7609\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3393 - val_loss: 35.3524\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2983 - val_loss: 34.9997\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2185 - val_loss: 40.9514\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3355 - val_loss: 36.3221\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.3918 - val_loss: 34.6314\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3141 - val_loss: 34.5323\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.4098 - val_loss: 33.4635\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2340 - val_loss: 34.0513\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2612 - val_loss: 36.2198\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.3525 - val_loss: 37.5805\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.2485 - val_loss: 35.6870\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1383 - val_loss: 34.8543\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2105 - val_loss: 41.5791\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2732 - val_loss: 36.0024\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2533 - val_loss: 33.6594\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2567 - val_loss: 48.5387\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2868 - val_loss: 34.2161\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2183 - val_loss: 41.7040\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2025 - val_loss: 41.5247\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1801 - val_loss: 36.7068\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.3063 - val_loss: 34.1301\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2757 - val_loss: 40.8685\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2975 - val_loss: 34.1023\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1906 - val_loss: 34.0415\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1563 - val_loss: 35.0509\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2232 - val_loss: 42.1263\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1810 - val_loss: 33.8993\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1501 - val_loss: 37.5849\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1925 - val_loss: 34.6962\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1353 - val_loss: 36.2451\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1218 - val_loss: 41.1465\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2053 - val_loss: 34.2097\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.2332 - val_loss: 34.2971\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.2404 - val_loss: 46.0722\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1309 - val_loss: 37.0581\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1769 - val_loss: 34.3967\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1648 - val_loss: 37.0773\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1579 - val_loss: 35.2984\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1903 - val_loss: 58.2566\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1125 - val_loss: 38.4870\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0957 - val_loss: 34.9745\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.1463 - val_loss: 33.9188\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1539 - val_loss: 34.9052\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1375 - val_loss: 41.8828\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2033 - val_loss: 34.2536\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.2051 - val_loss: 34.6737\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1087 - val_loss: 35.2900\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1605 - val_loss: 36.9760\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0103 - val_loss: 35.2365\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1621 - val_loss: 34.7189\n",
            "Epoch 386/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1887 - val_loss: 34.4858\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.0996 - val_loss: 33.6571\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0627 - val_loss: 35.2090\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0330 - val_loss: 35.6254\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9931 - val_loss: 47.2472\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1930 - val_loss: 36.1402\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1283 - val_loss: 35.4513\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0272 - val_loss: 35.2874\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0595 - val_loss: 32.8829\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1416 - val_loss: 35.9287\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 22.0602 - val_loss: 38.9092\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1109 - val_loss: 35.8570\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.1034 - val_loss: 33.2026\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0585 - val_loss: 35.6038\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9899 - val_loss: 34.1066\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.0680 - val_loss: 35.9055\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0710 - val_loss: 33.9266\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9434 - val_loss: 38.1014\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0412 - val_loss: 34.4502\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.0061 - val_loss: 34.6371\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 21.9500 - val_loss: 33.6122\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0707 - val_loss: 35.0605\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9610 - val_loss: 34.6325\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.9931 - val_loss: 36.2340\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0251 - val_loss: 33.0151\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0566 - val_loss: 36.1793\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9709 - val_loss: 33.4910\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0364 - val_loss: 33.6587\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9897 - val_loss: 36.2230\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.9724 - val_loss: 35.6656\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 21.9807 - val_loss: 65.2162\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0730 - val_loss: 35.1959\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9637 - val_loss: 35.6551\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.1030 - val_loss: 35.7022\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0208 - val_loss: 34.3615\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0187 - val_loss: 34.5798\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.9669 - val_loss: 34.6733\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9693 - val_loss: 38.8663\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9535 - val_loss: 41.0974\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 21.9431 - val_loss: 34.4409\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0182 - val_loss: 32.5117\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9814 - val_loss: 35.0543\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9721 - val_loss: 33.3884\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0106 - val_loss: 35.7552\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8828 - val_loss: 33.7869\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9678 - val_loss: 34.1612\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9464 - val_loss: 34.9775\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 21.9568 - val_loss: 40.9757\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8263 - val_loss: 34.0233\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9720 - val_loss: 38.6534\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.9993 - val_loss: 40.9977\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9419 - val_loss: 37.6085\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9112 - val_loss: 34.4295\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8818 - val_loss: 38.9097\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8857 - val_loss: 35.3576\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8462 - val_loss: 33.0617\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8310 - val_loss: 38.1670\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.8659 - val_loss: 35.7588\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 21.8865 - val_loss: 34.3403\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8460 - val_loss: 39.0482\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9139 - val_loss: 35.1270\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8997 - val_loss: 35.0920\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9711 - val_loss: 37.6822\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9335 - val_loss: 35.6943\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8727 - val_loss: 32.5939\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8864 - val_loss: 39.1133\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9298 - val_loss: 37.2686\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 21.8927 - val_loss: 35.2039\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8616 - val_loss: 36.7552\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.0060 - val_loss: 34.5811\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8140 - val_loss: 36.7998\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9302 - val_loss: 33.8486\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9555 - val_loss: 35.2506\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8588 - val_loss: 34.0867\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8514 - val_loss: 37.5401\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9129 - val_loss: 34.7005\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 21.8429 - val_loss: 34.5714\n",
            "Epoch 463/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9116 - val_loss: 34.3242\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8441 - val_loss: 35.9003\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8343 - val_loss: 33.4047\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8166 - val_loss: 34.1132\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8233 - val_loss: 38.9903\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.7547 - val_loss: 35.2495\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9083 - val_loss: 34.1371\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.8760 - val_loss: 35.3513\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 21.8582 - val_loss: 38.3807\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8402 - val_loss: 39.4428\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.7705 - val_loss: 35.6152\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.8726 - val_loss: 35.8441\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8511 - val_loss: 36.8924\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8611 - val_loss: 36.9726\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.7499 - val_loss: 42.9253\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8971 - val_loss: 39.4321\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9897 - val_loss: 33.9037\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9210 - val_loss: 33.0717\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 21.8144 - val_loss: 35.2510\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.7926 - val_loss: 34.6235\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8420 - val_loss: 37.3431\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8325 - val_loss: 39.7268\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8834 - val_loss: 34.4721\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.8761 - val_loss: 38.6714\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.8574 - val_loss: 41.4480\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.7607 - val_loss: 34.6510\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 21.8258 - val_loss: 37.5215\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.7680 - val_loss: 34.9111\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8434 - val_loss: 41.2059\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 21.8581 - val_loss: 35.2751\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8439 - val_loss: 34.4675\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8084 - val_loss: 36.7203\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.7083 - val_loss: 34.0292\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.7449 - val_loss: 43.4896\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.9488 - val_loss: 36.0411\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.8759 - val_loss: 35.0014\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.7999 - val_loss: 38.5513\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 21.7922 - val_loss: 35.2564\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -0.47359086744211343 \n",
            "MAE:  4.468160824936043 \n",
            "SD:  5.918789646067035\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_60 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_55 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_57 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_65 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 3762.2190 - val_loss: 3787.7617\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 3627.3018 - val_loss: 3630.6763\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 3479.4846 - val_loss: 3337.5525\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 3293.5906 - val_loss: 2999.8286\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 3027.5686 - val_loss: 2424.8550\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 2686.5452 - val_loss: 1812.3091\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 2318.3547 - val_loss: 1559.4709\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 1927.1267 - val_loss: 1187.8822\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 1530.2141 - val_loss: 581.3887\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 1175.5795 - val_loss: 240.7218\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 871.4245 - val_loss: 357.0337\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 622.1439 - val_loss: 804.9149\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 428.1283 - val_loss: 341.7252\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 286.9359 - val_loss: 519.7410\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 187.3492 - val_loss: 45.3397\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 120.4466 - val_loss: 181.1042\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 81.3123 - val_loss: 132.2570\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 56.0338 - val_loss: 166.5625\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 48.2525 - val_loss: 54.6203\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 48.0997 - val_loss: 74.7412\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 46.1992 - val_loss: 56.6642\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 42.6487 - val_loss: 44.1783\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 39.8561 - val_loss: 58.1854\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 38.7713 - val_loss: 45.6824\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.9077 - val_loss: 42.4789\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.2663 - val_loss: 59.1179\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.5283 - val_loss: 47.5350\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 36.6418 - val_loss: 63.8352\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 36.0061 - val_loss: 44.8362\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 35.4579 - val_loss: 41.3293\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 35.0362 - val_loss: 42.1908\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.8429 - val_loss: 48.1206\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 34.6354 - val_loss: 39.6419\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.4544 - val_loss: 43.8756\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 34.0422 - val_loss: 50.6662\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 33.9170 - val_loss: 42.1113\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.6973 - val_loss: 40.4748\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.1515 - val_loss: 45.9800\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 33.0757 - val_loss: 42.4184\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.0053 - val_loss: 44.2081\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 32.7459 - val_loss: 41.8568\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 32.6978 - val_loss: 44.9394\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 32.2537 - val_loss: 40.7151\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 32.1843 - val_loss: 38.2832\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.8155 - val_loss: 59.3330\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 31.6203 - val_loss: 51.6785\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.3540 - val_loss: 41.5907\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 31.3957 - val_loss: 61.9993\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 31.2194 - val_loss: 40.0203\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 30.8928 - val_loss: 43.7963\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.0215 - val_loss: 42.3863\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.7573 - val_loss: 37.1862\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.6100 - val_loss: 39.7933\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 30.3726 - val_loss: 39.9765\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.2478 - val_loss: 42.3681\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 30.3246 - val_loss: 36.7281loss: 30.27\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 30.2770 - val_loss: 41.5683\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.9880 - val_loss: 55.1194\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.1551 - val_loss: 39.6548\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 30.0976 - val_loss: 42.4270\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.2352 - val_loss: 58.4049\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.7377 - val_loss: 40.5378\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.6487 - val_loss: 47.4250\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 29.4241 - val_loss: 37.0467\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 29.2587 - val_loss: 41.5353\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 29.0466 - val_loss: 40.9039\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 28.9828 - val_loss: 46.7506\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 28.9711 - val_loss: 41.3093\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 28.7956 - val_loss: 39.4833\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.7007 - val_loss: 41.6057\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.6952 - val_loss: 44.5249\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.4229 - val_loss: 39.9945\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 28.3298 - val_loss: 36.6145\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3407 - val_loss: 43.4407\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 28.1947 - val_loss: 51.3785\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2647 - val_loss: 37.8981\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.0729 - val_loss: 39.5667\n",
            "Epoch 78/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 15ms/step - loss: 27.8699 - val_loss: 35.4716\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.8410 - val_loss: 37.0136\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8294 - val_loss: 36.4243\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 27.7308 - val_loss: 39.6194\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5948 - val_loss: 39.8075\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.5455 - val_loss: 41.0460\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.7261 - val_loss: 49.5488\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.8801 - val_loss: 60.0908\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6056 - val_loss: 51.1873\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.3838 - val_loss: 50.1705\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.3073 - val_loss: 46.9955\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2851 - val_loss: 46.0951\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.2863 - val_loss: 35.5241\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.0286 - val_loss: 35.8539\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.9371 - val_loss: 37.3103\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 27.1559 - val_loss: 46.4202\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9852 - val_loss: 44.4306\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9552 - val_loss: 42.0610\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.0142 - val_loss: 45.8796\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.9893 - val_loss: 43.0522\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.8938 - val_loss: 56.9882\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.7201 - val_loss: 46.1870\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.7458 - val_loss: 34.9096\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6157 - val_loss: 38.8183\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.6010 - val_loss: 34.6963\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3605 - val_loss: 35.2114\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 26.2526 - val_loss: 38.2725\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4258 - val_loss: 37.7105\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2475 - val_loss: 34.5032\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1862 - val_loss: 35.1446\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2202 - val_loss: 53.8292\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2145 - val_loss: 35.1544\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.0430 - val_loss: 39.7103\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.0241 - val_loss: 42.6569\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9197 - val_loss: 34.6050\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 25.9849 - val_loss: 49.9990\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8855 - val_loss: 37.4665\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8075 - val_loss: 34.7772\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0293 - val_loss: 37.5361\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8165 - val_loss: 33.5628\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7503 - val_loss: 44.7647\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7722 - val_loss: 39.6951\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7486 - val_loss: 34.7801\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7833 - val_loss: 40.1154\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4952 - val_loss: 38.1686\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 25.4780 - val_loss: 34.6368\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4583 - val_loss: 33.3317\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.4628 - val_loss: 35.3826\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6777 - val_loss: 37.1978\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.4596 - val_loss: 37.7005\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.4243 - val_loss: 45.1880\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3856 - val_loss: 38.2468\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.1671 - val_loss: 34.7283\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.2069 - val_loss: 51.5266\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2267 - val_loss: 33.9370\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.1970 - val_loss: 36.5931\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.0549 - val_loss: 38.8375\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.2623 - val_loss: 37.0423\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 25.0405 - val_loss: 35.6793\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.0996 - val_loss: 46.2580\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.9980 - val_loss: 36.1939\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.9680 - val_loss: 33.2956\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.8591 - val_loss: 45.5890\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.8009 - val_loss: 34.7057\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.9362 - val_loss: 40.6324\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.8507 - val_loss: 35.4877\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.8669 - val_loss: 36.3759\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.7537 - val_loss: 36.2082\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.5990 - val_loss: 38.6382\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.7124 - val_loss: 42.6205\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 24.7892 - val_loss: 43.4347\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.5574 - val_loss: 55.4510\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.8102 - val_loss: 37.9586\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.7016 - val_loss: 37.7685\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.4671 - val_loss: 36.0349\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.5647 - val_loss: 39.5848\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.5802 - val_loss: 46.2458\n",
            "Epoch 155/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 14ms/step - loss: 24.5394 - val_loss: 36.3075\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.4912 - val_loss: 56.7041\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.3362 - val_loss: 34.9711\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.4139 - val_loss: 35.3301\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.4457 - val_loss: 39.9718\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.2781 - val_loss: 38.9591\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 24.2753 - val_loss: 33.8225\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.2138 - val_loss: 43.4392\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.3328 - val_loss: 32.2887\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.3615 - val_loss: 33.6487\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.2033 - val_loss: 43.5323\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.2708 - val_loss: 33.8230\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.2287 - val_loss: 42.4691\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.2745 - val_loss: 33.2771\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.2073 - val_loss: 35.4788\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.1545 - val_loss: 36.2603\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.1955 - val_loss: 34.7081\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.1485 - val_loss: 51.6638\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 24.1263 - val_loss: 34.2026\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.0289 - val_loss: 35.8135\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.1277 - val_loss: 34.3782\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.1115 - val_loss: 40.0495\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.0461 - val_loss: 37.2885\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.0558 - val_loss: 47.2617\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.0474 - val_loss: 38.1198\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.0105 - val_loss: 39.5594\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.9211 - val_loss: 33.4261\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 24.0003 - val_loss: 34.4273\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.9292 - val_loss: 36.7956\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.8815 - val_loss: 32.5151\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.9831 - val_loss: 38.2615\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.9082 - val_loss: 35.3882\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.9092 - val_loss: 34.0340\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.9121 - val_loss: 39.1052\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.9360 - val_loss: 44.1610\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.8968 - val_loss: 33.5727\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.7754 - val_loss: 33.5171\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.7644 - val_loss: 41.7637\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.8616 - val_loss: 34.9077\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.7845 - val_loss: 36.6768\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.8444 - val_loss: 37.3575\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 24.1799 - val_loss: 35.8025\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.8705 - val_loss: 35.8530\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.6864 - val_loss: 36.7467\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6743 - val_loss: 35.0894\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6395 - val_loss: 35.6934\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.7632 - val_loss: 40.5577\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6417 - val_loss: 45.1981\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.7213 - val_loss: 45.9252\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6198 - val_loss: 34.8539\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6432 - val_loss: 36.0219\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6567 - val_loss: 37.8738\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 23.65 - 2s 14ms/step - loss: 23.6141 - val_loss: 36.2287\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6386 - val_loss: 33.5032\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.7115 - val_loss: 34.5845\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6694 - val_loss: 41.2579\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.6120 - val_loss: 35.5493\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 4s 24ms/step - loss: 23.5658 - val_loss: 36.7786\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 23.5086 - val_loss: 36.1939\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 4s 22ms/step - loss: 23.4581 - val_loss: 44.0553\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5270 - val_loss: 35.2716\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3638 - val_loss: 37.1589\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.3754 - val_loss: 36.9201\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.4485 - val_loss: 38.4240\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.3508 - val_loss: 34.8193\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.4213 - val_loss: 34.8587\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5575 - val_loss: 40.7523\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.5543 - val_loss: 41.1408\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5639 - val_loss: 33.6454\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.5473 - val_loss: 39.8426\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.4472 - val_loss: 38.6263\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.3123 - val_loss: 33.1132\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 23.2932 - val_loss: 54.2678\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.2681 - val_loss: 34.8776\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.4541 - val_loss: 37.3921\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.3198 - val_loss: 36.0126\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.3186 - val_loss: 37.8835\n",
            "Epoch 232/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2709 - val_loss: 36.6083\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2929 - val_loss: 38.7356\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.1790 - val_loss: 42.1798\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2626 - val_loss: 34.3087\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.1918 - val_loss: 35.4759\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.2154 - val_loss: 34.8014\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 23.1343 - val_loss: 34.4753\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.1622 - val_loss: 37.2267\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0903 - val_loss: 45.1433\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.1392 - val_loss: 35.1879\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.1829 - val_loss: 37.8808\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.1204 - val_loss: 42.1611\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.1484 - val_loss: 33.7320\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0902 - val_loss: 34.3572\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0577 - val_loss: 51.6136\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.0227 - val_loss: 39.8555\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 23.1342 - val_loss: 40.0856\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0569 - val_loss: 35.2343\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9923 - val_loss: 33.8623\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9384 - val_loss: 38.3356\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9658 - val_loss: 38.4568\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 23.0608 - val_loss: 34.3270\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0190 - val_loss: 33.4465\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9452 - val_loss: 36.9958\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9657 - val_loss: 36.4428\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9821 - val_loss: 33.5061\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 23.0198 - val_loss: 37.0135\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.8742 - val_loss: 42.0072\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8682 - val_loss: 39.3285\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9958 - val_loss: 35.6381\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8670 - val_loss: 36.7228\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8337 - val_loss: 34.3940\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8409 - val_loss: 41.7435\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8850 - val_loss: 35.4245\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.9633 - val_loss: 37.6013\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8670 - val_loss: 33.2726\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.9264 - val_loss: 34.4898\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8641 - val_loss: 38.3552\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.7845 - val_loss: 40.7824\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8266 - val_loss: 34.6698\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7771 - val_loss: 36.0964\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7716 - val_loss: 36.6928\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6963 - val_loss: 35.4991\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7682 - val_loss: 39.7903\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8493 - val_loss: 52.3946\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7329 - val_loss: 35.3973\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7027 - val_loss: 35.0519\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7429 - val_loss: 35.9744\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.7788 - val_loss: 42.3770\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.8243 - val_loss: 43.8302\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7294 - val_loss: 36.0603\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.7524 - val_loss: 35.9843\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.8480 - val_loss: 39.6154\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7282 - val_loss: 34.9822\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7274 - val_loss: 35.5333\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6713 - val_loss: 34.8150\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6707 - val_loss: 38.7215\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6632 - val_loss: 35.2993\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6649 - val_loss: 34.1104\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6819 - val_loss: 36.1986\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.7378 - val_loss: 39.8403\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5904 - val_loss: 36.5009\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5981 - val_loss: 38.4196\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.6337 - val_loss: 35.4539\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6380 - val_loss: 35.7408\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6285 - val_loss: 35.1425\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.5969 - val_loss: 35.2400\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5829 - val_loss: 41.8895\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5638 - val_loss: 37.4470\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5753 - val_loss: 40.0037\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.4899 - val_loss: 35.5695\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5533 - val_loss: 37.0549\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5145 - val_loss: 35.2653\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.6193 - val_loss: 37.2320\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5451 - val_loss: 33.6403\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5141 - val_loss: 37.3616\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5278 - val_loss: 40.0246\n",
            "Epoch 309/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 14ms/step - loss: 22.5120 - val_loss: 36.0043\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 4s 23ms/step - loss: 22.4795 - val_loss: 37.0210\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 22.5281 - val_loss: 39.6215\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 22.5041 - val_loss: 34.4553\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 22.4502 - val_loss: 34.8796\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.5201 - val_loss: 35.2671\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4312 - val_loss: 34.3298\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4718 - val_loss: 35.8006\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.5399 - val_loss: 37.7007\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4271 - val_loss: 36.9129\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4565 - val_loss: 41.1417\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4376 - val_loss: 34.0597\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4190 - val_loss: 36.2126\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4168 - val_loss: 47.4400\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4616 - val_loss: 34.0341\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3249 - val_loss: 34.6793\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3900 - val_loss: 67.9385\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4353 - val_loss: 36.3327\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3443 - val_loss: 43.9364\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4853 - val_loss: 35.4751\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4404 - val_loss: 45.4263\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3232 - val_loss: 34.2318\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3715 - val_loss: 36.3397\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.2751 - val_loss: 34.6267\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4125 - val_loss: 35.6435\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.2790 - val_loss: 40.5189\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3284 - val_loss: 37.9079\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.2769 - val_loss: 41.5113\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3759 - val_loss: 37.6276\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4401 - val_loss: 45.5247\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3465 - val_loss: 36.9994\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3701 - val_loss: 38.7276\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3126 - val_loss: 33.9947\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3538 - val_loss: 38.0198\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3356 - val_loss: 35.3672\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4179 - val_loss: 51.6006\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3416 - val_loss: 33.4708\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3235 - val_loss: 33.8821\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3204 - val_loss: 35.9621\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 22.2949 - val_loss: 35.1997\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3127 - val_loss: 41.8832\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.4024 - val_loss: 35.9194\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 22.1587 - val_loss: 32.8005\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.2608 - val_loss: 34.2714\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3536 - val_loss: 37.4313\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3036 - val_loss: 38.1526\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3234 - val_loss: 37.7263\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1734 - val_loss: 34.1087\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.2232 - val_loss: 43.2620\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.2493 - val_loss: 38.0292\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.2164 - val_loss: 41.3570\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.3083 - val_loss: 36.1248\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.2614 - val_loss: 35.7610\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1833 - val_loss: 35.8566\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1421 - val_loss: 35.8131\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1436 - val_loss: 35.5739\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1623 - val_loss: 35.2672\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.2380 - val_loss: 34.3822\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1422 - val_loss: 34.6471\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1449 - val_loss: 34.2675\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1614 - val_loss: 56.3871\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 22.2786 - val_loss: 34.7884\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 22.1472 - val_loss: 39.2099\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1638 - val_loss: 41.7974\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1247 - val_loss: 40.9677\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1376 - val_loss: 44.5666\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1021 - val_loss: 41.1612\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 22.1704 - val_loss: 38.2235\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 22.2163 - val_loss: 35.6730\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 22.2465 - val_loss: 34.5400\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1058 - val_loss: 38.7212\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1055 - val_loss: 37.4139\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 22.1015 - val_loss: 38.7945\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1191 - val_loss: 36.3764\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0919 - val_loss: 37.6012\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 22.0586 - val_loss: 39.8719\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1298 - val_loss: 33.9611\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0670 - val_loss: 46.0894\n",
            "Epoch 387/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0978 - val_loss: 37.8834\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 21.9966 - val_loss: 38.8159\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 22.1375 - val_loss: 35.7344\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0867 - val_loss: 36.3256\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0504 - val_loss: 45.8680\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0506 - val_loss: 37.9086\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0266 - val_loss: 35.9201\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0545 - val_loss: 35.0809\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.1193 - val_loss: 36.2429\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0319 - val_loss: 34.8461\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0653 - val_loss: 41.8044\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0730 - val_loss: 38.5345\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0117 - val_loss: 47.5704\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 21.9560 - val_loss: 37.3447\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0693 - val_loss: 47.9551\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0371 - val_loss: 35.6011\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0934 - val_loss: 34.8981\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0176 - val_loss: 35.0174\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0307 - val_loss: 34.7310\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 21.9647 - val_loss: 34.6197\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0351 - val_loss: 36.5751\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 21.9943 - val_loss: 36.4715\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0101 - val_loss: 33.6661\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0273 - val_loss: 33.6273\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 21.9958 - val_loss: 33.8718\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 21.9641 - val_loss: 36.0349\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0116 - val_loss: 35.7435\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 21.9857 - val_loss: 34.1855\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 21.9523 - val_loss: 33.7593\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0069 - val_loss: 35.6948\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0082 - val_loss: 42.3344\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0419 - val_loss: 33.5856\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 21.9609 - val_loss: 50.4279\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 22.0409 - val_loss: 37.5036\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9821 - val_loss: 41.7184\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 21.9389 - val_loss: 36.2347\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 21.9118 - val_loss: 36.9878\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9560 - val_loss: 34.4405\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9407 - val_loss: 35.2062\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9921 - val_loss: 38.0033\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9681 - val_loss: 36.9700\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9319 - val_loss: 35.5447\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9252 - val_loss: 34.9363\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9632 - val_loss: 33.6600\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9223 - val_loss: 35.3685\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8927 - val_loss: 44.1823\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9941 - val_loss: 34.2796\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9957 - val_loss: 38.8624\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9271 - val_loss: 37.3813\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8799 - val_loss: 36.7926\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9377 - val_loss: 36.5119\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9418 - val_loss: 40.2347\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9110 - val_loss: 33.6744\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8634 - val_loss: 33.7220\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9517 - val_loss: 39.8837\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8567 - val_loss: 38.5657\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8272 - val_loss: 35.0987\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9598 - val_loss: 40.9427\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9653 - val_loss: 44.4382\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8625 - val_loss: 37.4356\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8222 - val_loss: 35.2212\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8942 - val_loss: 37.4709\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 22.0210 - val_loss: 39.3913\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8875 - val_loss: 37.4457\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.9104 - val_loss: 32.9297\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8305 - val_loss: 34.1067\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8818 - val_loss: 37.3536\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8761 - val_loss: 38.2191\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.8318 - val_loss: 37.5732\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 21.8154 - val_loss: 37.7026\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7642 - val_loss: 35.0921\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 21.7916 - val_loss: 36.7320\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 21.8941 - val_loss: 34.9904\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 21.7863 - val_loss: 37.9559\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 5s 28ms/step - loss: 21.7934 - val_loss: 38.8741\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7982 - val_loss: 35.9762\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 10s 63ms/step - loss: 21.8265 - val_loss: 34.0166\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.8442 - val_loss: 34.3478\n",
            "Epoch 465/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 6s 35ms/step - loss: 21.8438 - val_loss: 34.8142\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7910 - val_loss: 41.7301\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 6s 34ms/step - loss: 21.8726 - val_loss: 35.1716\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 6s 34ms/step - loss: 21.7385 - val_loss: 36.6080\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7865 - val_loss: 40.7599\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.8324 - val_loss: 38.7235\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.8444 - val_loss: 33.9466\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7767 - val_loss: 40.9630\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7643 - val_loss: 34.0780\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.8204 - val_loss: 33.9583\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7862 - val_loss: 33.9446\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7527 - val_loss: 41.3752\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7464 - val_loss: 42.3171\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7355 - val_loss: 34.3660\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7840 - val_loss: 39.9803\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7688 - val_loss: 50.9346\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7384 - val_loss: 41.5472\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.8283 - val_loss: 42.5200\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7336 - val_loss: 33.7932\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7534 - val_loss: 35.6224\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7214 - val_loss: 36.1168\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.6657 - val_loss: 38.3200\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.6880 - val_loss: 37.4039\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.6429 - val_loss: 33.4366\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7087 - val_loss: 38.8547\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 6s 34ms/step - loss: 21.7336 - val_loss: 38.3382\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7724 - val_loss: 32.9321\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7622 - val_loss: 38.6626\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.6918 - val_loss: 33.2632\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7206 - val_loss: 35.6676\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.6853 - val_loss: 35.0868\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7395 - val_loss: 35.6909\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.6979 - val_loss: 41.8500\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7229 - val_loss: 37.0068\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7500 - val_loss: 36.6101\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 21.7660 - val_loss: 36.3544\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  1.0599419889383335 \n",
            "MAE:  4.428841768921792 \n",
            "SD:  5.935563231229617\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "outputId": "27e01dab-bfdf-463c-da82-64d5d2b5d7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  -0.410428222036694 \n",
            "Ensemble_std:  5.918011418910268\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "_BP_hv3_4(2).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}