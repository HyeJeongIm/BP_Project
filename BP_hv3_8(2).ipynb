{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/BP_hv3_8(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiiiBla2-j1S"
      },
      "source": [
        "# batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCoux5AOZnK",
        "outputId": "05ec15c5-4d35-4fbf-df2c-7869c9eda218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version :  3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]\n",
            "TensorFlow version :  2.3.0\n",
            "Keras version :  2.4.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러오기\n",
        "# train_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/train_shuffled_raw_v1.mat')\n",
        "# test_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/test_not_shuffled_raw_v1.mat')\n",
        "\n",
        "train_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/new/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/new/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "b570a293-9e53-473a-f3ed-a9b19951a83d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "8f731ca9-0203-46e7-87ab-30015e694029"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "8abc45d0-bd5c-49cd-8d12-1cde358bdca2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "outputs": [],
      "source": [
        "#parameter\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN",
        "outputId": "1159e6fd-8d70-4858-e9cd-392ba288e282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_44 (Dense)             (None, 8)                 1024      \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_54 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_55 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_57 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 3,145\n",
            "Trainable params: 2,809\n",
            "Non-trainable params: 336\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8SHBwBOZnO"
      },
      "outputs": [],
      "source": [
        "# model = model1()\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGT6-7NcOZnO",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 12413.4277 - val_loss: 12426.8613\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 12136.9639 - val_loss: 11977.1943\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 11769.5869 - val_loss: 11308.8809\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 11311.0703 - val_loss: 10510.4355\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 10723.3359 - val_loss: 10208.2969\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 9982.2705 - val_loss: 9780.9590\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 9115.8525 - val_loss: 9350.4443\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 8056.0625 - val_loss: 6901.6777\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 6820.3628 - val_loss: 5795.8013\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 5632.9956 - val_loss: 4791.5630\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 4529.5386 - val_loss: 3593.1943\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3554.0100 - val_loss: 2971.1479\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2716.8750 - val_loss: 2336.3513\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2013.0809 - val_loss: 1627.2522\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1460.7708 - val_loss: 1152.2429\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1040.3662 - val_loss: 717.1084\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 743.5624 - val_loss: 719.8412\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 544.1295 - val_loss: 404.6678\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 412.5424 - val_loss: 368.8566\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 335.6196 - val_loss: 311.4525\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 303.3554 - val_loss: 286.2395\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 286.1638 - val_loss: 271.3548\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 275.8802 - val_loss: 272.6797\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 271.7849 - val_loss: 265.2994\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 268.0418 - val_loss: 263.1129\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 267.3790 - val_loss: 265.4344\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 267.1518 - val_loss: 257.6725\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 267.6135 - val_loss: 259.9574\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 264.2375 - val_loss: 252.7695\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 263.5848 - val_loss: 250.2811\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 263.3741 - val_loss: 252.1699\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 262.3735 - val_loss: 251.8343\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 262.7977 - val_loss: 249.9025\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 261.9379 - val_loss: 249.4373\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 262.0589 - val_loss: 250.8833\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 261.5922 - val_loss: 252.7027\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 261.8173 - val_loss: 254.7712\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 260.6281 - val_loss: 254.7881\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 259.9084 - val_loss: 251.1738\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 258.9196 - val_loss: 253.7859\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 258.0609 - val_loss: 252.7314\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 255.9257 - val_loss: 252.5828\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 254.9727 - val_loss: 249.5750\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 255.2193 - val_loss: 246.1725\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 257.0404 - val_loss: 255.2922\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 256.3446 - val_loss: 251.3553\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 255.1706 - val_loss: 251.2837\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 253.3569 - val_loss: 243.6060\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 250.9602 - val_loss: 258.1115\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 246.9498 - val_loss: 250.8912\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 246.6015 - val_loss: 242.3595\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 244.5096 - val_loss: 236.0180\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 242.8844 - val_loss: 246.6734\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 242.0563 - val_loss: 249.1519\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 241.1311 - val_loss: 259.8271\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 239.1530 - val_loss: 264.5796\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 234.4935 - val_loss: 235.2707\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 229.5507 - val_loss: 229.6863\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 224.9538 - val_loss: 226.2670\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 219.5978 - val_loss: 223.3573\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 213.8632 - val_loss: 245.2116\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 207.0829 - val_loss: 448.3035\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 198.4206 - val_loss: 202.1611\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 190.2750 - val_loss: 184.9638\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 183.7406 - val_loss: 414.6053\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 176.9036 - val_loss: 179.5310\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 168.3239 - val_loss: 197.6937\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 161.2499 - val_loss: 207.6768\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 154.9586 - val_loss: 153.1611\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 152.6141 - val_loss: 177.9083\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 150.0632 - val_loss: 142.3068\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 148.3215 - val_loss: 346.2616\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 153.7795 - val_loss: 368.2142\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 149.6410 - val_loss: 220.8096\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 140.4506 - val_loss: 167.1118\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 146.0619 - val_loss: 549.7325\n",
            "Epoch 77/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 146.9017 - val_loss: 149.0202\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 142.8302 - val_loss: 184.5125\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 135.8167 - val_loss: 181.1685\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 137.0387 - val_loss: 217.6524\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 144.1260 - val_loss: 176.5981\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 148.3069 - val_loss: 382.6118\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 174.3730 - val_loss: 224.3286\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 188.7648 - val_loss: 272.0720\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 186.5823 - val_loss: 172.9378\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 169.1125 - val_loss: 156.9466\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 154.8879 - val_loss: 149.4703\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 149.3575 - val_loss: 156.0046\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 148.3909 - val_loss: 163.4837\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 141.5895 - val_loss: 149.7999\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 140.1388 - val_loss: 139.8078\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 133.3961 - val_loss: 172.6064\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 132.3422 - val_loss: 186.8690\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 130.8006 - val_loss: 148.8658\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 130.5044 - val_loss: 143.9812\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 129.0836 - val_loss: 134.7468\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 129.3844 - val_loss: 129.3141\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 128.3450 - val_loss: 239.5932\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 127.0686 - val_loss: 158.2366\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 125.5479 - val_loss: 127.5649\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 122.7354 - val_loss: 125.9541\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 121.8250 - val_loss: 143.5080\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 121.6165 - val_loss: 129.6471\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 120.3672 - val_loss: 127.9494\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 120.0615 - val_loss: 125.8197\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 120.3819 - val_loss: 200.0479\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 119.3707 - val_loss: 142.0712\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 118.1688 - val_loss: 157.3227\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 116.7878 - val_loss: 131.3531\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 116.7016 - val_loss: 142.4335\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 115.4276 - val_loss: 156.8748\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 114.3587 - val_loss: 139.3996\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 115.3917 - val_loss: 142.8763\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 115.5124 - val_loss: 125.9100\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 113.0665 - val_loss: 213.4692\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.7705 - val_loss: 135.3523\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.9094 - val_loss: 155.1428\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 118.4571 - val_loss: 137.5659\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 112.9567 - val_loss: 181.3510\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 115.5360 - val_loss: 167.9197\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.9360 - val_loss: 275.5333\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 115.0866 - val_loss: 121.2363\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 120.4165 - val_loss: 125.7964\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 116.5256 - val_loss: 144.0778\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 115.5251 - val_loss: 125.1467\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 112.9962 - val_loss: 116.4577\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 115.3025 - val_loss: 139.9567\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 115.2667 - val_loss: 163.6534\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 118.6320 - val_loss: 124.3258\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 115.8449 - val_loss: 120.6961\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 114.4724 - val_loss: 196.9050\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 113.6602 - val_loss: 175.4283\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 113.4156 - val_loss: 143.9743\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 118.6785 - val_loss: 474.8600\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 122.6783 - val_loss: 186.5839\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 118.5941 - val_loss: 468.0580\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.8643 - val_loss: 341.3704\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 115.4401 - val_loss: 205.0251\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.8329 - val_loss: 185.9031\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.4218 - val_loss: 120.3456\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.8142 - val_loss: 128.6463\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.3874 - val_loss: 124.9950\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.6616 - val_loss: 134.7590\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 112.3433 - val_loss: 126.8801\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.2426 - val_loss: 131.3825\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.5621 - val_loss: 348.5593\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 112.2077 - val_loss: 201.4797\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.3520 - val_loss: 125.0981\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.7306 - val_loss: 123.7363\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.9160 - val_loss: 120.2738\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.0592 - val_loss: 174.0352\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.7394 - val_loss: 117.8573\n",
            "Epoch 153/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 106.9160 - val_loss: 122.7914\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.0608 - val_loss: 118.9700\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.8389 - val_loss: 155.5064\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.5868 - val_loss: 224.3470\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.3447 - val_loss: 164.7857\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 107.8857 - val_loss: 168.1666\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.8332 - val_loss: 116.4041\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.2795 - val_loss: 142.9622\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 107.6515 - val_loss: 126.8602\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.3571 - val_loss: 195.4774\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.0816 - val_loss: 138.9731\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.4104 - val_loss: 529.7252\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.9870 - val_loss: 116.0454\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.8222 - val_loss: 131.5481\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.1320 - val_loss: 126.1160\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.4490 - val_loss: 146.2099\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.0632 - val_loss: 111.4983\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.0663 - val_loss: 137.9444\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.8992 - val_loss: 175.9562\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.7838 - val_loss: 168.6802\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.2355 - val_loss: 157.2920\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.0515 - val_loss: 119.3522\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.1623 - val_loss: 120.8413\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.2310 - val_loss: 118.6282\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.2506 - val_loss: 133.2870\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.4851 - val_loss: 128.6420\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.4526 - val_loss: 170.2651\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.8196 - val_loss: 113.8067\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.0849 - val_loss: 159.4539\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.0468 - val_loss: 124.3233\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.6933 - val_loss: 175.1918\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.0646 - val_loss: 107.0457\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.9366 - val_loss: 113.5662\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.9372 - val_loss: 116.9115\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.5256 - val_loss: 110.0574\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.3866 - val_loss: 110.8834\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.5695 - val_loss: 120.6096\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.3624 - val_loss: 111.5798\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.2763 - val_loss: 115.3560\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.2286 - val_loss: 127.5796\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.7809 - val_loss: 139.0652\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.0670 - val_loss: 113.0006\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.7578 - val_loss: 199.7425\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.6816 - val_loss: 114.5085\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.7045 - val_loss: 172.3125\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.3285 - val_loss: 122.0459\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.9935 - val_loss: 120.9103\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.9573 - val_loss: 254.3023\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.1974 - val_loss: 154.2892\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.2112 - val_loss: 105.9882\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.7988 - val_loss: 105.8507\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.4459 - val_loss: 172.9988\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.3007 - val_loss: 115.5834\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.0573 - val_loss: 133.9660\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.0088 - val_loss: 133.5812\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.2816 - val_loss: 196.4024\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.4279 - val_loss: 106.8288\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.2403 - val_loss: 146.7869\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.0857 - val_loss: 229.3021\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.3886 - val_loss: 114.1018\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.1725 - val_loss: 117.9740\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.1180 - val_loss: 106.6077\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.7354 - val_loss: 122.5187\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 98.3288 - val_loss: 205.6380\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 101.4269 - val_loss: 111.4236\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.1668 - val_loss: 122.0498\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.3886 - val_loss: 119.6966\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.3297 - val_loss: 318.8631\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.5667 - val_loss: 134.2888\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.6090 - val_loss: 126.3160\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.9432 - val_loss: 143.7438\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.7120 - val_loss: 112.5049\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.4967 - val_loss: 116.0297\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.5013 - val_loss: 104.5016\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.9986 - val_loss: 116.2842\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.2373 - val_loss: 105.2152\n",
            "Epoch 229/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 97.3764 - val_loss: 114.9902\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.5345 - val_loss: 206.0852\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.3847 - val_loss: 108.1662\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.7008 - val_loss: 119.7944\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.4584 - val_loss: 137.0208\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.9492 - val_loss: 102.2753\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.6240 - val_loss: 189.5241\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.3652 - val_loss: 100.6655\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.3145 - val_loss: 120.2293\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.5207 - val_loss: 104.0321\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.1545 - val_loss: 105.4509\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.2236 - val_loss: 158.6926\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.7403 - val_loss: 110.1103\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.3870 - val_loss: 123.4192\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.4614 - val_loss: 200.7178\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.0100 - val_loss: 124.2548\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.9439 - val_loss: 215.3922\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.6412 - val_loss: 119.0794\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.0575 - val_loss: 108.9003\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.5503 - val_loss: 144.1881\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.0887 - val_loss: 167.0160\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.4612 - val_loss: 154.4778\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.2144 - val_loss: 111.1130\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.6194 - val_loss: 106.5992\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.6023 - val_loss: 137.0621\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.8418 - val_loss: 209.4005\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.5844 - val_loss: 120.5900\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.3234 - val_loss: 114.1639\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.1220 - val_loss: 148.6041\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.0555 - val_loss: 379.3376\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.2177 - val_loss: 125.9079\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.5350 - val_loss: 121.7904\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.0770 - val_loss: 117.4486\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.5610 - val_loss: 176.7697\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.9344 - val_loss: 111.7937\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.4515 - val_loss: 138.1794\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.3650 - val_loss: 102.6554\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.6725 - val_loss: 99.1283\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.9827 - val_loss: 111.7188\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.2008 - val_loss: 105.9937\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.9105 - val_loss: 159.3901\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.7545 - val_loss: 134.6628\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.5287 - val_loss: 117.3799\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.6752 - val_loss: 167.2030\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.9676 - val_loss: 103.0075\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.8330 - val_loss: 107.8165\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.0807 - val_loss: 152.1814\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.3460 - val_loss: 268.6996\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.9020 - val_loss: 141.7681\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.7467 - val_loss: 381.3711\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.8562 - val_loss: 113.8085\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.7239 - val_loss: 98.8298\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.5965 - val_loss: 114.4629\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.9578 - val_loss: 163.9971\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.0455 - val_loss: 114.7722\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.1797 - val_loss: 154.5378\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.9980 - val_loss: 114.4295\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.4955 - val_loss: 102.2658\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.0771 - val_loss: 145.9343\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.7449 - val_loss: 121.2993\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.1149 - val_loss: 100.6291\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.5988 - val_loss: 149.7626\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.0893 - val_loss: 108.2568\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.1675 - val_loss: 106.7275\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.0784 - val_loss: 116.7852\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.4550 - val_loss: 104.0310\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.3138 - val_loss: 110.3453\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.8271 - val_loss: 108.3848\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.7643 - val_loss: 117.7375\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.7804 - val_loss: 208.3131\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.8804 - val_loss: 177.3690\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.1712 - val_loss: 103.2972\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.0277 - val_loss: 100.5625\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.0382 - val_loss: 113.6574\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.2932 - val_loss: 97.2575\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.2539 - val_loss: 154.9926\n",
            "Epoch 305/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 89.6332 - val_loss: 100.1040\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.5073 - val_loss: 100.6780\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.4495 - val_loss: 191.2479\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.1152 - val_loss: 127.8593\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.0896 - val_loss: 105.1322\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.9465 - val_loss: 103.7347\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.8731 - val_loss: 112.5565\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.9602 - val_loss: 110.4613\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.3867 - val_loss: 110.1338\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.5404 - val_loss: 106.9445\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.7420 - val_loss: 136.8715\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.9577 - val_loss: 131.5057\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.8271 - val_loss: 95.6589\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.6802 - val_loss: 122.0862\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.6870 - val_loss: 103.7633\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.6340 - val_loss: 176.5160\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.3868 - val_loss: 149.7490\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.5684 - val_loss: 115.4043\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.9231 - val_loss: 136.4970\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.1210 - val_loss: 96.9942\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.5569 - val_loss: 124.6454\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.4193 - val_loss: 98.9813\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.8070 - val_loss: 110.1365\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.8640 - val_loss: 104.4599\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.5139 - val_loss: 121.9157\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.2336 - val_loss: 109.1527\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.6221 - val_loss: 121.0379\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.8972 - val_loss: 144.6080\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.3864 - val_loss: 109.6880\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.1213 - val_loss: 101.0274\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.6608 - val_loss: 105.5565\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.2204 - val_loss: 119.4816\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.7283 - val_loss: 119.8627\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.2468 - val_loss: 104.5714\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.4959 - val_loss: 112.4282\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.0535 - val_loss: 187.9505\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.9576 - val_loss: 106.7277\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.1513 - val_loss: 100.6286\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.5624 - val_loss: 102.2993\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.6256 - val_loss: 122.1841\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.8232 - val_loss: 106.6340\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.3640 - val_loss: 115.7028\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.7861 - val_loss: 118.7645\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.7281 - val_loss: 100.1120\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.5608 - val_loss: 133.4596\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.8874 - val_loss: 105.0683\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.6205 - val_loss: 105.2546\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.9773 - val_loss: 101.9262\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.2353 - val_loss: 449.0690\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.6832 - val_loss: 98.5239\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.8858 - val_loss: 107.6625\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.7060 - val_loss: 98.3497\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.6607 - val_loss: 102.6526\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.9367 - val_loss: 139.1409\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.8835 - val_loss: 191.7919\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.8608 - val_loss: 113.4297\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.2285 - val_loss: 103.8261\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.4683 - val_loss: 179.8300\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.7486 - val_loss: 131.9909\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.4493 - val_loss: 124.8970\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.6742 - val_loss: 100.3203\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.8750 - val_loss: 135.5831\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.5048 - val_loss: 98.2997\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.8804 - val_loss: 106.4144\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.4397 - val_loss: 159.2286\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.8019 - val_loss: 124.0827\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.2473 - val_loss: 101.6851\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.5514 - val_loss: 117.3331\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.6166 - val_loss: 108.3089\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.5581 - val_loss: 102.7854\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.3401 - val_loss: 106.0392\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.3500 - val_loss: 99.7553\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.4397 - val_loss: 151.6644\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.9333 - val_loss: 114.9284\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.7548 - val_loss: 103.6867\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.3259 - val_loss: 108.9737\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.5449 - val_loss: 109.1009\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.5183 - val_loss: 109.6902\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.8160 - val_loss: 102.4535\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.3342 - val_loss: 97.0156\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.1720 - val_loss: 156.6009\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.7121 - val_loss: 99.9922\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.9113 - val_loss: 143.1286\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.1734 - val_loss: 122.3088\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.8424 - val_loss: 118.8504\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.6185 - val_loss: 99.6540\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.4908 - val_loss: 143.0543\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.3852 - val_loss: 116.8929\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.1422 - val_loss: 108.0718\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.7589 - val_loss: 118.4478\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.6650 - val_loss: 96.6730\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.4160 - val_loss: 100.3471\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.4509 - val_loss: 117.0711\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.1924 - val_loss: 116.1402\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.2732 - val_loss: 108.0209\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.1672 - val_loss: 104.0442\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.9082 - val_loss: 105.3235\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.5832 - val_loss: 106.0459\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.6828 - val_loss: 130.4489\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.5591 - val_loss: 111.9689\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.6215 - val_loss: 138.9933\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.1860 - val_loss: 110.6115\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.3274 - val_loss: 97.2457\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.3700 - val_loss: 103.8678\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.4888 - val_loss: 108.5134\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.3860 - val_loss: 98.7951\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.4779 - val_loss: 103.4689\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.2380 - val_loss: 97.0835\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.4266 - val_loss: 100.6701\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.7222 - val_loss: 210.8887\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.8875 - val_loss: 145.6635\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.7379 - val_loss: 138.0964\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.0706 - val_loss: 104.7758\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.2126 - val_loss: 119.1761\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.2279 - val_loss: 125.1079\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.5656 - val_loss: 103.9761\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.1290 - val_loss: 104.1090\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.9171 - val_loss: 102.2403\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.3377 - val_loss: 108.8759\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.9981 - val_loss: 98.2919\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.1187 - val_loss: 106.5479\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.9923 - val_loss: 170.4055\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.0816 - val_loss: 106.2445\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.7335 - val_loss: 113.4297\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.3152 - val_loss: 167.3247\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.7389 - val_loss: 119.2762\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.6806 - val_loss: 136.9147\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.6075 - val_loss: 109.0897\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.7696 - val_loss: 125.9441\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.6467 - val_loss: 101.0288\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.6755 - val_loss: 145.4427\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.7397 - val_loss: 116.6237\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 87.83 - 2s 11ms/step - loss: 87.8253 - val_loss: 104.7472\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.4143 - val_loss: 102.0110\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.3755 - val_loss: 123.4150\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.4964 - val_loss: 116.5341\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.2121 - val_loss: 102.8039\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.0102 - val_loss: 97.8647\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.4049 - val_loss: 100.3078\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.2186 - val_loss: 113.2989\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.6909 - val_loss: 105.9313\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.6100 - val_loss: 146.2309\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.4391 - val_loss: 128.9740\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.5229 - val_loss: 101.6774\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.7640 - val_loss: 99.1599\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.5430 - val_loss: 101.3064\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.5719 - val_loss: 100.9869\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.5131 - val_loss: 94.7862\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.5937 - val_loss: 106.9722\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.3069 - val_loss: 176.7645\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.8825 - val_loss: 132.5618\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.8048 - val_loss: 175.6611\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.9623 - val_loss: 146.5811\n",
            "Epoch 458/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 85.7264 - val_loss: 117.8221\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.3359 - val_loss: 110.5189\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.2320 - val_loss: 106.5770\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.1462 - val_loss: 266.4337\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.8622 - val_loss: 102.4337\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.2084 - val_loss: 102.1921\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.5805 - val_loss: 116.2352\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.7758 - val_loss: 97.2011\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.3693 - val_loss: 136.7466\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.4658 - val_loss: 129.6941\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.5690 - val_loss: 161.7105\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 88.4289 - val_loss: 131.7733\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.2238 - val_loss: 127.8493\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.2403 - val_loss: 115.2362\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.5338 - val_loss: 104.7124\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.6068 - val_loss: 101.8995\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.2146 - val_loss: 105.5854\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.3205 - val_loss: 111.9280\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.7645 - val_loss: 101.3354\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.7946 - val_loss: 175.7762\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.6248 - val_loss: 115.0061\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.3105 - val_loss: 124.0818\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.4502 - val_loss: 163.6292\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.6124 - val_loss: 115.2816\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.6476 - val_loss: 114.5928\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 86.89 - 2s 10ms/step - loss: 86.7068 - val_loss: 105.1149\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.6542 - val_loss: 112.0891\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.5034 - val_loss: 94.9199\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.0105 - val_loss: 112.6609\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.0250 - val_loss: 142.8645\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.4496 - val_loss: 137.2295\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.9285 - val_loss: 98.5510\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.4988 - val_loss: 100.3231\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.6141 - val_loss: 117.8394\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.2221 - val_loss: 191.6899\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.3132 - val_loss: 106.1752\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.2712 - val_loss: 145.2126\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.1476 - val_loss: 100.3746\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.2153 - val_loss: 151.7892\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.5955 - val_loss: 110.3991\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.4740 - val_loss: 154.6121\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.6396 - val_loss: 108.9971\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.7698 - val_loss: 119.2762\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -2.7485498159107142 \n",
            "MAE:  8.320481128130838 \n",
            "SD:  10.569847645632391\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAFBCAYAAAAsfIegAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABYJElEQVR4nO2dd5gV1fnHv+8WdhcWWHpXinRXQYoaxdi7YkeDDVESY4/dyM8SW4wtdo0aUVGxRmJUVCQiGgsgVdqCIAtIZ2Fp287vjzOHOXd26r0zt+37eZ773LlTzpyZO/Odd97znveQEAIMwzBMeOSkugIMwzDZBgsrwzBMyLCwMgzDhAwLK8MwTMiwsDIMw4QMCyvDMEzIRCasRFRIRN8T0Wwimk9EdxnzuxHRd0RURkQTiKiRMb/A+F1mLO8aVd0YhmGiJEqLdTeAI4UQ+wMYAOB4IjoIwF8BPCqE2AfAZgCjjfVHA9hszH/UWI9hGCbjiExYhaTS+JlvfASAIwG8Y8wfB+A0Y3q48RvG8qOIiKKqH8MwTFRE6mMlolwimgVgHYDPACwFsEUIUWOsUg6gkzHdCcBKADCWVwBoFWX9GIZhoiAvysKFELUABhBRCYD3AfRJtEwiGgNgDAA0adJkUJ8+3kVuXbgaS7Z3RO/eQHFxojWwMGNG7O9Bg4DaWmDWrNj5/fsDa9YAmzYBnTsD7dqFXBEHfv5Z7lOvn0LVfeBAIMflGavW69MHaNIk/Dq6UVYGVFQAPXoAJSXhlLl5M7BsGdC8ObDPPu7r7twJ/PSTnNbPXbqxYQOwYgXQtCnQq1eqa5PxzJgxY4MQok3cBQghkvIB8H8AbgSwAUCeMe9gAJOM6UkADjam84z1yK3MQYMGCT9MO3KsAISYNMnX6sEAYj9CCLFpU/35ixYJcf75cvrhhyOoiANqn+pTXV2/7lu3upeh1vv222jrascpp8h9f/BBeGW+8YYs89RTvdedPTv2v01XXnpJ1vHYY1Ndk6wAwHSRgN5FGRXQxrBUQURFAI4BsADAFABnGatdBOADY3qi8RvG8i+MA0yYooI6ANL4SAp21dbnJTPxjXVfXnULUlamki3HoZONx5TBROkK6ABgHBHlQvpy3xJCfEhEPwF4k4juAfAjgBeN9V8E8CoRlQHYBODcsCpSVCS/UyqsQZZHSSLC2hDhc8PEQWTCKoSYA2CgzfxlAIbazN8F4Owo6lJUKG+OpAmrF5lqsaaSMOsYpKxMODdA5tSzgdAgel4VNZZRWzt2JGmHTuKloscy1WJNRb2jiLhjEWIipkEIa+Mm8uZskK6ATLdYU123VO/fL5lSzwZCgxBWZbE2yMYrK3V19eels8WqCNNyzUZXAJNWNAhhzSvMQx6qsXO7jaikArZYU4s63mzq2NfQ/sM0p0EIKwoKUISdyRNWp4s8032sqSSKOvopMxPODZN2NAxhbdQIRdiJHZW1ydlfOrkCMj2ONdWNV5kmrJlW3yylQQlryi1Wv8ujJFMt1jBhVwATMQ1DWAsK0Bg7sKMySRdftlqs2UY2ugKy6WGRwTQMYW3cGM2wFVsrUnyTZLqPNRX1TpVvNcr9R0Gm1LOB0GCEtQRbUFGRpP2lkyvAui+7cCu7edlMNroCmLSiYQhrkyZojgps2ZqkGymdXAFu9XCb53fbqEm1+GWKJZgp9WwgNChhrajMTc7+nC5yNZ99rKklG10BTFrRMIRVuQK2R5rX251ssFizhWw83mw8pgymYQirYbHuqsrF7t1J2F+2WqzZcvOyxcpETMMQ1saN0Ryy5SopDVjpJKxOdfCal26kqo6ZcG6AzKlnA6FhCGuTJijBFgApFFYh0kNYE0nCki00tONlkk6DEVZlsW7enOK6AOHf2HPnAu+8Y78sW1wBnN3KH5lW3ywlha05SSQ/H3vlrgZqgeXLgaH1xi8ImWRbrPvt57/cTHUFpIpMOTeZUs8GQsOwWAH0bLIahDosXJiEnaWTjzVbLNYw953NIpTquF8GQAMS1sZbf8XeWIGFM7ZHvzMvYX333dT1dso0izXK7FZ+yk7nc6OTKfVsIDQYYQWAvliAhXOro9+Rl3iVlQFPPx19PezqkmnCGiXZmISFSQsajrDedBP6YCEWrioOz1gMKlL6sjVrQqpEQDLVFRAm2XIcOtl4TBlMwxHW229HHyzEzqo8rFwZUpler/zWefr8ZPnCMj0JS5TZrfz8B1Onhr9/JutpOMJaXIw++csASBfnN99EtB+rgFqXKXJSdOrZFWDiddxlZcDYscmpS6I01P8wTWkY4VYAQIQhbVeg97Y1uP76DgBkTGtJSQJl+rFMndZPlrBmelRAKodmSYugZ8aRpUuB5s2B1q1TXZN6NByLFUBRu2b4qtO5OPXQTQBkXH1CJBJonqqwGLZYszsfa0P6L/fZB+jWLdW1sKVBCSs6dkSbBVPx9LRSAMDs2XGW8/LLwJdfssWaTBqSYMRDQz0/lZWproEtDUtYhw0DAHTEanTuLPDgg8Dnn8dxTY4aBRx+uP0yJ2G1zmcfazBS0UEgE84Lk5Y0LGE96SQAAAF458WtqKsDjjkG6N4dmDLFp0tt61ZzOkhUgHV+Jlqs2UI2Hm82HlMG07CEtX9/4PHHAQAH7rUGX3wBtGwp8wcceSTQsSO8Q7F+/tmcTkSkUmWxJpLdqqG5AjLRB5uJdc5CGpawAkDv3vL75ZfRqxewcaNsxLrpJmDXLuDKKz3COpWwtmkTzMeaKldAtlis7ApwJ5Pq2gBoeMLapo38/utfgR07AAD77it/PvooMHGiR2/T8nL53aFDYq4AjgoIBidhYTKIhieseszb+vUxi665Bjj+eODmmy0ugSeeAJ57Tk5v2SK/mzSxL9+vxZqqnlccFZCd4Vb8sEgrGp6wtm1rTq9bF7OICHjySWnITpgAXHwx8OabAF55BfjnP+VK+hAEQcOtdDgqIBhRdm1lmJBpeMJaUAB8/bWctggrAPToAfTrB9x4IzBuHHDeecDiTa1N61YJa1VV8Bsz08OtUtmAxK4Ad7LxmDKYhiesgGz+B2KFddo04NBDgXffxWWXxa7+17UXm+sqYa2uzszGq3RIwrJtm/9ub6lMwsJixcRJZHc3EXUhoilE9BMRzSeia4z5dxLRKiKaZXxO1La5lYjKiGgRER0XVd32NGDpwvr3v0tL9qyzcNW6sXj03u34HkNwIcbh3R3HY3dlFbBzZ/zCqpYpMtFiDYsTTjCHk/ELuwL8kY3HlIFEeXfXALheCNEPwEEAriCifsayR4UQA4zPRwBgLDsXQH8AxwN4mohyI6lZkybyM2kSMGOGFMyPP96zOPf+e3DtzIswBNNxNt5GhWiOrzBMugN0V4AT6SSs6dh4pVwxQWBXgDvZeEwZTGR3txBijRBipjG9DcACAJ1cNhkO4E0hxG4hxM8AygBEN+xf//6yu9XgwcD99wPbt8e29L/7LgDgt/gSuajBFBwhhVX1vHKzWO3gJCzx7TdKH2s2RQUwaUVSzCYi6gpgIIDvjFlXEtEcInqJiFoY8zoB0IOcyuEuxInRrp05/Ze/yPyBEycCxcXAXXftWdQUlRiEGfgah0jXQaa5AtLRYlUE8e2yReYOn5+0IvK7m4iKAbwL4FohxFYAzwDoAWAAgDUAHg5Y3hgimk5E09db4lAD8cADwJlnAscZrtwLL5T9WrdtA048MWbVfTEPC9GnvisgSAeBZMSx+rm5Ms1ijWfdZJaVbrAVnhZEKqxElA8pquOFEO8BgBBirRCiVghRB+AfMF/3VwHoom3e2ZgXgxDieSHEYCHE4DaqESoe+vUD3nlHBqw+9FBspvh99wVyDfduq1bojUVYi/bY8r8FZpqyaodBCf1arFHgRzTTSVhTZbFmo7Bm4zFlMFFGBRCAFwEsEEI8os3voK12OoB5xvREAOcSUQERdQPQE8D3UdVvD82bA9dfH9sjq7BQ+mABoGNH9MFCAMC/J8gusDjmmOA+VitRhDj52XfQJCz6srBvXhYDJkuJ0mI9BMAFAI60hFY9SERziWgOgCMAXAcAQoj5AN4C8BOATwBcIYSojbB+7hxwANC4MdCiBYbhK7THGty4+VbUFRQBgwYl7gpIVvgQW6zRlpUuZOMxZTCRjXklhJgGmfrUykcu29wL4N6o6hSIsWOBM84Ann4aLbAFf8ONuACvYXrx4RhaWAjU1jpbf34EN1UWazoJa7r7WFmsmDhpmD2v/NC9O3DKKUBREQDgBHyMPFTjLXEWkJ8v19m+vf52fnteJUtYE7VYo7Sy0z0qIJOENZPq2gBgYfXigQeAYcPQ6pRDcDI+xLitp2NTTTO5bNs2+23itRwTJdMs1nR3BbBYMXHCwupFr17A1KnAPvtgLP6CLbVN8Yf3j5XL7ITVr/XHFiu7AqIgE+uchbCw+qVNGxyAHzG255t4e1YvzEc/+xEi/boC0sliTXYSliD7TWXPq1Sdl3hgQU0rWFj9YsTMXtruQwDAv3Ca89C7iTRevfiivOHjGdbXjzWayJhXYZMql4nfslmsmDhhYfWLkSC7I61Bv45b8D2GOrsCErFYH3hAfq9eHbyOUfhYs7HxKhtdAZlU1wYAC6tfWraU31VV6NG+EsvRNbiPVcdJVBLpkshxrMHKyiZXgIK7tKYFLKx+McKukJODru12Yjm6QmyLwBXgVUai2yRisYZNqhqv/JaZSVZgJtW1AcDC6pcBA4DrrgPGjUPXDruxFc2xZe3u+usl6goI22L1s06yX43VMaa68Sqs9RjGQmQ9r7KO3FzgEZnyoGvPxQCAnxfuRgvremH1vArLYk1nV0Cqw62yyRXAD4G0gi3WOOh2QAkAYHlZjf+N/AgckJiF5mebdErC4ke41D655xWTQbCwxkHXA1oBAJavyq+/MFGLtSE1XqV6v0F8rJnSKMQPg7SAhTUOSlrlohltxc8729VfmG6uAD/rJLvxKl18rF5lsiuAiRMW1jggAroWrZUhV3akU8+rMC3WqBuvVq8GbrhBZg6Lat9BYLFKX9L8v2FhjZNubSrthdXNYtXhtIH193vxxcDDDwNffeW9bhj75XCrzCXNj5eFNU66ds+VsazWBW5ugFSEWyVqsUZ5AVsfLmpI8ajFPxtdAQ0NFtbspOv+zVGJptiIVvUXZpOPNZ71/BLk4cKuAHcyqa5hkObHy8IaJ10Pag8A9d0BfhuvkhVuZZ1nJ+jWeevXm37OZDZeJetmyUZXgCJTohcSJc3/GxbWOOnWpwBAAGG1zvcKt0qVxVpRIRPO3HBDfOUHwclNEbXF6resTHIFpLnQhE6aHy8La5zsvbf8dowMcGK//eS3100bz00dho91yxb5/d57wffvF69wq2S5ArLRYm0opPl/w8IaJyUlQEmJwM+X3he7wMti7dhRjv7q5QoIS1j9rOM2L5mNV8lqRMvGxqtU9lBLBWl+nCysCdC1K2H56kaxM718rETyk64Wq5sroqG5AtL85m3QpPl/w8KaAN26AT//bLPAS1hzctLfYg1SZlCCuAJS2fMqzW9eWzKxzvGQ5sfJwpoAnTsDq1ZZZjr94XV1wK5d3hZrIsLqJ8FKOg/Nkqp4Wif0c5Xure1pLjShk+bHy8KaAB06AFu3AjtQZM50cgXceCMwf74Mgs/JSZ4rwM86icyLhyAPjygbrYJYrGl+I+8hU+qZKGl+nCysCdChg/xegw6xC+z+9Fmz5PeuXcl1BQT1sSYjtjSeY2RXgDuZVNcwSPPjZWFNAFthdWu8ApLfeOVnHT/xtX7LD0KUjVdPPy3LWb8+vroBmeUKUKS54IRGmh8nC2sCdOwovwMJa21tOBbrTz8BmzfHzotCWJMZbqUIo/HqxRfl9y+/1F/GFisTMSysCeDoCnCjtjYci7V/f2Do0Nh5YboCknGjRtl45aesbBLWhkaa/zcsrAnQqhWQnw+sRkdzZrIsVgAoK4v9HYbFapcLNUj5fvDKFZAuSVi4g0D6kubHycKaAERA+/ZxugJS1UHAKwlLMl0BXg+XROrh5hNlizU6tmyR5378+Gj3k+b/DQtrgnTs6DMqQOHXFRDPhROmjzXKC9fLYg1j33ZlrFkD3H67/4dWmt+8MaSLxbpsmfx++OFo95Pq4/SAh79OkA4dgDJdWN1epdXydAi3corRTIYrQBFEWMPY96hRwKRJwKBB/sqM5z949VWZHey444Jvmw1E0VPODhbW7KZDB+ArXVj79gXOP995g5qa9Ai3chLWdHAFhCGsdq6AHTvkt1+rPJ5jv/DC+LcNg3QRHBZWJhHatwc2ojVqkIs8GNbeBx84b5AuFqvTPDfRSVbjVV0dsH277NbmtO/33wcaNQJOOsl+H2FEGKT5zRtDutSVLVYA7GNNmFbGyCyb0NKc2bSp8wZ1deltsVpdAamyWIcNMwOF7dY94wzg5JO996FbrkHCu4DMigpQpFpwWFgBsLAmTEtDT2PGvioudt4g1RZrdXX9+X57XoWF1zHW1QE//hi7bhQ3UjZFBaRLXXMMSWFhjQYi6kJEU4joJyKaT0TXGPNbEtFnRLTE+G5hzCciepyIyohoDhEdEFXdwiSwxVpT4x5u5UdYg/gmreza5b5dMlwBTuW5WdNh7jsbXQGKVNeZLVYA0VqsNQCuF0L0A3AQgCuIqB+AWwBMFkL0BDDZ+A0AJwDoaXzGAHgmwrqFhhLWQBZroq4Ap2V+xFAJazo0Xvk5jigFNZtcAekmNCys0SCEWCOEmGlMbwOwAEAnAMMBjDNWGwfgNGN6OIBXhORbACVEFKCvaGqwdQXkubQJxuMKqKgwX+Gty3SCWKxOYpoOPa/0+fFYrJMnm66EREjzmzctYYsVQJJ8rETUFcBAAN8BaCeEWGMs+hVAO2O6E4CV2mblxry0xtZiVeJkJ7DxWKwlJcDZZ5u/4w1TKiz0b7GG/Qo+dy7QrFlsZnA/rgCndd04+mj7+UEtVn15ume3SpcOAiysAJIgrERUDOBdANcKIbbqy4QQAkCgM0REY4hoOhFNX59ISriQaNoUyEN1rLDW1Mjv3Nz6G8TbeKWHcMVrsRYWAjt31p/v1qU1LJ58Eti2DfjwQ28/cpg+VjeRziZXQLrBwhodRJQPKarjhRBqPOW16hXf+F5nzF8FoIu2eWdjXgxCiOeFEIOFEIPbtGkTXeV9QgS0wsbYxitlsboJayKNV36Fde5coLLS/F1UlHpXgJv/NCxXgNP+krltqkh1nZNlOaf6OD2IMiqAALwIYIEQ4hFt0UQAFxnTFwH4QJt/oREdcBCACs1lkNa0xKZgFmuYjVdu8aj77QcsWWLOc3IF2FmsybhB1L5qaoB+/cxRFqKyWK1lcbhV+LCwAoi259UhAC4AMJeIZhnzbgPwAIC3iGg0gBUAzjGWfQTgRABlAHYAGBVh3UKlFTbaC6uTjzXROFa/wmpFdwV4CavikEOc6xEPagQFfV8bNwILFtjXIdEb1elc+SkzE10BqRYcFlYAEQqrEGIaACeP/1E26wsAV0RVnyhphY1Yhu7mDC9XQBCL1ashJ0igv26x2u3Lbr+LFtmXHwZON2GY+7E7D34botL85o0hXeqarETp6XK8DnDPqxAI5GNVHQT8WktB8qd6CauTj1W3br2yc4WB1WJ1E9ZUNl6l+c1rS6rrzBYrABbWUHD0sTrFswaxWL2ENV6L1a8rQCdZFmtUrgCF34dHWK6AiorYRsQosJ6fV18F3nvPft0oyTSLdfZs4JVXwilLg7NbhUAr2oxdogg7UITG2OneeAX487HatZAr4rVYCwvlTe6nvKhuDK86Oq0TpsUaZdpAO0pKZFze1q2eqyaMqnOq0hdmmsU6YID8VucrJNhiDYHW+VKsNqC1nOFHWP2GW9lZV1FarMlwBSj8+Fj9iODjj8vQMjvszpVfYU3EYrWWvW1b/GXFs79UkWnCGhEsrCHQrmALAGCt6kTm4mP9BV3Q5PsvMH3LPu6FurkCnEQxXh9rMl0BdlEB1n0GtaCvuQbYf3/7ZXbnx+/DI5Hj1bsgh0VdXWzPtXQk01wBEcHCGgLtiuQr3q9oL2e4WKzf4DfYUVeE28os0WRqrCAv0bHOi8rHGuWF63WMXmFjdjjVN1WugCiE9e67gc6dgRUr6i9LlqXoBVusAFhYQ6F9YymseyxWl8arQkhh+6aivznz88+BHj2A118350URFeCnS2sqXAFuFquqTxSNV0FcAUFzBUQhrJ98Ir9Xrw6/7LBgixUAC2sotG2yHYA/i3U7msjv2iJz5syZ8vvHH6OxWDt3lvsoKAB273ZfN2pXgN1+3UYt8LpRg4ijW4OgV9lBjz0KYVX4Gdo7VbDFCoCFNRQK//JnlGCzLx+rElaCdnMrsSsoMOeFabEeeCAwcKCsj51QJSMJi50YOPk8gwirV33txDFTXQFupIvQsMUKgIU1HE4/He3b1pkWqy9h1S4M5fcsKPBnscbb8yonx/412K+PNayLOUjjlddrezwt+/HEsaaDK8APqRYctlgBsLCGRrvGlfVdATY+Vlth1S3WoK6AzZvNaSexVGXqFqtTeVH5WN2EOkqLNREfayI3b1VV/NvGQ7oIDQsrABbW0GjfuMJX45US1lrkmfe8myvAK451Hy1sy6s1PYjFGhV+wq3sjiMMV4DfMt229UtDtVjZFQCAhTU02hVUmBarurhcXAGA1kCfiMWq48diFcL82K2bCleA9SEQxIIOYrEGbbwK+pDRz0+6+Fj79gW++Sb59cg0YQ25PBbWkGh/RF9sQzPsyNEGEvQQ1h07jAn12qj78cIUVoXT0MTJDLdya0yyWycKV0BUjVepFFa7OgDAwoXADTckb/+ZarGysKYn7feV3VnXtuxrzvQrrMpiVSkFAf+NV07z7SxWJay1tc5ug2RGBSQj3MpOyPX9Wa33IGW7rZ8uFqvXsqjqkWnCGvJ1z8IaEu0M9+qvlZrF6uJjBWyEVflmgXAtVqtroq4uMVdAeTlQVmZfhyCE0UEgUYs1Jyd2oMYgZbutr/+XbrzyCnDMMcH2E5So/eZ2+4pCWFu3Bnr1cn8YWtmyxV9DIgtreqKG39ootLysVmGdOBHbDzxyz09bYbUKahjCqsp3sliDdmnt0gXo2dN+2UMPAfPmOW8LePtYkx3H+u673tv6IZ5GwIsukj3v/ODHKo3SN+6HKC3WjRvlUEN+8hkrWrQATjnFez0W1vSktZHYar1obc60ugJOOQXbRRPkkhQSW2G13vxhCKu1J5idhfj227KRI5HBBIUAbrwRGDTI37ZhxLEmEm7lRTKENRlki7Da7ccPn37qvQ4La3qihHVDnWax2vhYd+8GWubLFHL1hLW6OhqLVfn73CzWc86R41sFucCcBNHr1UtZrBUVwLXXAtu3xy5Pl8aroB0EvBoP3QjSaBi0S2syhTVTG69CFlZOdB0SxcVAo0bAhuoW5kwbYa2qAlo02oH1VSWmsCrh0y1Wp+B5fZnbfDdh9eNj9cOmTeYTxamuCiUGl19uzvvLX+Q2y5fHrhtEoOJpvPJrVdlt60YiFmtVlUzr6Ieg4plM6zkdLdYUlMcWa0gQSY3ZILQhWhyEtXmB7MK6R1jVq3pUPlY7V4DTukFcAevXm9O//uq/wca6L6uFG7UrwC/JdAUE6anl5Wu1Lk8Xi7WqSiYaCoM0t1hZWEOkTRtgA7yFtaTIIqx2Fms8wup0Y9u5Ary283Phrlsnv7/4AujQwbkRyAurINuFgoUhrEEFJ6grIFnC6hWC59YYGDVuFuu11wIHHAD8/HN4+wkLFtb0pXVrbXgWwFlYG8ubKHRhDcNiVdPbtjmP0aS63iphnT5dfn/1lf36XkTZQSCRGzCRONYohNUpPM26TiqF1e3/+u47+b1pU+L7YWFtONQTVj3cyrAWq6qAkiZSSHdsNy6OqF0BfhqvFPpNecYZ9uU3aya/VQIY6w3vliLQjqAWdBABq6uTITpB/JJVVcC//52YK8BvY5T6TxqCxar47rv4H8LW/SS6joKFNX2pJ6xHH21OGyJbVQU0bVIHQh12bDNugEQs1o4dzelEwq3sppUlaqVxY/mtBsizil/QFHtuQpBo0piZM2VQ+WOP1V/mtO3YscCpp0oXRxDicQWoh2+QnlrpbLH6EdYrrgAOOyyx/fh5cLGwZgetWwOb0QI1MATsmGOA77+X05qwFhTmoDF2YMdWQ/DUTeU33MoqPEOG1J8fRlTAvvva71e5AqyuAj9RAXYkYrF63TzKnzd1av11neq7dKn81hvn/JCIsEZpsSYzKiBZ4VZ+zleQEDaOCkhfWrcGBHKwGVrIVROjC2teHoSQGtdICes2i7DGY7HW1AD5+XJabbt9e2yeVmWxBul5BUhLzw61jhJWP64AN/xYrHb7t07boQuXX2F1srq+/VZ+nIjHx6reIhIVVp10t1jDIGg3Va+HJMexpi+qW+t6tEEbbIgVmPz8PfrZqMgQ1vlrgS4nmgHydj5WL2GprTXFQ13Me+8tu/8p1I79hlvl58ttbHId7KknkBxhtbOA4hVWK17hYVZxOPhg+/nx1MtP/Zzqk86ugGRZrKpTjZ+6AEDbttK1c8QR3uuGAFusIbKn95Xys+oCk5u7595pVJQnhXV1hUxoogtrGBarLqpqHcA53Mqa3apxY6BTJ+cbVAm1k7AGJcrGK3XsdsIV1GL1Il1dAQ3FYp0yBVizxvxtPU9ObQZ26yYIC2uI7MkXAMN01bPl5+Xtecg2amwIa6XlzwxLWK0EsVjr6qQA5+Y6C4/VYrXWNSfgZRVl45UaTyweYQ1KKoVVYWexJtPH6iasYYqt3fk68khg6FDzt/W43d6kWFjTlz2pA9VIAvofmZdnWqxKWHdYLjS/4VZW4bG6AqwEabyqrZXr5eU5B+6r+SoqQKGSXQR1BbjFsXoJq9fNqoZpqK7252OdNcs5fteLeHysybZYf/3V/37iIdWugPLy+nXxQyqElYiaEFGOMd2LiE4lovxQa5IFtGkD5KAWa9DBnKlZrHuEtYkU1p11BbEF6Dd/FBarU+OVNQ9sbq78OIUAOflY9QazIAR1BThZhrt2AX/4Q2xZSlj9NF7V1clhwlWYVZDoA2td/I6rFXbjlZuP9euvZQ+5N97wv6+gpMoV4NUWAQRPXpMAfi3WqQAKiagTgE8BXADg5VBrkgXk5gLtsNYUViLzD8vP14S1kbRY0Ti2AN1iVReKfnEsXFh/nhDeFqtXHKsuoLorIKiwKsJsvLK7UZ2sswkTgOeeiy1LuQLsjsVqkbt1rfWD1wPBjrCE1U+41ezZ8nvqVP/7CkqqLFa7/zcDXAEkhNgB4AwATwshzgbQP9SaZAkdsCZWWNVFrjdeNcl3Flbr67b+h/ftK8v8179it/OyWL3CrfwKq7VulZX2+w1TWO1w6uFkt10QH2siuWit9bJ747ArIxk+VmvHDbdjWblSRj8EjeG17ivZFqudsFrPQzoKKxEdDGAkgP8Y8+p3hGfqC6s2FPYeYW3V1FtY1cp2N/vrr8f+9hJWhVPjlX5RKh9rbq7zza53wVXl6SQqrF4WmdUV4kYQV0CirenWOm7bFtsS7Sasifa88uNj1X3sTjzyiIzVffVV//Wxq1syLVYVIO5UFz+kSFivBXArgPeFEPOJqDuAKaHWJEvYC79gGbqjCvlYvMRBWJsVohiV2IpmsRvrrgB1odj94Vbh8nIFKJxuLKuw5uU5uwJ0iyhomkAn/AiaW2ObG24Wq7X+flwSADB4sP2+rBbr6aebsa9OdY3HYnX7nxO1WBMlWaFd+vnyK6xeFuu8eXKd//0v4er5ElYhxJdCiFOFEH81GrE2CCGudtuGiF4ionVENE+bdycRrSKiWcbnRG3ZrURURkSLiOi4uI8oxRyBKdiK5jgd76P3AU2wsMy4cXRhLSB0xGpsRzEqdHHVLVY3YbXi12L14wpQHQOcXAG6GKnpsF0BXhaZV88sHWWxWrez29avK2DGDPt1rMJqvUHtjiuZUQF+LFZF0P/Qrh5REo/F6iWsn3wip995J+Hq+Y0KeJ2ImhFREwDzAPxERDd6bPYygONt5j8qhBhgfD4yyu8H4FxIv+3xAJ4moox0NRw9/hLkoBYf4SQAwKKfG8kFWuNVQQHQGTIspBydzY31XAFuwmq9eP1arE6uAKtYKmF1s/IaNQomrG43cyIWq153u/0qYbUrM2xXgFccq51wJ7NLq53F+ssvwIoV9csII5Y3SsK2WHfuDEVQFX5dAf2EEFsBnAbgYwDdICMDHBFCTAWwyWf5wwG8KYTYLYT4GUAZgKEe26QlLX53Aobi+z2/l68yrEndYm0EdMFKAMBKdDE3TgeLVYVveVmshYXyd12dvzpGJaxeFqta7meIG+s6umj7ERovSzpRi9Uaiue0ThCLde+9ga5dzd/xWqrWfUWN1WK1c0sFEfmbbjLzxYaAX2HNN+JWTwMwUQhRDSDeM3glEc0xXAUqW0knwFAaSbkxLyM5DpP2TC/+pVBOWITV1mL162O1ogur24XtJ9zKzRWg6ghIYVXrW/dp1/PK7Tisy7yO2cnadkN/aDlhFSQvsausBP7v/+z/q7q6+iKVqMXqJqx+wq38+FgTFcZMtVjDGNVAw6+wPgdgOYAmAKYS0d4A4ume8gyAHgAGAFgD4OGgBRDRGCKaTkTT18cbEhIxurCu2GBkt+rYMUZYO553OAqwC5/iWHNDXaTibbxyu0Gdel5ZLVY/rgA18J2dsCXTFeA3NVw8wuq1/t13ywERX365fr28OnZYSUa4VRAfa7xkqo81FR0EhBCPCyE6CSFOFJIVABzSxLiWs1YIUSuEqAPwD5iv+6sA/Z0YnY15dmU8L4QYLIQY3Ealk0ozhuAHdDYM8M3UAuL5fwBPPbWnB2hREZD/2j9x9Z/y8TbOwRY0B9q3l7GD1nArP7463WL1I6xhuQLU+kFjPK34Ccz3I6xeOV+D+ia9UGPrqJs8Hh+rWi9IhEW8jVd+LNZEXQFRiba13LAt1lQIKxE1J6JHlKVIRA9DWq+BICKtrydOh2wIA4CJAM4logIi6gagJ6A5KjOMPNRiPvrjtFPr8M03hMIrL8WU6U2xdKnUqy5dAOTkYOBg+Rq4Bh2Anj1lVirVm0ldKHY3owoh2rNDzWJ1S6fmJ45Vt1itF+vu3cBHH8lp3WL1cwG7Xbh+XAHxxrHq64UprNu31++26lRHP/OiHEFArZ9uFqvXcfzrX85du/0Ia5A3kFQIK4CXAGwDcI7x2Qrgn24bENEbAP4HoDcRlRPRaAAPEtFcIpoDafFeBwBCiPkA3gLwE4BPAFwhhAhoPqQXzbBtT37Wqirg88+BxYuB7t1NA1MlbVmLdmZSaauwBvWxugmrU9pA/SJ187Fefz1w6aVyWrdY/fRwCdNiDRJupa/nt6HLD6WlwLPPxs6Lx2KNJyY4SovVrrwgBNmutlZGJVx9df06v/qqjANW59h6zPG4Atz+31W2L8hx4zfRdQ8hxJna77uIaJbbBkKI82xmv+iy/r0A7vVZn4ygRYl5kf3yixRWPSl/jLD27Bm7cRBhVRbrF1/EtvBa8WuxFhe797wCYoXVKgpBhdV6U9itu2uXtBKbNInPYvWzbhBLbqXW1mrXqBSGxTpkiOxg8Pjj/uvp5mN1sv7CRC+7qko2KDhRUwNcfLHMo3rWWbHjYK1dK79Vo1IYrgC/D85E3SHwb7HuJKJDzf3SIQB2uqzPAGjR0pxetAhYvhzo1s2cFyOs3bvHbrxihf9eIOqCee01mZPSyskny28/Q7PorgC3bFVuwmpHkJvZzur56CMp+Nay/PpYAe8GoiAWq10olpcrIKiPdfp04IknnLfR8RMV4DfjFiAb5bZsAd5/X168ftHL3rDBfV39LcJ6TOotzMkllojFWlUFfPmle90SxK/F+gcArxBRc+P3ZgAXRVOlLKBNG2D9ehRoD+uffpIGV/v25ryWLYFc1Mj8rSpLtpV//MN7f27+uVWrZKo4wN7HZk1orbsC3NDDrRJ1BQRdNx5XABCusNrhFa2QKh+rOl9OImZHRYV8RVc5A/y+4utlb9gQO4qwFZWXwq5OVmENw2JVD6877wTuvz+UrqtO+BJWIcRsAPsTUTPj91YiuhbAnMhqlsl88w3w2WfYvtEUJzX6Stu25mo5OUCnwo34aVc/eZEVFcV2wfSLm8WYl2cKnZ0rwCqsKirAy/rTG6/8uAKC+N6CvLJbc8m6EZWwOrkC/MSxhhUV4LYfq7D6jQrYssV/naz7ArwzZCUirIlYrMqNs2CBe/0SINAIAkKIrUYPLAD4UwT1yQ722Qe4/HL07i1/Hn20uUi9/itGdPoaH+JkrF1VA7RoAUeuvNJ5mdtNqVuedq4Aq2WquwLcCCqsQSxWL4Fzsgy9totHWE8+Wcar+iFsH6uf/Sj8NF4FcQUA8SXZ0evmJaw1Nd7C6hR2mIiPdY8Pbq17/RIgkaFZEvfwZjlnnSVzC//xj+Y83WIFgKOuH4Ba5GFJ29+4C6t69bbD7abUe0E5WazWsoK4AvwI67Rp0lfnlyDCqu/bazs3obCLbgDkufAawysRH6ua50dY7fZjt04YrgDAW1i3b5c5gvVX6qgs1jB8rOp41E2YpsKapC4WmQsRsN9+sWJqFdb2v5GNVmt3NPMnrGeeWX+Z2w2gi4JduJWdsObnxw593cmmd3FBgblvLx9rUF+Wl0A6WWeJ+Eh793ZP6+enPolYrNb/MEgXYL0OdmVFZbHOnClHtbjpJvt6+Gm8chJWdQ0l4mN1SrKjrvl169zrlwCuwkpE24hoq81nGwAXrzSjo7/+W10BewYg/BXuwqqEzK6R6+yznbfzslit1tju3fUtVrtX4cZGkm4/UQHWQQe9CGKxXnWV/+3cWLbMWVj9ht/4jWP99ltZ7po15nr//jfwww/mum5uC6/GKy9h9WuxerlO7ARaL9vakcVKly7Owmq15N18rNXVwCWX1K+XW1QAUH+YeEUI4Vauj2MhRNOE98DsEc9GjWQYpk7r1vJ/XLsW7sKqLgqrS8ArdtXOYnVzBShhdSpDoYS1utrbFRBUWOONN020Vd8tSYobQS3W55+X+/rPf2LXGzrULMuto4eXxWoXFzx2rOylYl3Xiv7fudXBaRtVdqNG/twbTsKqrgE/PlY9phgwk7V7iXWEeQ14+Osk0LQp8OabMo7VSl6ejM769VfI+Csn1IWkC2tuLnDAAe5PWDth/ewzuZ0qQ2f3bukKsLN0ddws1kSFNYjF6mc7vxZIvBarne/TzZ+qYu5+/dX5WMK2WO+5R1rKTtvbuQm8LE43i1XP1+uGus6sImwVQTcfq9UQcLJyVX2CdL6JExbWJDFihBlOaqVdO+Oh67QCYF5IurBu2gQ0b+5fWJVA6mNmObkC9JvGTliV6b1ypbS8dKz1Ud10/eKVyd/J0nDazq3hz2v73Nz4XQHW7dRyP8Lq12KdMUM68vWHl1cXYbvzZyeCXsJqR1CL1epLtdbHj4/Vuq3TNlaxDpJRLCAsrGnAMccAn34KzCeXgW/tLFYlikEtVh070czL824UUhbrbbc571sRpivALZlKFMIaZuOVKl+VuWaNc539CutNNwFz55rWqF1DjrUudnVT5zyIK8Atp0NQV4BV5LxcAdZOLTosrAwgtalRI+Avnwx1DrUoKZHf1h4GQHCLVcduXn5+7IVrZ7koYVWNajpRugL04Wv8bhe1xRrUx6pu6HXrgrkC7PZjl1jF+mDyk4vBTgS9hNXuf9VdAU7Dp+s4uQKcLNbTTgP23dd+XYV6Q/JqvHIS1kR99WBhTQtatQJGjQImfNEGd+EO1Nr9LTffDDz5JHCR1pM4DIvVzv1gtVjtbjAlrHYNblG6AuxGLPDaLmqLVd3AfuNYdb9hEFeAm7DqdfDybdotD+oK+Ppr4NRTY+ug1zE/P1gOCSeL1epjPfvs+v+nVZSXLIkt26lMr+HdE4CFNU148klg8KA63IU7cQMeqr9CURFwxRX2PanchFVfZies1qxaQDBh9dNyHKYrIB6LVfUS8yJei1XV16/Fqm5somDCapedyq5uXq/gbkPueNVB8dVX9vOVbzk/v/5+7OpqFTun+eqYc3LMzgMK67YLF8pvpzhWL2FlizV7yMkBXnhR/h1P5lyDZehWfwUr6kL127hi99pvFxcbxBVQUeG9Xz/Cqt8sbq21UboC7Mr1Y7H6FVbrq6jTeoB/YVW4uQKsuFmsejl+cvsC9S1WJX5+fKx250SvjzX2NienfipCtZ+JE6XLTOUAiNfHyhZrdrH//jIZVX6+wL34s/cGfixWu/V1/DRe2YmLigqoqKgfJmYVOD+uAP1mSScfq59wK7vAezc/pvp2a4izu+nt9hOPK8CucSdobKdTfK+yWPPy/AmUl8VqfWjl5jpbrHl5QOfOZlfVeIWVLdbso2NH4MKLcvAGzpNjYdmhsrskarF+9pm92OrCuv/+wOjR9ddRr9dbt5rWq0K/oHfv9me56I1g1huyXz9zuqoq/cKt7Kw9u7KUBagLa5CoADdXgJ1IetXXmtXMqd52OOVPECLWFTB3rns5n34qv/36WN0s1rw8oFkz58Yra6SBk0XOwpqdjDyfsBON8eXfZwNlZWZOTMXUqcAnnyRmsfbqJdNuqeTROrqw3nmnfRZ43WqwdifTL2i//lU3i/Xgg81zEJbFapcnNN7GKydXgPV/sRNW67HccEPsujpuwqoH+Pu1WKMSVuUKmDRJxth+8413eWH4WL2E1WqxOgkruwKyk8GD5TXy7Zq9gR49gPPPj12hbVvguOPM3x7CWldnXGP6zaDiHs+zGUFH97E6WWy64IQtrHY9uZSFHJaw9u3rb/swG6+sYT52x/Lww7Hr6NhFH6i66evHI6xBeyM5CaveeKVQyU7c3AyVlXKYlhUr5G+nONbcXHeLtWlT85pjVwCjU1QEDBwIvPeec56IGDzS2vXqJY2+GFeACpPaZ5/6G+gWq5MvzUlYr7oq9sL0G2ql34h22bL0NHJ+XAFPP236fu2iAnT3gtN+gXAbr6wW64IFZgZ0nQ0b5FuJwhpmVV0t+/5v3Fi/55JTpie7egSxWK3nXH/Y2DVe6eetadPY+tvxxRfAuHFmMhUnYQ3TYo2w8crv0CxMkhk7VoYJHnwwMG+e+5hsXhbV0qXy45lXVBFUWHUfa26ueUHv2CFHUQyKdRSFsrJYYfVjsTZt6py4BojGYtUzhvkRVieOOEL+6Xq99KQiH30kz8nSpe4hTE7EI6zV1e5vFQo7i1UJmJslqHzsatQCa7RAmD5Wr/PPFmv2csopwMsvy1jnggI5RI8jhYX4+9H/BkGgCsYFbYx4qVu8u3YbN6E1KezAgbG/N23ydgXoN46ed1MXlQEDzCBynYKC+vGz+k1QWRm77L//9SesTz9tThcXuwurXceGsKIC7DIrAaageXWl1EUVkIM6PvWUkalH297OzeLHx2onrF6uAGudnY5Bb7yy7s/NYlXrKGGN12Jt2lReP3v8XxpWi9UJu7eIgLCwpjEXXmgK6p131s+OBhix0ES4c7ociXUpegAA/n3N59i5enPMAJtLlkBm8p8xI7aQTz6JHZd75Upvi1W/uPXycnLMbVUPGCu7dsknh45+E+zYEbtszBh/wqrjJazWmxOIP22gXUhQIharlT/9SQ7NYxWzFSvqJ8DxU76qhy7AXhbr/Pn2ZVj3V1dXX/zshNyKekiouGj9nOo91Lx8rM2ayenKSucOAl4PtkmT3Jf7gIU1jSECbrlFphskkuKqM2GCfKP9+GPTzbkQffAjBuDUM/Nx3V0lKCsz11+wALKvdefOsQW1bRubLPtPf/LnCpg8WU5fc40530lUrFjL1bdRlpnimWf8+Vh1iovNbeyE1c53anfD+bFYrcJqjQNWxCus1ieqqo9dKJMfi3XXLmD1antXgNN/d9BB0gWhzr1TdillsernV3Uw0cu2+vaVsFotVlV+EIsVkO6AeF0BIcDCmgHsvTdw6aXASy/Fjob99dfye9480825AH0xHzJL1jffxLo4VU8/W66+Gvjd76TF0KlTrE9LYRWoI4+UF+9jj5nzcnKkgBx6qPtBWYXNTYz1V8vKSjmWuBfFxebTRq+3OlH5+cC118ZuYxd+46fxauJE4IEHTAEpKvIXbuUXq2vEb5yqG/p/rG/jZlX+/vcyx2VlZey5sg6T4sdi1TP+A6awqnX0Y6iqir0erSGC6sLWLdZt2+J3BYQAN15lCLfeKt9QrrhCvgG2bGlez7fdZl6HL+RdDpGfD+yUY7n9/LO8F3JzgTvukPqpEmXF0LYtMH68+Xv0aDlWle4LPfhgYMqU2O2s4qGEWKm+E1aL1avBQN2oZ53lvp6iSRN7Ye3QQTb6VFbK0KalS+WwKIApjKtWmeN8+Wm8qqiQf9C998rfrVol5mMlirXKrcNQew2RbhUOp8Y0XbCdEkrrlJfL7xdfjD0Gaxddq4/VzmK1Rmo4dRBQy3SL1dpG8Mor8lsXVjuLNYnCyhZrhtCpk9S06mp5/15/PTB9ulymrsEhQ4Cfa7pg+U6ZSPnXX6WVu9de5rX46KM+dzh6tLy527WTFsLUqcC//uW9nR+fpHW9sWO93Qd2PlE3nCxWlc1rwwZny0rvouvHYlVs3iwFo3HjYD5Wa2cF6zm05mNwE1Y7V4DTg0FvdAwSx7pypbsrwCrkdharV284J2HNzZVDbtihuwKsFuugQXLI5EWLWFiZWDp3Bh56SLoGgPptUOrBDch7vE8fOd2pk7nMy5B0ZNgw0xpwwyuk68EH5bcSj8MPl4MVJktYR46U34MHy2/9BlaWlS5submxx3TGGc773LBBvg7k5MhGJasgOgnriBGxbgmrmFstVq/M/lZhdfpP9OGpVUu4n1CjjRudXQHKYtXn2UUFuAnr+vXAu++av60+VqvFqnCyWD/4AHjtNflA+vrrSBNcK1hYM4zrr5cDilqvrQULpJCWlMg8wCUl8no680zggguA0lLpBvjmG3ldvfAC0L+/dIX+/HMIFSsrk8Mh293Eyic2YABw441yWomHNfh98WL78v32CFIUFprCqrciH3+8PAFDh8rfupCoHkK6sOblxYrAffc577O8XIZxOQnZCy/IB5Rd6JJVzHWCBKyrDgJ6pw2n+ugWq4r99COsGzZ4N17py9WDQC/bbeDMMWNif1t9rH6F9fvv5fRBB5lvKi+9JEduiBgW1gwkJ0c2CH/2mXzj2bLFtE7Ly83rqVcv4J13TCPrt7+VD+2HHgIuu0y2AX39NfDHP3q77Tzp0UPGw9o1AB1+uLSIvvvOnKfEwzo0s9UBrF6Te/c2D9IPRKa46MLUqFGs9as3sr35pvy2jrqgd4Bws9pXrnQX1ro6YNq0+q35VqFNdPTQmprYdJB+LFanoHo7vCzWnBxvi9UamaKjslPp5esWqx9XQFkZ8M9/mtuo+eqVLYQhrt1gYc1Q2rY1c6g015JgNWninNfZ6DOAP1syEn7yiQzr8sV//1vfB6Gj36yALPj116U46ZajVVj1UCXFc8/F3gh//avPShpYcxgA9V0KN90E/N//OZeRlxdbjl2ZikWL5A3s5Q6xvsrv3u0exxsE5WPVxcevxTp7tr8+1Mpi3W8/+RpkjQogihVeO4tVDahoh9VqtvpYW7Wy304XVj2GOien/jlQ60UEC2sDonVrmX9g2DD5+5hjpAEFSBeUL9fTb39rDp1th3qdHjRItpTff7/9RewUx6oL65gxQNeu5m8/Pl7AFEolgvqBWYU1J8dMw2iH1WL1Go1g/nz3pOR26MIRBtXVseLjx2K9/37pqvnf/7zLVxZrQYF8sm/fLl+vt20zG6+sroAnnpA9+rzqBNQ/F1aL1alBMS9P1qlRI8QEcNs1qLKwMmFy+unAl1/KiKJPPwUOOUR+b9oEnHOOvEd27pSG5nvvyW61gXz9SlgfecTdH+nkY3W74fzcDPvuC9x1l5y+/nqgWzfZKUJh1wjm1jBmtVi9GtF697Y/BuWn7dGj/rLdu+1f/5991n1fdqxdK/9cu9F8raj/KiibN0urWglrba102QwbZjZe6X7XuXOlZavTvr10/tthveCqq+3jqq2oa6pZM8R0ObTbxulaeu018/pJABbWBghRbITPMcdIg+WDD+R90rSpfOs+80w5yOHzzwcoXFl0doMU6igrwppj1O3GsVqsjz9efx39FbRXL9nSp7922gmjW0hV06b1E3m7MWGCvYWkzotdxwmrKwCQqSKPOML/fnXWro09TqdzatdH2g49Ri83Vz4EysqkZag/dGbPtrdYrT5TQF6EKkWiFas7yWqxqnpYUfOaNYvt7x9EWEeOdHcN+YSFlQEgLdR//EOKaevWsqfXiBHy7fCqq2QPxIcfjtWt2lqb5FWvvioLshukUEfd+Cqr0bRpwHXXSUvrkkuAY4+tv41VWK+6qv46TsmLn3xS+h2DWqzdurn7VQcONDsTEMkYWLsbWQlru3bSEr34YnOZnStg48b6Q94Asb5Tu2TdCv1h4TQ+lW7VAbH5InRGjDAjO1TCntWr5TFZe0Epi1X/H6wCrs6FU8o2u8Yr1bqqtn3xxfrbqeO0imYQYQ0JFlZmD5deKqMIfv1VauObb5pv80uXyuT2Rx8NzJkj75tRo2RM7fjxmi506CAL8mL4cCmkqiVt8GDpPiCSN41dIgz9ZhgwwL5cuxERANllbd0676Td1hu2Y0dnizU/Xy5XfklVP6fhbgAZ9fD73xsJcg3shpvZtMm+i9x++8nv7t2lheiEk7A6hSoB0i/uVJZ6AOr+9SuuqH++VeOVmq+nbwTkf75qlZzWh+PRUReTeiOpqjK7vKpzfNFF8mK1w/oAVsevPzj0a2nWLPtyEiAyYSWil4hoHRHN0+a1JKLPiGiJ8d3CmE9E9DgRlRHRHCJyaR1hkskJJ8iRjmfMkFEF06bJYbAKC83RUs4/X76FHXaYbDB2CxrYQ5s2UkiPOsp/ZYqK5Ovxiy86D/dx883+y1PoIjRiROyy3Fxni7VxY7lc3ci6L9eKyualLDldSO26wN53X2y9/vEPackrH21BgXtDmpMrwG5UXoXeUKiTl2e+ZnfpIr87dZIXh1VY335b7u+tt4C//c3siKEoLTVjWN3eFH77WxmmB9gLK+DcINi9e+xvdfx6uJ9ezv77O9cjTqK0WF8GcLxl3i0AJgshegKYbPwGgBMA9DQ+YwA8E2G9mIAceqg0VKZMiW17adMm1gj46is5b/Bgc4SNMLj9dqPDFpHMNH/JJfai8sILMtdiUPQb3M46dbJYCwul4DRvLhOBqAw5doLxhz/IZDVXXil/KyFt1Eg+KHRhPeEEmeBG58IL5Xrq9T831733Ul6eDFT+5ptYYbV7BX74Yfl64hQfqvyqgDzvy5aZiXCswrprl/yfOnWSrzjWNwA9WsGt+3PTprEZzbZti7WcAWdhtQqlOv6SEvOcOb3ZhERkwiqEmApgk2X2cADjjOlxAE7T5r8iJN8CKCEij9YPJtnk5AA//ijbBSZOlL29Fi2SroMLLojNPHj11dJVdu65cnrcOJlqwG+IZm2t7Njw8ccyN4KjIaqCwIH61pFf9trLnFY3a0WFGTRv9QWOGwcceKCcr8Shd29zPbthX/LzZXpFZTEqoRo9WlpwugVr94qsym7XTn4rUVfowcyAdMn07StdDrqwKmE+6SRz3uDB0lJ3anDU0yA2biz9zspKt7Pm9f116wbcc4/528li7tFDdhpQ7o/iYlMEx4wB/v53OU8X00GDZBSIlUMOca6PIst8rO2EEKo/2a8AjKsEnQDoHu5yYx6TZqhG8lNOMY2Pdu1kLoI33pCJojp0kMLbvr1sJH/iCdlWc/rp8ppXb8OzZkmhvuMOM5fBmDHyLXjxYpmT+8QTzX1vsj6mAVnwqFFy2u011w2VfEGnWTPz5rNaRhdeKAdjLCmxv0FVTwwdpyHC1U3v1I/+5JNjy1Mn3WqZK7+lQm881H2OAwbILnsTJkg/N2B2FFCv+VZ0YbW+KdhZflZLVO+RYg3uz8uT3ZwXLJB9q5U7okkTsz6VlfJpbu3au9de9jlphw6Vr0zffy9jqe0s24iFNWVpA4UQgogC990jojGQ7gLspVsaTMrJzZU6sGyZjI3973+lYfThh9K4WLtWvh2WlkrR/eqr2O1nzDDfpn/8sX75X34pXaz12nSeflq+YneK81nsdywwK2+9ZX+DnnSSDFG67jr5+y9/qd+FUwmpuumdhFWlNFSceaY8uUcfLX+fe6486VbLUU8k/Z//mH7H5s3NbUeMkDF2apkurCeeKOvx3nvyj1UJZazn2E5YnXpG2S2zJqRRwt2zZ32BtualdWOvveRnyBD75eocq2N+/XX3jiJBEUJE9gHQFcA87fciAB2M6Q4AFhnTzwE4z249t8+gQYMEk1l8+KEQQ4YIUVoqxLnnCnHzzUK89poQv/mNEPJ92P3TooUQq1cLsWVLyBV76y0hnnzSefneewtx6aX+y1u5Ula4Y0f75Y89JpdffbX8feGF5kEuXOh/PzqDBpllrF0bu+zPf5bz338/dv7mzeZ0dbW5vRBC7NolxC+/yGk1f9Wq2O23bKn/Jw0fXr9u3bvLZTU17sfQpYtc77vv5O8pU4Q45pjYellxW2alsFCu+/LL8vvZZx2KxHSRgPYl22KdCOAiAA8Y3x9o868kojcBHAigQpguAyaLOOmkWPee4oQTZDvHgQfKt1QhpEElhHxD3LlTRvds3my6CR9/XM57/31pETs1ait++UW2H/31rzbtS7qD2I7ly/0doEJZsk6JY6wWq/Krjh8fv+U0ZYp8XVi7tn5Y1e23y7qo13+Fbv5bO0oUFNR3D1j9sMpSLi2VwdAjR9oPqT51qvT9eOXrffFFmSVIhXUdfrisg12Z8aDO9zHHyAYCp9jdRElEld0+AN4AsAZANaTPdDSAVpDRAEsAfA6gpbEuAXgKwFIAcwEM9rMPtlgbHp9/HmscHXigacnW1dVf/9tvhZg6VU6feKJcd8qUJFV28mQhNm2yX/bQQ7Iy110nf2/aJMQNNwhRVZWkyjnw2mtCfPFF/flXXSUtRzsmTxZi/XpZ9/vuE6KyMtw6bd/ubpUuWiTEV1/5K6uoSJazcaPrakjQYo1MWJPxYWFtmMybJ90Bt9wSK7LHHivEv/8txO7d5rr6/Th0qJx+5hkhliwRYsEC733V1Qlx8cVCfPBByAexapV0LyxZEnLBWUqQ1303eveW5Wzf7rG7xISVZBmZyeDBg8V0NT4J0yCZM0cmlLr7bnNMud695dBYc+fK6ARAvskffrj8Li2Vy5o1k0mZ9Ab7XbvkGzCRfFv/4x9l9kJARl9F3JjMOPHcc2Zm9kQoL5etquef77oaEc0QQsQZvwe2WJnsYPduaQS+9JIQPXrEWrKAEG3byu9+/cz2C0C+uS5YIC3TSZOEaNZMiDvvlGW++279cp5/XogZM5wNnp07hVi2LHnHzUQD2BXAMPWZO1e63VauFOKJJ4To0EGIyy4TorZWiIoK2Zh9wgmmYHbuHCugzZvL7+JiIb7+ur7A9uwpxD//KcS++0r3gxCyEb1PH7n8v//1rmN5udzm0kulm8LNNVlbK6Mo3njDfR3dDZLOOLme0wUWVoaJk+pqIR5+WIhTT5XtMs89Z/pui4vl3fGK0R9w6lQhvv9eiFGjpMVbUhIrtM88I8TYsebv3r1lBNfmzUIsXizE7bcLcf/9Mrpn3jwh7r67vliPG+dc148+MtebO9d+nSuuEL4imqzs2iVFu7DQMfooVObNk/V89dXo9xUvLKwMEwHbtwsxc6Z9pIEQ0uI64wzZWN63ryl6I0cK8eCD9UXT69Ohg/xu3FgGCsyfL90UffpI94W+bps2UoRVQ3hdnRBLl5rLP/9cWq5vvinEnDlCTJsm19m2rf5x1NRIK1gvf8kSIR59VDacb9sm3Rth8uijcj8HHCDdKkuXCvHrr/KB1qOH8zn3Yt06IbZuDaeOiQorN14xTILU1cl0r1u2yN6ZRUWyQ9Htt8u42169ZK/Uvn1lY9u0aTLZft++Mq1pTY382KU62GsvM+ftI4/IQWaHDTOHprryStnb9BktbVHPnjK8VM+GV1wsY4EvvFAmjmrZUoawjh0re7TZ0aWLTKXaujXwww/uccJLlsj4YrfUtYrhw81GRUVBgdnV+aSTZPfovDx5XrwaDLduleG1O3bI+OQ77qg/rltQEm28YmFlmDThl1/M5PsTJsgkTSeeKGP+W7aUvVGJZCTDnDmyF6bKsQBIQRo4UOY8adxYbvP738vt9Vw1dnz8seytOm+eTC+7zz4yTl/lqD74YODyy2VERZcusq6TJ8tIDJXAqn9/4Kmn5PbTp8uetLt2yRwzl10mk+lccIFMpHXyybKrsxMlJfJBVVoqR6zu00f23m3USPayraiQx3XllTIlgEojsO++MvHWgQdKsR4+XIptjx4y/0R1tcyR849/yI4iTvnYWVhZWJkGzMaN8qN3IFq7Voqq6l0mhMy616SJzLr4008yF8umTWYWMbt0CSpb33vvyRwPKl2AHxo1ch8rbfFimbxHdUBbt06K8aZN8gHy97/H5vVVQgtI0dfHCtRZs0YOPmGXm8VK69bywXPooTKFwc8/S7GeMwd44w0W1lRXg2Gynro6KXRz5shkWD/9JN0IJSVSzLp3l+6C1aulddutm0wF27q1nJ44UVq4xcXSonYalUUhhIw53n9/M8c1IHu65uRIa/SUU6SVPmmSHOHijjukWNfUSBfMO+/IRF433CDdIPvuKwfHvO8+md/3iSdkDLSVDh2ANWtYWFNdDYZhIkII6f7Yvl2Kqdu4j36pqZHlCCFdGvPmyencXOnnbt0ayMlJTFhTljaQYRjGC5UzxU+jmF+UOBPJVLx26XgThQcTZBiGCRkWVoZhmJBhYWUYhgkZFlaGYZiQYWFlGIYJGRZWhmGYkGFhZRiGCRkWVoZhmJBhYWUYhgkZFlaGYZiQYWFlGIYJGRZWhmGYkGFhZRiGCRkWVoZhmJBhYWUYhgkZFlaGYZiQYWFlGIYJGRZWhmGYkGFhZRiGCRkWVoZhmJBhYWUYhgkZFlaGYZiQYWFlGIYJGRZWhmGYkGFhZRiGCRkWVoZhmJDJS8VOiWg5gG0AagHUCCEGE1FLABMAdAWwHMA5QojNqagfwzBMIqTSYj1CCDFACDHY+H0LgMlCiJ4AJhu/GYZhMo50cgUMBzDOmB4H4LTUVYVhGCZ+UiWsAsCnRDSDiMYY89oJIdYY078CaJeaqjEMwyRGSnysAA4VQqwiorYAPiOihfpCIYQgImG3oSHEYwBgr732ir6mDMMwAUmJxSqEWGV8rwPwPoChANYSUQcAML7XOWz7vBBisBBicJs2bZJVZYZhGN8kXViJqAkRNVXTAI4FMA/ARAAXGatdBOCDZNeNYRgmDFLhCmgH4H0iUvt/XQjxCRH9AOAtIhoNYAWAc1JQN4ZhmIRJurAKIZYB2N9m/kYARyW7PgzDMGGTqsaryKiurkZ5eTl27dqV6qowAAoLC9G5c2fk5+enuioMkzSyTljLy8vRtGlTdO3aFYa7gUkRQghs3LgR5eXl6NatW6qrwzBJI506CITCrl270KpVKxbVNICI0KpVK357YBocWSesAFhU0wj+L5iGSFYKK8MwTCphYW1AFBcXOy5bvnw59t133yTWhmGyFxZWhmGYkMm6qIAYrr0WmDUr3DIHDAAee8x1leXLl+P444/HQQcdhG+++QZDhgzBqFGjcMcdd2DdunUYP348du7ciWuuuQaA9ENOnToVTZs2xd/+9je89dZb2L17N04//XTcddddtvu45ZZb0KVLF1xxxRUAgDvvvBPFxcX4wx/+gOHDh2Pz5s2orq7GPffcg+HDhwc6xF27duHyyy/H9OnTkZeXh0ceeQRHHHEE5s+fj1GjRqGqqgp1dXV499130bFjR5xzzjkoLy9HbW0txo4dixEjRgTaH8NkG9ktrCmkrKwMb7/9Nl566SUMGTIEr7/+OqZNm4aJEyfivvvuQ21tLZ566ikccsghqKysRGFhIT799FMsWbIE33//PYQQOPXUUzF16lQcdthh9cofMWIErr322j3C+tZbb2HSpEkoLCzE+++/j2bNmmHDhg046KCDcOqppwZqRHrqqadARJg7dy4WLlyIY489FosXL8azzz6La665BiNHjkRVVRVqa2vx0UcfoWPHjvjPf/4DAKioqAjnBDJMBpPdwuphWUZJt27dUFpaCgDo378/jjrqKBARSktLsXz5cpx77rn405/+hJEjR+KMM85A586d8emnn+LTTz/FwIEDAQCVlZVYsmSJrbAOHDgQ69atw+rVq7F+/Xq0aNECXbp0QXV1NW677TZMnToVOTk5WLVqFdauXYv27dv7rvu0adNw1VVXAQD69OmDvffeG4sXL8bBBx+Me++9F+Xl5TjjjDPQs2dPlJaW4vrrr8fNN9+Mk08+GcOGDQvh7DFMZsM+1ogoKCjYM52Tk7Pnd05ODmpqanDLLbfghRdewM6dO3HIIYdg4cKFEELg1ltvxaxZszBr1iyUlZVh9OjRjvs4++yz8c4772DChAl7Xr/Hjx+P9evXY8aMGZg1axbatWsXWhzp7373O0ycOBFFRUU48cQT8cUXX6BXr16YOXMmSktLcfvtt+Puu+8OZV8Mk8lkt8WaxixduhSlpaUoLS3FDz/8gIULF+K4447D2LFjMXLkSBQXF2PVqlXIz89H27ZtbcsYMWIELrvsMmzYsAFffvklAPkq3rZtW+Tn52PKlClYsWJF4LoNGzYM48ePx5FHHonFixfjl19+Qe/evbFs2TJ0794dV199NX755RfMmTMHffr0QcuWLXH++eejpKQEL7zwQkLnhWGyARbWFPHYY49hypQpyMnJQf/+/XHCCSegoKAACxYswMEHHwxAhke99tprjsLav39/bNu2DZ06dUKHDh0AACNHjsQpp5yC0tJSDB48GH369Alctz/+8Y+4/PLLUVpairy8PLz88ssoKCjAW2+9hVdffRX5+flo3749brvtNvzwww+48cYbkZOTg/z8fDzzzDPxnxSGyRJICNtE/RnB4MGDxfTp02PmLViwAH379k1RjRg7+D9hMg0imqENdBoY9rEyDMOEDLsC0pyNGzfiqKPqp6mdPHkyWrVqFbi8uXPn4oILLoiZV1BQgO+++y7uOjIMEwsLa5rTqlUrzAqxk0NpaWmo5TEMUx92BTAMw4QMCyvDMEzIsLAyDMOEDAsrwzBMyLCwZjBu+VUZhkkdLKwMwzAhk9XhVilKx5qUfKw6QgjcdNNN+Pjjj0FEuP322zFixAisWbMGI0aMwNatW1FTU4NnnnkGv/nNbzB69GhMnz4dRIRLLrkE1113XeInhmGYPWS1sKaSqPOx6rz33nuYNWsWZs+ejQ0bNmDIkCE47LDD8Prrr+O4447Dn//8Z9TW1mLHjh2YNWsWVq1ahXnz5gEAtmzZkoSzwTANi6wW1hSmY408H6vOtGnTcN555yE3Nxft2rXDb3/7W/zwww8YMmQILrnkElRXV+O0007DgAED0L17dyxbtgxXXXUVTjrpJBx77LGRnwuGaWiwjzUikpGP1YvDDjsMU6dORadOnXDxxRfjlVdeQYsWLTB79mwcfvjhePbZZ3HppZcmfKwMw8TCwpoiVD7Wm2++GUOGDNmTj/Wll15CZWUlAGDVqlVYt26dZ1nDhg3DhAkTUFtbi/Xr12Pq1KkYOnQoVqxYgXbt2uGyyy7DpZdeipkzZ2LDhg2oq6vDmWeeiXvuuQczZ86M+lAZpsGR1a6AdCaMfKyK008/Hf/73/+w//77g4jw4IMPon379hg3bhz+9re/IT8/H8XFxXjllVewatUqjBo1CnV1dQCA+++/P/JjZZiGBudjZSKH/xMm0+B8rAzDMGkGuwLSnLDzsTIMEz0srGlO2PlYGYaJnqx0BWSy3zjb4P+CaYhknbAWFhZi48aNfEOnAUIIbNy4EYWFhamuCsMklaxzBXTu3Bnl5eVYv359qqvCQD7oOnfunOpqMExSSTthJaLjAfwdQC6AF4QQDwTZPj8/H926dYukbgzDMH5IK1cAEeUCeArACQD6ATiPiPqltlYMwzDBSCthBTAUQJkQYpkQogrAmwCGp7hODMMwgUg3Ye0EYKX2u9yYxzAMkzGknY/VCyIaA2CM8XM3Ec1LZX0ipjWADamuRITw8WUu2XxsANA7kY3TTVhXAeii/e5szNuDEOJ5AM8DABFNT6Q/b7rDx5fZZPPxZfOxAfL4Etk+3VwBPwDoSUTdiKgRgHMBTExxnRiGYQKRVharEKKGiK4EMAky3OolIcT8FFeLYRgmEGklrAAghPgIwEc+V38+yrqkAXx8mU02H182HxuQ4PFldD5WhmGYdCTdfKwMwzAZT8YKKxEdT0SLiKiMiG5JdX3igYheIqJ1esgYEbUkos+IaInx3cKYT0T0uHG8c4jogNTV3Bsi6kJEU4joJyKaT0TXGPOz5fgKieh7IpptHN9dxvxuRPSdcRwTjEZYEFGB8bvMWN41pQfgAyLKJaIfiehD43fWHBsAENFyIppLRLNUFEBY12dGCmsWdX19GcDxlnm3AJgshOgJYLLxG5DH2tP4jAHwTJLqGC81AK4XQvQDcBCAK4z/KFuObzeAI4UQ+wMYAOB4IjoIwF8BPCqE2AfAZgBqmN3RADYb8x811kt3rgGwQPudTcemOEIIMUALHQvn+hRCZNwHwMEAJmm/bwVwa6rrFeexdAUwT/u9CEAHY7oDgEXG9HMAzrNbLxM+AD4AcEw2Hh+AxgBmAjgQMmg+z5i/5zqFjHQ52JjOM9ajVNfd5Zg6G8JyJIAPAVC2HJt2jMsBtLbMC+X6zEiLFdnd9bWdEGKNMf0rgHbGdMYes/FqOBDAd8ii4zNelWcBWAfgMwBLAWwRQtQYq+jHsOf4jOUVANJ5bJ3HANwEoM743QrZc2wKAeBTIpph9OgEQro+0y7cijERQggiyuiwDSIqBvAugGuFEFuJaM+yTD8+IUQtgAFEVALgfQB9UlujcCCikwGsE0LMIKLDU1ydKDlUCLGKiNoC+IyIFuoLE7k+M9Vi9ez6msGsJaIOAGB8rzPmZ9wxE1E+pKiOF0K8Z8zOmuNTCCG2AJgC+XpcQkTKYNGPYc/xGcubA9iY3Jr65hAApxLRcsgMc0dC5kjOhmPbgxBilfG9DvLBOBQhXZ+ZKqzZ3PV1IoCLjOmLIH2Tav6FRuvkQQAqtFeWtIOkafoigAVCiEe0RdlyfG0MSxVEVATpP14AKbBnGatZj08d91kAvhCGsy7dEELcKoToLIToCnlvfSGEGIksODYFETUhoqZqGsCxAOYhrOsz1Q7kBBzPJwJYDOnX+nOq6xPnMbwBYA2AakifzWhI39RkAEsAfA6gpbEuQUZCLAUwF8DgVNff49gOhfRhzQEwy/icmEXHtx+AH43jmwfg/4z53QF8D6AMwNsACoz5hcbvMmN591Qfg8/jPBzAh9l2bMaxzDY+85WGhHV9cs8rhmGYkMlUVwDDMEzawsLKMAwTMiysDMMwIcPCyjAMEzIsrAzDMCHDwsowBkR0uMrkxDCJwMLKMAwTMiysTMZBROcbuVBnEdFzRjKUSiJ61MiNOpmI2hjrDiCib40cmu9r+TX3IaLPjXyqM4moh1F8MRG9Q0QLiWg86ckNGMYnLKxMRkFEfQGMAHCIEGIAgFoAIwE0ATBdCNEfwJcA7jA2eQXAzUKI/SB7zKj54wE8JWQ+1d9A9oADZBauayHz/HaH7DfPMIHg7FZMpnEUgEEAfjCMySLIRBl1ACYY67wG4D0iag6gRAjxpTF/HIC3jT7inYQQ7wOAEGIXABjlfS+EKDd+z4LMlzst8qNisgoWVibTIADjhBC3xswkGmtZL96+2ru16VrwPcLEAbsCmExjMoCzjByaaoyivSGvZZV56XcApgkhKgBsJqJhxvwLAHwphNgGoJyITjPKKCCixsk8CCa74acxk1EIIX4iotshM7/nQGYGuwLAdgBDjWXrIP2wgEz99qwhnMsAjDLmXwDgOSK62yjj7CQeBpPlcHYrJisgokohRHGq68EwALsCGIZhQoctVoZhmJBhi5VhGCZkWFgZhmFChoWVYRgmZFhYGYZhQoaFlWEYJmRYWBmGYULm/wFDB7uywdMqbAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 500])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH",
        "outputId": "4d8fa1fb-0fb4-4d98-ec64-5f8eb75a94ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_66 (Dense)             (None, 8)                 1024      \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_65 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_66 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_67 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_68 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_68 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_69 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_70 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_71 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_72 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_73 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_73 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_74 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_74 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_78 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_75 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_75 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_76 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_76 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_77 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_77 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_81 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_78 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_78 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_79 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_79 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_80 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_80 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_84 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_81 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_81 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_85 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_82 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_82 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_83 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_83 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 3,145\n",
            "Trainable params: 2,809\n",
            "Non-trainable params: 336\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su2Sj5jZCSwH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPRh6v-mCSwH",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 12406.0186 - val_loss: 12313.5137\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 12058.6787 - val_loss: 11936.0283\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 11673.9346 - val_loss: 11386.6904\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 11202.3252 - val_loss: 10896.2646\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 10639.0537 - val_loss: 10372.2842\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 9987.0811 - val_loss: 9577.1846\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 9282.0420 - val_loss: 8939.8535\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 8534.7354 - val_loss: 8075.4258\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 7760.1528 - val_loss: 7276.9985\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 6988.9585 - val_loss: 6787.6597\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 6221.9688 - val_loss: 5741.4360\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 5468.6016 - val_loss: 5129.3120\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 4746.5630 - val_loss: 4345.7939\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 4051.2478 - val_loss: 3597.9810\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 3398.5779 - val_loss: 3025.1580\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2766.4360 - val_loss: 2518.2104\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2238.1438 - val_loss: 1897.8358\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1797.0220 - val_loss: 1436.0352\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1412.0789 - val_loss: 1019.5965\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1090.1230 - val_loss: 1140.0891\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 855.4134 - val_loss: 857.3926\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 673.4994 - val_loss: 620.4503\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 535.9583 - val_loss: 430.2491\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 438.4125 - val_loss: 374.7825\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 364.4450 - val_loss: 286.5989\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 321.1466 - val_loss: 272.1158\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 293.4186 - val_loss: 259.7659\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 276.6451 - val_loss: 242.8048\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 266.5028 - val_loss: 245.8479\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 264.1151 - val_loss: 248.2150\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 260.0646 - val_loss: 248.0592\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 261.1810 - val_loss: 299.8516\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 260.2950 - val_loss: 249.3923\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 262.3495 - val_loss: 246.1619\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 255.6636 - val_loss: 275.5981\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 251.6919 - val_loss: 274.2574\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 252.8321 - val_loss: 257.1665\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 261.3673 - val_loss: 254.5536\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 261.8456 - val_loss: 272.0251\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 252.2623 - val_loss: 266.4713\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 252.6578 - val_loss: 310.6744\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 252.7302 - val_loss: 249.5513\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 252.1610 - val_loss: 243.6933\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 253.8438 - val_loss: 282.4679\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 260.3280 - val_loss: 231.5066\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 261.8938 - val_loss: 246.1188\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 256.5366 - val_loss: 258.9766\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 255.5094 - val_loss: 263.6427\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 252.0350 - val_loss: 236.1378\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 247.196 - 2s 10ms/step - loss: 247.3994 - val_loss: 255.2176\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 250.5144 - val_loss: 247.1590\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 258.9647 - val_loss: 241.1105\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 253.9065 - val_loss: 266.0532\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 251.6362 - val_loss: 244.1309\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 252.8038 - val_loss: 275.8272\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 246.3809 - val_loss: 233.1080\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 239.7464 - val_loss: 245.3251\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 238.2066 - val_loss: 227.4523\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 235.9451 - val_loss: 222.8229\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 232.3437 - val_loss: 225.4468\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 231.8821 - val_loss: 366.3148\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 229.4902 - val_loss: 206.7844\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 224.0358 - val_loss: 205.5311\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 215.5434 - val_loss: 205.6255\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 208.7983 - val_loss: 268.4104\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 217.2234 - val_loss: 265.4119\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 216.3130 - val_loss: 257.9793\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 208.1487 - val_loss: 235.7465\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 204.5219 - val_loss: 193.2001\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 201.9057 - val_loss: 220.7453\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 195.7868 - val_loss: 292.6846\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 194.6287 - val_loss: 195.6889\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 192.4222 - val_loss: 203.2442\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 191.9907 - val_loss: 229.6401\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 188.9115 - val_loss: 178.0201\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 182.8595 - val_loss: 235.0710\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 172.8630 - val_loss: 190.6996\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 169.9685 - val_loss: 197.2676\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 171.7435 - val_loss: 167.2457\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 169.0112 - val_loss: 258.7121\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 169.1212 - val_loss: 156.8037\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 161.6003 - val_loss: 263.8725\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 159.7534 - val_loss: 173.0779\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 151.6982 - val_loss: 148.3346\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 148.926 - 2s 10ms/step - loss: 148.9535 - val_loss: 203.7570\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 151.6694 - val_loss: 153.0562\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 146.8609 - val_loss: 176.0940\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 147.2807 - val_loss: 176.8964\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 142.6928 - val_loss: 159.7831\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 139.0056 - val_loss: 150.9229\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 139.1246 - val_loss: 262.1962\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 149.6200 - val_loss: 172.1557\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 143.9873 - val_loss: 150.6352\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 142.3204 - val_loss: 189.7434\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 145.3993 - val_loss: 145.8428\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 145.4918 - val_loss: 396.2556\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 140.4497 - val_loss: 194.5799\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 134.2586 - val_loss: 153.7895\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 132.7363 - val_loss: 149.3433\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 129.9789 - val_loss: 142.4751\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 128.8197 - val_loss: 155.9442\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 128.5653 - val_loss: 258.3813\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 128.5965 - val_loss: 253.5713\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 124.7634 - val_loss: 186.1097\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 123.6838 - val_loss: 132.8185\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 129.8953 - val_loss: 166.3259\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 123.5647 - val_loss: 140.5632\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 121.3222 - val_loss: 269.2924\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 121.9121 - val_loss: 137.0556\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 128.6106 - val_loss: 145.9428\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 131.2755 - val_loss: 194.8536\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 132.5957 - val_loss: 146.7293\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 129.2798 - val_loss: 153.2785\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 125.2823 - val_loss: 713.1104\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 122.6516 - val_loss: 136.8517\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 118.4878 - val_loss: 133.3592\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 116.6489 - val_loss: 141.6141\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 116.1603 - val_loss: 135.7022\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 116.7343 - val_loss: 123.0562\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 117.7617 - val_loss: 153.8809\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 118.4834 - val_loss: 126.3843\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 118.6992 - val_loss: 142.2589\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 120.6787 - val_loss: 156.6117\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 122.2523 - val_loss: 149.0926\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 116.2092 - val_loss: 164.3434\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 116.9230 - val_loss: 174.8428\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 117.4789 - val_loss: 140.0525\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.3857 - val_loss: 262.1960\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 116.1559 - val_loss: 341.1435\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 118.7142 - val_loss: 158.3042\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 121.0080 - val_loss: 367.6689\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 119.0716 - val_loss: 125.1517\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 117.4881 - val_loss: 159.0385\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 116.2570 - val_loss: 129.2297\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.8540 - val_loss: 149.1317\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.5409 - val_loss: 187.4936\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.9008 - val_loss: 138.3083\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.6367 - val_loss: 272.2751\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.9048 - val_loss: 163.9713\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.9412 - val_loss: 173.7890\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.5198 - val_loss: 142.6529\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.9600 - val_loss: 122.4585\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.0897 - val_loss: 304.8159\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.4239 - val_loss: 191.4174\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.6037 - val_loss: 133.6415\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.8922 - val_loss: 157.7013\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.1776 - val_loss: 115.3009\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.8583 - val_loss: 142.5301\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.1836 - val_loss: 143.7667\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.4877 - val_loss: 132.2166\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.4751 - val_loss: 183.8503\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.2504 - val_loss: 150.1299\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.3695 - val_loss: 211.5007\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.9493 - val_loss: 143.4198\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.9222 - val_loss: 133.3058\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.3967 - val_loss: 179.7940\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.5808 - val_loss: 184.8660\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.7813 - val_loss: 183.4695\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.4608 - val_loss: 176.4545\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.0085 - val_loss: 179.0397\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.0928 - val_loss: 206.3781\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.5946 - val_loss: 123.7249\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.6566 - val_loss: 298.5195\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.1292 - val_loss: 170.4502\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.4430 - val_loss: 264.0211\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.8305 - val_loss: 195.3924\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.2518 - val_loss: 130.7011\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.2921 - val_loss: 462.0186\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 115.7757 - val_loss: 142.6451\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.7412 - val_loss: 146.9440\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.7643 - val_loss: 170.1233\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 116.8496 - val_loss: 135.8429\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 119.5585 - val_loss: 243.6580\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 112.1334 - val_loss: 131.6599\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.1963 - val_loss: 131.3698\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.1655 - val_loss: 127.8330\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.0607 - val_loss: 285.2621\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.0105 - val_loss: 136.6865\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.3114 - val_loss: 160.6776\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.6987 - val_loss: 207.6899\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.5952 - val_loss: 131.0928\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.4656 - val_loss: 180.6061\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.9847 - val_loss: 165.1704\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.7934 - val_loss: 122.1440\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.2937 - val_loss: 146.7477\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.2911 - val_loss: 200.9669\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.9141 - val_loss: 141.3240\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 115.9945 - val_loss: 220.4340\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 113.0753 - val_loss: 152.1339\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 112.3374 - val_loss: 216.8693\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.2829 - val_loss: 211.5335\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.7429 - val_loss: 178.9796\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.5530 - val_loss: 116.5309\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.0172 - val_loss: 534.8957\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 117.0827 - val_loss: 131.9194\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 117.3975 - val_loss: 142.3736\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 133.1903 - val_loss: 221.4467\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 119.2602 - val_loss: 170.4352\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 113.6914 - val_loss: 199.9679\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 114.5467 - val_loss: 261.0300\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 116.3814 - val_loss: 163.6297\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 113.2961 - val_loss: 139.1270\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 112.3460 - val_loss: 134.3254\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.3244 - val_loss: 242.0058\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 113.7358 - val_loss: 139.5571\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.6799 - val_loss: 182.6591\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.4183 - val_loss: 211.7012\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.2558 - val_loss: 247.3712\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.4989 - val_loss: 119.3198\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.3164 - val_loss: 455.7699\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 131.2396 - val_loss: 784.9230\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 135.9311 - val_loss: 191.4524\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 124.8438 - val_loss: 407.0697\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 116.4414 - val_loss: 283.6443\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.0180 - val_loss: 149.1840\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.6965 - val_loss: 113.2028\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.4884 - val_loss: 180.8370\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.2773 - val_loss: 357.4389\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.3907 - val_loss: 335.6182\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.9293 - val_loss: 121.1097\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.7570 - val_loss: 261.1154\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.5620 - val_loss: 362.2097\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.6779 - val_loss: 114.6394\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.5517 - val_loss: 158.0645\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.6241 - val_loss: 170.1214\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.4401 - val_loss: 110.1293\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.9269 - val_loss: 764.4861\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.7248 - val_loss: 630.8469\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.0257 - val_loss: 321.6561\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.2697 - val_loss: 179.8416\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.9767 - val_loss: 149.9060\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.7701 - val_loss: 337.9929\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 117.0479 - val_loss: 133.7896\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.4352 - val_loss: 289.0583\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.9959 - val_loss: 222.1791\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.9404 - val_loss: 153.2147\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.2721 - val_loss: 117.4600\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.0862 - val_loss: 119.2109\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.7077 - val_loss: 130.6360\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.1870 - val_loss: 109.8842\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.7616 - val_loss: 160.7367\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.3884 - val_loss: 154.0909\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.2101 - val_loss: 129.8973\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.8819 - val_loss: 139.8792\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.0881 - val_loss: 149.8895\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.5040 - val_loss: 127.3946\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.1835 - val_loss: 118.7150\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.2018 - val_loss: 307.3491\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.7318 - val_loss: 115.8023\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.8494 - val_loss: 121.5995\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.8033 - val_loss: 112.8168\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.9620 - val_loss: 135.4319\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.5500 - val_loss: 126.1914\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.8181 - val_loss: 123.9859\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.6905 - val_loss: 115.0257\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.1071 - val_loss: 138.4263\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.9693 - val_loss: 117.3650\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.1659 - val_loss: 114.7942\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.8419 - val_loss: 132.3147\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.5976 - val_loss: 136.1405\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.8362 - val_loss: 501.7194\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 118.9072 - val_loss: 130.4593\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.2313 - val_loss: 193.8377\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.9222 - val_loss: 139.9063\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.4034 - val_loss: 118.0246\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.9337 - val_loss: 109.7659\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.2552 - val_loss: 118.3485\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.3764 - val_loss: 114.4684\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.6234 - val_loss: 113.5008\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.6229 - val_loss: 113.2085\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.2412 - val_loss: 126.4657\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.4766 - val_loss: 113.2212\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.2661 - val_loss: 108.4871\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.2149 - val_loss: 116.5927\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.0788 - val_loss: 109.6441\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.8153 - val_loss: 109.5051\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.2198 - val_loss: 124.7161\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.3084 - val_loss: 110.5425\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.3197 - val_loss: 167.7641\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.7220 - val_loss: 121.9119\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.2444 - val_loss: 120.4261\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.6218 - val_loss: 132.2827\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.0909 - val_loss: 115.8828\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.1748 - val_loss: 113.8597\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.8475 - val_loss: 128.1090\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.6384 - val_loss: 154.4554\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.0795 - val_loss: 106.8084\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.5121 - val_loss: 116.6004\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.9738 - val_loss: 110.6010\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.5132 - val_loss: 117.3279\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.0902 - val_loss: 118.8555\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.8304 - val_loss: 145.4394\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.1539 - val_loss: 117.8253\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.4520 - val_loss: 109.1116\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.8193 - val_loss: 104.9442\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.0925 - val_loss: 174.5550\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.7637 - val_loss: 117.6278\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.5174 - val_loss: 115.0500\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.7887 - val_loss: 122.6178\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.4346 - val_loss: 107.9202\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.5123 - val_loss: 125.1516\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.0886 - val_loss: 129.6956\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.1825 - val_loss: 113.1111\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.2419 - val_loss: 109.6583\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.2569 - val_loss: 171.0334\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.1683 - val_loss: 113.0736\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.4488 - val_loss: 117.8365\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.8901 - val_loss: 124.3411\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.5135 - val_loss: 139.5939\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.9085 - val_loss: 109.1758\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.9684 - val_loss: 108.7199\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.1677 - val_loss: 113.2367\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.4995 - val_loss: 115.7825\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.6521 - val_loss: 206.3482\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 115.8242 - val_loss: 262.2411\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 114.4515 - val_loss: 123.5380\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 113.4360 - val_loss: 122.8272\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 113.3773 - val_loss: 158.5717\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 111.1097 - val_loss: 157.1109\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.4321 - val_loss: 190.2725\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.3671 - val_loss: 178.5496\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.6318 - val_loss: 170.4313\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.6650 - val_loss: 136.5036- loss: 105. - ETA: 0s - loss: 10\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.5591 - val_loss: 121.4646\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.5704 - val_loss: 184.9638\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.8663 - val_loss: 221.4498\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.0684 - val_loss: 116.7033\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.2178 - val_loss: 110.4070\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.8123 - val_loss: 186.4374\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.3857 - val_loss: 109.0101\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.5234 - val_loss: 141.8482\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.7529 - val_loss: 152.2971\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.9858 - val_loss: 143.3734\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.6770 - val_loss: 111.7173\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.1629 - val_loss: 180.4333\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.1377 - val_loss: 115.9627\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.3130 - val_loss: 174.2402\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.1140 - val_loss: 255.2381\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.6146 - val_loss: 129.7127\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.0925 - val_loss: 150.1490\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.7579 - val_loss: 136.2663\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.6002 - val_loss: 125.5596\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.2587 - val_loss: 122.1911\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.6492 - val_loss: 112.7613\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.9993 - val_loss: 117.7126\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.3072 - val_loss: 124.5115\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.5205 - val_loss: 134.2928\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.3763 - val_loss: 115.9388\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.0576 - val_loss: 109.9599\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.1185 - val_loss: 115.6026\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.7156 - val_loss: 157.4630\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.4734 - val_loss: 107.5348\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.5108 - val_loss: 126.3431\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.7445 - val_loss: 109.3421\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.4844 - val_loss: 114.1500\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.6815 - val_loss: 119.4967\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.0708 - val_loss: 106.7165\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.9607 - val_loss: 108.4625\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.0995 - val_loss: 107.3820\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.2201 - val_loss: 120.6781\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.3840 - val_loss: 121.3286\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.2985 - val_loss: 107.8290\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.6417 - val_loss: 104.9810\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.7779 - val_loss: 105.8414\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.8601 - val_loss: 107.4812\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.9759 - val_loss: 166.1520\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.4854 - val_loss: 195.6537\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.7484 - val_loss: 105.4472\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.7197 - val_loss: 117.8304\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.5036 - val_loss: 119.7718\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.3347 - val_loss: 154.5412\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.2700 - val_loss: 139.8131\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.5554 - val_loss: 140.9874\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.4926 - val_loss: 118.4001\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.4165 - val_loss: 115.0165\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.1701 - val_loss: 125.4526\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.1568 - val_loss: 142.3105\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.9357 - val_loss: 120.3920\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.2480 - val_loss: 126.7968\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.7095 - val_loss: 116.5285\n",
            "Epoch 381/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 97.2943 - val_loss: 178.1584\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.5142 - val_loss: 118.2327\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.7985 - val_loss: 105.1955\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.0085 - val_loss: 105.8489\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.3890 - val_loss: 167.7007\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.6170 - val_loss: 113.5781\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.3969 - val_loss: 105.4034\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.4709 - val_loss: 192.4523\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.5471 - val_loss: 111.0631\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.0485 - val_loss: 132.9482\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.7582 - val_loss: 114.7719\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.5330 - val_loss: 145.2282\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.0944 - val_loss: 111.6736\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.3382 - val_loss: 105.5705\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.4150 - val_loss: 189.4387\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.0342 - val_loss: 118.0032\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.8568 - val_loss: 137.1750\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.9898 - val_loss: 106.3967\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.0644 - val_loss: 125.0572\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.8541 - val_loss: 118.2503\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.0390 - val_loss: 105.0780\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.4764 - val_loss: 130.4552\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.5417 - val_loss: 106.2340\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.2277 - val_loss: 111.4283\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.1798 - val_loss: 116.5284\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.5876 - val_loss: 107.4244\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.5799 - val_loss: 105.4236\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.2707 - val_loss: 106.2510\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.0483 - val_loss: 101.2325\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.2732 - val_loss: 108.1928\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.9229 - val_loss: 120.1614\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.5427 - val_loss: 110.1394\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.1826 - val_loss: 103.5310\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.9016 - val_loss: 114.0854\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.0246 - val_loss: 121.1266\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.6053 - val_loss: 110.7894\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.0566 - val_loss: 118.7379\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.8447 - val_loss: 111.2404\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.5791 - val_loss: 114.8444\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.8988 - val_loss: 118.1883\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.3523 - val_loss: 146.1295\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.6228 - val_loss: 106.3035\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.7261 - val_loss: 302.4198\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.0163 - val_loss: 103.3588\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.2019 - val_loss: 115.4187\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.0958 - val_loss: 101.3292\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.5618 - val_loss: 144.1909\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.5657 - val_loss: 138.7245\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.2568 - val_loss: 148.7375\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.0471 - val_loss: 107.2820\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.6664 - val_loss: 126.2052\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.2245 - val_loss: 103.0831\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.8921 - val_loss: 110.4860\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.7395 - val_loss: 132.2094\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.2039 - val_loss: 108.1414\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.6925 - val_loss: 117.6515\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.9641 - val_loss: 147.5709\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.7744 - val_loss: 152.7527\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.0996 - val_loss: 106.9573\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.4370 - val_loss: 131.0290\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.0027 - val_loss: 158.8163\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.3238 - val_loss: 103.7190\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.9884 - val_loss: 108.1133\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.5121 - val_loss: 134.6787\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.7554 - val_loss: 107.0389\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.6842 - val_loss: 131.0779\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.0879 - val_loss: 138.8651\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.7319 - val_loss: 117.8613\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.2111 - val_loss: 103.7890\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.1948 - val_loss: 246.6173\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.7161 - val_loss: 104.4993\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.7023 - val_loss: 105.1918\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.5892 - val_loss: 106.7074\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.5884 - val_loss: 111.6488\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.5544 - val_loss: 116.7838\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.3048 - val_loss: 121.3658\n",
            "Epoch 457/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 93.8195 - val_loss: 154.5824\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.5244 - val_loss: 115.1778\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.6595 - val_loss: 149.3887\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.7300 - val_loss: 398.2302\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.5437 - val_loss: 123.1124\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.9474 - val_loss: 108.1655\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.2474 - val_loss: 117.1965\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.2336 - val_loss: 102.5618\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.7886 - val_loss: 114.9236\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.8590 - val_loss: 107.5048\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.7929 - val_loss: 119.3363\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.4137 - val_loss: 155.2523\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.1904 - val_loss: 105.3065\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.1533 - val_loss: 102.0389\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.9325 - val_loss: 108.3222\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.7172 - val_loss: 104.4087\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.8457 - val_loss: 155.1371\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.9925 - val_loss: 106.1842\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.4562 - val_loss: 99.3824\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.6448 - val_loss: 116.9450\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.9197 - val_loss: 99.9555\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.2621 - val_loss: 110.3062\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.4927 - val_loss: 99.4597\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.7027 - val_loss: 98.1960\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 90.7644 - val_loss: 105.5357\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.0583 - val_loss: 107.9848\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.3825 - val_loss: 108.3275\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.1861 - val_loss: 104.8683\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.0820 - val_loss: 106.5022\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.8105 - val_loss: 219.0276\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.8998 - val_loss: 103.7482\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.0037 - val_loss: 136.7181\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.4084 - val_loss: 114.1003\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.7534 - val_loss: 100.5626\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.8254 - val_loss: 181.9209\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.9942 - val_loss: 105.8850\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.1497 - val_loss: 115.3762\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.5240 - val_loss: 136.9372\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.1445 - val_loss: 112.5077\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.1618 - val_loss: 119.5781\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.5423 - val_loss: 107.5964\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.9177 - val_loss: 114.3969\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.8603 - val_loss: 184.8956\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.7422 - val_loss: 99.9552\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  1.0329855182989263 \n",
            "MAE:  7.458221552626488 \n",
            "SD:  9.944249486968442\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 500])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "outputId": "4a85f233-7730-4d19-a30a-66ad769ace4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_110 (Dense)            (None, 8)                 1024      \n",
            "_________________________________________________________________\n",
            "batch_normalization_105 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_105 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_111 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_106 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_106 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_112 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_107 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_107 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_108 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_108 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_109 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_109 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_110 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_110 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_116 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_111 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_111 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_117 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_112 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_112 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_118 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_113 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_113 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_119 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_114 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_114 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_120 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_115 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_115 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_121 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_116 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_116 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_122 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_117 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_117 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_123 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_118 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_118 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_124 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_119 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_119 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_125 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_120 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_120 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_126 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_121 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_121 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_127 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_122 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_122 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_128 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_123 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_123 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_129 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_124 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_124 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_130 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_125 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_125 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_131 (Dense)            (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 3,145\n",
            "Trainable params: 2,809\n",
            "Non-trainable params: 336\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXAOqd2CTsa",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 12378.5020 - val_loss: 12355.9336: 1\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 12106.9697 - val_loss: 12085.8799\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 11766.1777 - val_loss: 11451.0957\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 11303.8145 - val_loss: 9892.4473\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 10690.3125 - val_loss: 10115.8838\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 10003.8301 - val_loss: 9675.7734\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 9259.3311 - val_loss: 9220.0518\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 8471.9883 - val_loss: 7367.6177\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 7682.8926 - val_loss: 7264.9536\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 6887.0269 - val_loss: 6779.6074\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 6061.7305 - val_loss: 6127.0864\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 5295.0420 - val_loss: 5058.6421\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 4578.0811 - val_loss: 4985.6333\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3913.5312 - val_loss: 3674.0071\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3286.3022 - val_loss: 3322.7070\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2718.4768 - val_loss: 2664.6631\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2217.9353 - val_loss: 1992.5342\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1787.9658 - val_loss: 1571.7836\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1435.7885 - val_loss: 1276.4150\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1130.5911 - val_loss: 964.4640\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 884.1705 - val_loss: 938.3765\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 705.1264 - val_loss: 726.4662\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 565.7734 - val_loss: 526.7188\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 461.3983 - val_loss: 373.2467\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 387.8891 - val_loss: 358.0079\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 338.1160 - val_loss: 337.6157\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 310.2738 - val_loss: 289.1187\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 295.8091 - val_loss: 276.3146\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 282.8828 - val_loss: 271.8423\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 277.1572 - val_loss: 268.2502\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 274.4384 - val_loss: 262.9838\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 272.5345 - val_loss: 262.3785\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 271.8561 - val_loss: 261.1752\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 272.5061 - val_loss: 268.8612\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 271.7050 - val_loss: 260.0667\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 269.7826 - val_loss: 262.5991\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 270.0362 - val_loss: 261.6193\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 269.6169 - val_loss: 260.7857\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 269.1329 - val_loss: 257.6874\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 268.7249 - val_loss: 263.9434\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 266.9863 - val_loss: 262.1472\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 269.0195 - val_loss: 267.0776\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 270.5224 - val_loss: 256.2297\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 269.7140 - val_loss: 270.3254\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 269.4299 - val_loss: 264.9315\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 270.5578 - val_loss: 263.5802\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 268.9065 - val_loss: 261.7270\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 264.2048 - val_loss: 258.3761\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 264.1969 - val_loss: 254.0313\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 263.4657 - val_loss: 258.5441\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 266.0636 - val_loss: 271.0012\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 263.0980 - val_loss: 264.0102\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 260.9202 - val_loss: 262.3622\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 260.7469 - val_loss: 261.0981\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 259.8853 - val_loss: 258.1907\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 257.5766 - val_loss: 257.7516\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 263.7294 - val_loss: 261.9344\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 266.0939 - val_loss: 250.9207\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 265.0905 - val_loss: 263.6726\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 263.9828 - val_loss: 260.1138\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 262.2956 - val_loss: 250.0800\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 260.9326 - val_loss: 259.7422\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 260.9921 - val_loss: 259.8620\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 261.4868 - val_loss: 260.1353\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 259.8259 - val_loss: 271.5077\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 259.3438 - val_loss: 250.9941\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 257.8602 - val_loss: 261.1505\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 254.5247 - val_loss: 253.0226\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 255.3186 - val_loss: 245.1695\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 255.7464 - val_loss: 248.7628\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 253.1104 - val_loss: 268.5848\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 252.1973 - val_loss: 248.9382\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 252.0010 - val_loss: 247.9252\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 253.4483 - val_loss: 283.4178\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 254.0366 - val_loss: 262.6270\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 249.8914 - val_loss: 263.8080\n",
            "Epoch 77/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 250.2589 - val_loss: 260.2677\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 246.5319 - val_loss: 239.3481\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 251.2995 - val_loss: 255.8787\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 246.4509 - val_loss: 244.5150\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 241.6509 - val_loss: 228.6660\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 248.4642 - val_loss: 256.9294\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 243.9913 - val_loss: 253.7012\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 239.5600 - val_loss: 242.1552\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 236.3156 - val_loss: 247.1880\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 235.9165 - val_loss: 230.7098\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 232.3788 - val_loss: 222.5364\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 233.8910 - val_loss: 223.3750\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 231.7698 - val_loss: 237.8397\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 231.8822 - val_loss: 219.3270\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 231.2467 - val_loss: 229.7194\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 227.9193 - val_loss: 233.6871\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 228.2426 - val_loss: 231.1504\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 231.0659 - val_loss: 236.8431\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 225.2746 - val_loss: 225.9326\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 221.1087 - val_loss: 229.1601\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 214.7423 - val_loss: 308.6093\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 205.6136 - val_loss: 359.7672\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 197.1689 - val_loss: 288.9002\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 187.4246 - val_loss: 229.8183\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 179.0587 - val_loss: 180.5410\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 173.2461 - val_loss: 299.0792\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 168.7323 - val_loss: 174.2208\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 162.9444 - val_loss: 184.5903\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 159.7814 - val_loss: 173.1782\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 155.4957 - val_loss: 198.2907\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 154.1285 - val_loss: 157.7076\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 151.0349 - val_loss: 214.0828\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 150.7775 - val_loss: 185.1712\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 152.8219 - val_loss: 197.7464\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 148.1848 - val_loss: 277.3528\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 146.4292 - val_loss: 155.8300\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 145.0212 - val_loss: 410.4469\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 141.1108 - val_loss: 308.6697\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 140.1350 - val_loss: 182.2657\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 132.5605 - val_loss: 198.0896\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 135.0171 - val_loss: 163.7598\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 136.8041 - val_loss: 144.1160\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 129.6086 - val_loss: 159.4484\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 128.9031 - val_loss: 360.7325\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 125.2041 - val_loss: 158.9670\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 124.5272 - val_loss: 127.8204\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 126.6066 - val_loss: 144.9020\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 124.8681 - val_loss: 131.2538\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 124.5424 - val_loss: 283.9982\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 128.4272 - val_loss: 185.8033\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 130.1071 - val_loss: 343.7621\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 136.1181 - val_loss: 333.0208\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 133.1186 - val_loss: 389.4553\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 130.9947 - val_loss: 519.4960\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 127.8539 - val_loss: 215.2645\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 127.5760 - val_loss: 151.5766\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 128.6208 - val_loss: 151.6662\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 129.4732 - val_loss: 163.0747\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 125.0424 - val_loss: 174.8053\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 121.6792 - val_loss: 148.6012\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 120.8181 - val_loss: 175.9847\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 120.0178 - val_loss: 214.3126\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 120.4700 - val_loss: 158.8593\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 119.8445 - val_loss: 156.7448\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 119.0830 - val_loss: 148.1773\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 120.6116 - val_loss: 521.8592\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 125.4169 - val_loss: 423.3590\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 121.6570 - val_loss: 123.1537\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 128.0410 - val_loss: 315.0330\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 137.4941 - val_loss: 148.6341\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 145.3108 - val_loss: 259.5351\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 133.0541 - val_loss: 351.4346\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 121.8609 - val_loss: 143.6662\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 123.4955 - val_loss: 585.7082\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 122.1823 - val_loss: 182.9922\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 121.3517 - val_loss: 127.9416\n",
            "Epoch 153/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 121.6253 - val_loss: 628.8359\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 123.3524 - val_loss: 173.5537\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 125.1659 - val_loss: 314.1222\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 124.4229 - val_loss: 201.2024\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 127.9261 - val_loss: 303.3357\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 125.9913 - val_loss: 200.5519\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 133.6082 - val_loss: 236.1246\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 137.0491 - val_loss: 162.7316\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 123.1934 - val_loss: 154.3097\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 119.5937 - val_loss: 148.5974\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 123.2336 - val_loss: 129.6528\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 121.2406 - val_loss: 201.0051\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 119.4467 - val_loss: 137.3504\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 120.2118 - val_loss: 154.2065\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 124.1868 - val_loss: 134.8215\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 121.0773 - val_loss: 136.8836\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 120.7138 - val_loss: 178.8195\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 121.7612 - val_loss: 129.4811\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 119.1303 - val_loss: 125.8829\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 120.2444 - val_loss: 146.5194\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 117.5687 - val_loss: 158.2976\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 119.7298 - val_loss: 242.8273\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 125.1313 - val_loss: 135.0556\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 121.5000 - val_loss: 356.6069\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 132.1105 - val_loss: 280.5620\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 125.3413 - val_loss: 133.1252\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 127.2547 - val_loss: 256.6519\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 133.9309 - val_loss: 140.3409\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 131.5543 - val_loss: 175.8105\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 131.0952 - val_loss: 158.7265\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 127.1286 - val_loss: 130.6642\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 124.2479 - val_loss: 183.3341\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 125.2520 - val_loss: 422.9022\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 124.9927 - val_loss: 133.4848\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 123.6376 - val_loss: 180.3180\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 125.1500 - val_loss: 145.2643\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 122.0631 - val_loss: 147.0628\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 119.5493 - val_loss: 134.7350\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 118.8204 - val_loss: 226.4867\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 117.8144 - val_loss: 186.9788\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 125.7782 - val_loss: 188.6981\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 122.3504 - val_loss: 127.1846\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 119.9169 - val_loss: 167.0059\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 118.1684 - val_loss: 207.9257\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 117.1145 - val_loss: 119.4577\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 116.8013 - val_loss: 152.6726\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 118.0329 - val_loss: 129.6791\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 120.7128 - val_loss: 139.4503\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 119.1470 - val_loss: 323.9074\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 119.8785 - val_loss: 349.2629\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 119.7538 - val_loss: 131.1804\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 124.4958 - val_loss: 164.4595\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 123.4217 - val_loss: 136.8081\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 117.3191 - val_loss: 203.5693\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 114.8311 - val_loss: 121.1863\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 117.0105 - val_loss: 152.8327\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 120.7092 - val_loss: 147.1297\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 122.2490 - val_loss: 258.8188\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 121.6073 - val_loss: 142.9239\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 116.7306 - val_loss: 546.9351\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 116.2242 - val_loss: 198.9171\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 115.3723 - val_loss: 189.0563\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 114.6572 - val_loss: 131.4748\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.0578 - val_loss: 134.5216\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.7968 - val_loss: 160.7520\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 111.6674 - val_loss: 145.5138\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.7055 - val_loss: 173.4056\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.9289 - val_loss: 131.2377\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 112.4121 - val_loss: 168.3709\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.6995 - val_loss: 155.0574\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.3510 - val_loss: 134.6561\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 113.4721 - val_loss: 158.4553\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 116.3032 - val_loss: 256.2160\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.1807 - val_loss: 128.2854\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 115.7384 - val_loss: 123.3691\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.6476 - val_loss: 167.5467\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.1480 - val_loss: 154.1722\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 115.3759 - val_loss: 140.7303\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 113.4882 - val_loss: 193.2385\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.5154 - val_loss: 389.1688\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.9490 - val_loss: 120.8152\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.9440 - val_loss: 131.0912\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.1357 - val_loss: 124.2906\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 115.1194 - val_loss: 216.5801\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.6129 - val_loss: 132.6592\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.2196 - val_loss: 114.5076\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.2455 - val_loss: 121.4849\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.7885 - val_loss: 123.3982\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.6819 - val_loss: 238.1024\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.2280 - val_loss: 146.8638\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.0029 - val_loss: 126.4215\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.5238 - val_loss: 133.4595\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.0800 - val_loss: 233.4531\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.4045 - val_loss: 135.8269\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.6625 - val_loss: 236.3652\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.4838 - val_loss: 115.6558\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.2261 - val_loss: 126.9641\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.7697 - val_loss: 163.5667\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.5451 - val_loss: 140.4872\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.0052 - val_loss: 161.4943\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.4363 - val_loss: 195.3815\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.1275 - val_loss: 266.5523\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.4608 - val_loss: 186.8092\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.1278 - val_loss: 191.0751\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.6587 - val_loss: 110.1117\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.3223 - val_loss: 129.1308\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.4400 - val_loss: 132.4616\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.2486 - val_loss: 124.3401\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.1192 - val_loss: 156.0293\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.1377 - val_loss: 118.8716\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.4735 - val_loss: 119.8131\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 113.9302 - val_loss: 181.1243\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.0275 - val_loss: 178.7529\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.2915 - val_loss: 118.1062\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.8437 - val_loss: 119.8636\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.8856 - val_loss: 185.1806\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.9389 - val_loss: 125.8733\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.1244 - val_loss: 117.7070\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.0512 - val_loss: 139.8710\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.6953 - val_loss: 112.9863\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.5753 - val_loss: 132.8853\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.8785 - val_loss: 142.2832\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.1934 - val_loss: 110.6745\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.0492 - val_loss: 125.1880\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.9575 - val_loss: 113.5558\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.9405 - val_loss: 126.2442\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.3228 - val_loss: 119.8097\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.3173 - val_loss: 111.2203\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.6369 - val_loss: 136.6334\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.1782 - val_loss: 148.7095\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.6451 - val_loss: 151.4857\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.6042 - val_loss: 216.1503\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.3590 - val_loss: 232.1259\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.4577 - val_loss: 145.0127\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.3995 - val_loss: 130.5965\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.4005 - val_loss: 137.6947\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.7080 - val_loss: 157.1024\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.0331 - val_loss: 145.9134\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.0807 - val_loss: 141.2307\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.6045 - val_loss: 147.0106\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.1544 - val_loss: 133.6996\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.3974 - val_loss: 173.7725\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.1108 - val_loss: 144.0475\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.5587 - val_loss: 175.0925\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.5250 - val_loss: 148.1753\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.8916 - val_loss: 141.4506\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.7811 - val_loss: 411.8009\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.4688 - val_loss: 132.6704\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.8202 - val_loss: 196.0513\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.2651 - val_loss: 233.2282\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.5735 - val_loss: 114.6099\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.2343 - val_loss: 108.4364\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.5424 - val_loss: 200.4361\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.3712 - val_loss: 131.4486\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.7632 - val_loss: 182.1300\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.3186 - val_loss: 255.2592\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.2787 - val_loss: 154.2353\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.4115 - val_loss: 195.8333\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.0036 - val_loss: 129.2461\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.2037 - val_loss: 520.6661\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 111.1479 - val_loss: 183.1167\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.2978 - val_loss: 122.9770\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.3158 - val_loss: 129.4709\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.7186 - val_loss: 671.2310\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.8969 - val_loss: 209.3329\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.0785 - val_loss: 167.7281\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.5636 - val_loss: 113.6095\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.9020 - val_loss: 136.1878\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.5587 - val_loss: 127.5940\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.6057 - val_loss: 128.6669\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.4133 - val_loss: 117.7969\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.7183 - val_loss: 431.2379\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.6896 - val_loss: 139.0870\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.6722 - val_loss: 118.3527\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.3238 - val_loss: 171.5512\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.6720 - val_loss: 117.5989\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.8233 - val_loss: 107.9056\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.8860 - val_loss: 164.0609\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.5056 - val_loss: 164.1716\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.3770 - val_loss: 246.6797\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.6785 - val_loss: 106.9871\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.3987 - val_loss: 114.2123\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.1244 - val_loss: 111.9713\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.2312 - val_loss: 120.6581\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.8120 - val_loss: 136.9567\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.9435 - val_loss: 127.6287\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.0993 - val_loss: 224.5390\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.0914 - val_loss: 158.9098\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.5352 - val_loss: 238.8862\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.8143 - val_loss: 129.7350\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.8173 - val_loss: 200.1898\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.7405 - val_loss: 116.3655\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.4041 - val_loss: 147.2686\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.7875 - val_loss: 260.0070\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.0267 - val_loss: 382.9873\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.6805 - val_loss: 157.9204\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.2105 - val_loss: 136.4634\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.0803 - val_loss: 153.0058\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.7984 - val_loss: 174.2277\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 112.4028 - val_loss: 134.8315\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.8512 - val_loss: 172.7030\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.5117 - val_loss: 326.4805\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 114.4265 - val_loss: 127.5531\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 115.8437 - val_loss: 224.7432\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 114.0263 - val_loss: 142.5775\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 111.2074 - val_loss: 122.0062\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.4082 - val_loss: 152.5320\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.6669 - val_loss: 191.5025\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.8147 - val_loss: 136.9184\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.6592 - val_loss: 114.2932\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.2411 - val_loss: 161.1799\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.3907 - val_loss: 155.0960\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.1723 - val_loss: 114.1695\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.9806 - val_loss: 148.6447\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.9784 - val_loss: 125.7157\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.6130 - val_loss: 246.3421\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.3721 - val_loss: 114.5015\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.1185 - val_loss: 123.7393\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.7565 - val_loss: 205.5390\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.6610 - val_loss: 148.9171\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.9323 - val_loss: 202.5127\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.3292 - val_loss: 144.4853\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.2585 - val_loss: 639.2798\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.7771 - val_loss: 248.0879\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.1434 - val_loss: 114.1292\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.9437 - val_loss: 133.2299\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.1204 - val_loss: 149.7964\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.9726 - val_loss: 134.9627\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.5747 - val_loss: 127.0462\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.3869 - val_loss: 127.0559\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.8407 - val_loss: 126.3146\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.5673 - val_loss: 155.4365\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.5758 - val_loss: 119.8330\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.3690 - val_loss: 244.9383\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.9967 - val_loss: 113.7498\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.0842 - val_loss: 240.3636\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.8962 - val_loss: 123.2808\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.2242 - val_loss: 180.0719\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.1251 - val_loss: 116.9227\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.4387 - val_loss: 114.8198\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.0957 - val_loss: 130.2761\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.7351 - val_loss: 140.1128\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.8716 - val_loss: 117.0495\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.4504 - val_loss: 109.1834\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.4810 - val_loss: 174.3188\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.4118 - val_loss: 216.4365\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.3358 - val_loss: 120.5319\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.7697 - val_loss: 178.2574\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.2906 - val_loss: 130.1349\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.1610 - val_loss: 112.6952\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.3387 - val_loss: 112.9986\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.1576 - val_loss: 120.7605\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.8281 - val_loss: 111.3310\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.1297 - val_loss: 135.7907\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.6827 - val_loss: 137.2859\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.1060 - val_loss: 165.3786\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.0522 - val_loss: 157.6206\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.7325 - val_loss: 137.5815\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.7626 - val_loss: 147.9978\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.1615 - val_loss: 110.1557\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.7576 - val_loss: 110.7272\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.4692 - val_loss: 111.1401\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 101.9922 - val_loss: 113.3115\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.3653 - val_loss: 110.2503\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.7697 - val_loss: 142.9620\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.6865 - val_loss: 111.8141\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.0148 - val_loss: 108.7161\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.1116 - val_loss: 107.7645\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.5251 - val_loss: 132.2860\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.2536 - val_loss: 137.0309\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.2437 - val_loss: 111.7101\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.5070 - val_loss: 144.2942\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.2397 - val_loss: 115.8107\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.8287 - val_loss: 102.7961\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.6641 - val_loss: 170.2364\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.0742 - val_loss: 136.6148\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.4297 - val_loss: 117.9316\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.8015 - val_loss: 126.6255\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.7786 - val_loss: 130.0883\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.1650 - val_loss: 107.3385\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.1851 - val_loss: 137.7808\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.3806 - val_loss: 281.1002\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.6783 - val_loss: 104.8041\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.7521 - val_loss: 123.0897\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.3913 - val_loss: 110.7411\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.7582 - val_loss: 123.3984\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.8585 - val_loss: 115.0626\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.8980 - val_loss: 102.5268\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.2840 - val_loss: 137.1983\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.1294 - val_loss: 140.7719\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.4992 - val_loss: 118.6983\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.7346 - val_loss: 106.8448\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.0802 - val_loss: 106.2369\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.2541 - val_loss: 117.8631\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.6044 - val_loss: 119.8122\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.9813 - val_loss: 123.5903\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.4494 - val_loss: 115.1581\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.8361 - val_loss: 109.3945\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.0544 - val_loss: 131.0861\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.4097 - val_loss: 112.0100\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.6717 - val_loss: 110.1619\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.8897 - val_loss: 102.3678\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.3045 - val_loss: 105.9734\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.6957 - val_loss: 138.6436\n",
            "Epoch 457/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 97.8603 - val_loss: 110.1488\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.4631 - val_loss: 128.4176\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.6404 - val_loss: 136.1563\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.0823 - val_loss: 107.3792\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.5116 - val_loss: 124.1242\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.0340 - val_loss: 107.7411\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.8689 - val_loss: 119.3526\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.9853 - val_loss: 139.0999\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.6182 - val_loss: 149.9754\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.4700 - val_loss: 109.8306\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.5119 - val_loss: 119.8528\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 97.2647 - val_loss: 130.6721\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.7357 - val_loss: 112.7081\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.7013 - val_loss: 103.4248\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.0088 - val_loss: 122.5682\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.8186 - val_loss: 115.1763\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.5364 - val_loss: 107.5602\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.2057 - val_loss: 105.0422\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.1948 - val_loss: 139.8137\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.3058 - val_loss: 115.3755\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.0068 - val_loss: 129.1727\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.6232 - val_loss: 132.0567\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.4308 - val_loss: 152.5032\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.3083 - val_loss: 138.6445\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.4662 - val_loss: 105.9831\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.4084 - val_loss: 102.8923\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.1358 - val_loss: 258.7150\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.1704 - val_loss: 110.0568\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.3633 - val_loss: 173.9429\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 100.9840 - val_loss: 128.1646\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.2691 - val_loss: 210.4895\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.4742 - val_loss: 139.2282\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.4973 - val_loss: 127.4669\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.6417 - val_loss: 107.1806\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.9743 - val_loss: 118.0260\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.9446 - val_loss: 109.9380\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.6217 - val_loss: 118.4949\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.1033 - val_loss: 143.9303\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.0544 - val_loss: 104.6014\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.2441 - val_loss: 114.7360\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.1481 - val_loss: 118.5963\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.9217 - val_loss: 125.6758\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.2063 - val_loss: 104.0996\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.6699 - val_loss: 129.6610\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696v_fuFCTsa",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  4.8419052432691805 \n",
            "MAE:  8.722902504702974 \n",
            "SD:  10.306159608649516\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 500])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdZF2osWCUQS",
        "outputId": "cbb0a52d-bbee-4a9d-b169-0b473d52c33c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  1.042113648552464 \n",
            "Ensemble_std:  10.273418913750115\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_132 (Dense)            (None, 8)                 1024      \n",
            "_________________________________________________________________\n",
            "batch_normalization_126 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_126 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_133 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_127 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_127 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_134 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_128 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_128 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_135 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_129 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_129 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_136 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_130 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_130 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_137 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_131 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_131 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_138 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_132 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_132 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_139 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_133 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_133 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_140 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_134 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_134 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_141 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_135 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_135 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_142 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_136 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_136 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_143 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_137 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_137 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_144 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_138 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_138 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_145 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_139 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_139 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_146 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_140 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_140 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_147 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_141 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_141 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_148 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_142 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_142 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_149 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_143 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_143 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_150 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_144 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_144 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_151 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_145 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_145 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_152 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_146 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_146 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_153 (Dense)            (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 3,145\n",
            "Trainable params: 2,809\n",
            "Non-trainable params: 336\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 3714.6606 - val_loss: 3737.1338\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3507.5618 - val_loss: 3405.9512\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3243.0776 - val_loss: 3089.1885\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2910.5498 - val_loss: 2764.0798\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2536.8662 - val_loss: 2148.6882\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2136.2627 - val_loss: 1815.5328\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1718.6030 - val_loss: 1581.4941\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1332.2218 - val_loss: 1171.0190\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 999.2702 - val_loss: 869.6205\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 720.4376 - val_loss: 575.5736\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 504.8925 - val_loss: 387.6065\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 346.9285 - val_loss: 265.5440\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 244.0404 - val_loss: 210.2866\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 176.6197 - val_loss: 139.7867\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 138.0065 - val_loss: 126.8901\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 121.2790 - val_loss: 109.1181\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 113.6994 - val_loss: 108.5900\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.6683 - val_loss: 106.5561\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.3221 - val_loss: 105.8845\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 108.4394 - val_loss: 105.3791\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.7057 - val_loss: 105.0469\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.4839 - val_loss: 103.1001\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.9455 - val_loss: 106.1380\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.9734 - val_loss: 106.5215\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.9442 - val_loss: 101.8068\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.1572 - val_loss: 104.5575\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.8993 - val_loss: 110.3762\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.7665 - val_loss: 108.4982\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.3940 - val_loss: 105.3068\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.5160 - val_loss: 105.9004\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 106.0161 - val_loss: 104.8271\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 105.5887 - val_loss: 104.2294\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.9041 - val_loss: 102.3038\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.5657 - val_loss: 97.8771\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.4062 - val_loss: 104.2285\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.5748 - val_loss: 102.0126\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.1558 - val_loss: 98.2313\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 102.8022 - val_loss: 103.1007\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.6911 - val_loss: 109.5052\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 99.0301 - val_loss: 96.2066\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 99.9337 - val_loss: 98.4421\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.8785 - val_loss: 101.3871\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.0551 - val_loss: 92.9497\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.2509 - val_loss: 99.3898\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.6441 - val_loss: 92.7827\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.8977 - val_loss: 89.8319\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 92.2618 - val_loss: 93.8901\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.0095 - val_loss: 84.5508\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 87.5215 - val_loss: 83.7566\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 85.6681 - val_loss: 133.0633\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.2749 - val_loss: 85.0633\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.5931 - val_loss: 102.0121\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.8661 - val_loss: 109.8943\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.2609 - val_loss: 84.5874\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 84.3211 - val_loss: 87.8963\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 80.2605 - val_loss: 79.4029\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.2349 - val_loss: 76.9941\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.2379 - val_loss: 77.6957\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 76.1615 - val_loss: 118.6031\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 74.1413 - val_loss: 95.8530\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8092 - val_loss: 91.2231\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 72.6782 - val_loss: 84.9389\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 72.2121 - val_loss: 236.6888\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 72.5103 - val_loss: 85.6838\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 70.4099 - val_loss: 85.4807\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 68.8464 - val_loss: 65.8286\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 67.6102 - val_loss: 61.5513\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.1469 - val_loss: 73.8170\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 64.1077 - val_loss: 66.9487\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 66.9207 - val_loss: 69.5753\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 62.7590 - val_loss: 60.2028\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 62.5855 - val_loss: 96.8836\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 64.3994 - val_loss: 93.7533\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 64.9624 - val_loss: 68.6555\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 62.6908 - val_loss: 79.4430\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 61.0962 - val_loss: 105.0139\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 62.8958 - val_loss: 86.9588\n",
            "Epoch 78/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 61.2115 - val_loss: 86.6697\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 59.2323 - val_loss: 62.4501\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 57.8529 - val_loss: 55.5025\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 58.1025 - val_loss: 62.0758\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 56.3886 - val_loss: 80.1986\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 55.4450 - val_loss: 64.4866\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 54.4543 - val_loss: 53.3117\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.9030 - val_loss: 54.4586\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.9837 - val_loss: 100.7651\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 53.5541 - val_loss: 77.5971\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.5608 - val_loss: 68.9566\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 51.3632 - val_loss: 133.0185\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 51.1500 - val_loss: 74.5776\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.3849 - val_loss: 71.0388\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 48.9736 - val_loss: 55.2618\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 48.6270 - val_loss: 49.2189\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 48.7142 - val_loss: 77.1142\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 48.8441 - val_loss: 76.4039\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 48.1988 - val_loss: 69.7442\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 46.8892 - val_loss: 118.0462\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 48.5947 - val_loss: 142.3115\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 52.9652 - val_loss: 60.4658\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 49.7282 - val_loss: 76.0540\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 48.3364 - val_loss: 56.6207\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 47.5181 - val_loss: 74.0257\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.6999 - val_loss: 109.9007\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.6999 - val_loss: 57.9174\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 44.5380 - val_loss: 75.3801\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.1582 - val_loss: 73.1608\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 48.6747 - val_loss: 121.0623\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 46.0122 - val_loss: 57.7768\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 46.0751 - val_loss: 114.6865\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 45.4846 - val_loss: 57.5201\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.0009 - val_loss: 135.3625\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.0376 - val_loss: 50.1252\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 44.2570 - val_loss: 125.4469\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 47.3892 - val_loss: 185.0508\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 46.4898 - val_loss: 123.2325\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 46.5302 - val_loss: 56.0673\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 49.2921 - val_loss: 93.1010\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 49.7396 - val_loss: 50.2990\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 50.4719 - val_loss: 67.2538\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 47.3072 - val_loss: 64.8608\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.4976 - val_loss: 71.9675\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 44.5353 - val_loss: 51.6139\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 44.2761 - val_loss: 145.2666\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 43.7078 - val_loss: 89.0515\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.5796 - val_loss: 51.0305\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.2080 - val_loss: 46.0242\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 41.5501 - val_loss: 68.1285\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.4141 - val_loss: 64.8796\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.3253 - val_loss: 107.2589\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.8879 - val_loss: 59.3204\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.8232 - val_loss: 95.7002\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.1531 - val_loss: 47.6165\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.0020 - val_loss: 42.4035\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.8020 - val_loss: 45.7692\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.9075 - val_loss: 59.3763\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.4931 - val_loss: 51.4529\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.0071 - val_loss: 50.6144\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.6471 - val_loss: 43.3362\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.4456 - val_loss: 84.8810\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.6751 - val_loss: 45.9530\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.3868 - val_loss: 65.6213\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.7920 - val_loss: 66.2694\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 39.1208 - val_loss: 153.6664\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.5787 - val_loss: 43.7120\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 40.2538 - val_loss: 53.8104\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.6048 - val_loss: 49.3978\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 38.9612 - val_loss: 41.4591\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 38.2716 - val_loss: 56.3381\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 38.6385 - val_loss: 143.2236\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 38.3567 - val_loss: 44.9100\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.7763 - val_loss: 44.2712\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.6034 - val_loss: 41.3127\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.5898 - val_loss: 40.1613\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.8629 - val_loss: 63.6285\n",
            "Epoch 155/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 37.5454 - val_loss: 42.9194\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.9384 - val_loss: 57.2533\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 38.7322 - val_loss: 44.5537\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.3845 - val_loss: 56.0069\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.0430 - val_loss: 42.8060\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.2994 - val_loss: 63.3406\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.2166 - val_loss: 63.7364\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.6519 - val_loss: 60.4536\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.3101 - val_loss: 41.9268\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.3885 - val_loss: 46.0059\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.3073 - val_loss: 40.0493\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.2354 - val_loss: 72.7613\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.6801 - val_loss: 39.6798\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.5382 - val_loss: 44.1688\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.5627 - val_loss: 40.6209\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.3977 - val_loss: 39.6233\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.2444 - val_loss: 41.7378\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.9513 - val_loss: 38.4315\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.9710 - val_loss: 43.1680\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.3229 - val_loss: 47.1993\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.3304 - val_loss: 41.3304\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.3097 - val_loss: 110.6040\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.4099 - val_loss: 53.6303\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.2583 - val_loss: 114.8957\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 37.6622 - val_loss: 97.1356\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.4765 - val_loss: 48.2130\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.0108 - val_loss: 48.8633\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.8967 - val_loss: 41.0794\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.6214 - val_loss: 39.5662\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.2463 - val_loss: 40.1858\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.4460 - val_loss: 56.1376\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.0408 - val_loss: 57.8682\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.6988 - val_loss: 42.2688\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.4422 - val_loss: 38.3133\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.5203 - val_loss: 40.1824\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.1947 - val_loss: 40.8035\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.3096 - val_loss: 58.6880\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.0427 - val_loss: 64.2561\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.3200 - val_loss: 58.2448\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9140 - val_loss: 44.7073\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.9348 - val_loss: 41.2455\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.8334 - val_loss: 44.6324\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.0580 - val_loss: 47.5162\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.8965 - val_loss: 41.3815\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.7314 - val_loss: 48.8361\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9709 - val_loss: 80.9188\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9899 - val_loss: 42.7023\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.7738 - val_loss: 45.0060\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5982 - val_loss: 40.1735\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.4938 - val_loss: 48.6024\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.4758 - val_loss: 39.7636\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5034 - val_loss: 37.1061\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.4466 - val_loss: 40.8170\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5899 - val_loss: 60.5553\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.3420 - val_loss: 39.9835\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.3309 - val_loss: 39.1066\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.3612 - val_loss: 38.0159\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.2280 - val_loss: 41.4747\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.1944 - val_loss: 39.3650\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.2616 - val_loss: 38.8365\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0811 - val_loss: 39.7053\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.1058 - val_loss: 38.2739\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.2470 - val_loss: 48.6908\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0047 - val_loss: 38.8626\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.1143 - val_loss: 39.4656\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9173 - val_loss: 40.7069\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0796 - val_loss: 40.5387\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0248 - val_loss: 39.7530\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.1649 - val_loss: 37.0181\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8030 - val_loss: 46.1582\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.7699 - val_loss: 40.9055\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6943 - val_loss: 41.3223\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0304 - val_loss: 71.8063\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8975 - val_loss: 43.9135\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.2725 - val_loss: 66.0860\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0338 - val_loss: 40.4547\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.1738 - val_loss: 47.1272\n",
            "Epoch 232/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6996 - val_loss: 51.0497\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6895 - val_loss: 42.5666\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.9543 - val_loss: 38.9834\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6513 - val_loss: 50.7209\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9620 - val_loss: 43.8780\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6398 - val_loss: 40.2011\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5869 - val_loss: 44.0420\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6191 - val_loss: 42.7340\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7495 - val_loss: 45.2092\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5253 - val_loss: 42.9168\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4175 - val_loss: 52.9800\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3195 - val_loss: 37.8093\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3806 - val_loss: 46.3201\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3114 - val_loss: 37.0949\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.2367 - val_loss: 38.7178\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5590 - val_loss: 53.0080\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 32.2835 - val_loss: 40.1397\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.4372 - val_loss: 37.2184\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5261 - val_loss: 41.2883\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0906 - val_loss: 38.4322\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.2749 - val_loss: 49.4062\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.2348 - val_loss: 40.6013\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1711 - val_loss: 39.8535\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0695 - val_loss: 36.8595\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3225 - val_loss: 38.8287\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1473 - val_loss: 39.2332\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1026 - val_loss: 47.5310\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1517 - val_loss: 59.0137\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9771 - val_loss: 42.7709\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.4450 - val_loss: 38.4805\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0367 - val_loss: 43.3378\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0223 - val_loss: 40.2364\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 32.0741 - val_loss: 40.7752\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9686 - val_loss: 38.9066\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9542 - val_loss: 37.7213\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 31.9162 - val_loss: 48.7315\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8005 - val_loss: 45.3982\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8311 - val_loss: 44.2674\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0585 - val_loss: 37.7301\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7529 - val_loss: 39.3969\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8465 - val_loss: 38.0756\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9880 - val_loss: 38.5154\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.2750 - val_loss: 47.7142\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8715 - val_loss: 38.9828\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7595 - val_loss: 62.1304\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7941 - val_loss: 37.0558\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7271 - val_loss: 40.6290\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8446 - val_loss: 38.3528\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7764 - val_loss: 44.9893\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9097 - val_loss: 68.3515\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8126 - val_loss: 37.2039\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5670 - val_loss: 44.7367\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1954 - val_loss: 48.6808\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7852 - val_loss: 37.5472\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6140 - val_loss: 84.8262\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8213 - val_loss: 43.1136\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1793 - val_loss: 41.0305\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1849 - val_loss: 44.4539\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6249 - val_loss: 43.2371\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7455 - val_loss: 39.0800\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6963 - val_loss: 46.0118\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6720 - val_loss: 40.9684\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5222 - val_loss: 44.6896\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8133 - val_loss: 74.8903\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5817 - val_loss: 39.7220\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5080 - val_loss: 54.5407\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5324 - val_loss: 39.3623\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 31.4822 - val_loss: 43.4130\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6488 - val_loss: 40.6086\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5211 - val_loss: 40.1176\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6797 - val_loss: 37.8210\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7352 - val_loss: 41.5788\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9389 - val_loss: 37.3231\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6639 - val_loss: 36.6888\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3925 - val_loss: 37.3302\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8696 - val_loss: 42.1653\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5779 - val_loss: 38.1213\n",
            "Epoch 309/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3888 - val_loss: 36.8254\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5414 - val_loss: 74.6492\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4622 - val_loss: 35.5406\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.8576 - val_loss: 46.6953\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6118 - val_loss: 53.6569\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7831 - val_loss: 41.9967\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 31.4491 - val_loss: 66.1047\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6479 - val_loss: 38.0633\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2481 - val_loss: 48.8383\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2980 - val_loss: 51.0033\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3205 - val_loss: 40.4708\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2870 - val_loss: 39.3584\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2009 - val_loss: 37.2739\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3487 - val_loss: 45.2103\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4301 - val_loss: 74.0855\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9885 - val_loss: 40.7013\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4546 - val_loss: 45.3125\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0177 - val_loss: 38.7300\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5559 - val_loss: 42.8345\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6446 - val_loss: 76.1555\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3329 - val_loss: 37.1874\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3877 - val_loss: 42.0747\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2865 - val_loss: 44.3657\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1048 - val_loss: 39.2221\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2313 - val_loss: 44.0011\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4261 - val_loss: 38.0015\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4254 - val_loss: 56.0722\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2823 - val_loss: 43.1855\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2227 - val_loss: 105.9006\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5412 - val_loss: 37.7249\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3962 - val_loss: 37.3649\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2837 - val_loss: 42.3344\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1540 - val_loss: 37.6269\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1823 - val_loss: 40.3941\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1700 - val_loss: 37.3657\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4118 - val_loss: 49.7019\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9573 - val_loss: 39.6663\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2127 - val_loss: 36.4043\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3676 - val_loss: 41.6216\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1548 - val_loss: 38.7704\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2416 - val_loss: 36.4223\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1325 - val_loss: 39.0893\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3174 - val_loss: 37.4657\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4486 - val_loss: 43.6444\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6493 - val_loss: 37.9085\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4478 - val_loss: 45.1407\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3252 - val_loss: 37.4957\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0448 - val_loss: 38.1236\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1591 - val_loss: 39.9911\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1318 - val_loss: 37.5356\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5678 - val_loss: 36.5341\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2602 - val_loss: 43.5792\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9765 - val_loss: 45.2986\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2685 - val_loss: 52.9268\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2611 - val_loss: 35.5910\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2785 - val_loss: 112.6159\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7442 - val_loss: 39.7580\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1274 - val_loss: 44.6199\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6113 - val_loss: 58.3667\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4945 - val_loss: 48.2206\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3560 - val_loss: 52.3820\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6349 - val_loss: 83.0926\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5918 - val_loss: 47.1367\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2905 - val_loss: 43.4575\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1984 - val_loss: 37.7322\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0816 - val_loss: 38.9326\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9556 - val_loss: 40.3605\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8908 - val_loss: 37.7019\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0629 - val_loss: 50.5682\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9835 - val_loss: 38.2019\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0683 - val_loss: 38.8757\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2916 - val_loss: 35.1731\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9080 - val_loss: 38.3713\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7689 - val_loss: 37.5133\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7898 - val_loss: 35.9053\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7647 - val_loss: 38.9441\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7881 - val_loss: 39.4581\n",
            "Epoch 386/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1396 - val_loss: 36.6501\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1244 - val_loss: 44.8163\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8591 - val_loss: 39.8968\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7508 - val_loss: 38.7836\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7768 - val_loss: 35.8597\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5731 - val_loss: 54.2013\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1474 - val_loss: 36.0502\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1284 - val_loss: 39.3033\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2360 - val_loss: 81.9267\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1262 - val_loss: 36.4152\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9534 - val_loss: 41.0489\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5357 - val_loss: 37.9504\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9928 - val_loss: 36.3589\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9522 - val_loss: 68.3268\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6739 - val_loss: 52.3491\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0808 - val_loss: 46.0482\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2034 - val_loss: 127.4221\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4041 - val_loss: 39.4414\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0287 - val_loss: 45.4930\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3424 - val_loss: 38.8345\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8742 - val_loss: 36.9005\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8541 - val_loss: 38.5381\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1143 - val_loss: 36.8654\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1609 - val_loss: 40.1820\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3334 - val_loss: 36.4652\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7245 - val_loss: 37.3652\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0676 - val_loss: 67.0810\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9266 - val_loss: 40.7474\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9665 - val_loss: 36.6048\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6650 - val_loss: 37.3159\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 30.63 - 2s 11ms/step - loss: 30.6323 - val_loss: 37.0765\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6510 - val_loss: 35.8935\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6616 - val_loss: 35.4974\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6855 - val_loss: 49.2579\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8327 - val_loss: 37.2487\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1117 - val_loss: 40.1986\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8754 - val_loss: 37.5803\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8732 - val_loss: 38.3144\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7033 - val_loss: 36.7589\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6810 - val_loss: 36.6651\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6092 - val_loss: 38.1690\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8471 - val_loss: 41.8368\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9340 - val_loss: 36.1660\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8013 - val_loss: 50.6278\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8772 - val_loss: 36.5459\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6987 - val_loss: 43.1581\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9106 - val_loss: 37.7715\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7301 - val_loss: 51.9649\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7600 - val_loss: 63.1909\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5714 - val_loss: 37.0860\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5751 - val_loss: 56.2126\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6912 - val_loss: 39.6125\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7044 - val_loss: 44.8119\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.5896 - val_loss: 35.2935\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0001 - val_loss: 65.3695\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8263 - val_loss: 36.5721\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7186 - val_loss: 36.0611\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5901 - val_loss: 40.5014\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6772 - val_loss: 43.9626\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7484 - val_loss: 41.1314\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5491 - val_loss: 39.8390\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6593 - val_loss: 52.1905\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6600 - val_loss: 38.5067\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6508 - val_loss: 37.3894\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5059 - val_loss: 41.2500\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5799 - val_loss: 36.3760\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8474 - val_loss: 40.7059\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9991 - val_loss: 77.3278\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8715 - val_loss: 37.6631\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5219 - val_loss: 36.3009\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6257 - val_loss: 45.8190\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4525 - val_loss: 37.2715\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8007 - val_loss: 62.3607\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8124 - val_loss: 35.5664\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5357 - val_loss: 54.0174\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4701 - val_loss: 36.6908\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4335 - val_loss: 49.5913\n",
            "Epoch 463/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5927 - val_loss: 36.2423\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3744 - val_loss: 37.8354\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3000 - val_loss: 38.7502\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6234 - val_loss: 34.6118\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8171 - val_loss: 99.6728\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0774 - val_loss: 56.0663\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4870 - val_loss: 37.5067\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3251 - val_loss: 36.2355\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2431 - val_loss: 35.8140\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3986 - val_loss: 38.6367\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5595 - val_loss: 80.3484\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4494 - val_loss: 36.2594\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3536 - val_loss: 42.0067\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3181 - val_loss: 39.0882\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3383 - val_loss: 34.5214\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2106 - val_loss: 35.6820\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2524 - val_loss: 36.0696\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3192 - val_loss: 47.5625\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3722 - val_loss: 36.3360\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.2666 - val_loss: 37.4649\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6047 - val_loss: 38.0717\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4926 - val_loss: 40.2256\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 30.41 - 2s 11ms/step - loss: 30.4108 - val_loss: 36.7173\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5946 - val_loss: 36.9282\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3457 - val_loss: 39.1785\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7638 - val_loss: 46.7364\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5869 - val_loss: 36.2522\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4039 - val_loss: 37.2065\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5496 - val_loss: 48.2107\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9504 - val_loss: 39.6314\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4980 - val_loss: 35.0979\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3948 - val_loss: 38.7764\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6644 - val_loss: 48.4359\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2059 - val_loss: 37.1695\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4286 - val_loss: 37.7315\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4363 - val_loss: 35.0981\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.3279 - val_loss: 37.6984\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.4962 - val_loss: 36.4955\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -0.01323045140450665 \n",
            "MAE:  4.428357312605733 \n",
            "SD:  6.041138602423505\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cip38xZOZnV",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 500])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_154 (Dense)            (None, 8)                 1024      \n",
            "_________________________________________________________________\n",
            "batch_normalization_147 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_147 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_155 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_148 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_148 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_156 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_149 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_149 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_157 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_150 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_150 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_158 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_151 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_151 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_159 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_152 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_152 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_160 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_153 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_153 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_161 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_154 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_154 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_162 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_155 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_155 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_163 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_156 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_156 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_164 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_157 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_157 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_165 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_158 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_158 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_166 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_159 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_159 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_167 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_160 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_160 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_168 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_161 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_161 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_169 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_162 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_162 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_170 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_163 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_163 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_171 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_164 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_164 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_172 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_165 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_165 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_173 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_166 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_166 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_174 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_167 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_167 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_175 (Dense)            (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 3,145\n",
            "Trainable params: 2,809\n",
            "Non-trainable params: 336\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 3696.0144 - val_loss: 3706.5476\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3490.7959 - val_loss: 3467.6990\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3257.2048 - val_loss: 3349.0850\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2956.4221 - val_loss: 2627.9431\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2596.8679 - val_loss: 2386.6282\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 2171.4954 - val_loss: 1987.0756\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1762.1978 - val_loss: 1435.5187\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1378.7898 - val_loss: 1118.4993\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 1042.3131 - val_loss: 903.6013\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 767.7731 - val_loss: 494.3893\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 542.3671 - val_loss: 406.6146\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 377.4712 - val_loss: 306.2700\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 260.8733 - val_loss: 183.3557\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 190.1404 - val_loss: 153.9728\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 146.7290 - val_loss: 139.3687\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 123.0221 - val_loss: 111.8433\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 110.4922 - val_loss: 112.3845\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 104.6238 - val_loss: 103.0917\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 101.3327 - val_loss: 103.3553\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.6163 - val_loss: 98.4924\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.2383 - val_loss: 92.1604\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.4435 - val_loss: 103.9821\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.1634 - val_loss: 108.4048\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 97.6251 - val_loss: 113.6463\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 95.9938 - val_loss: 97.5151\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 96.7350 - val_loss: 95.3627\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 94.7186 - val_loss: 91.7044\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 93.1591 - val_loss: 178.7733\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 91.6403 - val_loss: 119.4988\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.9787 - val_loss: 181.7577\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 89.6949 - val_loss: 106.2187\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 86.2726 - val_loss: 113.3279\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 81.7052 - val_loss: 86.2538\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.2452 - val_loss: 77.7250\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 77.4408 - val_loss: 89.5038\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 74.5450 - val_loss: 192.3355\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 70.6324 - val_loss: 72.9443\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 68.5311 - val_loss: 66.7647\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 67.2121 - val_loss: 83.2416\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 64.8656 - val_loss: 83.2916\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 64.7143 - val_loss: 90.0716\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 64.8915 - val_loss: 141.4942\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 65.1886 - val_loss: 95.3859\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 66.3190 - val_loss: 388.3037\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 71.2065 - val_loss: 104.6319\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 67.7109 - val_loss: 77.4331\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 64.0945 - val_loss: 165.5497\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 63.6186 - val_loss: 79.7695\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 61.4494 - val_loss: 69.7753\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 58.9313 - val_loss: 72.4836\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 57.3461 - val_loss: 66.7189\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 55.9382 - val_loss: 61.6650\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 55.4588 - val_loss: 61.2267\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 55.1134 - val_loss: 76.1332\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 55.3921 - val_loss: 63.4509\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 53.4142 - val_loss: 58.1509\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 52.57 - 2s 11ms/step - loss: 52.5679 - val_loss: 54.3832\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 52.2796 - val_loss: 66.8394\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 51.6749 - val_loss: 91.6560\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 50.8354 - val_loss: 67.0294\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 50.2262 - val_loss: 56.5770\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 49.7452 - val_loss: 126.9996\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 49.9033 - val_loss: 70.2828\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 49.9687 - val_loss: 132.0496\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 49.8535 - val_loss: 60.3114\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 48.7665 - val_loss: 110.0530\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 48.6912 - val_loss: 57.1796\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 52.2894 - val_loss: 70.3144\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 49.9408 - val_loss: 186.1694\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 46.5981 - val_loss: 48.8919\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 46.0170 - val_loss: 54.3051\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.0405 - val_loss: 55.7458\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 44.4511 - val_loss: 67.7021\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 43.6371 - val_loss: 58.6358\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 43.7820 - val_loss: 82.8440\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.1729 - val_loss: 47.0944\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 43.6430 - val_loss: 70.6305\n",
            "Epoch 78/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 42.5718 - val_loss: 52.3350\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.6371 - val_loss: 46.5124\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.9968 - val_loss: 44.4413\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.4037 - val_loss: 85.3105\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.5490 - val_loss: 71.8849\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.4057 - val_loss: 54.3738\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.2719 - val_loss: 60.9872\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.6297 - val_loss: 51.5140\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.5897 - val_loss: 69.8298\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.7018 - val_loss: 50.2325\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.5038 - val_loss: 47.4167\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.5641 - val_loss: 52.6605\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.4101 - val_loss: 54.6727\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.7334 - val_loss: 107.0437\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 43.2751 - val_loss: 74.5966\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.5334 - val_loss: 47.5483\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.8211 - val_loss: 73.9349\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 41.4260 - val_loss: 84.3490\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.4077 - val_loss: 61.5561\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.9465 - val_loss: 58.2975\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.1697 - val_loss: 61.1101\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.5079 - val_loss: 123.2868\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.4097 - val_loss: 62.4219\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.8527 - val_loss: 44.2658\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.8016 - val_loss: 72.1149\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.3610 - val_loss: 59.0636\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.5674 - val_loss: 85.1562\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.4295 - val_loss: 49.7350\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 39.3919 - val_loss: 84.5300\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.4436 - val_loss: 104.3087\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.3760 - val_loss: 57.4768\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.8284 - val_loss: 73.4712\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.4397 - val_loss: 52.0440\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.7175 - val_loss: 50.4280\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.4178 - val_loss: 43.9511\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.6101 - val_loss: 41.0083\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.1071 - val_loss: 49.6143\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.1954 - val_loss: 56.5558\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.3313 - val_loss: 52.4358\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.0599 - val_loss: 150.9225\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.1690 - val_loss: 46.7028\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.9732 - val_loss: 46.2148\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.6006 - val_loss: 59.7992\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.1746 - val_loss: 69.1785\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.5821 - val_loss: 55.0747\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.4872 - val_loss: 47.9204\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.0319 - val_loss: 49.5939\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.4434 - val_loss: 39.7862\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.2314 - val_loss: 40.4696\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.9340 - val_loss: 45.8706\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.7917 - val_loss: 41.7381\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.6624 - val_loss: 40.0511\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.6635 - val_loss: 45.3283\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.5153 - val_loss: 50.2712\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.3004 - val_loss: 43.3116\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.2187 - val_loss: 48.1846\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.5450 - val_loss: 49.5666\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.2662 - val_loss: 38.2929\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.0912 - val_loss: 40.2149\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.2661 - val_loss: 54.4909\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.0000 - val_loss: 39.9906\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.7889 - val_loss: 77.1349\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.1828 - val_loss: 55.0208\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.7050 - val_loss: 64.1815\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.6293 - val_loss: 49.0949\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.8395 - val_loss: 51.1596\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.8663 - val_loss: 59.6964\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.7060 - val_loss: 55.0297\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.3836 - val_loss: 43.4398\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.0790 - val_loss: 59.6987\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.1405 - val_loss: 45.5470\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.6456 - val_loss: 71.6366\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.2833 - val_loss: 49.6081\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.1186 - val_loss: 60.0316\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.8345 - val_loss: 48.4780\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.7668 - val_loss: 40.1845\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.7791 - val_loss: 38.1546\n",
            "Epoch 155/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 34.7855 - val_loss: 89.4902\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.7535 - val_loss: 46.0144\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.4280 - val_loss: 42.3557\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.6028 - val_loss: 57.0219\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.2689 - val_loss: 63.2532\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.8586 - val_loss: 59.5308\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.7063 - val_loss: 42.4830\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.8058 - val_loss: 40.3623\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.3302 - val_loss: 38.6040\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.4992 - val_loss: 45.0680\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.6517 - val_loss: 41.4900\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.5484 - val_loss: 48.2650\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.0001 - val_loss: 42.2894\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 35.2296 - val_loss: 43.6491\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.0864 - val_loss: 41.8256\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.3399 - val_loss: 37.4156\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9886 - val_loss: 40.4288\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.7716 - val_loss: 39.1015\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.7520 - val_loss: 41.1807\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.7981 - val_loss: 46.6414\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.7008 - val_loss: 41.9534\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9678 - val_loss: 42.6349\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.2783 - val_loss: 39.6104\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 34.1959 - val_loss: 45.3527\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.8443 - val_loss: 69.2564\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.4726 - val_loss: 68.9708\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.9411 - val_loss: 44.7961\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.9790 - val_loss: 57.2596\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9370 - val_loss: 39.5002\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.8897 - val_loss: 39.7693\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.2640 - val_loss: 42.9891\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9505 - val_loss: 45.7332\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 34.3413 - val_loss: 40.2643\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 33.7001 - val_loss: 48.6217\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 33.3038 - val_loss: 36.0038\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.5269 - val_loss: 49.8857\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.3895 - val_loss: 41.5145\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.4006 - val_loss: 49.1071\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.5382 - val_loss: 39.1690\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 33.2871 - val_loss: 43.7445\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 33.7113 - val_loss: 58.6680\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 33.5675 - val_loss: 42.5343\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 33.6606 - val_loss: 41.5647\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9089 - val_loss: 40.8535\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.5467 - val_loss: 103.0267\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9918 - val_loss: 57.5541\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 33.2235 - val_loss: 40.6150\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 33.3289 - val_loss: 42.1079\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 33.3582 - val_loss: 79.7651\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 33.7314 - val_loss: 56.1045\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 33.2304 - val_loss: 36.9566\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.1114 - val_loss: 54.4903\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.3826 - val_loss: 52.9248\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.1945 - val_loss: 53.3165\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9971 - val_loss: 38.5826\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.3705 - val_loss: 43.2702\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0529 - val_loss: 40.2395\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9295 - val_loss: 39.5125\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.7785 - val_loss: 40.7577\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8192 - val_loss: 44.3631\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8328 - val_loss: 40.8683\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0262 - val_loss: 43.4992\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9006 - val_loss: 42.0981\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7412 - val_loss: 45.9814\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7156 - val_loss: 36.6383\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7342 - val_loss: 42.0301\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8054 - val_loss: 67.0974\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.9555 - val_loss: 41.1939\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0493 - val_loss: 41.1089\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7712 - val_loss: 39.8744\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5108 - val_loss: 41.5280\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5716 - val_loss: 37.0232\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4430 - val_loss: 38.3318\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8581 - val_loss: 45.9899\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6296 - val_loss: 40.4555\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5460 - val_loss: 38.1851\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3903 - val_loss: 79.5982\n",
            "Epoch 232/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5573 - val_loss: 39.3302\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3566 - val_loss: 47.3690\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.2191 - val_loss: 36.6211\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3814 - val_loss: 37.8631\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3993 - val_loss: 49.1043\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3635 - val_loss: 36.3011\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3412 - val_loss: 37.3107\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1751 - val_loss: 47.0534\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.4179 - val_loss: 42.3126\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.2222 - val_loss: 38.2679\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1668 - val_loss: 43.4099\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5593 - val_loss: 48.7274\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0820 - val_loss: 38.8110\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.2543 - val_loss: 37.8603\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6113 - val_loss: 39.6435\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6103 - val_loss: 38.9133\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3309 - val_loss: 57.0133\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.1388 - val_loss: 41.0740\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0196 - val_loss: 36.2575\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0509 - val_loss: 40.4642\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9782 - val_loss: 40.5329\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9196 - val_loss: 43.8657\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.3720 - val_loss: 38.8600\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0535 - val_loss: 46.1698\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2331 - val_loss: 37.1042\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8593 - val_loss: 37.1970\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9815 - val_loss: 39.5017\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9345 - val_loss: 39.7605\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8725 - val_loss: 36.0373\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.9096 - val_loss: 43.6080\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0053 - val_loss: 47.5706\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8172 - val_loss: 37.1699\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8081 - val_loss: 46.1418\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1545 - val_loss: 42.0123\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9128 - val_loss: 41.1384\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6777 - val_loss: 41.4977\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7534 - val_loss: 36.7741\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3161 - val_loss: 42.9394\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.9496 - val_loss: 40.7679\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8215 - val_loss: 42.8075\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6866 - val_loss: 38.1918\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5595 - val_loss: 36.9160\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7213 - val_loss: 51.0425\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4322 - val_loss: 41.9613\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6390 - val_loss: 40.2960\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5063 - val_loss: 37.4449\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5464 - val_loss: 37.3710\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9407 - val_loss: 37.0442\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7021 - val_loss: 36.9004\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4750 - val_loss: 39.6301\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5394 - val_loss: 38.5340\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8746 - val_loss: 36.4147\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6732 - val_loss: 35.2796\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3996 - val_loss: 35.2931\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2836 - val_loss: 34.6996\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4730 - val_loss: 44.3994\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2169 - val_loss: 51.0071\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8039 - val_loss: 42.4990\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5713 - val_loss: 52.2563\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4236 - val_loss: 59.0580\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3475 - val_loss: 39.3335\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5479 - val_loss: 36.0576\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2404 - val_loss: 36.8603\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6259 - val_loss: 38.2278\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3924 - val_loss: 39.9199\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3845 - val_loss: 37.7948\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4729 - val_loss: 51.8085\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4500 - val_loss: 57.0617\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5950 - val_loss: 52.7891\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7139 - val_loss: 38.4741\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4690 - val_loss: 64.7122\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6643 - val_loss: 92.0501\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5212 - val_loss: 46.5042\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1636 - val_loss: 44.4816\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2509 - val_loss: 38.3365\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3977 - val_loss: 38.5554\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1714 - val_loss: 38.0361\n",
            "Epoch 309/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4423 - val_loss: 41.4791\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1612 - val_loss: 37.9869\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4752 - val_loss: 54.0223\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4124 - val_loss: 40.0069\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2771 - val_loss: 38.9748\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2305 - val_loss: 39.0857\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1940 - val_loss: 40.4923\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8630 - val_loss: 36.7708\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2782 - val_loss: 42.6770\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2756 - val_loss: 35.6065\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1953 - val_loss: 38.1603\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4506 - val_loss: 43.7245\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6465 - val_loss: 40.0766\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2330 - val_loss: 36.9038\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0272 - val_loss: 37.7696\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4382 - val_loss: 46.4279\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5274 - val_loss: 38.1791\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0730 - val_loss: 46.8880\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4964 - val_loss: 55.9858\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3516 - val_loss: 48.8988\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2547 - val_loss: 37.9133\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1191 - val_loss: 39.0613\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1981 - val_loss: 36.8378\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0059 - val_loss: 37.7400\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9872 - val_loss: 40.3085\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3370 - val_loss: 40.6119\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2959 - val_loss: 69.1882\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5693 - val_loss: 52.3774\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2140 - val_loss: 50.5925\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4858 - val_loss: 44.2834\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7436 - val_loss: 44.7435\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3584 - val_loss: 47.2264\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1663 - val_loss: 38.4211\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0669 - val_loss: 36.4559\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1009 - val_loss: 46.2328\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0601 - val_loss: 43.7562\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2925 - val_loss: 88.3652\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2715 - val_loss: 36.2295\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2320 - val_loss: 37.2544\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6995 - val_loss: 42.4855\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1898 - val_loss: 35.8846\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4657 - val_loss: 45.3842\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2859 - val_loss: 38.7667\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1519 - val_loss: 36.9067\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2238 - val_loss: 38.7704\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1664 - val_loss: 40.5474\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4888 - val_loss: 57.1700\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0199 - val_loss: 39.9268\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3213 - val_loss: 37.6117\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1490 - val_loss: 37.6612\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5057 - val_loss: 39.0145\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1638 - val_loss: 40.6087\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6676 - val_loss: 150.1409\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3617 - val_loss: 43.6772\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3637 - val_loss: 40.7726\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3644 - val_loss: 59.6512\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3919 - val_loss: 58.1155\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3715 - val_loss: 42.2098\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2116 - val_loss: 43.4178\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3586 - val_loss: 37.0100\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3851 - val_loss: 47.9099\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3286 - val_loss: 58.6751\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3877 - val_loss: 40.6497\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1539 - val_loss: 37.9748\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9976 - val_loss: 37.2052\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0854 - val_loss: 37.1388\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1455 - val_loss: 38.2567\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0022 - val_loss: 35.7921\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7863 - val_loss: 39.3889\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9888 - val_loss: 41.5260\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0958 - val_loss: 38.9889\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0950 - val_loss: 36.1753\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9950 - val_loss: 54.8357\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0789 - val_loss: 37.0949\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.4276 - val_loss: 109.9316\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2010 - val_loss: 36.6636\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1173 - val_loss: 68.6881\n",
            "Epoch 386/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9752 - val_loss: 34.8707\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8564 - val_loss: 62.3951\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0125 - val_loss: 35.3192\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0337 - val_loss: 38.1903\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1440 - val_loss: 92.7074\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9795 - val_loss: 35.9497\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1017 - val_loss: 40.1446\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9772 - val_loss: 38.1393\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8921 - val_loss: 37.5123\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0791 - val_loss: 80.1596\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8633 - val_loss: 34.6499\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0702 - val_loss: 44.3623\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2465 - val_loss: 36.3334\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1530 - val_loss: 66.2481\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8010 - val_loss: 36.6798\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1285 - val_loss: 35.8318\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7763 - val_loss: 36.9680\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1433 - val_loss: 38.2336\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8465 - val_loss: 36.2027\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7880 - val_loss: 39.1657\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9681 - val_loss: 43.4538\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8499 - val_loss: 36.5705\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7789 - val_loss: 53.7589\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0605 - val_loss: 35.4911\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.5742 - val_loss: 35.5540\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8278 - val_loss: 35.7053\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8162 - val_loss: 43.5761\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8228 - val_loss: 39.3749\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0444 - val_loss: 203.0518\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1093 - val_loss: 51.5429\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8773 - val_loss: 37.1195\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7142 - val_loss: 38.6298\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8275 - val_loss: 40.9297\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0148 - val_loss: 36.7648\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0804 - val_loss: 36.9216\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8867 - val_loss: 46.0911\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8368 - val_loss: 36.8599\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2616 - val_loss: 51.9596\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9731 - val_loss: 50.6504\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7394 - val_loss: 39.9973\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1159 - val_loss: 39.5034\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0770 - val_loss: 43.0129\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8984 - val_loss: 48.5756\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0938 - val_loss: 36.0266\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6046 - val_loss: 65.2113\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7107 - val_loss: 63.3232\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6478 - val_loss: 36.6892\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.8778 - val_loss: 35.0088\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9309 - val_loss: 36.1289\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8024 - val_loss: 42.4193\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.6123 - val_loss: 38.4899\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.5140 - val_loss: 35.5718\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7499 - val_loss: 36.8571\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7966 - val_loss: 35.1603\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.6551 - val_loss: 39.7433\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1572 - val_loss: 70.6305\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7275 - val_loss: 36.8214\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7541 - val_loss: 37.3682\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7564 - val_loss: 37.5076\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8508 - val_loss: 39.2442\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8935 - val_loss: 37.8457\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1429 - val_loss: 43.7522\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0400 - val_loss: 41.0380\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8771 - val_loss: 40.7550\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9134 - val_loss: 36.0053\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0538 - val_loss: 41.7081\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2454 - val_loss: 35.9955\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0210 - val_loss: 68.8044\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1512 - val_loss: 46.9979\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1396 - val_loss: 35.6113\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2786 - val_loss: 52.4382\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0490 - val_loss: 39.2104\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.5316 - val_loss: 34.9163\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7318 - val_loss: 41.7108\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.7557 - val_loss: 37.9449\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7961 - val_loss: 36.9337\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4106 - val_loss: 38.5836\n",
            "Epoch 463/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8109 - val_loss: 80.9520\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.6922 - val_loss: 40.1521\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.4325 - val_loss: 35.9696\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8313 - val_loss: 57.2137\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6774 - val_loss: 35.0777\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8840 - val_loss: 36.2290\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.5525 - val_loss: 35.2815\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.5674 - val_loss: 39.0113\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3043 - val_loss: 40.7912\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9829 - val_loss: 37.1950\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9487 - val_loss: 34.9087\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.6922 - val_loss: 36.7294\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.5312 - val_loss: 39.9638\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7786 - val_loss: 36.3140\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8085 - val_loss: 41.9947\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.6598 - val_loss: 36.7744\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9100 - val_loss: 44.4334\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.0951 - val_loss: 50.9464\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.1328 - val_loss: 49.4806\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9852 - val_loss: 38.7176\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8429 - val_loss: 35.4253\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0097 - val_loss: 37.0813\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2596 - val_loss: 35.8059\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8868 - val_loss: 35.4457\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 30.9950 - val_loss: 61.4414\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7470 - val_loss: 52.3873\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9299 - val_loss: 41.2876\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7775 - val_loss: 46.5178\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0812 - val_loss: 48.6602\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3825 - val_loss: 45.1455\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.1044 - val_loss: 57.5378\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9778 - val_loss: 43.0370\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.6913 - val_loss: 34.7035\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.7343 - val_loss: 36.3284\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1380 - val_loss: 47.2308\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5005 - val_loss: 52.0825\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1738 - val_loss: 37.1735\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9552 - val_loss: 35.1851\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -0.5690157472269776 \n",
            "MAE:  4.396892628270079 \n",
            "SD:  5.904346263680352\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 500])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_176 (Dense)            (None, 8)                 1024      \n",
            "_________________________________________________________________\n",
            "batch_normalization_168 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_168 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_177 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_169 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_169 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_178 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_170 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_170 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_179 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_171 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_171 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_180 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_172 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_172 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_181 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_173 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_173 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_182 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_174 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_174 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_183 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_175 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_175 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_184 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_176 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_176 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_185 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_177 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_177 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_186 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_178 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_178 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_187 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_179 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_179 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_188 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_180 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_180 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_189 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_181 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_181 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_190 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_182 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_182 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_191 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_183 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_183 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_192 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_184 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_184 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_193 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_185 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_185 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_194 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_186 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_186 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_195 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_187 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_187 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_196 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_188 (Bat (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_188 (Activation)  (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_197 (Dense)            (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 3,145\n",
            "Trainable params: 2,809\n",
            "Non-trainable params: 336\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 3677.6626 - val_loss: 3679.7021\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 3471.3064 - val_loss: 3349.3345\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 3233.3701 - val_loss: 3048.6792\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2940.5764 - val_loss: 3068.2471\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2588.1116 - val_loss: 2324.6509\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2178.0339 - val_loss: 1889.6835\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1769.0425 - val_loss: 1552.6787\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1382.4397 - val_loss: 1120.4761\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1038.8549 - val_loss: 882.8595\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 754.9399 - val_loss: 574.7086\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 528.2219 - val_loss: 327.1589\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 364.4371 - val_loss: 282.9655\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 252.4355 - val_loss: 193.8202\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 180.2859 - val_loss: 121.6822\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 141.3429 - val_loss: 120.7454\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 122.8419 - val_loss: 117.1953\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.5521 - val_loss: 124.3086\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 111.6230 - val_loss: 108.9936\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.7534 - val_loss: 106.1743\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 109.0266 - val_loss: 115.9136\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.6998 - val_loss: 106.2588\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 108.2260 - val_loss: 110.2727\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 107.9166 - val_loss: 106.1502\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.1572 - val_loss: 104.8942\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.8659 - val_loss: 105.6653\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.4719 - val_loss: 102.5803\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.6630 - val_loss: 105.1759\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.6771 - val_loss: 107.8320\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.6645 - val_loss: 105.4542\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.0617 - val_loss: 103.6243\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.0892 - val_loss: 102.3908\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.2030 - val_loss: 103.7493\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 103.4718 - val_loss: 101.9175\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.5031 - val_loss: 102.0545\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.8740 - val_loss: 99.1558\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.1525 - val_loss: 98.0576\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 98.7039 - val_loss: 106.2709\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 97.4455 - val_loss: 96.3223\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.4924 - val_loss: 132.9538\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.6551 - val_loss: 207.2991\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.6762 - val_loss: 101.0829\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.5765 - val_loss: 96.6104\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.2667 - val_loss: 100.0918\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.8010 - val_loss: 188.6451\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.4602 - val_loss: 93.8983\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.7997 - val_loss: 238.5956\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 79.3161 - val_loss: 86.1635\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.2838 - val_loss: 81.5659\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8135 - val_loss: 79.0984\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.9432 - val_loss: 75.3200\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 69.5244 - val_loss: 80.8596\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 66.9077 - val_loss: 79.3972\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 63.9823 - val_loss: 80.0234\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 62.7289 - val_loss: 295.6631\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 61.9951 - val_loss: 90.0855\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 60.3626 - val_loss: 187.0859\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 58.5484 - val_loss: 70.7620\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 60.6841 - val_loss: 136.4996\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 56.7732 - val_loss: 63.6884\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.7825 - val_loss: 78.0387\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.5633 - val_loss: 84.3010\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 53.7934 - val_loss: 56.4201\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 54.9559 - val_loss: 61.5540\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 52.2818 - val_loss: 100.8936\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 50.4880 - val_loss: 61.2692\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 49.5940 - val_loss: 53.1942\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 47.7311 - val_loss: 68.2766\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 47.3413 - val_loss: 62.1957\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 46.9721 - val_loss: 55.3780\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.9384 - val_loss: 49.9801\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.9222 - val_loss: 105.6547\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 45.0604 - val_loss: 55.5473\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 44.3842 - val_loss: 119.7113\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 43.1708 - val_loss: 138.4566\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 43.3073 - val_loss: 50.3482\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 43.3511 - val_loss: 77.6846\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 42.0140 - val_loss: 45.2890\n",
            "Epoch 78/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 41.5936 - val_loss: 49.3235\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.8129 - val_loss: 77.4541\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.6791 - val_loss: 45.2619\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.7032 - val_loss: 65.6068\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.4113 - val_loss: 53.1303\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.2274 - val_loss: 63.8923\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.5636 - val_loss: 55.8534\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 39.3119 - val_loss: 46.1907\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 38.8100 - val_loss: 44.8696\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.4338 - val_loss: 108.8573\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 39.3722 - val_loss: 84.8952\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.8252 - val_loss: 127.3808\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.4412 - val_loss: 55.7464\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 40.2276 - val_loss: 53.1490\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.2066 - val_loss: 62.1959\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.8431 - val_loss: 59.4544\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.5481 - val_loss: 48.4467\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.9303 - val_loss: 44.5034\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.6357 - val_loss: 41.3072\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.8211 - val_loss: 42.7118\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.9438 - val_loss: 51.5272\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.6596 - val_loss: 52.2607\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.5283 - val_loss: 42.4206\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.2072 - val_loss: 44.3011\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 38.0284 - val_loss: 76.1398\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.1896 - val_loss: 74.3894\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.0298 - val_loss: 45.2210\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.8363 - val_loss: 60.4847\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.6471 - val_loss: 131.0852\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 37.1025 - val_loss: 53.7755\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.7957 - val_loss: 50.5699\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.5822 - val_loss: 60.5638\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.8777 - val_loss: 130.9467\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.4738 - val_loss: 58.4069\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.3871 - val_loss: 41.3627\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.9853 - val_loss: 74.2744\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.9877 - val_loss: 48.5624\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.9234 - val_loss: 65.0162\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.8389 - val_loss: 40.2247\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.3381 - val_loss: 73.6732\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.9750 - val_loss: 44.4917\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.9814 - val_loss: 45.5250\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.9241 - val_loss: 41.2228\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.8962 - val_loss: 38.8526\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.7274 - val_loss: 65.3867\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 36.0598 - val_loss: 40.4523\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.5258 - val_loss: 50.2614\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.4007 - val_loss: 39.6194\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.4809 - val_loss: 42.9790\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.7751 - val_loss: 56.1625\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.8679 - val_loss: 43.1191\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.3989 - val_loss: 56.3143\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.5816 - val_loss: 47.5667\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.1449 - val_loss: 64.6214\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.6308 - val_loss: 63.5378\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.3861 - val_loss: 48.0094\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.0521 - val_loss: 41.0791\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.8920 - val_loss: 43.2820\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 34.9807 - val_loss: 78.7644\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 34.9994 - val_loss: 41.5761\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 34.6311 - val_loss: 45.4660\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.6955 - val_loss: 51.8914\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.8645 - val_loss: 40.7581\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.6136 - val_loss: 47.3335\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.8236 - val_loss: 74.7202\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.8773 - val_loss: 47.4889\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.5826 - val_loss: 39.2397\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.2691 - val_loss: 61.3980\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.8638 - val_loss: 75.5110\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.5546 - val_loss: 66.8542\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 35.1459 - val_loss: 37.3441\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.6789 - val_loss: 42.1197\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.2881 - val_loss: 48.7096\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.6440 - val_loss: 48.9888\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.7153 - val_loss: 52.3970\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.4789 - val_loss: 48.7524\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.1516 - val_loss: 43.0900\n",
            "Epoch 155/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 34.1329 - val_loss: 39.5983\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.4202 - val_loss: 53.9626\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.8227 - val_loss: 71.7672\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 34.5037 - val_loss: 43.6280\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.3408 - val_loss: 49.6622\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.3914 - val_loss: 63.9953\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.1681 - val_loss: 55.1741\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.2147 - val_loss: 74.6536\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.1608 - val_loss: 38.2767\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.3168 - val_loss: 48.4681\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.8923 - val_loss: 45.9478\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.8862 - val_loss: 42.2796\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.0364 - val_loss: 44.8001\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.1739 - val_loss: 46.1922\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.1866 - val_loss: 42.5641\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.0441 - val_loss: 52.9576\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.0187 - val_loss: 39.6044\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.2775 - val_loss: 46.9506\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5050 - val_loss: 44.1454\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9878 - val_loss: 82.8207\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.1595 - val_loss: 61.1270\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.8106 - val_loss: 43.1959\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.8188 - val_loss: 76.9773\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.7506 - val_loss: 62.3395\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 34.3579 - val_loss: 45.4410\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9852 - val_loss: 41.6419\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.0060 - val_loss: 66.3148\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.8206 - val_loss: 45.7925\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.8832 - val_loss: 50.6103\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.6142 - val_loss: 46.8135\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.5433 - val_loss: 36.6010\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5106 - val_loss: 41.7046\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.9145 - val_loss: 46.6880\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.7961 - val_loss: 55.5018\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.5200 - val_loss: 37.0726\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5322 - val_loss: 62.9561\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.9534 - val_loss: 45.3507\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.6587 - val_loss: 59.7343\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.6114 - val_loss: 60.8077\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.8459 - val_loss: 49.0310\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.3758 - val_loss: 39.4065\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.8515 - val_loss: 48.6322\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.6214 - val_loss: 58.1178\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.3640 - val_loss: 42.9159\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.2946 - val_loss: 46.3608\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.4638 - val_loss: 41.4939\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5798 - val_loss: 45.4756\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.8229 - val_loss: 48.9143\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.1392 - val_loss: 54.3613\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.2465 - val_loss: 91.8979\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5311 - val_loss: 41.9206\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0575 - val_loss: 42.0218\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8633 - val_loss: 118.4672\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0134 - val_loss: 48.8780\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.3476 - val_loss: 37.5241\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0979 - val_loss: 41.4482\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.2427 - val_loss: 50.9688\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.2445 - val_loss: 47.3088\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5318 - val_loss: 47.2500\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.1567 - val_loss: 42.6352\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8392 - val_loss: 38.8419\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.1734 - val_loss: 56.0016\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0980 - val_loss: 90.1390\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.6177 - val_loss: 49.7297\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.6321 - val_loss: 38.0640\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.9488 - val_loss: 40.6070\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0492 - val_loss: 64.9321\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.3462 - val_loss: 53.4867\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.9594 - val_loss: 44.3210\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.3014 - val_loss: 42.9453\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.1737 - val_loss: 59.6752\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5772 - val_loss: 77.4426\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.4887 - val_loss: 46.5720\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0965 - val_loss: 37.6361\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.9967 - val_loss: 75.4288\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.1105 - val_loss: 62.3734\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0661 - val_loss: 42.4097\n",
            "Epoch 232/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0175 - val_loss: 49.1837\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.1365 - val_loss: 56.1418\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.5357 - val_loss: 77.0648\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0201 - val_loss: 37.0638\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0414 - val_loss: 79.6450\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8854 - val_loss: 39.4066\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.9894 - val_loss: 47.7427\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8781 - val_loss: 47.6950\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.9086 - val_loss: 46.8740\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8267 - val_loss: 43.6918\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.3579 - val_loss: 38.0430\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8634 - val_loss: 40.4210\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7350 - val_loss: 38.8153\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7227 - val_loss: 54.1675\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5580 - val_loss: 46.9784\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7169 - val_loss: 38.3568\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4789 - val_loss: 43.1060\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5339 - val_loss: 37.4735\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8987 - val_loss: 39.5286\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.0382 - val_loss: 39.4460\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6118 - val_loss: 42.6041\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5243 - val_loss: 39.0472\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4980 - val_loss: 54.8866\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5194 - val_loss: 41.0992\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6878 - val_loss: 63.8398\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6155 - val_loss: 39.5632\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7104 - val_loss: 43.4063\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7939 - val_loss: 38.5779\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5992 - val_loss: 49.2056\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8603 - val_loss: 40.7939\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6567 - val_loss: 68.5114\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0460 - val_loss: 74.9067\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.6390 - val_loss: 86.4989\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8463 - val_loss: 39.6798\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 33.1500 - val_loss: 47.7880\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4788 - val_loss: 39.4712\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6950 - val_loss: 47.8499\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5623 - val_loss: 42.1048\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5436 - val_loss: 37.2505\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8102 - val_loss: 44.9293\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.9053 - val_loss: 48.3556\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.4449 - val_loss: 36.2641\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6130 - val_loss: 37.0289\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.8160 - val_loss: 40.3353\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3673 - val_loss: 36.7979\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.9315 - val_loss: 37.7326\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.5270 - val_loss: 82.5723\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0426 - val_loss: 44.1405\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4626 - val_loss: 36.9106\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3749 - val_loss: 49.0500\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0705 - val_loss: 107.2790\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8641 - val_loss: 49.6921\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2999 - val_loss: 36.3942\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3206 - val_loss: 35.8637\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3514 - val_loss: 36.6702\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3093 - val_loss: 37.5745\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0712 - val_loss: 36.0930\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2534 - val_loss: 42.6999\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1626 - val_loss: 87.1387\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6163 - val_loss: 55.7952\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.5585 - val_loss: 50.1039\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0373 - val_loss: 46.3417\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7253 - val_loss: 45.5338\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.0047 - val_loss: 40.5904\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.4605 - val_loss: 37.7332\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3065 - val_loss: 38.6018\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.7407 - val_loss: 36.6794\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6889 - val_loss: 37.8236\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8392 - val_loss: 40.5942\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6316 - val_loss: 38.5416\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8232 - val_loss: 80.4084\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4325 - val_loss: 59.0646\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5484 - val_loss: 61.0604\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1523 - val_loss: 38.0672\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3743 - val_loss: 42.6369\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7825 - val_loss: 56.7438\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3949 - val_loss: 37.6422\n",
            "Epoch 309/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2975 - val_loss: 41.6738\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0206 - val_loss: 38.3854\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1272 - val_loss: 35.8232\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8953 - val_loss: 37.1923\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2006 - val_loss: 55.9827\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1260 - val_loss: 37.1858\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2202 - val_loss: 36.2047\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0455 - val_loss: 36.7807\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8441 - val_loss: 41.3470\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0410 - val_loss: 38.3818\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.9826 - val_loss: 60.8581\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2784 - val_loss: 39.0093\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.4349 - val_loss: 58.5871\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1677 - val_loss: 37.2501\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4798 - val_loss: 52.7405\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1918 - val_loss: 38.4507\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6852 - val_loss: 42.3147\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4151 - val_loss: 39.5713\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1723 - val_loss: 39.6628\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0462 - val_loss: 39.5392\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0148 - val_loss: 39.5203\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2663 - val_loss: 38.3791\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0884 - val_loss: 36.9564\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.9241 - val_loss: 55.6411\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8576 - val_loss: 38.7105\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0404 - val_loss: 43.7720\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.9206 - val_loss: 57.1218\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1164 - val_loss: 40.7080\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1533 - val_loss: 47.4883\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1935 - val_loss: 52.5203\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4850 - val_loss: 39.3596\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.2770 - val_loss: 37.0759\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0409 - val_loss: 37.9005\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.9471 - val_loss: 37.2741\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1828 - val_loss: 45.3389\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0374 - val_loss: 40.8494\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1505 - val_loss: 37.3299\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.9161 - val_loss: 37.1175\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1563 - val_loss: 51.4461\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3035 - val_loss: 37.0253\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0896 - val_loss: 44.1077\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8503 - val_loss: 40.0941\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2916 - val_loss: 40.3430\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4253 - val_loss: 40.4924\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.9754 - val_loss: 41.0274\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6125 - val_loss: 51.3908\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5150 - val_loss: 68.8945\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5445 - val_loss: 38.6786\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2364 - val_loss: 51.8489\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3523 - val_loss: 38.7681\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0365 - val_loss: 44.3961\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2010 - val_loss: 44.0643\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1850 - val_loss: 37.4039\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.9131 - val_loss: 40.9598\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0022 - val_loss: 40.6280\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7235 - val_loss: 49.0527\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3800 - val_loss: 35.7890\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7586 - val_loss: 62.2222\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7893 - val_loss: 36.1619\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5080 - val_loss: 40.1307\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.9330 - val_loss: 37.3001\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8936 - val_loss: 46.9003\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7442 - val_loss: 37.6634\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5783 - val_loss: 36.3879\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6645 - val_loss: 38.3315\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.9575 - val_loss: 37.4297\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6934 - val_loss: 35.7309\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6960 - val_loss: 70.1631\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6155 - val_loss: 37.3864\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3629 - val_loss: 43.4168\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8366 - val_loss: 43.8228\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1951 - val_loss: 37.7094\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0416 - val_loss: 55.5785\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1814 - val_loss: 75.6276\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6237 - val_loss: 43.7230\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2544 - val_loss: 50.8991\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0205 - val_loss: 40.6741\n",
            "Epoch 386/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 31.9903 - val_loss: 41.0222\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8506 - val_loss: 37.7858\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0614 - val_loss: 37.4471\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0372 - val_loss: 47.5929\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.9520 - val_loss: 41.3147\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8201 - val_loss: 53.1421\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 32.0626 - val_loss: 64.7023\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.9313 - val_loss: 35.9321\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8961 - val_loss: 35.2715\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6778 - val_loss: 36.4073\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8031 - val_loss: 45.4108\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6993 - val_loss: 44.3328\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7233 - val_loss: 39.4529\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5910 - val_loss: 37.6945\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6812 - val_loss: 39.4333\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4664 - val_loss: 45.2179\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5277 - val_loss: 35.7933\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5217 - val_loss: 44.2379\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5241 - val_loss: 39.3735\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8253 - val_loss: 55.8396\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1033 - val_loss: 42.2864\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7651 - val_loss: 37.2957\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8861 - val_loss: 44.1356\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5445 - val_loss: 56.2365\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7645 - val_loss: 69.5024\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2211 - val_loss: 42.9125\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8255 - val_loss: 41.1592\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6272 - val_loss: 38.6093\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6116 - val_loss: 40.7526\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.5074 - val_loss: 41.0214\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5182 - val_loss: 36.7875\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.3538 - val_loss: 40.8670\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5055 - val_loss: 49.0044\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5657 - val_loss: 38.1007\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5743 - val_loss: 50.4857\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0299 - val_loss: 37.8107\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7473 - val_loss: 44.6456\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7653 - val_loss: 41.8269\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8439 - val_loss: 83.6929\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 31.53 - 2s 10ms/step - loss: 31.5153 - val_loss: 35.5796\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5850 - val_loss: 42.4443\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3200 - val_loss: 43.3390\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3246 - val_loss: 40.8995\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5076 - val_loss: 40.8696\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.2990 - val_loss: 42.9468\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3092 - val_loss: 57.7657\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6401 - val_loss: 46.4710\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5892 - val_loss: 40.4446\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4347 - val_loss: 35.6938\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4336 - val_loss: 40.8670\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6019 - val_loss: 39.2873\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6456 - val_loss: 36.4094\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4040 - val_loss: 50.6303\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 31.50 - 2s 10ms/step - loss: 31.5095 - val_loss: 41.4557\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5074 - val_loss: 35.7694\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4108 - val_loss: 56.8673\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4057 - val_loss: 34.8042\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4171 - val_loss: 43.8562\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5921 - val_loss: 39.8153\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4885 - val_loss: 35.4219\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6298 - val_loss: 64.9533\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5602 - val_loss: 43.3804\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3886 - val_loss: 42.5476\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5645 - val_loss: 49.9006\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5362 - val_loss: 37.7467\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2667 - val_loss: 40.4609\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.9081 - val_loss: 36.6162\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7123 - val_loss: 62.1213\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4542 - val_loss: 54.5067\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5886 - val_loss: 36.5704\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.6876 - val_loss: 39.3121\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8724 - val_loss: 38.1230\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 31.55 - 2s 10ms/step - loss: 31.5640 - val_loss: 40.0240\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8417 - val_loss: 41.0823\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8219 - val_loss: 42.5555\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8529 - val_loss: 42.9514\n",
            "Epoch 462/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 11ms/step - loss: 31.7748 - val_loss: 42.1924\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.4569 - val_loss: 59.1845\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1128 - val_loss: 37.6521\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.1210 - val_loss: 51.2591\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5330 - val_loss: 38.5896\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7936 - val_loss: 39.6481\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3004 - val_loss: 45.1744\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8547 - val_loss: 37.0624\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.8033 - val_loss: 37.4889\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3433 - val_loss: 37.8282\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4303 - val_loss: 34.8572\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5551 - val_loss: 44.1642\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2661 - val_loss: 39.0914\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3775 - val_loss: 40.2930\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5301 - val_loss: 36.4496\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.2353 - val_loss: 35.4797\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4366 - val_loss: 37.4696\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4334 - val_loss: 43.7863\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4356 - val_loss: 38.4107\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7423 - val_loss: 36.8454\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4297 - val_loss: 40.0933\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6391 - val_loss: 61.6661\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5923 - val_loss: 35.2682\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0535 - val_loss: 40.3045\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6660 - val_loss: 47.3016\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7343 - val_loss: 63.9942\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5291 - val_loss: 39.6464\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 31.69 - 2s 10ms/step - loss: 31.6959 - val_loss: 36.3987\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3851 - val_loss: 47.7347\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6616 - val_loss: 52.2009\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7248 - val_loss: 48.6350\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5224 - val_loss: 36.1868\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4019 - val_loss: 36.0947\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5781 - val_loss: 38.9508\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4722 - val_loss: 47.2646\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 31.8008 - val_loss: 50.2115\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.6632 - val_loss: 37.5485\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3935 - val_loss: 34.9512\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3473 - val_loss: 38.2479\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -1.7612950425025418 \n",
            "MAE:  4.638151143457244 \n",
            "SD:  5.928381240070159\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 500])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "outputId": "3e1673e6-629c-4289-e103-073a6f03429c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  -0.781180413711342 \n",
            "Ensemble_std:  5.957955368724672\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFBjlEnhUOpv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BP_hv3_8(2).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}