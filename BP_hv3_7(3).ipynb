{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/%08BP_hv3_7(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YTF6cMiY1Hw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiiiBla2-j1S"
      },
      "source": [
        "# batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCoux5AOZnK",
        "outputId": "d1e974ee-190f-4b63-f1ce-3b7ad182d6d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "TensorFlow version :  2.8.2\n",
            "Keras version :  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XlHICkovd809",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e266eb0d-c809-4b64-ba1b-51f628823fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러z오기\n",
        "train_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "75KxLEi8kLbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "009f82f0-65b3-48a3-a56c-576b83ad5a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IEfYfZC5qWsR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "28106f29-1a2e-404d-9eb1-25f66858ee29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b5883e3-4ea0-4cc9-9b07-82c96a6afc2c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b5883e3-4ea0-4cc9-9b07-82c96a6afc2c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0b5883e3-4ea0-4cc9-9b07-82c96a6afc2c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0b5883e3-4ea0-4cc9-9b07-82c96a6afc2c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TtAXH0aCrBEF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "011032ea-2d2e-42c6-a471-bfb6a1e5c9b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85403c01-2f59-4d58-911e-2e4bc7c2414c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85403c01-2f59-4d58-911e-2e4bc7c2414c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-85403c01-2f59-4d58-911e-2e4bc7c2414c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-85403c01-2f59-4d58-911e-2e4bc7c2414c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "outputs": [],
      "source": [
        "#parameter\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ptBRJtSOZnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI8SHBwBOZnO",
        "outputId": "9a26eaac-e16d-44fe-e527-3e482e968a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 16)               64        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,393\n",
            "Trainable params: 2,329\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGT6-7NcOZnO",
        "scrolled": true,
        "outputId": "660c24b9-349e-4c34-9898-3d704db16e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 5s 6ms/step - loss: 12108.7373 - val_loss: 12083.1494\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 11637.2510 - val_loss: 11820.7109\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10986.6836 - val_loss: 10323.6270\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10120.5205 - val_loss: 9543.3779\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9115.5508 - val_loss: 8782.1855\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 8011.6777 - val_loss: 7866.6973\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6860.2046 - val_loss: 7529.6230\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5708.4116 - val_loss: 4640.9468\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 4608.9570 - val_loss: 4006.0273\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 3601.1213 - val_loss: 2473.8022\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2717.8232 - val_loss: 2164.3152\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1983.6193 - val_loss: 1605.3507\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1399.7339 - val_loss: 974.4820\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 952.2639 - val_loss: 650.0756\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 635.8171 - val_loss: 283.2971\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 417.2938 - val_loss: 389.7463\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 278.7592 - val_loss: 309.7568\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 199.1683 - val_loss: 131.0984\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 154.7826 - val_loss: 137.2790\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 130.8134 - val_loss: 219.5072\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 120.1604 - val_loss: 125.4855\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 114.4969 - val_loss: 125.9696\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 112.2739 - val_loss: 167.1940\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.8425 - val_loss: 127.7828\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.4542 - val_loss: 137.8289\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.2981 - val_loss: 138.6422\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.7432 - val_loss: 127.1703\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.9664 - val_loss: 142.0860\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.5588 - val_loss: 127.5237\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4707 - val_loss: 120.7321\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5188 - val_loss: 140.8574\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1475 - val_loss: 143.7954\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9445 - val_loss: 216.3596\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.4228 - val_loss: 153.7085\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.3114 - val_loss: 123.5226\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1993 - val_loss: 159.4551\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8449 - val_loss: 125.3434\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3310 - val_loss: 170.0048\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.6136 - val_loss: 139.7611\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.3179 - val_loss: 117.4930\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.5291 - val_loss: 131.3336\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.0267 - val_loss: 114.1192\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.6941 - val_loss: 128.5420\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 94.9701 - val_loss: 181.6239\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 95.3802 - val_loss: 174.2753\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4506 - val_loss: 140.1454\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.3544 - val_loss: 151.2292\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.7419 - val_loss: 216.5243\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.3542 - val_loss: 117.3709\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.0919 - val_loss: 112.7706\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.9057 - val_loss: 118.2898\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.3770 - val_loss: 144.1349\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.0758 - val_loss: 108.6256\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7085 - val_loss: 118.6079\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7732 - val_loss: 129.0469\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.3161 - val_loss: 108.1651\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.1252 - val_loss: 127.3985\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8255 - val_loss: 122.3388\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5809 - val_loss: 126.6748\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.6111 - val_loss: 123.9728\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.5934 - val_loss: 105.0843\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.3772 - val_loss: 117.0357\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.1087 - val_loss: 210.4122\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.9782 - val_loss: 111.6739\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.9800 - val_loss: 118.4597\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.1554 - val_loss: 126.6194\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0037 - val_loss: 115.7963\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.5493 - val_loss: 114.2933\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.2366 - val_loss: 123.8937\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.0958 - val_loss: 122.0673\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.0856 - val_loss: 130.2911\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.6342 - val_loss: 126.3706\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.3824 - val_loss: 125.7586\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.2045 - val_loss: 151.1661\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8654 - val_loss: 165.8106\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4859 - val_loss: 102.0836\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.0699 - val_loss: 136.5986\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.0373 - val_loss: 112.5971\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.5617 - val_loss: 131.6763\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.2541 - val_loss: 106.8890\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.2141 - val_loss: 117.8127\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.3393 - val_loss: 135.7216\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.9836 - val_loss: 110.6635\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.8974 - val_loss: 107.5130\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.5077 - val_loss: 110.7458\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.4090 - val_loss: 124.4967\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0855 - val_loss: 202.8550\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1456 - val_loss: 159.6843\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.9760 - val_loss: 119.1101\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0096 - val_loss: 120.6820\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.7238 - val_loss: 105.7326\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.2041 - val_loss: 119.5018\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.3500 - val_loss: 110.0220\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.3709 - val_loss: 99.3087\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.3123 - val_loss: 112.9708\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9793 - val_loss: 113.5047\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.7151 - val_loss: 134.0776\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.6126 - val_loss: 102.3285\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.5426 - val_loss: 129.4974\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.5447 - val_loss: 112.6704\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.4510 - val_loss: 106.0392\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.9919 - val_loss: 101.3486\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.1234 - val_loss: 110.6459\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.7343 - val_loss: 106.5515\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.8187 - val_loss: 113.8237\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.2033 - val_loss: 97.5319\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.3453 - val_loss: 102.5722\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.3068 - val_loss: 109.0328\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.1441 - val_loss: 113.0654\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 80.1752 - val_loss: 110.8424\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.9799 - val_loss: 112.9352\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.9769 - val_loss: 105.7115\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.1168 - val_loss: 208.1381\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.9632 - val_loss: 93.4105\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.7383 - val_loss: 107.7871\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.6050 - val_loss: 117.3551\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.6281 - val_loss: 121.3658\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.6078 - val_loss: 115.7818\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.1885 - val_loss: 106.4099\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.3723 - val_loss: 94.5118\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.3292 - val_loss: 111.9766\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.0404 - val_loss: 116.0152\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 79.0535 - val_loss: 144.6286\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7368 - val_loss: 123.0627\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.8072 - val_loss: 124.2195\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7941 - val_loss: 113.1298\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.8389 - val_loss: 119.8579\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.5031 - val_loss: 95.6260\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.3876 - val_loss: 94.5727\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.4056 - val_loss: 107.4348\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7104 - val_loss: 124.7510\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.2128 - val_loss: 96.3779\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.3603 - val_loss: 120.6241\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.2491 - val_loss: 100.5118\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.2970 - val_loss: 133.5733\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.2564 - val_loss: 113.9325\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.1073 - val_loss: 99.5002\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.7788 - val_loss: 109.3533\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0482 - val_loss: 97.2572\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9997 - val_loss: 96.1349\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6039 - val_loss: 95.5327\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9494 - val_loss: 140.9733\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5900 - val_loss: 102.2455\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5748 - val_loss: 112.6772\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5914 - val_loss: 90.6293\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.3219 - val_loss: 111.1495\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5974 - val_loss: 139.5514\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4753 - val_loss: 112.5728\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3305 - val_loss: 103.4998\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4873 - val_loss: 97.0264\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.3672 - val_loss: 114.7728\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3596 - val_loss: 97.2655\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.3781 - val_loss: 120.4974\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.2375 - val_loss: 94.6346\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.1739 - val_loss: 103.2573\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0751 - val_loss: 125.0377\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.9337 - val_loss: 120.0435\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9873 - val_loss: 100.9915\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.1270 - val_loss: 145.4677\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.7744 - val_loss: 123.2451\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9621 - val_loss: 108.6117\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.8618 - val_loss: 101.0562\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9582 - val_loss: 105.3353\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.8106 - val_loss: 94.4623\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.8060 - val_loss: 93.2633\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5321 - val_loss: 98.0390\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5457 - val_loss: 99.2955\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5053 - val_loss: 93.8982\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.4493 - val_loss: 103.0542\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1875 - val_loss: 105.7563\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.4006 - val_loss: 100.4901\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3499 - val_loss: 99.4725\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3909 - val_loss: 97.1438\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1541 - val_loss: 104.6934\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3322 - val_loss: 108.6685\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1774 - val_loss: 95.4682\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1199 - val_loss: 135.2407\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9231 - val_loss: 100.2622\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1787 - val_loss: 91.3527\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0845 - val_loss: 93.1655\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9924 - val_loss: 97.3508\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0030 - val_loss: 111.6105\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9638 - val_loss: 100.4041\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.6688 - val_loss: 108.7796\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.7820 - val_loss: 105.0489\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.8521 - val_loss: 95.6463\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5592 - val_loss: 91.0769\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5954 - val_loss: 98.1899\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.8042 - val_loss: 91.6818\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5446 - val_loss: 107.4965\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5865 - val_loss: 97.5054\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.3064 - val_loss: 97.5824\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5725 - val_loss: 97.1328\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.1078 - val_loss: 92.5212\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5224 - val_loss: 147.5655\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5841 - val_loss: 118.6957\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.3280 - val_loss: 91.8762\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.0317 - val_loss: 93.0484\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.4332 - val_loss: 90.6581\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.2408 - val_loss: 91.5546\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.0691 - val_loss: 95.1070\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.0978 - val_loss: 95.5615\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.1009 - val_loss: 132.5669\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.1479 - val_loss: 104.1020\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9177 - val_loss: 92.6117\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.0400 - val_loss: 102.0071\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9793 - val_loss: 101.7352\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9482 - val_loss: 93.0158\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.1009 - val_loss: 101.6033\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.8931 - val_loss: 97.0098\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.8075 - val_loss: 136.8512\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.8324 - val_loss: 121.8747\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9684 - val_loss: 98.2875\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.9186 - val_loss: 108.2226\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.7381 - val_loss: 89.3094\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.7840 - val_loss: 106.1888\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.8384 - val_loss: 92.4177\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.8449 - val_loss: 104.7468\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5573 - val_loss: 91.4072\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6237 - val_loss: 88.4103\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5229 - val_loss: 99.9921\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6814 - val_loss: 100.4204\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5537 - val_loss: 94.9885\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6550 - val_loss: 95.9842\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.6622 - val_loss: 114.5191\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.7004 - val_loss: 146.7910\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4911 - val_loss: 92.4217\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.4708 - val_loss: 98.7433\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4458 - val_loss: 96.0040\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.5796 - val_loss: 109.4394\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4848 - val_loss: 112.3778\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5375 - val_loss: 100.3278\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.3120 - val_loss: 120.3840\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3611 - val_loss: 95.3606\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2850 - val_loss: 115.8259\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1204 - val_loss: 95.9902\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4246 - val_loss: 97.5044\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2524 - val_loss: 98.3340\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2294 - val_loss: 140.3037\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.4633 - val_loss: 110.7868\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1537 - val_loss: 120.6977\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9893 - val_loss: 105.3844\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2908 - val_loss: 96.4359\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3534 - val_loss: 95.7552\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1931 - val_loss: 101.2318\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0604 - val_loss: 111.0569\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0359 - val_loss: 106.1546\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9500 - val_loss: 104.0969\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.0186 - val_loss: 92.4128\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 74.0390 - val_loss: 90.1008\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 73.9684 - val_loss: 130.5845\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.9395 - val_loss: 109.0578\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9977 - val_loss: 100.3022\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8067 - val_loss: 97.7557\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.0074 - val_loss: 124.0481\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8673 - val_loss: 102.6420\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8423 - val_loss: 221.3553\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7969 - val_loss: 92.9996\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8686 - val_loss: 108.7825\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8060 - val_loss: 98.0584\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.8552 - val_loss: 93.0534\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.9206 - val_loss: 101.5661\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8146 - val_loss: 96.4238\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5786 - val_loss: 96.5099\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8811 - val_loss: 127.7249\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7490 - val_loss: 102.6440\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7489 - val_loss: 112.1352\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8573 - val_loss: 91.3030\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7673 - val_loss: 135.8719\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8064 - val_loss: 94.6158\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6788 - val_loss: 101.9223\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6685 - val_loss: 116.1288\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6805 - val_loss: 103.1661\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7541 - val_loss: 122.9218\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6350 - val_loss: 98.8653\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6299 - val_loss: 112.2374\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.5816 - val_loss: 93.2939\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6296 - val_loss: 103.7066\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5403 - val_loss: 136.2817\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7649 - val_loss: 95.5046\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6661 - val_loss: 126.0374\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4604 - val_loss: 95.4959\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3437 - val_loss: 90.6327\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5558 - val_loss: 94.2400\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5851 - val_loss: 103.1046\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4162 - val_loss: 122.0948\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4810 - val_loss: 96.0478\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4537 - val_loss: 91.3967\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.4571 - val_loss: 99.0158\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6640 - val_loss: 96.9699\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3184 - val_loss: 114.5037\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4726 - val_loss: 123.1152\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5244 - val_loss: 99.6641\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3097 - val_loss: 112.3860\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5975 - val_loss: 118.0801\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4703 - val_loss: 98.9938\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2098 - val_loss: 93.3630\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3684 - val_loss: 100.9306\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2323 - val_loss: 96.4925\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3020 - val_loss: 92.3678\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4573 - val_loss: 96.3307\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3994 - val_loss: 97.0296\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2150 - val_loss: 93.6382\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2615 - val_loss: 116.2433\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2977 - val_loss: 100.0555\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2106 - val_loss: 97.4109\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3018 - val_loss: 105.8074\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3664 - val_loss: 128.0283\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3493 - val_loss: 92.8109\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2048 - val_loss: 89.0449\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1442 - val_loss: 97.5726\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2983 - val_loss: 95.6754\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0445 - val_loss: 89.3067\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1846 - val_loss: 94.7060\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0437 - val_loss: 95.2770\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1193 - val_loss: 94.8816\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0544 - val_loss: 101.7251\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1854 - val_loss: 111.9014\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1554 - val_loss: 94.3308\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2575 - val_loss: 111.6640\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1406 - val_loss: 92.7615\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0554 - val_loss: 102.6891\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0183 - val_loss: 96.1070\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0390 - val_loss: 127.0232\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3338 - val_loss: 93.1609\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0474 - val_loss: 103.8412\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.9338 - val_loss: 93.1445\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4042 - val_loss: 89.0726\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.8211 - val_loss: 102.8677\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.9488 - val_loss: 96.2874\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.9683 - val_loss: 95.4966\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.9784 - val_loss: 91.8067\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.9716 - val_loss: 89.0716\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0417 - val_loss: 95.0869\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.9386 - val_loss: 94.7042\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8832 - val_loss: 93.0512\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0307 - val_loss: 106.7503\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.9028 - val_loss: 100.5209\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.8824 - val_loss: 111.4641\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.9637 - val_loss: 118.8040\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8241 - val_loss: 104.8255\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.8092 - val_loss: 89.2654\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.9447 - val_loss: 110.5950\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8119 - val_loss: 130.2964\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.8147 - val_loss: 94.5298\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7766 - val_loss: 95.0607\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6985 - val_loss: 90.4419\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8439 - val_loss: 93.0307\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6953 - val_loss: 109.1653\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6991 - val_loss: 112.9859\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8334 - val_loss: 95.0615\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8595 - val_loss: 96.4369\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8815 - val_loss: 105.6111\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7139 - val_loss: 103.9298\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.7880 - val_loss: 108.0432\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7022 - val_loss: 99.8291\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8341 - val_loss: 99.5151\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5779 - val_loss: 113.4031\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6832 - val_loss: 105.3087\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8957 - val_loss: 89.5165\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7755 - val_loss: 89.0187\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4988 - val_loss: 94.1121\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.8671 - val_loss: 96.1230\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5957 - val_loss: 91.9678\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4460 - val_loss: 94.2973\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6006 - val_loss: 112.5338\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6299 - val_loss: 131.8372\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6051 - val_loss: 87.8861\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6030 - val_loss: 98.7221\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.4556 - val_loss: 104.8012\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5466 - val_loss: 95.7668\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7090 - val_loss: 95.5090\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6584 - val_loss: 93.7384\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5439 - val_loss: 95.9795\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5259 - val_loss: 93.6732\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6703 - val_loss: 90.9748\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4692 - val_loss: 107.4317\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5705 - val_loss: 105.9182\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.5332 - val_loss: 96.9299\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5806 - val_loss: 98.6889\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6867 - val_loss: 116.2697\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6158 - val_loss: 101.1361\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.4303 - val_loss: 95.8237\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.5627 - val_loss: 96.5082\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6856 - val_loss: 90.5830\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3989 - val_loss: 95.5649\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.3559 - val_loss: 90.3130\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4395 - val_loss: 115.2711\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6071 - val_loss: 104.0680\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5323 - val_loss: 113.8372\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4913 - val_loss: 97.2164\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5431 - val_loss: 99.9412\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6481 - val_loss: 89.3354\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5417 - val_loss: 137.8082\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4204 - val_loss: 96.2022\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6167 - val_loss: 94.0063\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4312 - val_loss: 101.1835\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4256 - val_loss: 89.5691\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4048 - val_loss: 99.1182\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4711 - val_loss: 90.4634\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2251 - val_loss: 104.7702\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5008 - val_loss: 98.4981\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.3412 - val_loss: 102.9853\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5352 - val_loss: 95.2307\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.3312 - val_loss: 97.3816\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3084 - val_loss: 98.5516\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2419 - val_loss: 98.5932\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3748 - val_loss: 139.4951\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1829 - val_loss: 93.6058\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1724 - val_loss: 93.2805\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3132 - val_loss: 112.2066\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.0951 - val_loss: 96.8703\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1350 - val_loss: 93.1914\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3318 - val_loss: 89.1971\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1302 - val_loss: 92.4297\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2738 - val_loss: 92.8409\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2551 - val_loss: 112.8638\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3821 - val_loss: 167.5361\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4284 - val_loss: 94.7193\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.1109 - val_loss: 89.2625\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1416 - val_loss: 97.6306\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1762 - val_loss: 99.7154\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2802 - val_loss: 95.8597\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0850 - val_loss: 115.4516\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2110 - val_loss: 182.4002\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2928 - val_loss: 91.2964\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1287 - val_loss: 95.8344\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2011 - val_loss: 88.5247\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1423 - val_loss: 98.3951\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1398 - val_loss: 93.7652\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1427 - val_loss: 100.7046\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0215 - val_loss: 92.2090\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9593 - val_loss: 127.8330\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0338 - val_loss: 94.6566\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.1357 - val_loss: 97.8706\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0283 - val_loss: 100.7364\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.0523 - val_loss: 97.2479\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9941 - val_loss: 123.4782\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.0897 - val_loss: 90.0902\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0801 - val_loss: 100.6435\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9403 - val_loss: 107.7364\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.1508 - val_loss: 103.7582\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.1160 - val_loss: 105.2619\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.0234 - val_loss: 116.9244\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9992 - val_loss: 103.2013\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0468 - val_loss: 118.8184\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9836 - val_loss: 94.9389\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0866 - val_loss: 93.8975\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0051 - val_loss: 89.6488\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8070 - val_loss: 108.7782\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.0162 - val_loss: 107.0263\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9642 - val_loss: 87.6366\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9669 - val_loss: 90.1870\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8969 - val_loss: 95.8387\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1481 - val_loss: 121.2849\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7393 - val_loss: 99.9713\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7961 - val_loss: 112.0033\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9874 - val_loss: 102.1716\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8000 - val_loss: 145.4203\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9034 - val_loss: 91.2616\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9499 - val_loss: 99.4459\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0059 - val_loss: 90.8370\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9144 - val_loss: 88.2695\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7218 - val_loss: 89.6205\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8139 - val_loss: 126.1435\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6567 - val_loss: 87.8400\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7274 - val_loss: 102.1312\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8598 - val_loss: 89.8458\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 72.0073 - val_loss: 93.0462\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7803 - val_loss: 89.3520\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8980 - val_loss: 95.9917\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6633 - val_loss: 87.8698\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8437 - val_loss: 102.6984\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5961 - val_loss: 92.5705\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6859 - val_loss: 93.5068\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8619 - val_loss: 102.8739\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6320 - val_loss: 101.0976\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7859 - val_loss: 97.5429\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 71.7058 - val_loss: 86.0014\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5707 - val_loss: 95.4351\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6748 - val_loss: 91.1865\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8139 - val_loss: 108.2692\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6981 - val_loss: 92.4053\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6684 - val_loss: 113.2466\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6193 - val_loss: 97.3587\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6891 - val_loss: 95.9586\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7430 - val_loss: 95.7636\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7526 - val_loss: 86.2928\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5936 - val_loss: 85.8671\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6256 - val_loss: 95.3251\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.3821 - val_loss: 90.4606\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5675 - val_loss: 89.1757\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5575 - val_loss: 103.5485\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7475 - val_loss: 108.9553\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5120 - val_loss: 116.7454\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6286 - val_loss: 91.3002\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4009 - val_loss: 90.9231\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6601 - val_loss: 110.2416\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6232 - val_loss: 97.5954\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5019 - val_loss: 99.6845\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6Dc0xVwOZnO",
        "outputId": "a313a547-9281-49d3-f84e-ea1e8b84d7b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -2.048756819378884 \n",
            "MAE:  7.601697294777297 \n",
            "SD:  9.771747715672356\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "qQZLKCzHOZnO",
        "outputId": "8dda0c22-1c06-4ac2-bf20-f1504ff50c64"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su2Sj5jZCSwH",
        "outputId": "9b840e41-9b77-4f5c-901a-8c4122383979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,393\n",
            "Trainable params: 2,329\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kPRh6v-mCSwH",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30728028-c88f-4c4e-f43d-4da06bfc6fe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 12404.4834 - val_loss: 12321.2061\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 12062.0752 - val_loss: 12019.2012\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11657.2734 - val_loss: 11536.8623\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11152.7295 - val_loss: 10584.5518\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10559.5010 - val_loss: 9819.2402\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 9891.8711 - val_loss: 8698.6230\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9168.4346 - val_loss: 9155.9658\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 8386.4795 - val_loss: 6980.6772\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7515.3203 - val_loss: 7142.1836\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6642.8394 - val_loss: 8281.3867\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5772.3706 - val_loss: 4352.6450\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4871.2627 - val_loss: 3842.0168\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 4064.1196 - val_loss: 4298.1279\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3337.1140 - val_loss: 3416.2693\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2694.4360 - val_loss: 2537.1819\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2140.2292 - val_loss: 2325.3074\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1673.4220 - val_loss: 1156.8920\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1278.3191 - val_loss: 1983.6378\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 966.8669 - val_loss: 1693.7090\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 717.0878 - val_loss: 762.1201\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 527.6885 - val_loss: 944.1129\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 390.9389 - val_loss: 246.6738\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 289.0848 - val_loss: 221.1348\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 220.3242 - val_loss: 222.7356\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 172.0033 - val_loss: 215.3101\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 143.2369 - val_loss: 130.2967\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 125.5409 - val_loss: 130.6943\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 114.1699 - val_loss: 934.7452\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 109.5776 - val_loss: 130.1355\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8462 - val_loss: 113.7857\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8799 - val_loss: 133.0716\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4020 - val_loss: 111.1382\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0967 - val_loss: 154.7695\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.0151 - val_loss: 111.2981\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.6115 - val_loss: 116.8780\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.2371 - val_loss: 112.9662\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.1052 - val_loss: 115.8879\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.2508 - val_loss: 118.5972\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.5517 - val_loss: 111.7962\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.6642 - val_loss: 105.0721\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.0884 - val_loss: 115.8832\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.7019 - val_loss: 112.9568\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.5544 - val_loss: 123.8973\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.4059 - val_loss: 103.9655\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.4692 - val_loss: 110.0232\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.8999 - val_loss: 118.0808\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.3567 - val_loss: 117.6086\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7177 - val_loss: 110.2762\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.6606 - val_loss: 145.4912\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.4940 - val_loss: 109.2455\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.4422 - val_loss: 141.7341\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.4390 - val_loss: 119.2983\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.5779 - val_loss: 141.8771\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.1603 - val_loss: 117.5512\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.2165 - val_loss: 106.7875\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8589 - val_loss: 116.8996\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.6143 - val_loss: 112.1247\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.5778 - val_loss: 99.9367\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.8452 - val_loss: 164.0240\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.6257 - val_loss: 104.1099\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.1047 - val_loss: 101.3964\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.6903 - val_loss: 108.5000\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.9515 - val_loss: 115.3748\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.0892 - val_loss: 119.8912\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.5277 - val_loss: 99.8027\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.1179 - val_loss: 108.7896\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.5058 - val_loss: 98.9443\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.1397 - val_loss: 96.0573\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.7163 - val_loss: 137.8336\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8967 - val_loss: 112.3204\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6541 - val_loss: 100.7056\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.1680 - val_loss: 102.0499\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.4176 - val_loss: 100.6450\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.6005 - val_loss: 102.4491\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.7049 - val_loss: 96.4320\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.8967 - val_loss: 95.0587\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 86.7514 - val_loss: 105.4100\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.6752 - val_loss: 152.7480\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.2534 - val_loss: 117.8333\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.0696 - val_loss: 107.2779\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.4439 - val_loss: 97.8663\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.9700 - val_loss: 120.0634\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.1049 - val_loss: 114.8004\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.3672 - val_loss: 130.7021\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4287 - val_loss: 104.1114\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5244 - val_loss: 100.4453\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.3451 - val_loss: 104.2056\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5392 - val_loss: 106.8751\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1711 - val_loss: 99.6235\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.0668 - val_loss: 96.8587\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.5688 - val_loss: 125.7908\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.9428 - val_loss: 144.6618\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.8340 - val_loss: 135.6006\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.6998 - val_loss: 102.0550\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.2890 - val_loss: 110.4603\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.2760 - val_loss: 120.6125\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.0762 - val_loss: 111.5765\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.2794 - val_loss: 120.3429\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.7566 - val_loss: 119.0588\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.8308 - val_loss: 114.2761\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.6897 - val_loss: 108.8835\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.9976 - val_loss: 116.8055\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.8068 - val_loss: 109.4446\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.8936 - val_loss: 119.4128\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0984 - val_loss: 98.1182\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3198 - val_loss: 106.4992\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1296 - val_loss: 102.2995\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3194 - val_loss: 122.1036\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.7717 - val_loss: 104.3520\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.8381 - val_loss: 99.4329\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.7718 - val_loss: 101.9267\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0369 - val_loss: 120.6398\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1450 - val_loss: 99.9656\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.9600 - val_loss: 125.6528\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.8232 - val_loss: 99.8485\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.6149 - val_loss: 101.8309\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.2145 - val_loss: 97.3905\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.2773 - val_loss: 109.7204\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.3272 - val_loss: 106.4013\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.1890 - val_loss: 92.2640\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9268 - val_loss: 94.7309\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.1128 - val_loss: 102.7885\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.3622 - val_loss: 98.2558\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.4924 - val_loss: 98.0439\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.5980 - val_loss: 102.6208\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.6249 - val_loss: 136.6754\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 81.4132 - val_loss: 99.5404\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 81.5124 - val_loss: 95.5304\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 81.3943 - val_loss: 115.6061\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.8019 - val_loss: 95.7116\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9741 - val_loss: 112.6705\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.3217 - val_loss: 105.9997\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.5175 - val_loss: 94.2022\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.5362 - val_loss: 123.2544\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.4915 - val_loss: 103.6699\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.3862 - val_loss: 133.9726\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.2958 - val_loss: 130.7603\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.1473 - val_loss: 95.4527\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.0109 - val_loss: 94.2354\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.9882 - val_loss: 102.8373\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.1563 - val_loss: 90.2795\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.0418 - val_loss: 138.4401\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.0455 - val_loss: 105.5576\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.4912 - val_loss: 102.8640\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.6951 - val_loss: 120.6572\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.7197 - val_loss: 111.9058\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.6923 - val_loss: 95.7727\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 80.5505 - val_loss: 129.3105\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.3732 - val_loss: 94.9391\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.4512 - val_loss: 98.9727\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.3205 - val_loss: 122.7262\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.3887 - val_loss: 100.4978\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.2943 - val_loss: 100.1488\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.1920 - val_loss: 104.1891\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 80.3391 - val_loss: 97.8296\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.0610 - val_loss: 103.5103\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.8774 - val_loss: 149.5831\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.4184 - val_loss: 104.4550\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.0174 - val_loss: 106.0212\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.1287 - val_loss: 145.7413\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.2175 - val_loss: 97.0404\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.7313 - val_loss: 97.0083\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.9800 - val_loss: 109.2304\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.9917 - val_loss: 97.9193\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.9021 - val_loss: 158.9012\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.8710 - val_loss: 92.7209\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.8672 - val_loss: 97.2112\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.8152 - val_loss: 106.3651\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.8536 - val_loss: 96.4874\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.9194 - val_loss: 96.5657\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.3606 - val_loss: 105.8732\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.5663 - val_loss: 107.1413\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.6464 - val_loss: 133.0009\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4062 - val_loss: 95.8053\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4300 - val_loss: 103.0051\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4993 - val_loss: 99.2993\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.3638 - val_loss: 92.0280\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.5861 - val_loss: 99.4299\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.5322 - val_loss: 104.7817\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4162 - val_loss: 113.2410\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4662 - val_loss: 102.4847\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 79.4657 - val_loss: 94.8392\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.3544 - val_loss: 96.5762\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.6774 - val_loss: 117.9824\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.2437 - val_loss: 126.7087\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4061 - val_loss: 102.4398\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4149 - val_loss: 109.4716\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.3196 - val_loss: 102.6805\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4692 - val_loss: 108.7111\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.1558 - val_loss: 122.3941\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.2036 - val_loss: 112.9963\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.9168 - val_loss: 93.9188\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.2671 - val_loss: 94.3049\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.0215 - val_loss: 97.2023\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.9561 - val_loss: 96.8571\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.1947 - val_loss: 107.6475\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.3132 - val_loss: 98.1733\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.1773 - val_loss: 113.7267\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.0141 - val_loss: 135.8395\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.0385 - val_loss: 123.1691\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.6914 - val_loss: 96.0902\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7717 - val_loss: 125.3173\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.8519 - val_loss: 104.6982\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.8307 - val_loss: 107.0698\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.9226 - val_loss: 104.0401\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 79.0607 - val_loss: 125.5121\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.9564 - val_loss: 96.7449\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.6314 - val_loss: 101.9336\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.8489 - val_loss: 102.2682\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.5946 - val_loss: 95.1631\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.8181 - val_loss: 109.0333\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.9654 - val_loss: 103.4360\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.5773 - val_loss: 125.3214\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7518 - val_loss: 104.1744\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.6936 - val_loss: 99.5622\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.0032 - val_loss: 110.2362\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.8342 - val_loss: 102.7630\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.5522 - val_loss: 109.6613\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.6562 - val_loss: 107.3043\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7109 - val_loss: 105.4316\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.6990 - val_loss: 145.6584\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.8151 - val_loss: 94.0284\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.7047 - val_loss: 95.0216\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7541 - val_loss: 175.9488\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.5418 - val_loss: 96.6807\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7620 - val_loss: 98.1716\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.4997 - val_loss: 94.8812\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.5793 - val_loss: 97.6371\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.3933 - val_loss: 154.7223\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7811 - val_loss: 106.5189\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.5797 - val_loss: 130.3108\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.4915 - val_loss: 93.5514\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.6138 - val_loss: 107.5128\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.4171 - val_loss: 102.3451\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.4506 - val_loss: 96.0088\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.3774 - val_loss: 102.0587\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.4127 - val_loss: 96.4523\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.3847 - val_loss: 104.5745\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.2383 - val_loss: 126.8735\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.2736 - val_loss: 92.6221\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0537 - val_loss: 97.4227\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.1663 - val_loss: 94.5507\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.1065 - val_loss: 106.8348\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.4891 - val_loss: 94.3826\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.4235 - val_loss: 110.9448\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.4609 - val_loss: 97.3252\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.3056 - val_loss: 96.2859\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.2700 - val_loss: 195.4556\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.2805 - val_loss: 95.4576\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.2285 - val_loss: 103.2639\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0289 - val_loss: 99.3645\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.1555 - val_loss: 106.7693\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.1677 - val_loss: 125.2543\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.1500 - val_loss: 115.7433\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.2404 - val_loss: 101.0382\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0287 - val_loss: 96.1307\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.3549 - val_loss: 103.5992\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0561 - val_loss: 125.2550\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0651 - val_loss: 100.2916\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0844 - val_loss: 97.9938\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0576 - val_loss: 95.5144\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0527 - val_loss: 107.4630\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.2339 - val_loss: 105.8207\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.8218 - val_loss: 97.3755\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.8816 - val_loss: 97.3612\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0855 - val_loss: 131.4035\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0580 - val_loss: 108.7312\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9360 - val_loss: 101.6769\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.8727 - val_loss: 103.0897\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0649 - val_loss: 96.8770\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9512 - val_loss: 112.2719\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9086 - val_loss: 108.3187\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9676 - val_loss: 94.5217\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.8850 - val_loss: 95.5851\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0810 - val_loss: 96.4823\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.4872 - val_loss: 99.2479\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.8646 - val_loss: 99.9229\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.9528 - val_loss: 92.8209\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.7003 - val_loss: 99.5939\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0766 - val_loss: 101.2626\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.8763 - val_loss: 98.7707\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.7450 - val_loss: 95.9089\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6678 - val_loss: 95.3606\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0035 - val_loss: 96.3761\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9472 - val_loss: 100.2143\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6143 - val_loss: 91.9213\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.7793 - val_loss: 104.8563\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6870 - val_loss: 116.5907\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6808 - val_loss: 100.1086\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.6844 - val_loss: 99.7678\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.8698 - val_loss: 109.2516\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.7962 - val_loss: 95.7815\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5656 - val_loss: 107.3385\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6215 - val_loss: 125.4400\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9014 - val_loss: 96.7390\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.7612 - val_loss: 121.6893\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.7914 - val_loss: 124.5562\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.7924 - val_loss: 100.0848\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5116 - val_loss: 107.5287\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6555 - val_loss: 104.6516\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4253 - val_loss: 131.5533\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6746 - val_loss: 113.8358\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.7207 - val_loss: 113.5965\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5305 - val_loss: 94.0258\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4796 - val_loss: 121.3435\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5816 - val_loss: 97.6063\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4906 - val_loss: 119.8210\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.4751 - val_loss: 97.5455\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6441 - val_loss: 95.5012\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6455 - val_loss: 109.8356\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5223 - val_loss: 116.9409\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.6242 - val_loss: 108.2242\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3628 - val_loss: 90.9455\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.7484 - val_loss: 107.9485\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3784 - val_loss: 94.5285\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3535 - val_loss: 105.9669\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6547 - val_loss: 99.4657\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.7217 - val_loss: 161.8198\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4705 - val_loss: 110.4090\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3633 - val_loss: 101.1505\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5678 - val_loss: 109.0684\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5462 - val_loss: 97.1306\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3944 - val_loss: 122.0951\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3004 - val_loss: 95.0239\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3706 - val_loss: 105.7126\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3219 - val_loss: 99.7511\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.2391 - val_loss: 97.9610\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3240 - val_loss: 92.1738\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.4478 - val_loss: 100.9112\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3281 - val_loss: 101.9182\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3045 - val_loss: 106.3321\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3042 - val_loss: 90.8883\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.2583 - val_loss: 100.5308\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.2496 - val_loss: 108.7436\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.3030 - val_loss: 104.4472\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.1309 - val_loss: 96.0250\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.0713 - val_loss: 110.6519\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.1716 - val_loss: 121.0332\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0575 - val_loss: 100.7187\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0504 - val_loss: 106.9024\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.1485 - val_loss: 106.7611\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0892 - val_loss: 95.9437\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.1131 - val_loss: 98.5831\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0379 - val_loss: 102.4900\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.1367 - val_loss: 116.9413\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.1942 - val_loss: 101.3178\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9741 - val_loss: 101.7164\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0477 - val_loss: 110.8405\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0618 - val_loss: 99.8526\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9467 - val_loss: 107.8265\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0447 - val_loss: 96.3267\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.1099 - val_loss: 105.9739\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.9027 - val_loss: 115.1148\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.8288 - val_loss: 106.1831\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9405 - val_loss: 110.1724\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9786 - val_loss: 95.7131\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.1033 - val_loss: 96.4935\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9742 - val_loss: 97.6727\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.7942 - val_loss: 112.5040\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.0211 - val_loss: 97.4743\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.9818 - val_loss: 103.8414\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.8153 - val_loss: 92.3616\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.9730 - val_loss: 96.4092\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 76.7279 - val_loss: 93.0300\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.7798 - val_loss: 96.8305\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.8436 - val_loss: 96.6600\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.9271 - val_loss: 102.2197\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.6061 - val_loss: 109.6742\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.7769 - val_loss: 94.1487\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.7614 - val_loss: 98.0766\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 76.6729 - val_loss: 108.9322\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.6379 - val_loss: 98.4210\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.7055 - val_loss: 111.6984\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.5083 - val_loss: 102.5736\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.6685 - val_loss: 112.8109\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.8834 - val_loss: 106.6500\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.6309 - val_loss: 107.3802\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.8294 - val_loss: 110.7271\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.7169 - val_loss: 96.2142\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.7781 - val_loss: 132.6165\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.5397 - val_loss: 94.2878\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0134 - val_loss: 104.3877\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.6151 - val_loss: 94.1544\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.8017 - val_loss: 93.9229\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.5807 - val_loss: 116.9177\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.7706 - val_loss: 118.7400\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.6259 - val_loss: 93.7079\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.8794 - val_loss: 128.3482\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5473 - val_loss: 112.5919\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.6604 - val_loss: 93.8888\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.6860 - val_loss: 99.0643\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5464 - val_loss: 93.0964\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.5447 - val_loss: 117.2599\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9009 - val_loss: 107.0897\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5778 - val_loss: 94.3924\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.6292 - val_loss: 96.7208\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3554 - val_loss: 103.3367\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5308 - val_loss: 94.7771\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.6183 - val_loss: 113.3373\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.4610 - val_loss: 134.0700\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.5517 - val_loss: 97.4766\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.4628 - val_loss: 109.1091\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.2622 - val_loss: 101.6904\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.5137 - val_loss: 96.3043\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.5422 - val_loss: 173.4916\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5029 - val_loss: 95.3904\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.4003 - val_loss: 118.5836\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3588 - val_loss: 99.0551\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.6413 - val_loss: 103.9006\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.3764 - val_loss: 91.1627\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3010 - val_loss: 104.0861\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3063 - val_loss: 101.2322\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.4362 - val_loss: 147.9119\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5699 - val_loss: 99.2718\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5351 - val_loss: 91.8640\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.4919 - val_loss: 95.6004\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.4069 - val_loss: 95.9149\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3250 - val_loss: 121.5226\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.4904 - val_loss: 90.3427\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.2515 - val_loss: 98.7119\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.3594 - val_loss: 105.5044\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1917 - val_loss: 106.5289\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2651 - val_loss: 94.2376\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2049 - val_loss: 105.5370\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.4949 - val_loss: 112.6683\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.6116 - val_loss: 99.0940\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3573 - val_loss: 105.7849\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.3227 - val_loss: 95.5709\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3583 - val_loss: 101.3893\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.2241 - val_loss: 106.3741\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2032 - val_loss: 98.5560\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3195 - val_loss: 91.7729\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3894 - val_loss: 96.0424\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.2813 - val_loss: 93.9329\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.3211 - val_loss: 95.1189\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.4918 - val_loss: 103.5487\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3530 - val_loss: 94.7170\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.2189 - val_loss: 107.2783\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2235 - val_loss: 97.1654\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3547 - val_loss: 113.8884\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.2427 - val_loss: 108.2094\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3075 - val_loss: 93.7094\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0502 - val_loss: 106.5023\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3203 - val_loss: 97.6908\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.1980 - val_loss: 107.6391\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0707 - val_loss: 98.8275\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0567 - val_loss: 102.4705\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.2542 - val_loss: 107.0476\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2286 - val_loss: 112.2664\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3672 - val_loss: 94.7843\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.5153 - val_loss: 94.7755\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.1766 - val_loss: 96.0983\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.3134 - val_loss: 101.2659\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.0845 - val_loss: 98.2407\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0925 - val_loss: 101.0419\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.3608 - val_loss: 151.0516\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.2167 - val_loss: 103.8767\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.4578 - val_loss: 90.5570\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0091 - val_loss: 91.7782\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.9683 - val_loss: 91.8955\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.2016 - val_loss: 94.7965\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2465 - val_loss: 171.7798\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.9006 - val_loss: 104.7862\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2475 - val_loss: 102.7910\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0909 - val_loss: 102.5737\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.9926 - val_loss: 92.7681\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3286 - val_loss: 93.5189\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1783 - val_loss: 111.0670\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.1333 - val_loss: 100.2964\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1330 - val_loss: 91.6236\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1265 - val_loss: 105.8235\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9372 - val_loss: 93.0860\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.0710 - val_loss: 91.9266\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0304 - val_loss: 107.4747\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.8820 - val_loss: 100.2507\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.9387 - val_loss: 91.6147\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1059 - val_loss: 90.0099\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1485 - val_loss: 96.3879\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.0012 - val_loss: 114.4069\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.9655 - val_loss: 100.6065\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.9753 - val_loss: 94.3374\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.8951 - val_loss: 92.1209\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0528 - val_loss: 121.4647\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.9988 - val_loss: 96.1270\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9242 - val_loss: 104.8046\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.9588 - val_loss: 96.9126\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 75.9041 - val_loss: 93.6552\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 76.1710 - val_loss: 93.1235\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 75.9976 - val_loss: 98.3706\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1272 - val_loss: 95.3900\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.1252 - val_loss: 94.4045\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.8537 - val_loss: 130.7190\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9717 - val_loss: 104.1134\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0659 - val_loss: 94.2291\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9662 - val_loss: 97.5012\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.0318 - val_loss: 98.7220\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.0364 - val_loss: 151.9581\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.0325 - val_loss: 99.2149\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.7959 - val_loss: 93.2298\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1039 - val_loss: 98.2057\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7fe720a-0b4e-41c2-9c7e-6e5cee467f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -1.8723304507009264 \n",
            "MAE:  7.573426818743259 \n",
            "SD:  9.731394329978599\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "3104984a-ceb4-4808-f955-4aa9a9ebb9a3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fced9165-72a2-4038-d9dc-5be84dd827ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,393\n",
            "Trainable params: 2,329\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dcXAOqd2CTsa",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba4fe5b-d19c-4f77-e499-2d8052b5f7a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 8ms/step - loss: 12242.2246 - val_loss: 12188.7422\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11855.2295 - val_loss: 11749.3984\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11313.9844 - val_loss: 11107.6377\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10596.5059 - val_loss: 9980.5586\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9745.9932 - val_loss: 9412.0840\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8805.7725 - val_loss: 7698.8604\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7813.6431 - val_loss: 6496.7075\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6807.9795 - val_loss: 5392.4097\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5820.9521 - val_loss: 4839.5938\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4881.9072 - val_loss: 4049.3933\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4011.6182 - val_loss: 3919.2942\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3206.4868 - val_loss: 2776.6965\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2503.0137 - val_loss: 1740.2285\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1923.7563 - val_loss: 1582.3466\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1448.8438 - val_loss: 870.5934\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1067.8870 - val_loss: 1169.2445\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 772.9095 - val_loss: 533.0776\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 549.7985 - val_loss: 659.7994\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 389.2057 - val_loss: 328.7017\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 276.8513 - val_loss: 273.3089\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 201.7521 - val_loss: 303.2007\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 154.0890 - val_loss: 173.9479\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 124.5390 - val_loss: 165.4382\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.0755 - val_loss: 237.1884\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8400 - val_loss: 119.7486\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.2244 - val_loss: 107.5979\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.4410 - val_loss: 108.5578\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.7829 - val_loss: 117.1919\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.1430 - val_loss: 111.3134\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.9090 - val_loss: 119.8122\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 89.9798 - val_loss: 100.7365\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7253 - val_loss: 106.1257\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.2890 - val_loss: 124.4068\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.4124 - val_loss: 106.7694\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.3752 - val_loss: 106.6765\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.3602 - val_loss: 113.3216\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.3052 - val_loss: 139.5736\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.7861 - val_loss: 105.1840\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.2607 - val_loss: 106.4890\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.9011 - val_loss: 116.0136\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.7100 - val_loss: 113.4202\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.1264 - val_loss: 109.0207\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.2484 - val_loss: 125.3603\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.5813 - val_loss: 138.5437\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.3502 - val_loss: 103.5265\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.7477 - val_loss: 105.9418\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.7021 - val_loss: 98.9780\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.6284 - val_loss: 108.4189\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.9628 - val_loss: 114.2381\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.4092 - val_loss: 108.0934\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.7688 - val_loss: 112.5267\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8061 - val_loss: 181.3460\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.1798 - val_loss: 119.2399\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.1693 - val_loss: 108.7837\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5174 - val_loss: 124.8332\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5902 - val_loss: 108.5265\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8879 - val_loss: 140.5279\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.6445 - val_loss: 121.2734\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5605 - val_loss: 114.8542\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.9659 - val_loss: 114.5213\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.8293 - val_loss: 100.3625\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1098 - val_loss: 119.4895\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.7344 - val_loss: 131.6982\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.6923 - val_loss: 121.9748\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.4653 - val_loss: 130.4453\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1292 - val_loss: 126.7966\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.0081 - val_loss: 100.0821\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.2721 - val_loss: 100.7288\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.9165 - val_loss: 119.4504\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.9309 - val_loss: 103.3282\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1043 - val_loss: 116.6224\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.6731 - val_loss: 92.8891\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.7442 - val_loss: 100.1676\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.7125 - val_loss: 118.7461\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.6882 - val_loss: 108.3194\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.2954 - val_loss: 102.9602\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3525 - val_loss: 106.6819\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3674 - val_loss: 112.1717\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.2660 - val_loss: 98.1741\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0052 - val_loss: 105.8010\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.2593 - val_loss: 131.6702\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.9240 - val_loss: 104.2348\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.5356 - val_loss: 119.2550\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.5537 - val_loss: 106.0528\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.5058 - val_loss: 104.7139\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.5087 - val_loss: 117.7858\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.4092 - val_loss: 100.3770\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.3293 - val_loss: 96.2840\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.0206 - val_loss: 109.7214\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9488 - val_loss: 114.2179\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.0217 - val_loss: 124.6758\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.8658 - val_loss: 105.6607\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.5905 - val_loss: 120.7208\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.7689 - val_loss: 137.3911\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.1103 - val_loss: 94.2140\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.7137 - val_loss: 113.4394\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.6418 - val_loss: 96.3609\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.5802 - val_loss: 99.7735\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.3197 - val_loss: 160.4454\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.3220 - val_loss: 98.8730\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.0158 - val_loss: 102.4011\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.3016 - val_loss: 99.2281\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.2456 - val_loss: 106.1956\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.9531 - val_loss: 96.3732\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.8864 - val_loss: 106.0740\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.7324 - val_loss: 98.4644\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.7588 - val_loss: 103.1203\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.8065 - val_loss: 102.0799\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 80.8646 - val_loss: 131.2877\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.0050 - val_loss: 95.8785\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.6577 - val_loss: 104.7546\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 80.5679 - val_loss: 111.8067\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.5910 - val_loss: 99.9974\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.7097 - val_loss: 102.5892\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.5296 - val_loss: 124.7341\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.1544 - val_loss: 98.0522\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 80.2400 - val_loss: 114.6279\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.2898 - val_loss: 100.0720\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.0700 - val_loss: 108.6609\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.1386 - val_loss: 99.3782\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.8160 - val_loss: 98.4111\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.8918 - val_loss: 110.4586\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.7441 - val_loss: 109.5417\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 79.8691 - val_loss: 97.4931\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.7255 - val_loss: 92.5845\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.7886 - val_loss: 95.1773\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.5345 - val_loss: 98.5141\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 79.5863 - val_loss: 92.1545\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.3036 - val_loss: 104.3524\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4965 - val_loss: 105.4148\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4490 - val_loss: 94.1592\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.2197 - val_loss: 99.3188\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.3203 - val_loss: 99.9628\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.1250 - val_loss: 113.9414\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.3150 - val_loss: 106.8121\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.0234 - val_loss: 157.7620\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 79.2371 - val_loss: 95.4807\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.0546 - val_loss: 99.8899\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.0732 - val_loss: 105.0296\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.8058 - val_loss: 95.1877\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.9526 - val_loss: 100.1724\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.1485 - val_loss: 110.3035\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7916 - val_loss: 120.5185\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.8805 - val_loss: 99.1263\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7805 - val_loss: 97.6520\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7299 - val_loss: 101.6349\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.5733 - val_loss: 110.1911\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.8637 - val_loss: 112.0327\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.5854 - val_loss: 118.6286\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.4702 - val_loss: 110.9574\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.3129 - val_loss: 99.4302\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.3181 - val_loss: 108.7725\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0428 - val_loss: 107.4691\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9926 - val_loss: 106.6055\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.2700 - val_loss: 120.8005\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.1096 - val_loss: 112.4368\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.0224 - val_loss: 109.3635\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9476 - val_loss: 115.8521\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0882 - val_loss: 105.7596\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.8524 - val_loss: 114.5128\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9755 - val_loss: 97.0111\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.5946 - val_loss: 133.6938\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.7537 - val_loss: 104.2759\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.8748 - val_loss: 108.3795\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4926 - val_loss: 102.8090\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5265 - val_loss: 91.5997\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4669 - val_loss: 109.3767\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.5112 - val_loss: 178.7550\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5396 - val_loss: 96.3861\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5492 - val_loss: 101.1416\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.2806 - val_loss: 96.5364\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4077 - val_loss: 98.5309\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4428 - val_loss: 97.7307\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.2668 - val_loss: 92.3782\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.2565 - val_loss: 97.8570\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4961 - val_loss: 93.9433\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0665 - val_loss: 96.6385\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0665 - val_loss: 109.2955\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0483 - val_loss: 107.7442\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.1528 - val_loss: 99.0319\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.1144 - val_loss: 94.1339\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9241 - val_loss: 93.1775\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.9018 - val_loss: 113.2048\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.8502 - val_loss: 137.1737\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.8512 - val_loss: 91.0125\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.9204 - val_loss: 92.0787\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.9343 - val_loss: 100.7846\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.6187 - val_loss: 114.3823\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.8945 - val_loss: 101.4006\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.6414 - val_loss: 97.5819\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.7458 - val_loss: 125.7613\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5166 - val_loss: 133.6897\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.4780 - val_loss: 99.0524\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.4811 - val_loss: 96.9317\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.4833 - val_loss: 101.7638\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3192 - val_loss: 115.1120\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.4126 - val_loss: 98.3357\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3647 - val_loss: 93.5330\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5490 - val_loss: 92.0496\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3419 - val_loss: 101.3549\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3828 - val_loss: 98.0741\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.2259 - val_loss: 109.8051\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1189 - val_loss: 97.5547\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2535 - val_loss: 95.9791\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1124 - val_loss: 96.9282\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2003 - val_loss: 99.1914\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.1103 - val_loss: 94.5714\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1261 - val_loss: 99.2364\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.1859 - val_loss: 101.1778\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.0734 - val_loss: 92.9348\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9195 - val_loss: 106.4332\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.9147 - val_loss: 126.8340\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9724 - val_loss: 101.8081\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9508 - val_loss: 92.9473\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.8206 - val_loss: 93.9699\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.8361 - val_loss: 118.6882\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9597 - val_loss: 160.4449\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9152 - val_loss: 99.9644\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9222 - val_loss: 98.7244\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.6880 - val_loss: 91.7212\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.8276 - val_loss: 125.0371\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5852 - val_loss: 98.4198\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5269 - val_loss: 98.0629\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.7134 - val_loss: 94.4229\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.6467 - val_loss: 90.9311\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5611 - val_loss: 99.2877\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5005 - val_loss: 102.6597\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.8132 - val_loss: 100.6202\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.7489 - val_loss: 129.9342\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5843 - val_loss: 94.0061\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.6003 - val_loss: 93.6959\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.4168 - val_loss: 143.5159\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.5257 - val_loss: 94.7705\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.3139 - val_loss: 100.3642\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5329 - val_loss: 93.3917\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5121 - val_loss: 106.2204\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.4493 - val_loss: 98.7649\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.4634 - val_loss: 96.4971\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.3845 - val_loss: 106.3479\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.3794 - val_loss: 106.1309\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.3552 - val_loss: 93.0764\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.3561 - val_loss: 92.7922\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.4769 - val_loss: 93.1180\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.2957 - val_loss: 105.4715\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.1624 - val_loss: 120.4074\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.1954 - val_loss: 106.9913\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.1465 - val_loss: 93.0736\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.0829 - val_loss: 120.9096\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.1670 - val_loss: 97.0196\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.0422 - val_loss: 93.2092\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.0656 - val_loss: 92.3821\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.3458 - val_loss: 88.5845\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.1046 - val_loss: 103.3715\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.0290 - val_loss: 116.5237\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9294 - val_loss: 116.7952\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.1512 - val_loss: 104.8668\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.0163 - val_loss: 107.9187\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.0963 - val_loss: 93.0670\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9351 - val_loss: 101.1317\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9419 - val_loss: 93.9483\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.1207 - val_loss: 93.3782\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9587 - val_loss: 96.7816\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9165 - val_loss: 129.5129\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.8376 - val_loss: 92.5137\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.0976 - val_loss: 98.3040\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9467 - val_loss: 95.4290\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.8532 - val_loss: 89.5282\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9953 - val_loss: 89.3651\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.7234 - val_loss: 90.9956\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.8513 - val_loss: 100.9490\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.7051 - val_loss: 136.5015\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.8189 - val_loss: 96.1236\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9094 - val_loss: 98.9296\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.8425 - val_loss: 102.1611\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.6591 - val_loss: 98.0708\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.7049 - val_loss: 104.4423\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6367 - val_loss: 100.0915\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6762 - val_loss: 122.1991\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9146 - val_loss: 139.1086\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.8891 - val_loss: 99.4183\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.7219 - val_loss: 127.4991\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.7322 - val_loss: 105.9820\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6732 - val_loss: 124.9135\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.6312 - val_loss: 92.6784\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6980 - val_loss: 96.3463\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.7940 - val_loss: 122.9594\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6898 - val_loss: 99.7555\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6051 - val_loss: 99.8371\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5596 - val_loss: 99.4373\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5366 - val_loss: 139.5928\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5458 - val_loss: 131.5602\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6537 - val_loss: 91.5454\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.6253 - val_loss: 98.1318\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.6548 - val_loss: 105.9136\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4751 - val_loss: 90.8664\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5488 - val_loss: 95.8877\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4896 - val_loss: 89.7544\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6679 - val_loss: 111.1742\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6980 - val_loss: 97.0712\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3845 - val_loss: 91.3339\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4236 - val_loss: 102.1173\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5402 - val_loss: 95.5356\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4944 - val_loss: 91.3371\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.4318 - val_loss: 93.7983\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3450 - val_loss: 95.4256\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3113 - val_loss: 91.0375\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3680 - val_loss: 88.5208\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.4205 - val_loss: 93.9018\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3891 - val_loss: 94.5368\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3561 - val_loss: 89.5956\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3828 - val_loss: 136.5159\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.2319 - val_loss: 101.3573\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2950 - val_loss: 111.1056\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3647 - val_loss: 110.4090\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2372 - val_loss: 126.5838\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.3407 - val_loss: 104.1129\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.4871 - val_loss: 108.4199\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3826 - val_loss: 97.5176\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2491 - val_loss: 91.6583\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1556 - val_loss: 99.3888\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.1128 - val_loss: 107.6212\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1521 - val_loss: 99.8551\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.1615 - val_loss: 92.1411\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1047 - val_loss: 106.5070\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0006 - val_loss: 116.7850\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1334 - val_loss: 94.0132\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4332 - val_loss: 98.3447\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.0650 - val_loss: 92.4706\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0146 - val_loss: 96.6063\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0029 - val_loss: 91.8542\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.1888 - val_loss: 97.1358\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1726 - val_loss: 136.1059\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1324 - val_loss: 105.1550\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0427 - val_loss: 111.5180\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9905 - val_loss: 91.0958\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0101 - val_loss: 91.9673\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9524 - val_loss: 103.9647\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.1257 - val_loss: 100.1019\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1574 - val_loss: 105.9574\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9668 - val_loss: 102.6021\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0797 - val_loss: 106.1841\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 74.0753 - val_loss: 119.0060\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 73.8987 - val_loss: 131.7703\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 74.0368 - val_loss: 127.0828\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.8778 - val_loss: 94.5665\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8698 - val_loss: 88.5300\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0597 - val_loss: 97.4655\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0213 - val_loss: 93.9441\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9390 - val_loss: 102.9811\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9416 - val_loss: 110.8841\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.7970 - val_loss: 107.6425\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9641 - val_loss: 103.4917\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1408 - val_loss: 99.3680\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8123 - val_loss: 121.9336\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9324 - val_loss: 93.9889\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9901 - val_loss: 91.5130\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0329 - val_loss: 95.0342\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7849 - val_loss: 97.6204\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8458 - val_loss: 96.2239\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7905 - val_loss: 99.2554\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7858 - val_loss: 91.1679\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8307 - val_loss: 96.3865\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8656 - val_loss: 142.1686\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8180 - val_loss: 92.6941\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.8427 - val_loss: 93.6050\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.8339 - val_loss: 100.3216\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7127 - val_loss: 105.4334\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8024 - val_loss: 93.5760\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7140 - val_loss: 99.7305\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6523 - val_loss: 94.9631\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.7164 - val_loss: 126.8533\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.8682 - val_loss: 91.0948\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4776 - val_loss: 95.3706\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5943 - val_loss: 88.1654\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.7022 - val_loss: 92.3640\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7956 - val_loss: 97.2208\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7746 - val_loss: 109.7201\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.7718 - val_loss: 93.6054\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7297 - val_loss: 100.7911\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6368 - val_loss: 94.5466\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6669 - val_loss: 105.6542\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5489 - val_loss: 95.5433\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.5564 - val_loss: 111.9710\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5960 - val_loss: 99.9009\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5746 - val_loss: 92.1483\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5652 - val_loss: 125.9472\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6987 - val_loss: 100.2559\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6880 - val_loss: 91.5355\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5420 - val_loss: 92.1454\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.6149 - val_loss: 109.4373\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5031 - val_loss: 89.8428\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6193 - val_loss: 102.0945\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.4786 - val_loss: 89.6369\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.6774 - val_loss: 97.3370\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6214 - val_loss: 101.3807\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6027 - val_loss: 100.4733\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.5139 - val_loss: 105.1672\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.5356 - val_loss: 93.3969\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5415 - val_loss: 123.0421\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3748 - val_loss: 95.9198\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.4994 - val_loss: 95.4092\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4140 - val_loss: 92.1223\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.6685 - val_loss: 99.0167\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3762 - val_loss: 106.3434\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4248 - val_loss: 107.4381\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3942 - val_loss: 94.6421\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5676 - val_loss: 90.4239\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.5382 - val_loss: 94.0386\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4151 - val_loss: 104.5856\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.5658 - val_loss: 92.6580\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3755 - val_loss: 100.1476\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3903 - val_loss: 89.6401\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3723 - val_loss: 92.1249\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2766 - val_loss: 95.4155\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2724 - val_loss: 86.8511\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3595 - val_loss: 101.2605\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2234 - val_loss: 105.5840\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3752 - val_loss: 96.8425\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3029 - val_loss: 101.7666\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5025 - val_loss: 124.3969\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3913 - val_loss: 115.2474\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2824 - val_loss: 93.8954\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3893 - val_loss: 104.7932\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2981 - val_loss: 99.8097\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5043 - val_loss: 89.8633\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2100 - val_loss: 88.3696\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3662 - val_loss: 94.5581\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3358 - val_loss: 94.3421\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2649 - val_loss: 93.7669\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3741 - val_loss: 99.5014\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1906 - val_loss: 102.1001\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 73.3483 - val_loss: 98.7180\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4393 - val_loss: 89.7387\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3330 - val_loss: 92.5285\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3242 - val_loss: 89.1041\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1695 - val_loss: 94.6002\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4219 - val_loss: 104.4616\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3754 - val_loss: 101.7188\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1383 - val_loss: 122.2948\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3911 - val_loss: 92.4133\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3634 - val_loss: 109.6215\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3475 - val_loss: 127.6720\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1390 - val_loss: 93.6995\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2659 - val_loss: 106.7857\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2599 - val_loss: 105.6427\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3123 - val_loss: 102.0477\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3280 - val_loss: 102.6001\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3178 - val_loss: 91.6815\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1460 - val_loss: 99.3627\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3447 - val_loss: 92.1398\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1692 - val_loss: 103.7086\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3489 - val_loss: 104.5482\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2578 - val_loss: 99.6827\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3156 - val_loss: 112.4856\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2835 - val_loss: 102.6710\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1206 - val_loss: 117.3852\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1588 - val_loss: 90.6702\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3331 - val_loss: 192.1548\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3079 - val_loss: 95.0490\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0564 - val_loss: 91.0438\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3167 - val_loss: 97.9347\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3226 - val_loss: 106.9839\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2177 - val_loss: 95.6141\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.0613 - val_loss: 91.7557\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2852 - val_loss: 90.2398\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1652 - val_loss: 94.1326\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0740 - val_loss: 103.1623\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2911 - val_loss: 87.1853\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1630 - val_loss: 105.6328\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2310 - val_loss: 109.2443\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1735 - val_loss: 94.6799\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2053 - val_loss: 101.3992\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.0961 - val_loss: 93.9641\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2662 - val_loss: 123.7447\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1849 - val_loss: 116.1564\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0925 - val_loss: 95.2902\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2674 - val_loss: 100.3303\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.0771 - val_loss: 92.8846\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1909 - val_loss: 99.4719\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.0291 - val_loss: 92.1418\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1636 - val_loss: 94.8307\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0530 - val_loss: 91.5475\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2123 - val_loss: 88.7902\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2306 - val_loss: 93.5930\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.0350 - val_loss: 119.1845\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1201 - val_loss: 107.9062\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1173 - val_loss: 96.3366\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2462 - val_loss: 95.7240\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1829 - val_loss: 91.4935\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0734 - val_loss: 94.9961\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3380 - val_loss: 95.5622\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.9394 - val_loss: 96.2417\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1787 - val_loss: 100.1926\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 73.1923 - val_loss: 124.3090\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1304 - val_loss: 91.8064\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.9608 - val_loss: 93.9463\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 73.2526 - val_loss: 93.8735\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.0230 - val_loss: 98.1653\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1025 - val_loss: 93.1073\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0439 - val_loss: 103.0453\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "696v_fuFCTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148df601-165b-4ca2-c6f5-5d4f5a0e4122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  3.7445963574363588 \n",
            "MAE:  7.585457474398376 \n",
            "SD:  9.435216235328273\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "c3a706e7-eaea-47dc-b53d-f9aa48b05cbb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mdZF2osWCUQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0128436b-143a-4039-aa2c-e96c9be5df21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -0.058830304214483764 \n",
            "Ensemble_std:  9.646119426993076\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d94cc72-aa80-4e60-9d3e-6208a40aa604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,393\n",
            "Trainable params: 2,329\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea218160-984e-4147-e00d-103cb3f679c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 3590.3677 - val_loss: 3690.4373\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3318.6860 - val_loss: 3408.3281\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2977.6753 - val_loss: 2751.4497\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2574.4783 - val_loss: 2372.3281\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2134.0498 - val_loss: 1746.7018\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1687.1464 - val_loss: 1154.9036\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1260.5338 - val_loss: 1205.8855\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 872.3168 - val_loss: 1230.3439\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 565.5339 - val_loss: 1023.9561\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 343.1076 - val_loss: 311.1629\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 197.2868 - val_loss: 147.8754\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 113.2115 - val_loss: 129.5150\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.5520 - val_loss: 132.0463\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 50.1288 - val_loss: 75.9487\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 42.1315 - val_loss: 66.1684\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.7618 - val_loss: 51.4172\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.2127 - val_loss: 46.3677\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9916 - val_loss: 95.9015\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4504 - val_loss: 71.3932\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.1047 - val_loss: 43.6755\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7847 - val_loss: 39.7302\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5831 - val_loss: 48.8102\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2958 - val_loss: 42.0415\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9667 - val_loss: 42.2619\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9641 - val_loss: 41.0886\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9160 - val_loss: 50.1446\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5279 - val_loss: 41.0920\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3587 - val_loss: 40.9590\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3358 - val_loss: 71.3369\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1003 - val_loss: 56.1545\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7832 - val_loss: 43.1552\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7692 - val_loss: 46.6751\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7207 - val_loss: 44.9698\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5997 - val_loss: 44.0991\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4806 - val_loss: 40.5818\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4575 - val_loss: 40.9399\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2171 - val_loss: 40.6763\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.0714 - val_loss: 41.1318\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0057 - val_loss: 45.3346\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0166 - val_loss: 42.0061\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8520 - val_loss: 40.3520\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7822 - val_loss: 42.9235\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6247 - val_loss: 56.1499\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6034 - val_loss: 43.6250\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5330 - val_loss: 40.4591\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5041 - val_loss: 40.0497\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4012 - val_loss: 40.1433\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1821 - val_loss: 40.8559\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2330 - val_loss: 47.1655\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1955 - val_loss: 46.4316\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0739 - val_loss: 44.2319\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1672 - val_loss: 44.0044\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1015 - val_loss: 44.3085\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9172 - val_loss: 39.0720\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0311 - val_loss: 43.9994\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8745 - val_loss: 37.5723\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8374 - val_loss: 51.9131\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6605 - val_loss: 44.7041\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6827 - val_loss: 39.8866\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7411 - val_loss: 46.2710\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.6146 - val_loss: 47.5790\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6128 - val_loss: 46.5430\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5155 - val_loss: 40.0565\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5079 - val_loss: 52.1108\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5077 - val_loss: 40.8081\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.4448 - val_loss: 44.2834\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4989 - val_loss: 37.3554\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3867 - val_loss: 42.2220\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3231 - val_loss: 50.9947\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2909 - val_loss: 39.8121\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.3120 - val_loss: 44.1362\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3089 - val_loss: 38.9981\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2839 - val_loss: 46.6525\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2514 - val_loss: 40.3058\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1677 - val_loss: 39.0612\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1126 - val_loss: 43.4740\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1665 - val_loss: 37.7815\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1586 - val_loss: 62.3659\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0833 - val_loss: 45.3862\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0790 - val_loss: 39.8638\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0446 - val_loss: 41.9203\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0011 - val_loss: 36.0208\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0225 - val_loss: 41.9840\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9525 - val_loss: 37.8732\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9070 - val_loss: 53.7756\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9232 - val_loss: 37.3076\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8958 - val_loss: 40.3284\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8901 - val_loss: 38.5174\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8128 - val_loss: 37.3380\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7602 - val_loss: 37.5269\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6582 - val_loss: 36.5237\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7239 - val_loss: 38.8814\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7405 - val_loss: 41.4579\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6476 - val_loss: 47.7536\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6123 - val_loss: 41.1385\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6033 - val_loss: 42.8186\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6846 - val_loss: 37.4642\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5341 - val_loss: 41.9852\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5975 - val_loss: 38.7485\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5174 - val_loss: 48.6627\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5027 - val_loss: 42.1589\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.4884 - val_loss: 36.4355\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5409 - val_loss: 43.9026\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3872 - val_loss: 39.4827\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3827 - val_loss: 37.4783\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.4691 - val_loss: 40.3592\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3147 - val_loss: 36.9047\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3454 - val_loss: 37.2071\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3759 - val_loss: 44.3283\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2907 - val_loss: 38.3830\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2621 - val_loss: 42.5410\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2125 - val_loss: 43.5674\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2390 - val_loss: 38.6848\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1394 - val_loss: 42.2876\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1126 - val_loss: 37.7379\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1425 - val_loss: 40.4842\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1372 - val_loss: 39.0873\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1286 - val_loss: 37.6419\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.0317 - val_loss: 36.0723\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1015 - val_loss: 41.5774\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.9639 - val_loss: 45.6703\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0272 - val_loss: 39.5737\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0139 - val_loss: 36.0661\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9735 - val_loss: 46.6577\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9350 - val_loss: 39.2601\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9536 - val_loss: 34.8779\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.9187 - val_loss: 40.3099\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9428 - val_loss: 42.4318\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8954 - val_loss: 45.1073\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8783 - val_loss: 37.8933\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.8112 - val_loss: 37.6305\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8104 - val_loss: 41.0065\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8518 - val_loss: 36.8449\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7934 - val_loss: 42.9385\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7764 - val_loss: 39.2890\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7810 - val_loss: 36.5012\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7886 - val_loss: 38.1107\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.7463 - val_loss: 38.2774\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.7092 - val_loss: 37.3069\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.7461 - val_loss: 38.0351\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7497 - val_loss: 37.6772\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7312 - val_loss: 38.0354\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7378 - val_loss: 38.5058\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6631 - val_loss: 39.0287\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6497 - val_loss: 48.2610\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6386 - val_loss: 39.4101\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6844 - val_loss: 50.4700\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6571 - val_loss: 36.6731\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6199 - val_loss: 42.4439\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6398 - val_loss: 45.8607\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5528 - val_loss: 39.8394\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6240 - val_loss: 50.2196\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5797 - val_loss: 42.6497\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6060 - val_loss: 40.7259\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5352 - val_loss: 36.4897\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5298 - val_loss: 40.0144\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5378 - val_loss: 38.2819\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5120 - val_loss: 37.9400\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5582 - val_loss: 40.6263\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5376 - val_loss: 42.2077\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5365 - val_loss: 37.3428\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4140 - val_loss: 35.4861\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4133 - val_loss: 40.0206\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4323 - val_loss: 34.9437\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4406 - val_loss: 45.3297\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3991 - val_loss: 37.4669\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4201 - val_loss: 35.1106\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4695 - val_loss: 37.9217\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3733 - val_loss: 37.8599\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.4007 - val_loss: 37.3144\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.3470 - val_loss: 34.4437\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3592 - val_loss: 39.9647\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3171 - val_loss: 43.9308\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3572 - val_loss: 38.5847\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.3476 - val_loss: 36.5752\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3366 - val_loss: 39.0273\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3903 - val_loss: 36.5835\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3160 - val_loss: 43.8524\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3814 - val_loss: 39.2646\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3603 - val_loss: 42.4798\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.3526 - val_loss: 40.0880\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3245 - val_loss: 39.9270\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3155 - val_loss: 39.0000\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2597 - val_loss: 36.1240\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3185 - val_loss: 47.0510\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2922 - val_loss: 35.1642\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2644 - val_loss: 36.7853\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2923 - val_loss: 35.7379\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3051 - val_loss: 36.7743\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2792 - val_loss: 36.2823\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2997 - val_loss: 38.2959\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2410 - val_loss: 38.3139\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2692 - val_loss: 35.1571\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2343 - val_loss: 36.0552\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2405 - val_loss: 36.3239\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2472 - val_loss: 36.7018\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1932 - val_loss: 39.2914\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2121 - val_loss: 41.1734\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3283 - val_loss: 35.8498\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2164 - val_loss: 38.6216\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2016 - val_loss: 44.9728\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1719 - val_loss: 39.7641\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2643 - val_loss: 35.2555\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2135 - val_loss: 41.8211\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2307 - val_loss: 35.4552\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1692 - val_loss: 38.7328\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2220 - val_loss: 43.4798\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1742 - val_loss: 36.7971\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1120 - val_loss: 47.4921\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1701 - val_loss: 38.7977\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2406 - val_loss: 35.3752\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0294 - val_loss: 40.6583\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1458 - val_loss: 37.6347\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1818 - val_loss: 47.4635\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1019 - val_loss: 35.7149\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1416 - val_loss: 35.6980\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1664 - val_loss: 38.0574\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1471 - val_loss: 34.3945\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1844 - val_loss: 40.5789\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0875 - val_loss: 35.0468\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0469 - val_loss: 35.9398\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1094 - val_loss: 44.1673\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0453 - val_loss: 37.6011\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0676 - val_loss: 46.6051\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0745 - val_loss: 38.2034\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.0407 - val_loss: 34.4550\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0099 - val_loss: 34.3767\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0752 - val_loss: 34.0324\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0794 - val_loss: 36.7881\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0490 - val_loss: 47.1135\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9825 - val_loss: 38.6478\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0680 - val_loss: 35.3858\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0439 - val_loss: 36.1537\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9775 - val_loss: 37.9576\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0534 - val_loss: 36.4357\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9945 - val_loss: 40.4363\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9916 - val_loss: 36.1992\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9918 - val_loss: 35.2130\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9703 - val_loss: 41.1868\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9963 - val_loss: 35.0767\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9422 - val_loss: 43.7225\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9827 - val_loss: 35.3027\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8849 - val_loss: 35.3645\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.9520 - val_loss: 40.6915\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 29.0149 - val_loss: 46.5704\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9418 - val_loss: 36.1581\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9665 - val_loss: 41.5468\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8877 - val_loss: 44.9839\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9564 - val_loss: 37.8369\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9022 - val_loss: 35.8668\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9150 - val_loss: 36.5173\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9865 - val_loss: 56.7089\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8504 - val_loss: 38.2309\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8734 - val_loss: 37.4924\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8732 - val_loss: 36.4098\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8927 - val_loss: 45.2176\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9278 - val_loss: 35.2009\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9343 - val_loss: 36.9274\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9536 - val_loss: 33.8431\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8749 - val_loss: 34.3560\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8332 - val_loss: 43.9379\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8333 - val_loss: 41.4305\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9125 - val_loss: 38.6541\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8598 - val_loss: 36.5396\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8577 - val_loss: 36.6721\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8918 - val_loss: 39.0984\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8056 - val_loss: 41.5352\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8571 - val_loss: 38.7770\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7953 - val_loss: 46.0988\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8010 - val_loss: 38.1154\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8157 - val_loss: 39.2683\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7965 - val_loss: 37.6000\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8110 - val_loss: 34.3543\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8102 - val_loss: 34.7457\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8698 - val_loss: 34.1417\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8186 - val_loss: 42.7267\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8425 - val_loss: 45.9588\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8098 - val_loss: 44.3841\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8412 - val_loss: 37.0307\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8230 - val_loss: 38.3051\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7828 - val_loss: 33.9154\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7909 - val_loss: 37.4264\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8185 - val_loss: 38.1171\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7555 - val_loss: 38.2480\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7807 - val_loss: 38.3812\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7987 - val_loss: 34.7401\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7111 - val_loss: 40.4208\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7946 - val_loss: 42.4760\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7624 - val_loss: 34.7811\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7892 - val_loss: 35.9580\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7786 - val_loss: 36.0453\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6959 - val_loss: 41.7968\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7342 - val_loss: 37.0997\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7619 - val_loss: 44.0697\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7785 - val_loss: 49.0043\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6998 - val_loss: 35.6143\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7094 - val_loss: 35.1151\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7130 - val_loss: 34.8936\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7781 - val_loss: 42.6802\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6928 - val_loss: 43.4509\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6790 - val_loss: 36.0206\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7682 - val_loss: 41.4889\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6857 - val_loss: 38.4979\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6968 - val_loss: 41.3601\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6985 - val_loss: 35.6775\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7039 - val_loss: 38.0002\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6807 - val_loss: 38.7385\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7239 - val_loss: 43.7117\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6620 - val_loss: 34.8513\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6483 - val_loss: 38.2792\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6610 - val_loss: 37.3349\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6120 - val_loss: 39.5704\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6131 - val_loss: 40.0133\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5898 - val_loss: 38.9912\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6482 - val_loss: 36.3974\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5698 - val_loss: 40.8024\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6528 - val_loss: 35.3975\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6837 - val_loss: 35.4708\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6295 - val_loss: 37.1063\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6719 - val_loss: 41.7198\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5945 - val_loss: 36.1163\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5916 - val_loss: 38.2070\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6237 - val_loss: 42.2147\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5979 - val_loss: 39.0243\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6072 - val_loss: 33.5947\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5726 - val_loss: 37.3600\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5883 - val_loss: 35.9137\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6066 - val_loss: 45.1108\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5774 - val_loss: 33.9406\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5374 - val_loss: 34.7142\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5609 - val_loss: 44.4117\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5893 - val_loss: 35.2951\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5322 - val_loss: 36.9419\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5674 - val_loss: 35.5886\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5705 - val_loss: 34.2973\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5519 - val_loss: 34.3917\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5200 - val_loss: 37.9662\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5503 - val_loss: 38.2950\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5071 - val_loss: 40.2871\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5492 - val_loss: 34.5686\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4736 - val_loss: 34.1475\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5249 - val_loss: 34.4629\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5306 - val_loss: 40.8070\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5555 - val_loss: 35.1617\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5223 - val_loss: 34.7071\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4791 - val_loss: 38.4504\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4691 - val_loss: 34.5240\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5546 - val_loss: 37.8226\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5810 - val_loss: 34.3934\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4915 - val_loss: 36.1207\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5383 - val_loss: 43.2309\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4949 - val_loss: 34.4115\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5370 - val_loss: 35.6901\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4697 - val_loss: 34.2168\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4965 - val_loss: 36.5273\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4448 - val_loss: 35.2984\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4550 - val_loss: 37.2879\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4833 - val_loss: 36.3527\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4199 - val_loss: 35.4526\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4798 - val_loss: 37.1979\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5611 - val_loss: 33.4207\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5136 - val_loss: 33.3540\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4606 - val_loss: 35.6918\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4686 - val_loss: 43.2318\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4556 - val_loss: 43.0577\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.4620 - val_loss: 34.6398\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4245 - val_loss: 36.0047\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4177 - val_loss: 34.6052\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4064 - val_loss: 42.4098\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4320 - val_loss: 36.4598\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4130 - val_loss: 38.2320\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4604 - val_loss: 35.1671\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4187 - val_loss: 39.0793\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4072 - val_loss: 33.9786\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3486 - val_loss: 37.1962\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4059 - val_loss: 38.5383\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3754 - val_loss: 33.7045\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4265 - val_loss: 37.5381\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3589 - val_loss: 39.7118\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4239 - val_loss: 38.8700\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4616 - val_loss: 36.9381\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3457 - val_loss: 47.1488\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3242 - val_loss: 36.9805\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3707 - val_loss: 39.9821\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3865 - val_loss: 36.8193\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4030 - val_loss: 40.4133\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3981 - val_loss: 36.5640\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3609 - val_loss: 39.5513\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3501 - val_loss: 39.7237\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3916 - val_loss: 44.4841\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4066 - val_loss: 37.2766\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3693 - val_loss: 36.8634\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3939 - val_loss: 35.0974\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3994 - val_loss: 33.7495\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3690 - val_loss: 38.5355\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4131 - val_loss: 35.9226\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3170 - val_loss: 38.5047\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2741 - val_loss: 44.4327\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3775 - val_loss: 33.2364\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2797 - val_loss: 36.2582\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3433 - val_loss: 36.0147\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3216 - val_loss: 36.1111\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2837 - val_loss: 36.0339\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3044 - val_loss: 38.1118\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3263 - val_loss: 44.3531\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3019 - val_loss: 35.8470\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3454 - val_loss: 33.5826\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2950 - val_loss: 34.3977\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2936 - val_loss: 36.8237\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2950 - val_loss: 39.8453\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3171 - val_loss: 42.7234\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2609 - val_loss: 33.4169\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2888 - val_loss: 36.6359\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3176 - val_loss: 34.7776\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2720 - val_loss: 34.5728\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2663 - val_loss: 36.5896\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3233 - val_loss: 35.2211\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2743 - val_loss: 39.0599\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2198 - val_loss: 43.3788\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2429 - val_loss: 40.1138\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2470 - val_loss: 35.7491\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2717 - val_loss: 37.2219\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2371 - val_loss: 34.7338\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3058 - val_loss: 35.6218\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2398 - val_loss: 36.1862\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2616 - val_loss: 37.3988\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2739 - val_loss: 39.3841\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2216 - val_loss: 36.5017\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2069 - val_loss: 36.1936\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2643 - val_loss: 42.8461\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2411 - val_loss: 39.7261\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1957 - val_loss: 34.8960\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.2812 - val_loss: 36.6241\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2077 - val_loss: 38.7781\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2433 - val_loss: 38.4945\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1978 - val_loss: 38.7543\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1807 - val_loss: 38.2935\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2069 - val_loss: 37.9696\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1658 - val_loss: 34.9028\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1807 - val_loss: 45.8670\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.2362 - val_loss: 35.7135\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1958 - val_loss: 35.4033\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1752 - val_loss: 41.6059\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2101 - val_loss: 38.1761\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1897 - val_loss: 34.6786\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1590 - val_loss: 35.8751\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1508 - val_loss: 42.4650\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2243 - val_loss: 36.2470\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1773 - val_loss: 40.3600\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1245 - val_loss: 38.5289\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2466 - val_loss: 35.1040\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1380 - val_loss: 33.9127\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2185 - val_loss: 36.3079\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1385 - val_loss: 40.7569\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1753 - val_loss: 48.8866\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1389 - val_loss: 45.2591\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1330 - val_loss: 37.8734\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1322 - val_loss: 48.2769\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1189 - val_loss: 37.8408\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1511 - val_loss: 38.2094\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1537 - val_loss: 39.2278\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1067 - val_loss: 35.0100\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1380 - val_loss: 40.8316\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1188 - val_loss: 33.3372\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1667 - val_loss: 37.4374\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1329 - val_loss: 41.0392\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1440 - val_loss: 39.6537\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1572 - val_loss: 33.6027\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1338 - val_loss: 32.9033\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9888 - val_loss: 35.3912\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0539 - val_loss: 34.9355\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1045 - val_loss: 43.6833\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1635 - val_loss: 44.5079\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1213 - val_loss: 33.5596\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0672 - val_loss: 39.7893\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1575 - val_loss: 37.0345\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0936 - val_loss: 38.3859\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0348 - val_loss: 39.0995\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0639 - val_loss: 34.4413\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0746 - val_loss: 34.4906\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0431 - val_loss: 35.1878\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0896 - val_loss: 38.2578\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0708 - val_loss: 33.8564\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0646 - val_loss: 35.4057\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0608 - val_loss: 34.5369\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0808 - val_loss: 51.3036\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1005 - val_loss: 38.0764\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0695 - val_loss: 39.1214\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0303 - val_loss: 39.3829\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0631 - val_loss: 35.2942\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9962 - val_loss: 38.3272\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9912 - val_loss: 36.0817\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0514 - val_loss: 37.7202\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0877 - val_loss: 38.4947\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0139 - val_loss: 34.6968\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0654 - val_loss: 36.6605\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0184 - val_loss: 41.8899\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0696 - val_loss: 38.7623\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9860 - val_loss: 36.5000\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0608 - val_loss: 34.9947\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9e7405-c21d-40ce-d007-7d3c0bfc941b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.9595752618989815 \n",
            "MAE:  4.337255360420766 \n",
            "SD:  5.837284286192048\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0cip38xZOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "15ac57cc-f06e-40bc-b42d-cfd1b0f4a52d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26afdfea-0db0-4d2b-cceb-a11c5ccdee5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_16 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,393\n",
            "Trainable params: 2,329\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7aadb34-fde8-46da-e997-a31d62c50cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 3823.4409 - val_loss: 3737.8037\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3674.3965 - val_loss: 3612.2415\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3553.3789 - val_loss: 3524.9102\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3411.5308 - val_loss: 3189.1309\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 3246.8745 - val_loss: 2887.3667\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3063.5535 - val_loss: 2669.4883\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2864.2415 - val_loss: 2431.8213\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2614.8735 - val_loss: 2553.0432\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2324.6318 - val_loss: 2375.0081\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2016.7677 - val_loss: 2017.2460\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1710.4373 - val_loss: 1421.4757\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1414.7242 - val_loss: 942.9185\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1141.2053 - val_loss: 924.8093\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 900.9829 - val_loss: 941.3514\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 694.5364 - val_loss: 645.8783\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 522.3002 - val_loss: 551.2115\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 382.8742 - val_loss: 543.9614\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 274.1725 - val_loss: 257.5192\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 192.7225 - val_loss: 333.0656\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 134.2925 - val_loss: 152.3565\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.7053 - val_loss: 57.9317\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.3742 - val_loss: 104.7922\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 53.6919 - val_loss: 68.7193\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 45.0807 - val_loss: 61.5734\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.3586 - val_loss: 62.0728\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.9861 - val_loss: 48.5492\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.8321 - val_loss: 53.5912\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.1452 - val_loss: 53.1364\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.8948 - val_loss: 52.0602\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7291 - val_loss: 105.1689\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4252 - val_loss: 50.9513\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2253 - val_loss: 45.6142\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3128 - val_loss: 40.9640\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0200 - val_loss: 39.0759\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0090 - val_loss: 43.3942\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9868 - val_loss: 50.2097\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6695 - val_loss: 47.7812\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7410 - val_loss: 44.7692\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6440 - val_loss: 48.9317\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5438 - val_loss: 42.2393\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4100 - val_loss: 60.9318\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4335 - val_loss: 44.9481\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.2080 - val_loss: 45.4925\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.1785 - val_loss: 40.1543\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1620 - val_loss: 51.4720\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9883 - val_loss: 47.0285\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9628 - val_loss: 44.1662\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8657 - val_loss: 41.5859\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8162 - val_loss: 43.3130\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6746 - val_loss: 48.3597\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6228 - val_loss: 43.1970\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4458 - val_loss: 40.4183\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5651 - val_loss: 48.2034\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3737 - val_loss: 52.0208\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.3719 - val_loss: 40.3739\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1663 - val_loss: 51.7744\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.1670 - val_loss: 42.9392\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.0859 - val_loss: 43.2467\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0731 - val_loss: 47.0346\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9620 - val_loss: 38.4680\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9063 - val_loss: 47.3437\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7415 - val_loss: 37.7654\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7566 - val_loss: 43.6672\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6002 - val_loss: 47.0369\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7043 - val_loss: 43.7400\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5910 - val_loss: 42.1859\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5333 - val_loss: 45.6545\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5208 - val_loss: 42.9419\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3566 - val_loss: 42.2081\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3596 - val_loss: 41.5960\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3172 - val_loss: 38.7075\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3416 - val_loss: 38.1745\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1732 - val_loss: 41.9955\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1672 - val_loss: 52.4871\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1563 - val_loss: 41.8517\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0908 - val_loss: 38.9963\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9838 - val_loss: 37.2596\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9288 - val_loss: 41.6516\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0452 - val_loss: 40.1564\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8174 - val_loss: 36.9818\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9589 - val_loss: 47.7179\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7124 - val_loss: 37.3171\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7953 - val_loss: 36.0288\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.6809 - val_loss: 48.9285\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6854 - val_loss: 34.6884\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5823 - val_loss: 35.4997\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5071 - val_loss: 34.8487\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5754 - val_loss: 36.3220\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4481 - val_loss: 40.9513\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.4782 - val_loss: 44.5845\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4019 - val_loss: 42.7367\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3158 - val_loss: 45.0512\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3042 - val_loss: 41.6254\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2597 - val_loss: 38.3683\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2588 - val_loss: 41.3145\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2076 - val_loss: 36.5888\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1330 - val_loss: 35.8454\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2858 - val_loss: 57.0499\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1132 - val_loss: 41.5417\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0867 - val_loss: 39.9038\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1291 - val_loss: 40.1161\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.9825 - val_loss: 44.4125\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0964 - val_loss: 39.4073\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0725 - val_loss: 43.3921\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0592 - val_loss: 35.6221\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9949 - val_loss: 42.8268\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0138 - val_loss: 35.3210\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9508 - val_loss: 38.8474\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.0162 - val_loss: 40.0827\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9444 - val_loss: 37.2818\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8622 - val_loss: 53.8713\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8121 - val_loss: 38.5000\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.8285 - val_loss: 51.7660\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.8029 - val_loss: 35.2720\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 30.8058 - val_loss: 44.4214\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8219 - val_loss: 35.3677\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8106 - val_loss: 60.4502\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8494 - val_loss: 40.2175\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7928 - val_loss: 34.5246\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7238 - val_loss: 38.6614\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7158 - val_loss: 37.1530\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6821 - val_loss: 36.6023\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6825 - val_loss: 42.5007\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6533 - val_loss: 38.3488\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6698 - val_loss: 40.5301\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7012 - val_loss: 51.5084\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6678 - val_loss: 45.2985\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5757 - val_loss: 35.8216\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5689 - val_loss: 37.7451\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5937 - val_loss: 38.7042\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5423 - val_loss: 39.0306\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5912 - val_loss: 35.9874\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5089 - val_loss: 35.1413\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5586 - val_loss: 36.9871\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5446 - val_loss: 35.1496\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.3739 - val_loss: 35.9684\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.4855 - val_loss: 39.3507\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.4560 - val_loss: 40.8271\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3778 - val_loss: 36.8853\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3330 - val_loss: 43.7730\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3624 - val_loss: 40.9316\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3600 - val_loss: 36.4628\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3984 - val_loss: 38.7945\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3731 - val_loss: 53.0844\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3595 - val_loss: 40.2645\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3069 - val_loss: 37.2992\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2892 - val_loss: 38.0111\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2758 - val_loss: 45.2143\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2949 - val_loss: 37.7769\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2517 - val_loss: 38.6897\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2210 - val_loss: 37.0720\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1671 - val_loss: 39.1585\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2701 - val_loss: 39.8635\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1121 - val_loss: 45.8162\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1687 - val_loss: 35.3351\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2318 - val_loss: 38.1319\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1436 - val_loss: 38.4932\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1604 - val_loss: 35.8742\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0846 - val_loss: 39.3039\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0817 - val_loss: 37.5494\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1265 - val_loss: 35.9249\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0857 - val_loss: 39.7371\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9900 - val_loss: 37.4495\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.0373 - val_loss: 35.1425\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.0311 - val_loss: 38.7191\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0169 - val_loss: 44.2968\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.9916 - val_loss: 39.2591\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9044 - val_loss: 38.9037\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9860 - val_loss: 47.8651\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9249 - val_loss: 38.1990\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9334 - val_loss: 37.0427\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9374 - val_loss: 43.2399\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.8256 - val_loss: 45.4270\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8995 - val_loss: 35.3650\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.8879 - val_loss: 39.2444\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8290 - val_loss: 38.3714\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8983 - val_loss: 41.3162\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8851 - val_loss: 39.2400\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.7863 - val_loss: 37.5625\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.7980 - val_loss: 37.4269\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7368 - val_loss: 35.2435\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8074 - val_loss: 37.9371\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7736 - val_loss: 36.1502\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7115 - val_loss: 37.0876\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7477 - val_loss: 39.2662\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.7511 - val_loss: 35.3290\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7630 - val_loss: 37.9311\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6397 - val_loss: 56.6692\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6233 - val_loss: 35.4869\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5918 - val_loss: 36.6171\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.6865 - val_loss: 42.4814\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6296 - val_loss: 37.9392\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6672 - val_loss: 55.8097\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.6453 - val_loss: 38.0021\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6182 - val_loss: 39.4687\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6355 - val_loss: 42.1530\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.6233 - val_loss: 38.5944\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5995 - val_loss: 36.0774\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5888 - val_loss: 36.6493\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.6181 - val_loss: 35.2999\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5682 - val_loss: 40.6227\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5548 - val_loss: 35.4496\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5918 - val_loss: 35.4722\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5078 - val_loss: 41.2234\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5669 - val_loss: 40.1770\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5448 - val_loss: 36.2966\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6036 - val_loss: 33.8273\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5694 - val_loss: 49.4037\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5713 - val_loss: 43.6433\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5119 - val_loss: 38.2628\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5275 - val_loss: 36.7437\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4863 - val_loss: 38.8433\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4630 - val_loss: 35.1256\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4947 - val_loss: 37.2319\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4031 - val_loss: 36.2749\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3913 - val_loss: 41.3939\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.4581 - val_loss: 37.8968\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4590 - val_loss: 34.8721\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4566 - val_loss: 39.3034\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4160 - val_loss: 47.4797\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3968 - val_loss: 39.1710\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3835 - val_loss: 38.7573\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4164 - val_loss: 43.9562\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.4886 - val_loss: 37.9495\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3912 - val_loss: 36.0980\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4350 - val_loss: 39.9314\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.4042 - val_loss: 35.2379\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.3609 - val_loss: 39.5416\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3531 - val_loss: 36.8564\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3642 - val_loss: 35.9399\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3524 - val_loss: 35.0322\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3722 - val_loss: 44.0177\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3272 - val_loss: 35.0971\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3227 - val_loss: 39.3844\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2892 - val_loss: 35.6796\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2974 - val_loss: 35.5138\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3152 - val_loss: 36.3502\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3276 - val_loss: 38.7151\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2795 - val_loss: 40.6320\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2956 - val_loss: 41.5053\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2525 - val_loss: 46.1201\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3336 - val_loss: 36.2227\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2468 - val_loss: 36.6650\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3629 - val_loss: 38.5786\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3376 - val_loss: 37.7714\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3103 - val_loss: 41.8295\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2433 - val_loss: 39.0074\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2686 - val_loss: 37.9103\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2697 - val_loss: 36.3247\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2253 - val_loss: 38.9725\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2224 - val_loss: 41.5708\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1578 - val_loss: 37.2399\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2132 - val_loss: 35.9165\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2677 - val_loss: 44.5272\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2462 - val_loss: 35.8091\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1322 - val_loss: 43.9001\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1866 - val_loss: 35.0175\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1638 - val_loss: 46.2657\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1716 - val_loss: 45.1887\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1499 - val_loss: 39.4239\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2608 - val_loss: 36.6182\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1148 - val_loss: 38.2923\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1215 - val_loss: 34.5211\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 29.1325 - val_loss: 36.0785\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1070 - val_loss: 39.5793\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1074 - val_loss: 39.4675\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0441 - val_loss: 36.5522\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1938 - val_loss: 41.3755\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1318 - val_loss: 37.6609\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1084 - val_loss: 40.2672\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1071 - val_loss: 43.7213\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.0766 - val_loss: 45.3064\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1001 - val_loss: 40.1806\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1720 - val_loss: 37.3641\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0383 - val_loss: 42.9209\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1220 - val_loss: 48.3907\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1133 - val_loss: 42.5326\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 29.0886 - val_loss: 35.5336\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.0819 - val_loss: 34.8271\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0379 - val_loss: 36.9625\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1779 - val_loss: 36.8033\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9950 - val_loss: 37.3464\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.0441 - val_loss: 44.2939\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0550 - val_loss: 36.8783\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9699 - val_loss: 36.6641\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9414 - val_loss: 34.8381\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9791 - val_loss: 39.0130\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9082 - val_loss: 35.1889\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9879 - val_loss: 36.4620\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.0376 - val_loss: 39.7164\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9998 - val_loss: 37.7200\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9913 - val_loss: 36.1727\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9318 - val_loss: 39.3369\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9058 - val_loss: 34.5902\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9945 - val_loss: 36.4670\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9660 - val_loss: 42.2402\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9931 - val_loss: 34.3631\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8965 - val_loss: 36.1626\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9607 - val_loss: 39.1952\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9396 - val_loss: 37.3157\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9508 - val_loss: 35.5631\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0046 - val_loss: 39.7452\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9060 - val_loss: 39.1964\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8868 - val_loss: 43.0107\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8692 - val_loss: 44.9910\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9970 - val_loss: 34.2937\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8873 - val_loss: 61.7408\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9996 - val_loss: 35.1157\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9109 - val_loss: 56.0985\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9104 - val_loss: 41.2314\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8801 - val_loss: 36.9343\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9402 - val_loss: 35.5893\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8673 - val_loss: 42.8235\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8743 - val_loss: 40.0597\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8844 - val_loss: 37.4359\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8166 - val_loss: 41.7889\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.9407 - val_loss: 39.1650\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8664 - val_loss: 42.0269\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8676 - val_loss: 38.4408\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8452 - val_loss: 37.6465\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8323 - val_loss: 35.7431\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8557 - val_loss: 34.4836\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9014 - val_loss: 36.5027\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8136 - val_loss: 37.3505\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8426 - val_loss: 37.1270\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8032 - val_loss: 37.4255\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7825 - val_loss: 37.4538\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8635 - val_loss: 37.6059\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7997 - val_loss: 40.2186\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7825 - val_loss: 39.3999\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8322 - val_loss: 40.6206\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8201 - val_loss: 36.2063\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8478 - val_loss: 44.0610\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8064 - val_loss: 36.4048\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8019 - val_loss: 35.6421\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8028 - val_loss: 33.8396\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7494 - val_loss: 39.2255\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8046 - val_loss: 45.8877\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7687 - val_loss: 40.1235\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7849 - val_loss: 46.2208\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8105 - val_loss: 40.1532\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8286 - val_loss: 41.9249\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8354 - val_loss: 36.3450\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6975 - val_loss: 34.5339\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8435 - val_loss: 39.4598\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7222 - val_loss: 36.2799\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8484 - val_loss: 37.9132\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7857 - val_loss: 42.8409\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7963 - val_loss: 35.6387\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7980 - val_loss: 38.2608\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6950 - val_loss: 36.1465\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7253 - val_loss: 35.7807\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7840 - val_loss: 39.1107\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7324 - val_loss: 39.9543\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7334 - val_loss: 34.3795\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7756 - val_loss: 36.6506\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7036 - val_loss: 34.3348\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7401 - val_loss: 37.0876\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7130 - val_loss: 40.9062\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6992 - val_loss: 39.8725\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6963 - val_loss: 36.6596\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6953 - val_loss: 37.0224\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7419 - val_loss: 38.8968\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6529 - val_loss: 45.2474\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7242 - val_loss: 44.7232\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7246 - val_loss: 35.8141\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6376 - val_loss: 35.1489\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6536 - val_loss: 36.7377\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6994 - val_loss: 55.6183\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6947 - val_loss: 45.2861\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6981 - val_loss: 34.8576\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6892 - val_loss: 37.7963\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7026 - val_loss: 38.0172\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6938 - val_loss: 34.4933\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6526 - val_loss: 43.0263\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6536 - val_loss: 35.7971\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6180 - val_loss: 36.9348\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6265 - val_loss: 35.8207\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6327 - val_loss: 36.3307\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6528 - val_loss: 34.9464\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6646 - val_loss: 38.0799\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6226 - val_loss: 36.7510\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6212 - val_loss: 34.9457\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6566 - val_loss: 38.8679\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5914 - val_loss: 38.3993\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5985 - val_loss: 36.1971\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6428 - val_loss: 34.8916\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6059 - val_loss: 34.8634\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7159 - val_loss: 39.6168\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6291 - val_loss: 35.9205\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6250 - val_loss: 36.3472\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5657 - val_loss: 35.2761\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5731 - val_loss: 36.9813\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5908 - val_loss: 37.6820\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6464 - val_loss: 35.2602\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5524 - val_loss: 34.7075\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6185 - val_loss: 34.7301\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5315 - val_loss: 34.3878\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5451 - val_loss: 43.1284\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5537 - val_loss: 40.1513\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5381 - val_loss: 38.6066\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.5391 - val_loss: 39.6801\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5638 - val_loss: 45.6524\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5543 - val_loss: 34.0342\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5667 - val_loss: 39.9373\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5642 - val_loss: 44.1614\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5921 - val_loss: 42.6525\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.5056 - val_loss: 37.4934\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5664 - val_loss: 35.8696\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5416 - val_loss: 35.4132\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5710 - val_loss: 43.1994\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5938 - val_loss: 36.9444\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5227 - val_loss: 36.1078\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5346 - val_loss: 42.2636\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6091 - val_loss: 39.3275\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5181 - val_loss: 36.0535\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5906 - val_loss: 34.6758\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4856 - val_loss: 38.3470\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4283 - val_loss: 37.3943\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5339 - val_loss: 37.5453\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5532 - val_loss: 35.3154\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4893 - val_loss: 34.4736\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4695 - val_loss: 35.1311\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4225 - val_loss: 36.1226\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5577 - val_loss: 46.8827\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5233 - val_loss: 35.6250\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4925 - val_loss: 34.9403\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5337 - val_loss: 37.9747\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4374 - val_loss: 35.3983\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4531 - val_loss: 37.2051\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4816 - val_loss: 38.5076\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5283 - val_loss: 34.5906\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5281 - val_loss: 39.2288\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5398 - val_loss: 35.9304\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4476 - val_loss: 36.3743\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5099 - val_loss: 34.9850\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4359 - val_loss: 33.3463\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4403 - val_loss: 34.1449\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4490 - val_loss: 34.7344\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5440 - val_loss: 43.5289\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4210 - val_loss: 33.7788\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4775 - val_loss: 37.1520\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4004 - val_loss: 36.2000\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4254 - val_loss: 34.5743\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4029 - val_loss: 39.3014\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4650 - val_loss: 36.9162\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3652 - val_loss: 36.3750\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4315 - val_loss: 39.7013\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3872 - val_loss: 34.0962\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4164 - val_loss: 37.5098\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4275 - val_loss: 34.4398\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4464 - val_loss: 35.7027\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4449 - val_loss: 35.2845\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4087 - val_loss: 34.9171\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4342 - val_loss: 37.5643\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4258 - val_loss: 38.4399\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4376 - val_loss: 38.1115\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4553 - val_loss: 50.8855\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4177 - val_loss: 34.7727\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5091 - val_loss: 34.0748\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3744 - val_loss: 41.6650\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3494 - val_loss: 33.9259\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3604 - val_loss: 37.9679\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3905 - val_loss: 36.9920\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4280 - val_loss: 38.1245\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 28.4170 - val_loss: 38.1052\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3863 - val_loss: 37.1904\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3487 - val_loss: 40.6916\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4019 - val_loss: 33.9566\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3050 - val_loss: 34.8395\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.3854 - val_loss: 40.1054\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3525 - val_loss: 33.2714\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3333 - val_loss: 35.3987\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4229 - val_loss: 41.2958\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3919 - val_loss: 35.1143\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4268 - val_loss: 37.7314\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4178 - val_loss: 51.7084\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3294 - val_loss: 40.9869\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3562 - val_loss: 33.5245\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3493 - val_loss: 34.1934\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4156 - val_loss: 42.2438\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3697 - val_loss: 36.4991\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3794 - val_loss: 36.1779\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3236 - val_loss: 39.8113\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3082 - val_loss: 34.4400\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3229 - val_loss: 37.5540\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3590 - val_loss: 36.9621\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.3850 - val_loss: 43.3060\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3887 - val_loss: 38.4890\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3144 - val_loss: 40.5928\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3033 - val_loss: 40.9738\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3743 - val_loss: 36.1414\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3557 - val_loss: 34.7801\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3266 - val_loss: 34.9310\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3481 - val_loss: 44.3743\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3016 - val_loss: 40.0300\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3484 - val_loss: 33.1254\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2764 - val_loss: 36.9325\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3309 - val_loss: 39.0282\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2949 - val_loss: 35.8070\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43331f7-47ac-48d0-f4fe-d4fdcc1b5207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  1.2399119794613538 \n",
            "MAE:  4.417541371072891 \n",
            "SD:  5.854023916156944\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "24878c98-db4d-4702-92b6-2472ddeeb13c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd6c2893-fc4e-4cff-d475-5418377cb0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_20 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,393\n",
            "Trainable params: 2,329\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f64c80-0b97-4d77-f551-af7ee76def51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 3670.4236 - val_loss: 3673.4080\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3456.4609 - val_loss: 3070.6514\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3199.0295 - val_loss: 2788.5657\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2868.7131 - val_loss: 2708.8091\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2482.9248 - val_loss: 2285.1345\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2062.5679 - val_loss: 1672.0576\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1657.5175 - val_loss: 1384.9075\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1279.7100 - val_loss: 1190.0056\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 937.5019 - val_loss: 778.7426\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 652.8801 - val_loss: 494.7355\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 431.5896 - val_loss: 392.9880\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 273.6642 - val_loss: 131.7365\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 169.8230 - val_loss: 88.9933\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.3711 - val_loss: 66.6323\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.6381 - val_loss: 57.4591\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 51.2160 - val_loss: 61.2246\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 43.4183 - val_loss: 40.5080\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.8364 - val_loss: 49.0189\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.6612 - val_loss: 44.7277\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8118 - val_loss: 42.5813\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.1340 - val_loss: 45.2100\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.9417 - val_loss: 43.9271\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.6835 - val_loss: 48.0080\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5239 - val_loss: 43.9058\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2180 - val_loss: 39.6998\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.1997 - val_loss: 54.8452\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9677 - val_loss: 44.4507\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7481 - val_loss: 44.8231\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6258 - val_loss: 52.2535\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4488 - val_loss: 42.1548\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3633 - val_loss: 40.2461\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4514 - val_loss: 41.6711\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1301 - val_loss: 45.1623\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9682 - val_loss: 37.9103\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9342 - val_loss: 40.9900\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8548 - val_loss: 59.1500\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6921 - val_loss: 82.6864\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.5545 - val_loss: 39.2740\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.4087 - val_loss: 41.4368\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6195 - val_loss: 46.6931\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2812 - val_loss: 43.5478\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3474 - val_loss: 48.5634\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1338 - val_loss: 39.7679\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0925 - val_loss: 59.2178\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9629 - val_loss: 45.0413\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7465 - val_loss: 78.6235\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7722 - val_loss: 43.5098\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6082 - val_loss: 37.3928\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5619 - val_loss: 42.0007\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3285 - val_loss: 37.0686\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4988 - val_loss: 37.7010\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2606 - val_loss: 43.1884\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3165 - val_loss: 42.3632\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1328 - val_loss: 38.6187\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0631 - val_loss: 46.4155\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0272 - val_loss: 51.0474\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9235 - val_loss: 41.0186\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9256 - val_loss: 43.5953\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9257 - val_loss: 46.6659\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8000 - val_loss: 39.7928\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7111 - val_loss: 47.2947\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6950 - val_loss: 39.8900\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6539 - val_loss: 43.8170\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5680 - val_loss: 37.1906\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4417 - val_loss: 47.6545\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5404 - val_loss: 38.5275\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3613 - val_loss: 38.6110\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3405 - val_loss: 48.1780\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2720 - val_loss: 49.0218\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3392 - val_loss: 36.4108\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1650 - val_loss: 36.1097\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1710 - val_loss: 39.6219\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1259 - val_loss: 35.7704\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1265 - val_loss: 37.8188\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0677 - val_loss: 36.7849\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1109 - val_loss: 45.1209\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0388 - val_loss: 38.7060\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.0133 - val_loss: 39.1679\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8853 - val_loss: 36.1035\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8664 - val_loss: 36.5297\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9298 - val_loss: 59.5684\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7762 - val_loss: 36.3259\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7446 - val_loss: 35.2387\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6527 - val_loss: 48.5452\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6453 - val_loss: 48.4822\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6069 - val_loss: 38.0128\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6052 - val_loss: 36.3992\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5250 - val_loss: 41.9477\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6132 - val_loss: 53.9925\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5579 - val_loss: 46.2215\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5637 - val_loss: 38.9678\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.4974 - val_loss: 36.5044\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4203 - val_loss: 34.5113\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.4060 - val_loss: 38.5244\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4059 - val_loss: 36.0270\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4234 - val_loss: 43.2728\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3356 - val_loss: 37.4986\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2973 - val_loss: 38.9174\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2416 - val_loss: 40.3620\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2494 - val_loss: 40.4481\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2514 - val_loss: 39.4185\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2675 - val_loss: 39.6468\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1698 - val_loss: 35.2870\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1615 - val_loss: 35.6768\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1321 - val_loss: 42.8829\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0910 - val_loss: 43.6398\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0707 - val_loss: 35.7124\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0750 - val_loss: 38.4174\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9524 - val_loss: 36.6134\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0112 - val_loss: 50.5909\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0011 - val_loss: 35.6436\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.9109 - val_loss: 37.4752\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.8797 - val_loss: 34.6785\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8370 - val_loss: 42.0314\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8876 - val_loss: 37.1786\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8382 - val_loss: 39.6774\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.7825 - val_loss: 43.3956\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.8156 - val_loss: 38.0400\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7885 - val_loss: 50.4139\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7925 - val_loss: 42.2928\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8419 - val_loss: 36.1403\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7062 - val_loss: 36.2339\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7058 - val_loss: 37.8085\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6613 - val_loss: 49.9504\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6690 - val_loss: 38.5960\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7361 - val_loss: 36.3150\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6130 - val_loss: 42.1965\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5949 - val_loss: 35.9021\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5939 - val_loss: 34.6519\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6575 - val_loss: 34.1641\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5484 - val_loss: 34.9834\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5643 - val_loss: 34.3901\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4818 - val_loss: 37.5996\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5265 - val_loss: 47.9087\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5169 - val_loss: 39.3078\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4753 - val_loss: 44.0226\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4267 - val_loss: 38.5970\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4698 - val_loss: 45.7853\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3743 - val_loss: 35.7102\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4837 - val_loss: 36.4654\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4788 - val_loss: 41.8599\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4318 - val_loss: 36.6908\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3936 - val_loss: 41.5743\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4780 - val_loss: 38.1860\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4054 - val_loss: 36.0549\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3165 - val_loss: 45.3913\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3519 - val_loss: 36.6319\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3195 - val_loss: 47.4066\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3605 - val_loss: 34.5868\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3842 - val_loss: 36.1203\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2618 - val_loss: 34.5661\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.3252 - val_loss: 34.6322\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.3097 - val_loss: 38.7559\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2568 - val_loss: 36.3820\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2132 - val_loss: 34.4676\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2595 - val_loss: 34.9378\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2096 - val_loss: 40.9484\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2113 - val_loss: 39.6289\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1925 - val_loss: 39.0379\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1902 - val_loss: 36.4406\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2637 - val_loss: 44.5501\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1931 - val_loss: 35.1704\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1470 - val_loss: 40.7087\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1228 - val_loss: 35.1168\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1603 - val_loss: 38.0842\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2080 - val_loss: 35.4653\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.0242 - val_loss: 36.8310\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0722 - val_loss: 46.6887\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1566 - val_loss: 38.6240\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0931 - val_loss: 39.2660\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1528 - val_loss: 35.3762\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1174 - val_loss: 34.3621\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.0347 - val_loss: 34.2308\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1579 - val_loss: 51.7564\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0470 - val_loss: 41.2575\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9945 - val_loss: 39.8975\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0414 - val_loss: 37.7418\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0090 - val_loss: 34.2519\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0492 - val_loss: 50.8726\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0459 - val_loss: 39.5193\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9361 - val_loss: 33.9531\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9832 - val_loss: 37.1873\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0129 - val_loss: 34.8651\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9683 - val_loss: 35.6353\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0713 - val_loss: 43.3469\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9060 - val_loss: 34.0671\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9858 - val_loss: 36.3084\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9366 - val_loss: 32.9617\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.0883 - val_loss: 37.8541\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9015 - val_loss: 36.0100\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8842 - val_loss: 41.1839\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8439 - val_loss: 36.0632\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8931 - val_loss: 38.9364\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9469 - val_loss: 46.8838\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8709 - val_loss: 52.7080\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8927 - val_loss: 37.5347\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7601 - val_loss: 36.5191\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8954 - val_loss: 35.1262\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7911 - val_loss: 33.4745\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8063 - val_loss: 45.8238\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8386 - val_loss: 34.6338\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7927 - val_loss: 60.5268\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8387 - val_loss: 37.9560\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7804 - val_loss: 33.7175\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8588 - val_loss: 36.3921\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7539 - val_loss: 38.3259\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8026 - val_loss: 37.7935\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8045 - val_loss: 50.9442\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7550 - val_loss: 43.7943\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7811 - val_loss: 33.8273\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7978 - val_loss: 40.9374\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7317 - val_loss: 36.0553\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7355 - val_loss: 61.7854\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7374 - val_loss: 40.9283\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7314 - val_loss: 33.7344\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7276 - val_loss: 36.2992\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6939 - val_loss: 37.5401\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7101 - val_loss: 34.9844\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6742 - val_loss: 37.0670\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6538 - val_loss: 34.6240\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6707 - val_loss: 33.8249\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6319 - val_loss: 36.3883\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6472 - val_loss: 37.5218\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6751 - val_loss: 40.3529\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6311 - val_loss: 36.9788\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6706 - val_loss: 33.5619\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6520 - val_loss: 45.2839\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6126 - val_loss: 36.5163\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7064 - val_loss: 55.0967\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5962 - val_loss: 37.2115\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6050 - val_loss: 34.7233\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5373 - val_loss: 36.4339\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6359 - val_loss: 38.3089\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5698 - val_loss: 36.6923\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6118 - val_loss: 37.4789\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5249 - val_loss: 33.7460\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5607 - val_loss: 35.9983\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5471 - val_loss: 37.8983\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5444 - val_loss: 40.0044\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5396 - val_loss: 37.9705\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5335 - val_loss: 35.5799\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5617 - val_loss: 35.0219\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5105 - val_loss: 35.9891\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4952 - val_loss: 37.5096\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5162 - val_loss: 36.6559\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5185 - val_loss: 33.6481\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4725 - val_loss: 36.6064\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4520 - val_loss: 38.3642\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5082 - val_loss: 41.6870\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4870 - val_loss: 42.9930\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4819 - val_loss: 38.2276\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4727 - val_loss: 36.3697\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4728 - val_loss: 53.0610\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4317 - val_loss: 47.1651\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4866 - val_loss: 41.7409\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4865 - val_loss: 34.3444\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4732 - val_loss: 34.7004\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4687 - val_loss: 34.2922\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4396 - val_loss: 41.9010\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4291 - val_loss: 36.1622\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3578 - val_loss: 43.4448\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4082 - val_loss: 60.6201\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3761 - val_loss: 38.6888\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3857 - val_loss: 33.1991\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3900 - val_loss: 36.7386\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4263 - val_loss: 37.5421\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3906 - val_loss: 41.1542\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3746 - val_loss: 46.0996\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3521 - val_loss: 33.8568\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2972 - val_loss: 42.0241\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2898 - val_loss: 34.8267\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3805 - val_loss: 34.4986\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3487 - val_loss: 42.9814\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3102 - val_loss: 36.0638\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3313 - val_loss: 40.5863\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3927 - val_loss: 43.1956\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2897 - val_loss: 42.4126\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4182 - val_loss: 46.3739\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3229 - val_loss: 33.8341\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2672 - val_loss: 35.8708\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2285 - val_loss: 35.6212\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4096 - val_loss: 35.1943\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2933 - val_loss: 33.1474\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2868 - val_loss: 34.0975\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3338 - val_loss: 43.6071\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2721 - val_loss: 34.2776\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2839 - val_loss: 38.3134\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2861 - val_loss: 33.3214\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2246 - val_loss: 35.3114\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3014 - val_loss: 37.7679\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2814 - val_loss: 40.7123\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2939 - val_loss: 33.1746\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2313 - val_loss: 58.1761\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2895 - val_loss: 43.0666\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2800 - val_loss: 32.6891\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2491 - val_loss: 34.5283\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2693 - val_loss: 37.2046\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2267 - val_loss: 37.0323\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1726 - val_loss: 34.6692\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3034 - val_loss: 34.6595\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2067 - val_loss: 39.6450\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2241 - val_loss: 38.3398\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2501 - val_loss: 33.9287\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1882 - val_loss: 36.9123\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2291 - val_loss: 43.0159\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1807 - val_loss: 36.0561\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1942 - val_loss: 36.7694\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2458 - val_loss: 37.6920\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2279 - val_loss: 39.1809\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2236 - val_loss: 34.9001\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2177 - val_loss: 39.4614\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1878 - val_loss: 36.4027\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2057 - val_loss: 39.5764\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1829 - val_loss: 38.7608\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1605 - val_loss: 38.4359\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1739 - val_loss: 39.5062\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1985 - val_loss: 42.1297\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1408 - val_loss: 42.4823\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1272 - val_loss: 36.1949\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1807 - val_loss: 33.2775\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1625 - val_loss: 46.2765\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2058 - val_loss: 35.1124\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1197 - val_loss: 37.0408\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2016 - val_loss: 34.1590\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1394 - val_loss: 37.9746\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2304 - val_loss: 34.2519\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2098 - val_loss: 34.1057\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1726 - val_loss: 34.5408\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1975 - val_loss: 38.7842\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1103 - val_loss: 45.7479\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1109 - val_loss: 38.0561\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1301 - val_loss: 43.2644\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1583 - val_loss: 37.3586\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1059 - val_loss: 34.1613\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1471 - val_loss: 43.4005\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1086 - val_loss: 33.2798\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.1053 - val_loss: 35.7482\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 28.0629 - val_loss: 35.0370\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1290 - val_loss: 43.7162\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1452 - val_loss: 34.0165\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1642 - val_loss: 34.7268\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1378 - val_loss: 40.9978\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2003 - val_loss: 34.5446\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0974 - val_loss: 38.9447\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0808 - val_loss: 36.2796\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1191 - val_loss: 33.8931\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1627 - val_loss: 42.1980\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1201 - val_loss: 33.5070\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0975 - val_loss: 37.9836\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0747 - val_loss: 34.9960\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0559 - val_loss: 33.3053\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0741 - val_loss: 36.3512\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1247 - val_loss: 40.5044\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0518 - val_loss: 34.1129\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0233 - val_loss: 37.6268\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0666 - val_loss: 33.9306\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0995 - val_loss: 34.3823\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0457 - val_loss: 37.2013\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0825 - val_loss: 40.0189\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0483 - val_loss: 40.9648\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1183 - val_loss: 39.2570\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1123 - val_loss: 35.3996\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0924 - val_loss: 69.4382\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0916 - val_loss: 37.1013\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0863 - val_loss: 35.2658\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0253 - val_loss: 39.9277\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0425 - val_loss: 34.7981\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1528 - val_loss: 46.0120\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0721 - val_loss: 33.9568\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0669 - val_loss: 34.7698\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0214 - val_loss: 35.6961\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0602 - val_loss: 39.3200\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0120 - val_loss: 37.0925\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0778 - val_loss: 33.8448\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0350 - val_loss: 40.4561\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0310 - val_loss: 34.2260\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0684 - val_loss: 33.8652\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0383 - val_loss: 33.0195\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0581 - val_loss: 38.5125\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0171 - val_loss: 35.1580\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0338 - val_loss: 34.8875\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9724 - val_loss: 33.8800\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0146 - val_loss: 35.8059\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0813 - val_loss: 34.1572\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.0431 - val_loss: 39.4072\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0163 - val_loss: 41.2453\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0766 - val_loss: 46.9669\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0057 - val_loss: 34.4055\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0280 - val_loss: 35.0384\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9819 - val_loss: 39.0118\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0140 - val_loss: 35.7936\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9858 - val_loss: 34.0200\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9489 - val_loss: 39.8095\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0910 - val_loss: 35.9111\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0626 - val_loss: 33.4538\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0511 - val_loss: 39.9031\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0189 - val_loss: 45.7362\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0059 - val_loss: 36.4274\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9357 - val_loss: 35.2282\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9844 - val_loss: 37.9828\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9466 - val_loss: 37.1518\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0035 - val_loss: 40.5330\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0329 - val_loss: 36.1858\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.0445 - val_loss: 39.2448\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9713 - val_loss: 37.3690\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0329 - val_loss: 35.3134\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0411 - val_loss: 49.6478\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0088 - val_loss: 39.6719\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0262 - val_loss: 37.0115\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0112 - val_loss: 35.3746\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9855 - val_loss: 36.5162\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9703 - val_loss: 36.6423\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9588 - val_loss: 37.6228\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9446 - val_loss: 37.0480\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9745 - val_loss: 34.2836\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0061 - val_loss: 36.9131\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9671 - val_loss: 33.8006\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9411 - val_loss: 34.2980\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9546 - val_loss: 33.9366\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0158 - val_loss: 39.5062\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9555 - val_loss: 34.8735\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9439 - val_loss: 33.6925\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9963 - val_loss: 40.3545\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0355 - val_loss: 35.8786\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9724 - val_loss: 46.8221\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9959 - val_loss: 36.7505\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9563 - val_loss: 37.5133\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9582 - val_loss: 34.0983\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9633 - val_loss: 34.0187\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0342 - val_loss: 41.9110\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9255 - val_loss: 34.2821\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9656 - val_loss: 38.5959\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9108 - val_loss: 33.5170\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9710 - val_loss: 34.8866\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9384 - val_loss: 35.1209\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8871 - val_loss: 38.8758\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9538 - val_loss: 36.4337\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8914 - val_loss: 34.1773\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9408 - val_loss: 35.6166\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9680 - val_loss: 39.1832\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9105 - val_loss: 40.6905\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9945 - val_loss: 35.9702\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9472 - val_loss: 47.2879\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8824 - val_loss: 40.3150\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8842 - val_loss: 39.5943\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8892 - val_loss: 34.2671\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8984 - val_loss: 33.8382\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9593 - val_loss: 34.7167\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9120 - val_loss: 35.8502\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9065 - val_loss: 33.0724\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8935 - val_loss: 34.5619\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9893 - val_loss: 33.2313\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9240 - val_loss: 35.4037\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8934 - val_loss: 34.8635\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9736 - val_loss: 34.1108\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8996 - val_loss: 34.1128\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9713 - val_loss: 33.0868\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8993 - val_loss: 33.4922\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9163 - val_loss: 38.3491\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8975 - val_loss: 36.0329\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8797 - val_loss: 38.5833\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8750 - val_loss: 33.2518\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9384 - val_loss: 35.9194\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9312 - val_loss: 34.4314\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9146 - val_loss: 34.2609\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8726 - val_loss: 35.7188\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8405 - val_loss: 36.9821\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8596 - val_loss: 35.7324\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8881 - val_loss: 34.4020\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9056 - val_loss: 43.5177\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8889 - val_loss: 34.3112\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8622 - val_loss: 34.5602\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8501 - val_loss: 41.9415\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8894 - val_loss: 41.2757\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8631 - val_loss: 34.4402\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.8741 - val_loss: 33.5391\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 27.8510 - val_loss: 43.1174\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8958 - val_loss: 34.3902\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9221 - val_loss: 33.0152\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8352 - val_loss: 34.3856\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8849 - val_loss: 33.4086\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8795 - val_loss: 42.6072\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8714 - val_loss: 44.6243\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8366 - val_loss: 35.1273\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8184 - val_loss: 37.2724\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9423 - val_loss: 38.5845\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8286 - val_loss: 35.0064\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8334 - val_loss: 36.8693\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8933 - val_loss: 37.4821\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8353 - val_loss: 34.3464\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8295 - val_loss: 37.8959\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8418 - val_loss: 34.7755\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8367 - val_loss: 40.1451\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8653 - val_loss: 33.2672\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8789 - val_loss: 37.8552\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9233 - val_loss: 34.2797\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8871 - val_loss: 37.5845\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9092 - val_loss: 40.2336\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8422 - val_loss: 35.5545\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8187 - val_loss: 36.6006\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e499c9f-72a1-49f8-931f-96e67a559dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  1.4729895282541208 \n",
            "MAE:  4.500380747739948 \n",
            "SD:  5.867787571426671\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "1de03e39-09d5-40ae-b196-88eb7426ab66"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240a5827-8772-487e-fef5-2ca2a987c16f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  0.5844420819388311 \n",
            "Ensemble_std:  5.853031924591888\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "\bBP_hv3_7(3).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}