{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/%08BP_hv3_6(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2YTF6cMiY1Hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# batch_size"
      ],
      "metadata": {
        "id": "XiiiBla2-j1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsCoux5AOZnK",
        "outputId": "557813bb-b401-4263-c2a5-dce933aaf264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "TensorFlow version :  2.8.2\n",
            "Keras version :  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlHICkovd809",
        "outputId": "bd52188d-8f98-46f1-835a-fc87d55a8b4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러z오기\n",
        "train_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ],
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "376247db-19d4-4d1e-8328-3a22f6986a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "e9c21e13-bbb5-4e96-ddf5-ca26087d36df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a32a1af1-e8de-4091-89d8-ee362163c880\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a32a1af1-e8de-4091-89d8-ee362163c880')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a32a1af1-e8de-4091-89d8-ee362163c880 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a32a1af1-e8de-4091-89d8-ee362163c880');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "284c4217-fadb-4fb6-bad3-b1d7b64ee0e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42b96a24-c0d5-4ae7-b7da-2bd3eaa99561\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42b96a24-c0d5-4ae7-b7da-2bd3eaa99561')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-42b96a24-c0d5-4ae7-b7da-2bd3eaa99561 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-42b96a24-c0d5-4ae7-b7da-2bd3eaa99561');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ],
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8SHBwBOZnO",
        "outputId": "60b0323e-7ad1-44df-e856-5b8df82743bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 4)                 512       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 4)                16        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 569\n",
            "Trainable params: 553\n",
            "Non-trainable params: 16\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dGT6-7NcOZnO",
        "outputId": "16f11e92-d381-4d73-f17d-79c8ada95bce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 4s 6ms/step - loss: 12345.4434 - val_loss: 12431.1924\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 12088.1250 - val_loss: 12055.9775\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11783.0693 - val_loss: 11537.3691\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11428.8955 - val_loss: 11073.6152\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11026.3740 - val_loss: 10701.7256\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10583.7959 - val_loss: 10217.6201\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10113.2568 - val_loss: 9617.2568\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 9626.6426 - val_loss: 9114.3057\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 9136.7197 - val_loss: 8774.6064\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8655.0205 - val_loss: 8102.7124\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 8191.2188 - val_loss: 7846.9497\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 7754.4585 - val_loss: 7466.9829\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 7352.6689 - val_loss: 7016.3052\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6990.2246 - val_loss: 6720.6172\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 6671.6362 - val_loss: 6595.5952\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6397.4268 - val_loss: 6322.1152\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 6169.0649 - val_loss: 6052.1030\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 5984.0508 - val_loss: 5966.9458\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5837.9395 - val_loss: 6042.8320\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 5566.9468 - val_loss: 845.7557\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3274.2007 - val_loss: 4306.8442\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2508.8938 - val_loss: 3101.2830\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1971.0707 - val_loss: 2819.1980\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1484.6913 - val_loss: 3514.1013\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1030.1279 - val_loss: 2130.4053\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 754.9316 - val_loss: 1254.6638\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 568.8768 - val_loss: 713.0291\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 437.6937 - val_loss: 317.3591\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 345.8967 - val_loss: 353.7518\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 282.4529 - val_loss: 410.3583\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 241.8581 - val_loss: 336.6304\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 214.2640 - val_loss: 326.4764\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 199.0749 - val_loss: 254.4716\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 189.2419 - val_loss: 266.2410\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 180.8046 - val_loss: 228.3171\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 175.3490 - val_loss: 190.1076\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 170.5270 - val_loss: 237.3413\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 166.4056 - val_loss: 187.1797\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 162.3903 - val_loss: 172.0669\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 158.0777 - val_loss: 176.7189\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 154.3812 - val_loss: 170.1927\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 150.5332 - val_loss: 373.3558\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 147.4009 - val_loss: 179.3305\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 143.5276 - val_loss: 168.6039\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 140.6059 - val_loss: 147.5032\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 138.2794 - val_loss: 141.5083\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 135.0736 - val_loss: 176.5349\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 133.5350 - val_loss: 171.4207\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 131.0875 - val_loss: 239.6251\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 129.5195 - val_loss: 145.1644\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 127.8400 - val_loss: 131.9554\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 126.0550 - val_loss: 131.8419\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 124.8690 - val_loss: 140.3928\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 123.0483 - val_loss: 140.6625\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 122.1152 - val_loss: 128.6982\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 120.9049 - val_loss: 132.9102\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 119.3414 - val_loss: 135.4630\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 118.3483 - val_loss: 181.6423\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 117.4872 - val_loss: 143.8393\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 116.2375 - val_loss: 138.0209\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 115.0360 - val_loss: 134.9816\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 113.7652 - val_loss: 123.4382\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 112.9581 - val_loss: 119.4994\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 112.0547 - val_loss: 126.5822\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 111.1897 - val_loss: 126.5857\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.9030 - val_loss: 119.3845\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.0917 - val_loss: 114.0028\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 109.3959 - val_loss: 117.4440\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 109.1760 - val_loss: 127.3462\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.6407 - val_loss: 126.4764\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 107.6010 - val_loss: 120.7360\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.3327 - val_loss: 119.9009\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.8008 - val_loss: 117.1620\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.3567 - val_loss: 118.9760\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.6768 - val_loss: 117.4204\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.1817 - val_loss: 128.1108\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.9374 - val_loss: 116.4454\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.3423 - val_loss: 118.8617\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2223 - val_loss: 115.6247\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.8092 - val_loss: 134.1015\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6848 - val_loss: 119.1581\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5838 - val_loss: 120.3569\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.0756 - val_loss: 121.4631\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.3290 - val_loss: 112.9719\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.7615 - val_loss: 133.1769\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7902 - val_loss: 127.9456\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.9106 - val_loss: 112.4622\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6248 - val_loss: 113.0069\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4685 - val_loss: 139.3524\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.3633 - val_loss: 115.3752\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.2561 - val_loss: 109.6764\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1399 - val_loss: 125.3552\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0554 - val_loss: 111.6900\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9328 - val_loss: 108.8829\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8145 - val_loss: 109.5240\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8396 - val_loss: 116.5940\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.6312 - val_loss: 120.9631\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6356 - val_loss: 113.2705\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4359 - val_loss: 113.4062\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.3366 - val_loss: 116.5661\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.6137 - val_loss: 149.1309\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2222 - val_loss: 113.1517\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2902 - val_loss: 107.3316\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.2256 - val_loss: 114.4129\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.0895 - val_loss: 124.1507\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.2946 - val_loss: 115.9282\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.1327 - val_loss: 112.9839\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0190 - val_loss: 122.2367\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.9581 - val_loss: 165.6066\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.0542 - val_loss: 109.9411\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.9464 - val_loss: 109.6769\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.8855 - val_loss: 108.3721\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.8675 - val_loss: 110.2191\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.8653 - val_loss: 107.5893\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.8217 - val_loss: 117.2363\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.7177 - val_loss: 110.3223\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.6119 - val_loss: 112.3802\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.5307 - val_loss: 108.7242\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.5022 - val_loss: 114.3741\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.4403 - val_loss: 106.3892\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.4725 - val_loss: 121.1119\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.3843 - val_loss: 113.0497\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.4948 - val_loss: 116.0807\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.4577 - val_loss: 107.8292\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.5404 - val_loss: 116.6665\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.4973 - val_loss: 111.8421\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.2947 - val_loss: 109.8677\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.3326 - val_loss: 114.0610\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.1935 - val_loss: 116.5864\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.4639 - val_loss: 114.5032\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2081 - val_loss: 108.1927\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.1502 - val_loss: 127.5725\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.2808 - val_loss: 116.4389\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.1444 - val_loss: 110.0293\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.1407 - val_loss: 126.7884\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.2340 - val_loss: 106.3950\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.9691 - val_loss: 130.4833\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.3076 - val_loss: 108.1681\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.9303 - val_loss: 126.6115\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3892 - val_loss: 120.3566\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.1185 - val_loss: 108.2328\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.1032 - val_loss: 136.6511\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.2514 - val_loss: 110.7142\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.8875 - val_loss: 109.5666\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8920 - val_loss: 126.1434\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8049 - val_loss: 133.0085\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.0997 - val_loss: 114.7454\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.9467 - val_loss: 112.9336\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8038 - val_loss: 106.1945\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.0127 - val_loss: 105.5772\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.8505 - val_loss: 106.6213\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8721 - val_loss: 106.2320\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7536 - val_loss: 106.8582\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6861 - val_loss: 108.4031\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.8381 - val_loss: 110.0532\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6519 - val_loss: 118.8742\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8538 - val_loss: 109.5111\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.9234 - val_loss: 104.6721\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6846 - val_loss: 107.3808\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 100.0534 - val_loss: 109.1151\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7146 - val_loss: 119.1786\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7036 - val_loss: 112.3960\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.5842 - val_loss: 108.8169\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7569 - val_loss: 105.5689\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8397 - val_loss: 105.9024\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6846 - val_loss: 132.4077\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.6978 - val_loss: 122.0679\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.5699 - val_loss: 115.1051\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8717 - val_loss: 114.1554\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6589 - val_loss: 106.7923\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.7257 - val_loss: 115.3515\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.5063 - val_loss: 127.2863\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.6369 - val_loss: 111.0231\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.3887 - val_loss: 138.4039\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.4669 - val_loss: 105.5189\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7222 - val_loss: 106.0850\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.5196 - val_loss: 116.6384\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.4828 - val_loss: 109.3746\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.5214 - val_loss: 109.3002\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.4032 - val_loss: 112.3537\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4715 - val_loss: 129.0643\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.4944 - val_loss: 104.5161\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4393 - val_loss: 112.9320\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.3188 - val_loss: 108.0541\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4783 - val_loss: 132.3505\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.3323 - val_loss: 110.2324\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4002 - val_loss: 110.9207\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.5793 - val_loss: 112.0352\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4158 - val_loss: 105.1533\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.5230 - val_loss: 115.1630\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.5523 - val_loss: 105.5374\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.2906 - val_loss: 108.3093\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.3412 - val_loss: 108.3310\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.3477 - val_loss: 132.9581\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4675 - val_loss: 115.0147\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1629 - val_loss: 107.3792\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.2548 - val_loss: 108.8363\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.3271 - val_loss: 114.7677\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.3212 - val_loss: 212.4858\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4272 - val_loss: 114.7680\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1444 - val_loss: 108.6023\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.0390 - val_loss: 124.8272\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.2448 - val_loss: 108.2141\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.2173 - val_loss: 114.7281\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1981 - val_loss: 180.8642\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.3683 - val_loss: 122.4986\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.2516 - val_loss: 114.2552\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1686 - val_loss: 106.9998\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1356 - val_loss: 108.1177\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.1587 - val_loss: 128.2595\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0614 - val_loss: 123.6825\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.2180 - val_loss: 107.1306\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9427 - val_loss: 126.0021\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0742 - val_loss: 106.5351\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0526 - val_loss: 106.5529\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1662 - val_loss: 119.0445\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1076 - val_loss: 143.4457\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0173 - val_loss: 108.0077\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.1428 - val_loss: 106.5968\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1818 - val_loss: 118.7839\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0136 - val_loss: 106.8045\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1173 - val_loss: 124.2610\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.1347 - val_loss: 104.7822\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0723 - val_loss: 107.2073\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0271 - val_loss: 113.3011\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0426 - val_loss: 135.0036\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1616 - val_loss: 112.2663\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.0012 - val_loss: 118.0860\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9019 - val_loss: 104.3793\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0810 - val_loss: 144.4340\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9442 - val_loss: 107.0381\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8910 - val_loss: 124.5088\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9574 - val_loss: 103.9523\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0494 - val_loss: 180.6355\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.9647 - val_loss: 123.0590\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.8340 - val_loss: 107.9008\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8059 - val_loss: 114.5137\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9007 - val_loss: 126.8423\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.1115 - val_loss: 105.1969\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8587 - val_loss: 104.1323\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8536 - val_loss: 106.8186\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.9565 - val_loss: 105.1207\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.6439 - val_loss: 106.0501\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8103 - val_loss: 109.0408\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7851 - val_loss: 129.7736\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9986 - val_loss: 107.4976\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.8518 - val_loss: 106.2400\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7342 - val_loss: 118.0827\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7108 - val_loss: 119.7992\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8493 - val_loss: 106.6378\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.8098 - val_loss: 116.9028\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6940 - val_loss: 107.6684\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8481 - val_loss: 108.5868\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6796 - val_loss: 106.5482\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.9158 - val_loss: 124.0105\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8470 - val_loss: 107.0294\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 99.0178 - val_loss: 109.3237\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7949 - val_loss: 124.1747\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.8062 - val_loss: 110.2906\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5633 - val_loss: 104.1796\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.7510 - val_loss: 152.6575\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9033 - val_loss: 112.9836\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8637 - val_loss: 106.7318\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7971 - val_loss: 130.1227\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.6084 - val_loss: 105.6714\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8317 - val_loss: 128.0387\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7656 - val_loss: 127.0340\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6121 - val_loss: 109.3738\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.5792 - val_loss: 119.5259\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7737 - val_loss: 108.3067\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7810 - val_loss: 112.4021\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6500 - val_loss: 112.1549\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7719 - val_loss: 107.1297\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7278 - val_loss: 106.0813\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.8121 - val_loss: 107.5757\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6946 - val_loss: 106.2922\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9180 - val_loss: 196.8457\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4787 - val_loss: 104.8015\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.6244 - val_loss: 110.7636\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5679 - val_loss: 139.1440\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8174 - val_loss: 116.0481\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7405 - val_loss: 105.7378\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7289 - val_loss: 116.3178\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.6079 - val_loss: 108.5473\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5539 - val_loss: 116.4145\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5225 - val_loss: 107.0755\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7304 - val_loss: 112.3286\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.5733 - val_loss: 103.4482\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8491 - val_loss: 112.5218\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6508 - val_loss: 107.8473\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.7287 - val_loss: 115.9440\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5794 - val_loss: 106.4864\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6353 - val_loss: 115.2456\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4718 - val_loss: 130.4992\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.5098 - val_loss: 106.3570\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.8128 - val_loss: 110.3990\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.6748 - val_loss: 158.5869\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5586 - val_loss: 106.2360\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5726 - val_loss: 121.7607\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.6136 - val_loss: 108.5334\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7514 - val_loss: 105.6905\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6048 - val_loss: 119.8623\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5060 - val_loss: 117.5928\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.5131 - val_loss: 104.5180\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4520 - val_loss: 112.5644\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6322 - val_loss: 110.9763\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.5844 - val_loss: 117.3616\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.5215 - val_loss: 104.6078\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5815 - val_loss: 104.5319\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5191 - val_loss: 104.4265\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4946 - val_loss: 105.3620\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.5547 - val_loss: 104.6236\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7237 - val_loss: 114.2430\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5526 - val_loss: 114.6578\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4987 - val_loss: 127.4768\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5837 - val_loss: 106.4504\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4342 - val_loss: 114.7384\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5488 - val_loss: 105.2385\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5244 - val_loss: 145.5504\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4426 - val_loss: 110.9006\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.5070 - val_loss: 141.3283\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.5382 - val_loss: 107.2016\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5360 - val_loss: 105.0040\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5548 - val_loss: 106.1656\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6045 - val_loss: 104.3802\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3569 - val_loss: 105.2665\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5646 - val_loss: 139.9613\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4821 - val_loss: 109.3827\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4581 - val_loss: 107.2163\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4947 - val_loss: 142.6493\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5201 - val_loss: 108.3668\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4556 - val_loss: 110.5843\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4621 - val_loss: 114.9817\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5234 - val_loss: 117.5355\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4471 - val_loss: 104.4491\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4263 - val_loss: 105.6788\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5345 - val_loss: 104.5643\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.4009 - val_loss: 105.0746\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4529 - val_loss: 110.3472\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4621 - val_loss: 104.8105\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 98.4671 - val_loss: 114.5762\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3225 - val_loss: 106.4883\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5165 - val_loss: 125.1817\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4518 - val_loss: 115.6862\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4732 - val_loss: 109.6052\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3176 - val_loss: 117.2331\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4774 - val_loss: 107.2119\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4568 - val_loss: 124.8299\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5063 - val_loss: 112.1920\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5702 - val_loss: 106.1213\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5200 - val_loss: 106.6239\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3327 - val_loss: 133.3233\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6139 - val_loss: 115.1709\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6213 - val_loss: 109.4252\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4615 - val_loss: 108.6069\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4633 - val_loss: 104.2740\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2277 - val_loss: 105.8224\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4575 - val_loss: 125.9666\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2839 - val_loss: 114.8541\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4913 - val_loss: 104.6262\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5268 - val_loss: 132.0511\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4135 - val_loss: 107.4247\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5303 - val_loss: 106.4560\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3933 - val_loss: 109.2105\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4976 - val_loss: 113.0430\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5546 - val_loss: 112.6761\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4230 - val_loss: 106.1484\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4084 - val_loss: 106.2430\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3248 - val_loss: 109.7011\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3588 - val_loss: 124.2511\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2556 - val_loss: 106.5464\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3038 - val_loss: 108.1259\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.5139 - val_loss: 114.0994\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 98.2980 - val_loss: 109.7262\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 98.4760 - val_loss: 109.6141\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.5083 - val_loss: 111.7773\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2547 - val_loss: 105.2081\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4119 - val_loss: 105.9650\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4099 - val_loss: 127.0566\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4037 - val_loss: 114.2607\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3326 - val_loss: 105.1252\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3578 - val_loss: 111.0222\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4424 - val_loss: 108.7131\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3558 - val_loss: 110.2317\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4488 - val_loss: 104.5800\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3462 - val_loss: 110.2865\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2109 - val_loss: 125.3064\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2807 - val_loss: 106.7681\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5346 - val_loss: 110.9901\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2576 - val_loss: 109.2193\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3987 - val_loss: 111.3896\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3584 - val_loss: 122.7272\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3970 - val_loss: 111.5041\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3082 - val_loss: 108.1451\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3693 - val_loss: 105.5941\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5948 - val_loss: 118.2479\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5386 - val_loss: 111.4136\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3097 - val_loss: 103.8980\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3037 - val_loss: 109.6766\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3016 - val_loss: 112.1666\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4430 - val_loss: 111.9236\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2825 - val_loss: 115.8858\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5536 - val_loss: 107.6851\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3700 - val_loss: 115.3662\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4783 - val_loss: 129.9635\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3602 - val_loss: 132.5289\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4297 - val_loss: 150.8343\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3872 - val_loss: 104.3461\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2131 - val_loss: 111.3020\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4157 - val_loss: 104.8702\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3099 - val_loss: 118.2310\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2443 - val_loss: 115.8237\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4838 - val_loss: 107.9105\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3946 - val_loss: 111.3094\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2758 - val_loss: 108.0578\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4383 - val_loss: 105.3363\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3247 - val_loss: 126.4178\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3351 - val_loss: 109.7919\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4198 - val_loss: 105.2070\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3024 - val_loss: 109.1718\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3319 - val_loss: 109.8999\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2617 - val_loss: 107.1882\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2543 - val_loss: 108.4649\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4729 - val_loss: 131.3416\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2554 - val_loss: 104.7088\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.1987 - val_loss: 109.6914\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3410 - val_loss: 118.6527\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2762 - val_loss: 112.7883\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2755 - val_loss: 117.1616\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3317 - val_loss: 109.0155\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3178 - val_loss: 111.8399\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3150 - val_loss: 109.4706\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2992 - val_loss: 103.6686\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2216 - val_loss: 110.9643\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.1876 - val_loss: 106.4440\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3626 - val_loss: 105.9759\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3729 - val_loss: 106.1974\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2478 - val_loss: 117.6259\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3421 - val_loss: 104.5598\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3357 - val_loss: 107.0676\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5136 - val_loss: 106.4132\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2146 - val_loss: 106.0987\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2785 - val_loss: 125.1426\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3186 - val_loss: 106.1552\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3233 - val_loss: 104.6461\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.1893 - val_loss: 106.0429\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2594 - val_loss: 120.6302\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3826 - val_loss: 141.1122\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3402 - val_loss: 115.8123\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3137 - val_loss: 107.3183\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2043 - val_loss: 109.9829\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2273 - val_loss: 117.6984\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3541 - val_loss: 110.0958\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3867 - val_loss: 113.3469\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3289 - val_loss: 105.7510\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3653 - val_loss: 141.9709\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3646 - val_loss: 105.5776\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2223 - val_loss: 106.5958\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3702 - val_loss: 111.0967\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3079 - val_loss: 105.5021\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3692 - val_loss: 107.0838\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2379 - val_loss: 106.0838\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2114 - val_loss: 105.1066\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3398 - val_loss: 115.5109\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2408 - val_loss: 109.8966\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3264 - val_loss: 124.3062\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2059 - val_loss: 110.1064\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2723 - val_loss: 113.6592\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2080 - val_loss: 120.4879\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3661 - val_loss: 105.8082\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3353 - val_loss: 117.9270\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3201 - val_loss: 106.6837\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3199 - val_loss: 105.2381\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2907 - val_loss: 106.9845\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6013 - val_loss: 113.5036\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2132 - val_loss: 105.3006\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2572 - val_loss: 110.2472\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.1209 - val_loss: 107.8705\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2324 - val_loss: 114.6403\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.1725 - val_loss: 108.2545\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2882 - val_loss: 112.8510\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2898 - val_loss: 109.7917\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2305 - val_loss: 111.6890\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3751 - val_loss: 103.9008\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.1873 - val_loss: 107.9719\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3144 - val_loss: 108.6267\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2187 - val_loss: 108.8450\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.0939 - val_loss: 111.0518\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3320 - val_loss: 105.7165\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.1921 - val_loss: 124.9104\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3154 - val_loss: 112.5782\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2822 - val_loss: 108.0470\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.0819 - val_loss: 119.4653\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.1449 - val_loss: 118.5009\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.4052 - val_loss: 104.1119\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.1711 - val_loss: 133.4706\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.2323 - val_loss: 110.0307\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3019 - val_loss: 108.2821\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3069 - val_loss: 111.9319\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.1504 - val_loss: 119.8908\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "51cb5884-0ba3-46ba-dd53-31be78f9382a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  2.8826824055011646 \n",
            "MAE:  8.143562661878532 \n",
            "SD:  10.563184915844335\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "f60ecf7a-7ddd-4e49-d8ee-bffbf2393f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "34623232-a4aa-45ad-c90a-76cbcb710a4a",
        "id": "su2Sj5jZCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 4)                 512       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 569\n",
            "Trainable params: 553\n",
            "Non-trainable params: 16\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "outputId": "7f937545-1df7-4b73-c9c0-fa3aea279554",
        "id": "kPRh6v-mCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 12514.9893 - val_loss: 12479.8828\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 12326.7656 - val_loss: 12127.9961\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 12106.3633 - val_loss: 11626.2744\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 11838.3076 - val_loss: 11549.4824\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 11506.1084 - val_loss: 11132.8232\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11065.9756 - val_loss: 10510.9697\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10527.8838 - val_loss: 10859.6055\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 9927.9707 - val_loss: 9233.9639\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9289.5947 - val_loss: 8012.9219\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8627.0469 - val_loss: 7039.6504\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7953.5103 - val_loss: 6699.8530\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 7275.5034 - val_loss: 7006.2622\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 6602.4116 - val_loss: 6546.9053\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 5954.2109 - val_loss: 5124.0161\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5336.6670 - val_loss: 7702.6377\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 4746.8809 - val_loss: 5571.6187\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4195.4165 - val_loss: 4403.2695\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3681.5283 - val_loss: 6853.8916\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3207.6462 - val_loss: 3134.5371\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2774.6292 - val_loss: 3302.2158\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2384.2981 - val_loss: 1472.3953\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2032.7413 - val_loss: 1606.7175\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1729.9977 - val_loss: 2965.0815\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1452.6047 - val_loss: 2339.6699\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1215.7404 - val_loss: 1899.7566\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1012.8267 - val_loss: 825.9990\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 834.1456 - val_loss: 794.0242\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 686.5883 - val_loss: 869.2142\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 563.2176 - val_loss: 942.2758\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 465.9270 - val_loss: 322.8385\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 385.1361 - val_loss: 222.1779\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 323.5801 - val_loss: 529.1646\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 280.7153 - val_loss: 297.5815\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 250.8683 - val_loss: 198.5500\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 223.6109 - val_loss: 699.7159\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 195.7230 - val_loss: 286.5428\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 175.5597 - val_loss: 204.8893\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 151.2869 - val_loss: 217.5400\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 133.5533 - val_loss: 238.5473\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 121.9520 - val_loss: 152.4688\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 116.6626 - val_loss: 193.6706\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 112.6566 - val_loss: 256.8034\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.6667 - val_loss: 142.1717\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.7024 - val_loss: 267.0305\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.2502 - val_loss: 503.4388\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.0382 - val_loss: 206.7387\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.8366 - val_loss: 544.2321\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 107.3047 - val_loss: 116.5480\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.2394 - val_loss: 116.4033\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 107.1111 - val_loss: 502.2101\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.7468 - val_loss: 172.5544\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.2465 - val_loss: 290.7987\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.4886 - val_loss: 128.3790\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.6259 - val_loss: 185.9321\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.5817 - val_loss: 372.8226\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.0055 - val_loss: 188.2421\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.7135 - val_loss: 189.7561\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.2118 - val_loss: 144.2824\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.3378 - val_loss: 176.8650\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.1161 - val_loss: 127.8841\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.1250 - val_loss: 241.8137\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.9850 - val_loss: 603.8112\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.1542 - val_loss: 603.0645\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.0786 - val_loss: 423.6089\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.4002 - val_loss: 184.3419\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.1651 - val_loss: 582.1734\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.2678 - val_loss: 130.3468\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.3816 - val_loss: 142.3467\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.9644 - val_loss: 111.6282\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.0067 - val_loss: 213.0829\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.2565 - val_loss: 457.0371\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.3900 - val_loss: 225.2945\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.9525 - val_loss: 131.0020\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.2343 - val_loss: 220.7382\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.9159 - val_loss: 111.6678\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.9457 - val_loss: 250.1026\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.8632 - val_loss: 121.6421\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.7272 - val_loss: 294.3641\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.8361 - val_loss: 228.2165\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.8363 - val_loss: 586.8020\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4694 - val_loss: 149.6998\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.3902 - val_loss: 264.2546\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.9503 - val_loss: 129.5552\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.9591 - val_loss: 152.7530\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.7960 - val_loss: 577.1124\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4568 - val_loss: 137.8477\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.6536 - val_loss: 157.2930\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4943 - val_loss: 133.6124\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.9476 - val_loss: 415.8105\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.5730 - val_loss: 409.5617\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.3162 - val_loss: 192.6835\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.8474 - val_loss: 163.1398\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.6364 - val_loss: 124.7376\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.6894 - val_loss: 149.0027\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.8145 - val_loss: 310.7144\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.7455 - val_loss: 184.2827\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.4720 - val_loss: 125.3790\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.7728 - val_loss: 117.8732\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4388 - val_loss: 119.8390\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.4555 - val_loss: 181.2886\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.6399 - val_loss: 176.3088\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.4689 - val_loss: 130.1000\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2792 - val_loss: 131.7662\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2533 - val_loss: 148.1474\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.5242 - val_loss: 127.0755\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2240 - val_loss: 154.7665\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.3274 - val_loss: 124.6477\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.2474 - val_loss: 195.0942\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.7242 - val_loss: 401.0332\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2109 - val_loss: 117.5351\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.3841 - val_loss: 172.5362\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.1733 - val_loss: 126.4117\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.1237 - val_loss: 126.4390\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2498 - val_loss: 120.1244\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4170 - val_loss: 116.8270\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4687 - val_loss: 208.2928\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.4659 - val_loss: 194.1718\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4402 - val_loss: 332.3055\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.1823 - val_loss: 297.2631\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2807 - val_loss: 235.3074\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4827 - val_loss: 125.4778\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.3118 - val_loss: 120.7213\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2645 - val_loss: 117.1099\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.3482 - val_loss: 117.7885\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.1111 - val_loss: 121.9008\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0010 - val_loss: 166.5815\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.1690 - val_loss: 277.8222\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2675 - val_loss: 118.7975\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.9907 - val_loss: 149.2918\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.1382 - val_loss: 154.2204\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.9480 - val_loss: 113.1228\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.0577 - val_loss: 156.1280\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.0215 - val_loss: 116.1866\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0571 - val_loss: 111.1997\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.1463 - val_loss: 133.9819\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0812 - val_loss: 146.2880\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.3248 - val_loss: 125.6171\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.9150 - val_loss: 325.9861\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.0335 - val_loss: 155.7055\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.1343 - val_loss: 635.0606\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4329 - val_loss: 178.7253\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8625 - val_loss: 154.9993\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.8981 - val_loss: 137.2015\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.0757 - val_loss: 425.8241\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.9074 - val_loss: 115.8430\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2499 - val_loss: 117.9717\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.7045 - val_loss: 125.7474\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.7060 - val_loss: 112.2547\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.8099 - val_loss: 250.5762\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.7698 - val_loss: 328.5990\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.7295 - val_loss: 215.6875\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.8914 - val_loss: 161.3224\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.6760 - val_loss: 121.2605\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8177 - val_loss: 122.9752\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.6730 - val_loss: 232.5737\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8855 - val_loss: 140.2498\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8899 - val_loss: 333.2859\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2215 - val_loss: 173.0041\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.6025 - val_loss: 123.9268\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8780 - val_loss: 141.0379\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.6405 - val_loss: 125.7248\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.9463 - val_loss: 121.5250\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.5210 - val_loss: 185.4371\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.6577 - val_loss: 277.0031\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.7144 - val_loss: 186.8943\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8017 - val_loss: 144.0069\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.5467 - val_loss: 452.8178\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.4659 - val_loss: 130.9657\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.6666 - val_loss: 125.4650\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.5314 - val_loss: 213.6334\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.1323 - val_loss: 239.1336\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.4764 - val_loss: 349.7826\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.8129 - val_loss: 115.9670\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2642 - val_loss: 152.3810\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.5983 - val_loss: 236.6795\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.3967 - val_loss: 245.4917\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.6767 - val_loss: 122.6974\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.5115 - val_loss: 114.9317\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.4695 - val_loss: 232.1454\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.5590 - val_loss: 156.1016\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.4370 - val_loss: 193.5514\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.5479 - val_loss: 113.2904\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.3117 - val_loss: 161.7475\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8186 - val_loss: 111.4057\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.7347 - val_loss: 175.9676\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.7032 - val_loss: 121.1142\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.4350 - val_loss: 118.3831\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2159 - val_loss: 349.8655\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2491 - val_loss: 208.3821\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.2689 - val_loss: 110.8876\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.4338 - val_loss: 279.7658\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2580 - val_loss: 114.0593\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.1959 - val_loss: 112.1120\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.3248 - val_loss: 122.0640\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.3219 - val_loss: 111.1484\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.4612 - val_loss: 134.3369\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.3601 - val_loss: 323.8531\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.2499 - val_loss: 121.5907\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1533 - val_loss: 115.1903\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.2592 - val_loss: 114.9144\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.1291 - val_loss: 168.3789\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.1952 - val_loss: 117.1332\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.9557 - val_loss: 117.9692\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.3120 - val_loss: 128.3747\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1357 - val_loss: 142.8630\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.1561 - val_loss: 214.6858\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2113 - val_loss: 126.4303\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2251 - val_loss: 112.7728\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1623 - val_loss: 159.3189\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.4877 - val_loss: 258.1403\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.8913 - val_loss: 362.2707\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1066 - val_loss: 111.8011\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1175 - val_loss: 111.3192\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.0158 - val_loss: 136.9481\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.1883 - val_loss: 263.0025\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.3016 - val_loss: 223.2017\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.1257 - val_loss: 110.1618\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.0473 - val_loss: 246.7694\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.9116 - val_loss: 181.9382\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2765 - val_loss: 111.5261\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8152 - val_loss: 138.6807\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.8825 - val_loss: 112.7609\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.9191 - val_loss: 142.5249\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9311 - val_loss: 280.9059\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2076 - val_loss: 113.1526\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0454 - val_loss: 114.3282\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8690 - val_loss: 113.5120\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9474 - val_loss: 184.4127\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0158 - val_loss: 118.9978\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1244 - val_loss: 160.9279\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5874 - val_loss: 119.8568\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7349 - val_loss: 118.0873\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.0844 - val_loss: 117.9363\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7124 - val_loss: 109.9145\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6382 - val_loss: 144.8695\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7509 - val_loss: 112.6151\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7333 - val_loss: 109.0735\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.7568 - val_loss: 114.4165\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6775 - val_loss: 147.7273\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8166 - val_loss: 110.9620\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.6935 - val_loss: 116.5206\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.6550 - val_loss: 139.6404\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.7767 - val_loss: 112.7375\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.7398 - val_loss: 159.0757\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.9302 - val_loss: 158.3979\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.0760 - val_loss: 136.2821\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6404 - val_loss: 114.1533\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5788 - val_loss: 110.4293\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8240 - val_loss: 180.1144\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.7784 - val_loss: 109.5480\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7483 - val_loss: 117.0467\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.4954 - val_loss: 127.2252\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7118 - val_loss: 229.6722\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5410 - val_loss: 247.2554\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5712 - val_loss: 119.0359\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7007 - val_loss: 109.1620\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6676 - val_loss: 140.6025\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5399 - val_loss: 123.1184\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4678 - val_loss: 110.3943\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5593 - val_loss: 112.2748\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8406 - val_loss: 152.7899\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5270 - val_loss: 210.0773\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5144 - val_loss: 182.8650\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.6750 - val_loss: 210.7799\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4967 - val_loss: 114.0521\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7806 - val_loss: 110.1456\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6604 - val_loss: 111.8109\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6599 - val_loss: 129.9592\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5568 - val_loss: 194.8499\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4597 - val_loss: 178.0946\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5886 - val_loss: 115.3033\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4629 - val_loss: 142.4255\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6848 - val_loss: 187.8227\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5778 - val_loss: 183.5482\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4181 - val_loss: 120.8102\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.3982 - val_loss: 134.2771\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5870 - val_loss: 113.8462\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5337 - val_loss: 121.4942\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5182 - val_loss: 114.3587\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.2715 - val_loss: 110.1093\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.2837 - val_loss: 221.0965\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4731 - val_loss: 112.9583\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6710 - val_loss: 149.3353\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.3417 - val_loss: 122.8334\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.2877 - val_loss: 116.2001\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4211 - val_loss: 127.6479\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1917 - val_loss: 121.8519\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.3409 - val_loss: 124.7534\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1898 - val_loss: 114.0823\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.3189 - val_loss: 130.7343\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.3495 - val_loss: 117.2123\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.2644 - val_loss: 108.5228\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.1646 - val_loss: 222.1502\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.1229 - val_loss: 120.6600\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.0640 - val_loss: 109.7959\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4178 - val_loss: 108.1747\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1072 - val_loss: 145.4167\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.0957 - val_loss: 122.1131\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.1817 - val_loss: 113.2671\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.0387 - val_loss: 109.2337\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1797 - val_loss: 117.5691\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.1630 - val_loss: 123.9873\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.0404 - val_loss: 113.1239\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.0220 - val_loss: 110.3638\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.0875 - val_loss: 109.5205\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1141 - val_loss: 110.7924\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1290 - val_loss: 123.7811\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.0496 - val_loss: 126.3606\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.9592 - val_loss: 216.7584\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.9748 - val_loss: 136.9119\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8984 - val_loss: 116.0668\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8566 - val_loss: 131.9625\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.9199 - val_loss: 214.6890\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.9586 - val_loss: 112.1186\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7689 - val_loss: 119.0550\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8612 - val_loss: 142.5136\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7758 - val_loss: 113.1171\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7544 - val_loss: 117.7822\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.9176 - val_loss: 114.6355\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.0016 - val_loss: 123.8923\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6873 - val_loss: 175.0689\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.7439 - val_loss: 154.2327\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7195 - val_loss: 111.3004\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8025 - val_loss: 110.7747\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6884 - val_loss: 121.6550\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.7070 - val_loss: 125.2570\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7759 - val_loss: 117.4769\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7024 - val_loss: 113.9179\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7776 - val_loss: 153.3793\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6531 - val_loss: 132.0967\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6083 - val_loss: 123.3629\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8413 - val_loss: 144.5540\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6305 - val_loss: 111.9977\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5065 - val_loss: 167.3781\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6264 - val_loss: 132.5265\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5780 - val_loss: 117.3971\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5021 - val_loss: 155.7149\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6727 - val_loss: 116.6585\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.5921 - val_loss: 123.7210\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4932 - val_loss: 162.4923\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5925 - val_loss: 112.9841\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.5667 - val_loss: 135.7331\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4521 - val_loss: 108.1705\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5051 - val_loss: 114.8366\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4696 - val_loss: 115.2054\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7814 - val_loss: 116.5800\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4117 - val_loss: 112.5801\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5617 - val_loss: 121.3349\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5114 - val_loss: 167.5215\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5036 - val_loss: 143.2418\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3980 - val_loss: 111.4785\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4835 - val_loss: 117.0111\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5327 - val_loss: 111.2086\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5095 - val_loss: 107.3380\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5844 - val_loss: 111.2894\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4047 - val_loss: 135.7950\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4250 - val_loss: 137.9653\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3680 - val_loss: 109.2196\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.3358 - val_loss: 107.9719\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4068 - val_loss: 116.1447\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.3863 - val_loss: 128.8684\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3369 - val_loss: 108.8457\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3095 - val_loss: 121.6940\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.3200 - val_loss: 122.2982\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3523 - val_loss: 109.6470\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3149 - val_loss: 198.0882\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3360 - val_loss: 116.0913\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.3491 - val_loss: 107.8588\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.2556 - val_loss: 181.9055\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3617 - val_loss: 108.8864\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.2078 - val_loss: 115.0047\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3973 - val_loss: 110.1655\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3513 - val_loss: 124.8719\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1488 - val_loss: 110.2784\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.2702 - val_loss: 158.0029\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.2280 - val_loss: 109.8352\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.2592 - val_loss: 115.9441\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.2387 - val_loss: 131.0354\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.2253 - val_loss: 109.8830\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0941 - val_loss: 108.1665\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1183 - val_loss: 109.3420\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.1042 - val_loss: 115.9317\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1213 - val_loss: 108.9434\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1543 - val_loss: 125.0499\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3236 - val_loss: 108.0593\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3008 - val_loss: 108.8422\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0423 - val_loss: 111.1036\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0769 - val_loss: 112.3776\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1181 - val_loss: 109.0134\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0477 - val_loss: 130.3231\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.9552 - val_loss: 132.4170\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0305 - val_loss: 111.8942\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9801 - val_loss: 116.8313\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9792 - val_loss: 118.4721\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.0567 - val_loss: 107.3525\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0279 - val_loss: 134.6040\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0091 - val_loss: 113.4634\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8866 - val_loss: 113.5106\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9166 - val_loss: 118.5604\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0026 - val_loss: 126.2872\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9921 - val_loss: 111.5415\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7193 - val_loss: 147.4088\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9347 - val_loss: 111.4700\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8931 - val_loss: 114.6211\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8131 - val_loss: 110.8304\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8109 - val_loss: 137.4684\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7496 - val_loss: 118.4871\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9340 - val_loss: 114.7088\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7735 - val_loss: 108.4923\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6848 - val_loss: 118.5961\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7428 - val_loss: 115.4560\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7723 - val_loss: 110.3205\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6846 - val_loss: 112.4274\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7391 - val_loss: 123.4029\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6268 - val_loss: 107.9832\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6692 - val_loss: 135.7288\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6535 - val_loss: 118.8956\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6371 - val_loss: 163.5467\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6112 - val_loss: 125.4954\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6951 - val_loss: 114.0475\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7188 - val_loss: 124.7585\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4873 - val_loss: 122.6158\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5417 - val_loss: 112.0533\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5613 - val_loss: 173.9819\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4983 - val_loss: 112.8491\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5381 - val_loss: 120.7288\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4537 - val_loss: 125.9758\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4568 - val_loss: 126.5675\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6307 - val_loss: 126.1336\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3849 - val_loss: 153.9913\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3708 - val_loss: 110.9190\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3065 - val_loss: 111.9345\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2617 - val_loss: 111.9883\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2741 - val_loss: 130.6772\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1886 - val_loss: 166.8730\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4045 - val_loss: 124.9538\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4114 - val_loss: 182.2267\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1264 - val_loss: 123.9267\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1406 - val_loss: 142.1464\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1909 - val_loss: 123.4074\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9738 - val_loss: 112.7954\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1383 - val_loss: 140.7596\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9778 - val_loss: 109.6804\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0053 - val_loss: 107.5736\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9563 - val_loss: 122.1355\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0951 - val_loss: 108.1550\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.8907 - val_loss: 108.3051\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9270 - val_loss: 111.9606\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.8692 - val_loss: 110.8411\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.8218 - val_loss: 108.2854\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.7481 - val_loss: 108.3134\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.7350 - val_loss: 120.0074\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.6681 - val_loss: 109.4262\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.6997 - val_loss: 115.8706\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.6808 - val_loss: 110.9624\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.5889 - val_loss: 115.3389\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.5273 - val_loss: 113.2297\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.5227 - val_loss: 115.7243\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.4624 - val_loss: 113.0047\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.5218 - val_loss: 121.5543\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3619 - val_loss: 112.7298\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3650 - val_loss: 110.7726\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2851 - val_loss: 134.4145\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2493 - val_loss: 154.4196\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3205 - val_loss: 109.2896\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2268 - val_loss: 135.7597\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.1244 - val_loss: 111.9273\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.0624 - val_loss: 108.7170\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2283 - val_loss: 114.7308\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2160 - val_loss: 130.1871\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.9951 - val_loss: 106.7451\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.9083 - val_loss: 115.7553\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.9098 - val_loss: 108.5832\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.9713 - val_loss: 120.4402\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8743 - val_loss: 107.9136\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7610 - val_loss: 111.7673\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8334 - val_loss: 117.9857\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7296 - val_loss: 110.0939\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6849 - val_loss: 106.6788\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.6640 - val_loss: 108.2511\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7388 - val_loss: 109.0978\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7443 - val_loss: 113.1768\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7020 - val_loss: 109.5094\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4787 - val_loss: 110.3793\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4459 - val_loss: 117.6762\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.5728 - val_loss: 108.5158\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.3812 - val_loss: 108.9740\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.5194 - val_loss: 110.0914\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.5434 - val_loss: 109.5892\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.3711 - val_loss: 108.8138\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4719 - val_loss: 109.1955\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.2234 - val_loss: 106.9285\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.3740 - val_loss: 108.0372\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.2543 - val_loss: 119.2033\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.3220 - val_loss: 109.1051\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.2437 - val_loss: 106.9493\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0381 - val_loss: 108.1052\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.2065 - val_loss: 110.3461\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.2240 - val_loss: 111.6509\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.2614 - val_loss: 167.3219\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba147cd5-604b-462b-a5f6-fe4e3beeb4d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -7.188603195901 \n",
            "MAE:  10.412121373029496 \n",
            "SD:  10.753876063165539\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "77d616bd-b55f-44fd-f626-08a4b2349d2a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8e5aec-d86d-47f1-c403-c8e5c0d62611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 4)                 512       \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 569\n",
            "Trainable params: 553\n",
            "Non-trainable params: 16\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dcXAOqd2CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de83122-3d60-448e-bdd6-ee1a7828d69f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 8ms/step - loss: 12332.6523 - val_loss: 12195.0586\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 12066.8291 - val_loss: 11955.8818\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11750.1777 - val_loss: 11570.8447\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11365.5312 - val_loss: 11121.4775\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10910.2227 - val_loss: 10378.1660\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10381.6367 - val_loss: 9249.8730\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 9787.6846 - val_loss: 8555.4463\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 9154.2227 - val_loss: 8989.3027\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 8493.9883 - val_loss: 7335.3042\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7813.1172 - val_loss: 6707.8228\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7128.9863 - val_loss: 6027.0513\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6453.9604 - val_loss: 6569.6992\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 5793.2573 - val_loss: 4665.0439\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 5154.4995 - val_loss: 3640.9399\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 4543.3027 - val_loss: 5197.2476\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3965.9285 - val_loss: 4442.1470\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3427.8867 - val_loss: 2474.6611\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2932.0637 - val_loss: 3000.3254\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2480.8809 - val_loss: 2704.2710\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2077.3796 - val_loss: 1447.4264\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1717.5573 - val_loss: 1405.4609\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1407.2412 - val_loss: 897.5070\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1139.8564 - val_loss: 933.9705\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 916.2963 - val_loss: 890.5278\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 733.8108 - val_loss: 665.8583\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 585.4988 - val_loss: 932.5808\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 471.0954 - val_loss: 474.9001\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 382.8574 - val_loss: 651.9311\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 319.4172 - val_loss: 568.4873\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 268.8241 - val_loss: 673.7627\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 230.7171 - val_loss: 481.4533\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 206.1769 - val_loss: 344.5880\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 188.2146 - val_loss: 387.1071\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 177.3889 - val_loss: 254.7470\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 168.3866 - val_loss: 170.6891\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 163.1725 - val_loss: 172.8107\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 158.1327 - val_loss: 215.6066\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 154.2987 - val_loss: 217.4363\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 150.7980 - val_loss: 158.8657\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 148.2693 - val_loss: 155.1281\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 144.8023 - val_loss: 163.7622\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 142.3450 - val_loss: 168.0540\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 139.1103 - val_loss: 143.8174\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 136.5937 - val_loss: 144.9481\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 133.9011 - val_loss: 167.0527\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 131.9444 - val_loss: 150.1131\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 129.2760 - val_loss: 149.0594\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 127.9316 - val_loss: 195.1531\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 125.4292 - val_loss: 132.3174\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 124.0558 - val_loss: 163.0387\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 122.5802 - val_loss: 282.8933\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 120.6899 - val_loss: 186.8583\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 119.7248 - val_loss: 131.3066\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 118.1552 - val_loss: 148.4381\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 117.8618 - val_loss: 153.4576\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 116.6790 - val_loss: 150.8942\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 116.1646 - val_loss: 222.9264\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 116.6262 - val_loss: 139.7641\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 115.8124 - val_loss: 170.4310\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 114.3142 - val_loss: 178.2228\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 114.1840 - val_loss: 135.7679\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 113.6153 - val_loss: 203.8528\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 113.8784 - val_loss: 125.5631\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 112.8973 - val_loss: 239.4617\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 112.3794 - val_loss: 164.3993\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 112.2661 - val_loss: 120.7521\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 112.2988 - val_loss: 120.6429\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 111.9623 - val_loss: 119.8233\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.5160 - val_loss: 130.1084\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.5578 - val_loss: 127.7444\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.1660 - val_loss: 118.8340\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 110.7664 - val_loss: 128.5767\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 110.7482 - val_loss: 121.6333\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.6540 - val_loss: 152.7567\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.1421 - val_loss: 156.3759\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.5966 - val_loss: 135.6356\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.0078 - val_loss: 122.1941\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.1407 - val_loss: 122.6232\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.5759 - val_loss: 131.7683\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 109.1658 - val_loss: 137.6920\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.1186 - val_loss: 120.1332\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.8979 - val_loss: 144.7266\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.8928 - val_loss: 117.8729\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.9313 - val_loss: 142.5496\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.9856 - val_loss: 127.7808\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.9627 - val_loss: 116.2140\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.4707 - val_loss: 144.4359\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.5351 - val_loss: 132.9043\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.6219 - val_loss: 151.3576\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.2520 - val_loss: 143.7089\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.1813 - val_loss: 115.2966\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.2120 - val_loss: 127.1489\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.8712 - val_loss: 126.2253\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.6046 - val_loss: 120.0673\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.2744 - val_loss: 143.8321\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.8833 - val_loss: 134.9385\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 107.8022 - val_loss: 138.7337\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 108.0725 - val_loss: 154.4460\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.3628 - val_loss: 125.1249\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 107.7157 - val_loss: 188.1501\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 107.7926 - val_loss: 146.4847\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.7603 - val_loss: 131.3280\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.5781 - val_loss: 150.2495\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.1341 - val_loss: 147.5632\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.0425 - val_loss: 118.4792\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 107.3445 - val_loss: 132.3639\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.3890 - val_loss: 128.4183\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.0221 - val_loss: 134.7703\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.2185 - val_loss: 120.3022\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.7612 - val_loss: 114.1056\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.6977 - val_loss: 142.6808\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.7720 - val_loss: 126.4328\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.7131 - val_loss: 120.5447\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.6674 - val_loss: 119.8194\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.8151 - val_loss: 132.2858\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.5950 - val_loss: 161.2929\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.5636 - val_loss: 115.8414\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.5072 - val_loss: 151.5789\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.4332 - val_loss: 122.0176\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.4014 - val_loss: 117.9482\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.2764 - val_loss: 127.9723\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.3795 - val_loss: 115.9497\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.2485 - val_loss: 131.0739\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.2958 - val_loss: 143.1915\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.4795 - val_loss: 122.5805\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.2443 - val_loss: 116.4770\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.0617 - val_loss: 146.9695\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.2165 - val_loss: 133.4163\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.0327 - val_loss: 120.2352\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.2188 - val_loss: 114.1996\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.9327 - val_loss: 141.2155\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.8246 - val_loss: 115.5840\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.9921 - val_loss: 114.9438\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.8424 - val_loss: 114.5344\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.7235 - val_loss: 114.8912\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.9796 - val_loss: 122.5970\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 106.1514 - val_loss: 116.8147\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.7951 - val_loss: 124.7670\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.9149 - val_loss: 119.4413\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.6813 - val_loss: 140.1749\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.7683 - val_loss: 125.4999\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.6456 - val_loss: 125.4320\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.5454 - val_loss: 131.8934\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.5633 - val_loss: 114.4455\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.5827 - val_loss: 126.0587\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.8671 - val_loss: 112.6263\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.5944 - val_loss: 126.4525\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.7438 - val_loss: 112.9964\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.3104 - val_loss: 123.5621\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.5165 - val_loss: 115.3059\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.5608 - val_loss: 117.5036\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.2683 - val_loss: 121.5253\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.4902 - val_loss: 114.3970\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4363 - val_loss: 128.4918\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.5788 - val_loss: 120.2923\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2804 - val_loss: 123.5559\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2525 - val_loss: 115.5063\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.4130 - val_loss: 132.8338\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.2747 - val_loss: 117.1422\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.3931 - val_loss: 121.3662\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.2203 - val_loss: 117.0978\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2658 - val_loss: 119.2452\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.0873 - val_loss: 121.2295\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.1802 - val_loss: 117.5571\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.2303 - val_loss: 126.8763\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.0700 - val_loss: 115.4011\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2058 - val_loss: 116.2438\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.2097 - val_loss: 129.7327\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2069 - val_loss: 119.0508\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.1831 - val_loss: 117.1341\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.1044 - val_loss: 120.2480\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0078 - val_loss: 124.9026\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0597 - val_loss: 118.5290\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2086 - val_loss: 116.8222\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0442 - val_loss: 129.1958\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.0188 - val_loss: 113.9580\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.1278 - val_loss: 122.0309\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0778 - val_loss: 113.3181\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.9606 - val_loss: 113.8786\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.9316 - val_loss: 115.9231\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.9735 - val_loss: 141.5783\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.9071 - val_loss: 117.2471\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.7488 - val_loss: 118.1018\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0217 - val_loss: 117.0087\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.0215 - val_loss: 112.7888\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8527 - val_loss: 121.4476\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.9553 - val_loss: 138.0152\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.8972 - val_loss: 138.3832\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.8352 - val_loss: 116.9530\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.6423 - val_loss: 122.3209\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 105.2010 - val_loss: 118.1845\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.7455 - val_loss: 116.0912\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.9013 - val_loss: 127.7627\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.9026 - val_loss: 133.5494\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.6635 - val_loss: 115.2805\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.5609 - val_loss: 121.9379\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.6092 - val_loss: 124.3888\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.7195 - val_loss: 115.7692\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.5363 - val_loss: 137.9423\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.6224 - val_loss: 118.8525\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.5491 - val_loss: 155.4316\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.4845 - val_loss: 119.3173\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.4292 - val_loss: 115.5891\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.4610 - val_loss: 124.5962\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 104.4715 - val_loss: 127.0788\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.5669 - val_loss: 112.9055\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.6074 - val_loss: 119.7148\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.5051 - val_loss: 140.6212\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.3990 - val_loss: 112.8860\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2605 - val_loss: 112.5268\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.4303 - val_loss: 114.8935\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2715 - val_loss: 124.2815\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.3510 - val_loss: 114.2805\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2578 - val_loss: 129.4789\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.3407 - val_loss: 121.3214\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.3361 - val_loss: 119.1188\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.3934 - val_loss: 112.3965\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.1782 - val_loss: 124.7449\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2016 - val_loss: 116.2198\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.3671 - val_loss: 119.8433\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.3757 - val_loss: 130.5264\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2182 - val_loss: 123.4585\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2057 - val_loss: 132.6930\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1955 - val_loss: 145.9940\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0678 - val_loss: 117.8508\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0873 - val_loss: 121.5040\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2057 - val_loss: 122.1947\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1836 - val_loss: 115.1845\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0724 - val_loss: 140.9438\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.2778 - val_loss: 136.5648\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2690 - val_loss: 114.2225\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9493 - val_loss: 119.1470\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1589 - val_loss: 121.4959\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9406 - val_loss: 124.1320\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1678 - val_loss: 115.3563\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0205 - val_loss: 112.6437\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9409 - val_loss: 113.4983\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.2245 - val_loss: 119.2191\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0723 - val_loss: 117.4463\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0475 - val_loss: 131.3615\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9454 - val_loss: 113.9561\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 104.0557 - val_loss: 140.7040\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.9528 - val_loss: 122.9582\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9014 - val_loss: 149.0380\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0812 - val_loss: 120.0839\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8738 - val_loss: 119.6202\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0912 - val_loss: 112.3631\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0478 - val_loss: 118.5083\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0245 - val_loss: 124.7325\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9086 - val_loss: 135.2845\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9715 - val_loss: 141.0619\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0644 - val_loss: 126.9742\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9257 - val_loss: 122.9366\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9185 - val_loss: 127.3582\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.9283 - val_loss: 116.1173\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8663 - val_loss: 113.4211\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8133 - val_loss: 163.2650\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8215 - val_loss: 148.9690\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8472 - val_loss: 117.9210\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8715 - val_loss: 125.7361\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7823 - val_loss: 127.6920\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8702 - val_loss: 118.9389\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.7363 - val_loss: 131.3715\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8251 - val_loss: 117.1668\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8229 - val_loss: 124.4466\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8760 - val_loss: 113.2096\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7008 - val_loss: 116.6473\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7998 - val_loss: 125.4818\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7866 - val_loss: 117.9878\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7642 - val_loss: 114.4934\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8135 - val_loss: 117.5361\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.8504 - val_loss: 127.7153\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.8022 - val_loss: 119.6409\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6336 - val_loss: 112.3805\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.8452 - val_loss: 116.6111\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6338 - val_loss: 113.5832\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.7292 - val_loss: 115.4844\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6471 - val_loss: 112.3533\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7478 - val_loss: 113.1297\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7521 - val_loss: 118.6347\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.6309 - val_loss: 125.1064\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7672 - val_loss: 116.3964\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.7757 - val_loss: 115.1421\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.7660 - val_loss: 123.0794\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.6760 - val_loss: 114.9741\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.7316 - val_loss: 133.7172\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.7924 - val_loss: 123.6269\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5867 - val_loss: 112.4497\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6435 - val_loss: 118.6029\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.7358 - val_loss: 119.8939\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5230 - val_loss: 114.8437\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5504 - val_loss: 121.0492\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6445 - val_loss: 114.6190\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6237 - val_loss: 126.8882\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6836 - val_loss: 113.7974\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5787 - val_loss: 123.8174\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5727 - val_loss: 115.9910\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.6852 - val_loss: 117.7135\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5099 - val_loss: 123.9086\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5685 - val_loss: 113.0397\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.6384 - val_loss: 130.9334\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.5871 - val_loss: 121.5690\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.5700 - val_loss: 114.9710\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 103.6934 - val_loss: 113.3141\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 103.5107 - val_loss: 116.1831\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.4899 - val_loss: 113.5994\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 103.5794 - val_loss: 113.3736\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 103.4798 - val_loss: 117.4931\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 103.4956 - val_loss: 121.1289\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4249 - val_loss: 123.2737\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5710 - val_loss: 112.5642\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4865 - val_loss: 117.5257\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4546 - val_loss: 114.1834\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4871 - val_loss: 114.3149\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.4448 - val_loss: 123.7507\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.3841 - val_loss: 126.7689\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.2777 - val_loss: 123.4271\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.3592 - val_loss: 147.8304\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.2693 - val_loss: 129.5147\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.2343 - val_loss: 127.6218\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.3271 - val_loss: 124.9927\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.1292 - val_loss: 137.6288\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.3354 - val_loss: 112.5493\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.0011 - val_loss: 130.3595\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.0841 - val_loss: 111.3185\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.9939 - val_loss: 127.6035\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1788 - val_loss: 113.1027\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.9705 - val_loss: 115.5928\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.0169 - val_loss: 116.5717\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 103.1043 - val_loss: 112.1626\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.9745 - val_loss: 130.5893\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8763 - val_loss: 123.8020\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.9462 - val_loss: 113.1804\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.9449 - val_loss: 112.4629\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.9013 - val_loss: 113.7172\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7455 - val_loss: 114.9575\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.9299 - val_loss: 111.6148\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8862 - val_loss: 111.2719\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.7101 - val_loss: 115.3426\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.6680 - val_loss: 116.0545\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8504 - val_loss: 123.0403\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7532 - val_loss: 112.3371\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.6611 - val_loss: 118.6940\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7198 - val_loss: 114.8997\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8737 - val_loss: 123.8402\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6762 - val_loss: 116.8124\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7512 - val_loss: 122.1522\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.7522 - val_loss: 118.4820\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.6574 - val_loss: 114.6016\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.7177 - val_loss: 113.2791\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.5538 - val_loss: 116.9420\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6017 - val_loss: 121.9349\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5205 - val_loss: 131.7551\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.5900 - val_loss: 118.8512\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.5964 - val_loss: 116.9730\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5197 - val_loss: 117.1649\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5248 - val_loss: 118.4424\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6034 - val_loss: 111.8979\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5332 - val_loss: 136.7165\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4779 - val_loss: 119.2565\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4091 - val_loss: 115.0208\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.4082 - val_loss: 111.7287\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5011 - val_loss: 112.1517\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4709 - val_loss: 117.0165\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3601 - val_loss: 109.8858\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3121 - val_loss: 112.6215\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.2763 - val_loss: 119.0854\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.1656 - val_loss: 110.2181\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.2039 - val_loss: 110.4863\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.2626 - val_loss: 124.1306\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4664 - val_loss: 118.5193\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.2565 - val_loss: 122.2656\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1565 - val_loss: 111.8143\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1990 - val_loss: 122.9978\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3068 - val_loss: 162.2437\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3383 - val_loss: 112.9947\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1131 - val_loss: 110.1747\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1115 - val_loss: 115.7683\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1836 - val_loss: 114.1399\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1365 - val_loss: 114.6248\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0900 - val_loss: 111.2287\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0852 - val_loss: 111.1298\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9449 - val_loss: 110.2740\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9392 - val_loss: 119.6974\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9481 - val_loss: 112.7112\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1081 - val_loss: 114.3343\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1883 - val_loss: 112.6928\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9530 - val_loss: 114.5087\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.8693 - val_loss: 114.8590\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0453 - val_loss: 113.8656\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8890 - val_loss: 112.4516\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 102.1620 - val_loss: 115.1611\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9513 - val_loss: 117.5099\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0573 - val_loss: 127.2456\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9344 - val_loss: 112.7261\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0141 - val_loss: 114.1931\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9590 - val_loss: 113.7024\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8708 - val_loss: 124.2217\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9804 - val_loss: 114.3690\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0296 - val_loss: 122.6095\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.9868 - val_loss: 111.5901\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.7716 - val_loss: 111.7263\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.9228 - val_loss: 116.0671\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0524 - val_loss: 115.6211\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9258 - val_loss: 113.7859\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6511 - val_loss: 119.4910\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8696 - val_loss: 114.3212\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8282 - val_loss: 111.8717\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.0182 - val_loss: 118.2475\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8698 - val_loss: 111.2540\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.7103 - val_loss: 113.7014\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 101.7932 - val_loss: 110.5918\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 101.8882 - val_loss: 113.1155\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.7091 - val_loss: 111.4444\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7656 - val_loss: 139.5671\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7883 - val_loss: 116.9711\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5899 - val_loss: 112.6628\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.6241 - val_loss: 114.0870\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8033 - val_loss: 119.5813\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6738 - val_loss: 116.0764\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7699 - val_loss: 110.9216\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5626 - val_loss: 116.7751\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8407 - val_loss: 114.9033\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6814 - val_loss: 117.4000\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.6012 - val_loss: 110.0894\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7116 - val_loss: 119.7063\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4703 - val_loss: 112.7682\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3776 - val_loss: 113.9495\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5234 - val_loss: 118.3138\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3710 - val_loss: 110.0468\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5364 - val_loss: 116.0348\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5361 - val_loss: 124.6676\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.7365 - val_loss: 124.5125\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3593 - val_loss: 118.5746\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3727 - val_loss: 110.0441\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5224 - val_loss: 115.6272\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3987 - val_loss: 118.3320\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4252 - val_loss: 111.5297\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4780 - val_loss: 113.1041\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2676 - val_loss: 121.4434\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1507 - val_loss: 126.7749\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1515 - val_loss: 111.1921\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2151 - val_loss: 110.1946\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1328 - val_loss: 112.0884\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2624 - val_loss: 110.8651\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3552 - val_loss: 111.2412\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2975 - val_loss: 111.0568\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2730 - val_loss: 127.9764\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3230 - val_loss: 110.2097\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1558 - val_loss: 110.1693\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1260 - val_loss: 110.4327\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2212 - val_loss: 109.5899\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2109 - val_loss: 111.7847\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1620 - val_loss: 113.4930\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2550 - val_loss: 126.8863\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2869 - val_loss: 109.9241\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1269 - val_loss: 120.2729\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1620 - val_loss: 117.3027\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2189 - val_loss: 111.6049\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1168 - val_loss: 120.5326\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2516 - val_loss: 111.1115\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.3296 - val_loss: 131.0290\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2081 - val_loss: 121.2862\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0957 - val_loss: 112.6948\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1854 - val_loss: 112.2328\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0532 - val_loss: 109.1535\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1620 - val_loss: 123.1104\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2160 - val_loss: 120.7697\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2941 - val_loss: 114.2017\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1411 - val_loss: 126.1129\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9807 - val_loss: 116.8886\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0731 - val_loss: 112.2598\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0535 - val_loss: 113.7937\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1472 - val_loss: 115.6051\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1762 - val_loss: 119.5221\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1153 - val_loss: 109.4429\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0911 - val_loss: 111.1690\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0813 - val_loss: 113.0633\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0447 - val_loss: 115.6741\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0996 - val_loss: 109.0190\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2275 - val_loss: 115.1952\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9915 - val_loss: 113.0553\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9480 - val_loss: 112.0940\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9369 - val_loss: 122.8880\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0492 - val_loss: 120.8238\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2587 - val_loss: 126.8464\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0951 - val_loss: 118.4120\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1796 - val_loss: 120.7015\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1791 - val_loss: 114.1987\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1269 - val_loss: 112.2160\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9051 - val_loss: 117.2851\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1390 - val_loss: 109.2882\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2059 - val_loss: 110.9747\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0699 - val_loss: 115.1138\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.1201 - val_loss: 118.2730\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9981 - val_loss: 113.9977\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0621 - val_loss: 123.0418\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9435 - val_loss: 109.1061\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0136 - val_loss: 113.5125\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0825 - val_loss: 114.9275\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696v_fuFCTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355f5d15-761c-4d27-e2fe-c94be8d0023a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  1.7743090149652305 \n",
            "MAE:  7.9440614064125255 \n",
            "SD:  10.572571386874142\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "fd92fbe6-93ba-406e-de96-8b3c27991d6f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "mdZF2osWCUQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee4334e-e677-4500-afe5-b89fdd06606d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -0.8438705918115348 \n",
            "Ensemble_std:  10.629877455294672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3880669-5b4c-4f61-8e93-e131365ed2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_9 (Dense)             (None, 4)                 512       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 569\n",
            "Trainable params: 553\n",
            "Non-trainable params: 16\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af52f224-6e7a-4b63-99ba-335b29630a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 5ms/step - loss: 3730.4468 - val_loss: 3678.5923\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3573.9336 - val_loss: 3498.1086\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3410.6982 - val_loss: 3237.3137\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3228.8743 - val_loss: 3013.3569\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3022.9600 - val_loss: 2771.4905\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2796.4685 - val_loss: 2582.7705\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2557.0698 - val_loss: 2281.8518\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2312.6375 - val_loss: 1486.7318\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1959.4463 - val_loss: 1961.2330\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1644.8705 - val_loss: 1643.0204\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1370.2729 - val_loss: 1207.1798\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1121.7284 - val_loss: 1109.5220\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 901.5163 - val_loss: 782.6522\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 710.8403 - val_loss: 678.9592\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 549.5220 - val_loss: 474.8772\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 416.1845 - val_loss: 324.7222\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 308.9595 - val_loss: 456.7552\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 224.7596 - val_loss: 206.2928\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 150.2322 - val_loss: 174.9388\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.6390 - val_loss: 142.2876\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 80.9844 - val_loss: 108.3172\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 63.8105 - val_loss: 74.3507\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 53.4821 - val_loss: 57.0708\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 47.2470 - val_loss: 45.7273\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 43.7094 - val_loss: 43.8726\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 41.8302 - val_loss: 50.3040\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.5809 - val_loss: 47.7746\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.7537 - val_loss: 44.1904\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.4414 - val_loss: 43.4416\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.0682 - val_loss: 59.9233\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.8001 - val_loss: 42.5231\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.6337 - val_loss: 48.5889\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.5621 - val_loss: 66.9470\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.4269 - val_loss: 51.6866\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.3454 - val_loss: 52.4039\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.3111 - val_loss: 56.9758\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.2764 - val_loss: 50.9187\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.3119 - val_loss: 42.5102\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.0565 - val_loss: 55.3593\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.0758 - val_loss: 50.3449\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.9590 - val_loss: 46.9273\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.9730 - val_loss: 49.8739\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.0110 - val_loss: 43.9748\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.8586 - val_loss: 55.5921\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.8733 - val_loss: 41.1906\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.7135 - val_loss: 44.1150\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.7899 - val_loss: 47.6025\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.7926 - val_loss: 125.7189\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.8060 - val_loss: 79.4485\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.7701 - val_loss: 86.2234\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.8453 - val_loss: 58.7205\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.7315 - val_loss: 48.0477\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.6920 - val_loss: 41.9406\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.6091 - val_loss: 64.9834\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.5580 - val_loss: 41.9394\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5908 - val_loss: 47.0121\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5278 - val_loss: 59.0383\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5052 - val_loss: 51.5996\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.3607 - val_loss: 47.9538\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.3391 - val_loss: 43.9189\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3378 - val_loss: 41.2227\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.3859 - val_loss: 42.5060\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.2525 - val_loss: 40.4911\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.2708 - val_loss: 46.4019\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.2297 - val_loss: 46.5585\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.1018 - val_loss: 59.4693\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.2236 - val_loss: 43.4997\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1230 - val_loss: 43.4342\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.1879 - val_loss: 41.4226\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0602 - val_loss: 57.0613\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.0658 - val_loss: 47.3699\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0884 - val_loss: 45.0794\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0270 - val_loss: 43.1571\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0101 - val_loss: 40.7373\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.0023 - val_loss: 42.3528\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9299 - val_loss: 54.4238\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9266 - val_loss: 59.4484\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.8860 - val_loss: 45.2285\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.8726 - val_loss: 42.9142\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8368 - val_loss: 42.1671\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9131 - val_loss: 43.1964\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.8473 - val_loss: 46.6658\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.8627 - val_loss: 41.1194\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7678 - val_loss: 46.6076\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.7746 - val_loss: 52.9633\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8101 - val_loss: 41.1887\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7693 - val_loss: 64.2340\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7504 - val_loss: 45.0016\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6752 - val_loss: 57.9960\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7299 - val_loss: 39.9845\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7289 - val_loss: 45.6271\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6689 - val_loss: 43.9592\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6651 - val_loss: 55.3167\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5770 - val_loss: 43.4319\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6353 - val_loss: 55.4521\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6017 - val_loss: 40.3332\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.5446 - val_loss: 43.7707\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.5648 - val_loss: 44.7481\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5741 - val_loss: 39.8534\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.4744 - val_loss: 41.8076\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4435 - val_loss: 43.2961\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4500 - val_loss: 41.4182\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4779 - val_loss: 42.7647\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4099 - val_loss: 52.9839\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4198 - val_loss: 42.5046\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4079 - val_loss: 44.8747\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3322 - val_loss: 52.7554\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3646 - val_loss: 41.8554\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.2890 - val_loss: 43.4211\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.0674 - val_loss: 55.6061\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.9477 - val_loss: 43.7004\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.8136 - val_loss: 50.9382\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.7649 - val_loss: 41.9502\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7054 - val_loss: 41.9951\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7537 - val_loss: 45.7261\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.6612 - val_loss: 39.7304\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.6067 - val_loss: 45.8575\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.5544 - val_loss: 44.3654\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5406 - val_loss: 40.1810\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5161 - val_loss: 51.4485\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5471 - val_loss: 50.9157\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4134 - val_loss: 41.9704\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.5312 - val_loss: 53.4429\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3860 - val_loss: 51.2260\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4144 - val_loss: 42.5424\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3218 - val_loss: 38.2116\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3265 - val_loss: 40.9973\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2963 - val_loss: 40.7503\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3496 - val_loss: 53.0889\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2555 - val_loss: 40.5876\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3092 - val_loss: 41.6263\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2748 - val_loss: 41.4789\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2296 - val_loss: 62.6668\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2154 - val_loss: 46.3569\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2020 - val_loss: 39.1171\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2154 - val_loss: 55.7709\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.1729 - val_loss: 44.0182\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1252 - val_loss: 48.2648\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1316 - val_loss: 45.2432\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0793 - val_loss: 48.2669\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.1339 - val_loss: 41.2789\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0790 - val_loss: 38.1870\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0197 - val_loss: 45.9281\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0593 - val_loss: 43.9213\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0301 - val_loss: 44.8961\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0823 - val_loss: 47.7080\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0330 - val_loss: 50.2388\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0271 - val_loss: 49.2327\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0720 - val_loss: 39.7508\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9776 - val_loss: 42.5724\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9158 - val_loss: 44.4607\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9489 - val_loss: 41.3437\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9377 - val_loss: 41.7437\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0506 - val_loss: 45.5025\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9518 - val_loss: 92.4422\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8603 - val_loss: 41.6672\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9999 - val_loss: 47.8382\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9439 - val_loss: 56.7236\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9714 - val_loss: 39.5714\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9241 - val_loss: 38.2542\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8751 - val_loss: 43.6654\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9237 - val_loss: 44.2398\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9337 - val_loss: 37.6359\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8646 - val_loss: 38.7634\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8661 - val_loss: 43.9633\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8463 - val_loss: 45.8093\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9128 - val_loss: 44.5075\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8355 - val_loss: 38.6120\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8197 - val_loss: 45.7183\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8359 - val_loss: 62.4885\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9141 - val_loss: 40.7288\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8557 - val_loss: 58.9159\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8340 - val_loss: 42.2967\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8354 - val_loss: 50.1726\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8188 - val_loss: 37.8732\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8213 - val_loss: 40.8024\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8294 - val_loss: 48.5813\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7887 - val_loss: 37.7198\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8124 - val_loss: 39.0638\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7933 - val_loss: 40.2037\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7887 - val_loss: 37.6505\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7405 - val_loss: 49.1096\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7802 - val_loss: 42.4239\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7500 - val_loss: 39.2677\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7020 - val_loss: 38.9648\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7176 - val_loss: 41.4136\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7474 - val_loss: 61.0321\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7089 - val_loss: 43.5701\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7648 - val_loss: 39.4829\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7140 - val_loss: 47.1525\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6543 - val_loss: 39.8515\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6989 - val_loss: 42.5309\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6460 - val_loss: 37.4950\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6934 - val_loss: 42.3030\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6962 - val_loss: 47.4848\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6280 - val_loss: 43.8886\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7231 - val_loss: 46.5305\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7082 - val_loss: 41.9734\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7495 - val_loss: 41.2479\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7235 - val_loss: 42.2157\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6169 - val_loss: 39.3775\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6543 - val_loss: 42.1227\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6532 - val_loss: 45.3847\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6760 - val_loss: 53.5454\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6497 - val_loss: 37.5348\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6598 - val_loss: 46.9141\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6249 - val_loss: 39.4716\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6417 - val_loss: 53.3631\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6130 - val_loss: 38.3339\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6162 - val_loss: 39.4942\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5628 - val_loss: 37.7879\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 34.6370 - val_loss: 38.6396\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 34.5919 - val_loss: 39.7113\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5730 - val_loss: 44.2087\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6088 - val_loss: 44.3994\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5760 - val_loss: 41.3318\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5641 - val_loss: 49.6943\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6532 - val_loss: 42.0702\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5330 - val_loss: 38.6845\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5687 - val_loss: 43.0037\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5892 - val_loss: 50.7770\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5157 - val_loss: 45.4014\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5961 - val_loss: 49.7327\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5504 - val_loss: 41.5027\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5779 - val_loss: 45.6285\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5441 - val_loss: 43.1919\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5782 - val_loss: 37.9355\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6311 - val_loss: 50.4864\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5425 - val_loss: 37.8485\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5752 - val_loss: 37.8821\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5413 - val_loss: 53.8684\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5179 - val_loss: 39.2121\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5167 - val_loss: 38.6120\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5713 - val_loss: 49.2660\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5071 - val_loss: 45.1820\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5295 - val_loss: 38.8526\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5626 - val_loss: 39.1477\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5887 - val_loss: 41.7409\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5400 - val_loss: 51.0273\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4937 - val_loss: 41.8362\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5905 - val_loss: 38.5770\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5024 - val_loss: 38.5352\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5422 - val_loss: 47.6391\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5220 - val_loss: 42.9859\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4788 - val_loss: 40.0813\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5402 - val_loss: 42.8847\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5063 - val_loss: 47.3086\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4815 - val_loss: 39.4371\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4501 - val_loss: 40.0689\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5085 - val_loss: 40.0862\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4539 - val_loss: 37.9269\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4831 - val_loss: 58.8322\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4969 - val_loss: 39.1372\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4364 - val_loss: 39.9567\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4857 - val_loss: 38.7224\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4698 - val_loss: 39.1428\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4313 - val_loss: 46.9062\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5040 - val_loss: 45.0287\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5102 - val_loss: 51.6041\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4819 - val_loss: 52.3777\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4345 - val_loss: 42.6080\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4247 - val_loss: 41.0646\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4759 - val_loss: 50.0744\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4318 - val_loss: 38.8934\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4234 - val_loss: 42.7069\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4826 - val_loss: 38.9306\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4220 - val_loss: 40.2779\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3653 - val_loss: 42.5802\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4916 - val_loss: 40.8854\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4712 - val_loss: 40.3844\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4864 - val_loss: 43.1680\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3696 - val_loss: 38.1140\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4423 - val_loss: 39.7866\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4205 - val_loss: 43.9195\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4527 - val_loss: 50.3495\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4150 - val_loss: 48.7215\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4201 - val_loss: 37.9035\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4547 - val_loss: 44.1977\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4045 - val_loss: 42.4645\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4624 - val_loss: 39.5009\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4388 - val_loss: 38.8722\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3823 - val_loss: 43.3286\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4164 - val_loss: 39.5385\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4075 - val_loss: 38.0016\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4379 - val_loss: 40.7000\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4721 - val_loss: 39.5498\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4348 - val_loss: 43.2684\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3597 - val_loss: 37.6149\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3995 - val_loss: 42.3449\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4296 - val_loss: 38.4359\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4829 - val_loss: 49.4298\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4798 - val_loss: 38.9931\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3668 - val_loss: 40.0144\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3761 - val_loss: 37.6476\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4057 - val_loss: 43.3692\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3784 - val_loss: 51.8613\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4000 - val_loss: 42.6640\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4760 - val_loss: 45.7491\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3714 - val_loss: 38.3223\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3511 - val_loss: 53.9354\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4097 - val_loss: 38.4819\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3739 - val_loss: 41.5954\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4280 - val_loss: 45.1368\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3010 - val_loss: 39.6668\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3988 - val_loss: 38.5560\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3785 - val_loss: 38.0163\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4081 - val_loss: 39.2534\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3831 - val_loss: 38.9244\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4089 - val_loss: 38.5859\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3620 - val_loss: 43.0749\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3362 - val_loss: 39.4057\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3893 - val_loss: 39.0572\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3323 - val_loss: 40.7174\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3649 - val_loss: 44.8088\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4148 - val_loss: 40.5494\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3724 - val_loss: 40.0837\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3878 - val_loss: 37.8715\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3159 - val_loss: 42.0782\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3499 - val_loss: 38.1745\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3867 - val_loss: 41.0718\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3828 - val_loss: 47.2433\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3469 - val_loss: 44.1370\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3346 - val_loss: 39.7700\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4138 - val_loss: 39.3364\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3503 - val_loss: 40.4838\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3106 - val_loss: 43.0019\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3152 - val_loss: 38.1279\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3469 - val_loss: 38.8728\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3520 - val_loss: 40.5166\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3827 - val_loss: 44.9934\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3962 - val_loss: 39.2229\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3752 - val_loss: 45.8420\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3979 - val_loss: 38.9184\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3712 - val_loss: 39.4616\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3398 - val_loss: 39.8357\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3026 - val_loss: 40.1590\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2933 - val_loss: 39.3106\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3495 - val_loss: 38.0815\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3160 - val_loss: 39.8172\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3669 - val_loss: 40.3989\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3053 - val_loss: 39.5256\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3928 - val_loss: 40.5486\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3457 - val_loss: 62.6084\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3686 - val_loss: 46.7085\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3523 - val_loss: 38.8463\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3162 - val_loss: 50.1748\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3026 - val_loss: 38.0049\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2900 - val_loss: 40.2974\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3187 - val_loss: 48.0309\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3189 - val_loss: 40.3969\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2900 - val_loss: 49.2515\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2435 - val_loss: 39.7009\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2944 - val_loss: 40.9565\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3753 - val_loss: 42.9780\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3315 - val_loss: 38.7168\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2662 - val_loss: 39.3131\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3273 - val_loss: 42.3280\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3138 - val_loss: 39.5650\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3245 - val_loss: 45.9569\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3823 - val_loss: 39.1258\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2825 - val_loss: 50.5338\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3556 - val_loss: 45.2766\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3376 - val_loss: 39.8169\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3130 - val_loss: 39.7065\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3218 - val_loss: 38.6761\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2457 - val_loss: 40.8850\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3079 - val_loss: 40.7728\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2995 - val_loss: 39.4209\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2456 - val_loss: 52.2923\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2892 - val_loss: 43.5847\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2887 - val_loss: 41.1912\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3026 - val_loss: 38.3036\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2540 - val_loss: 38.2378\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2396 - val_loss: 38.7373\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2862 - val_loss: 38.5774\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2929 - val_loss: 37.9784\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2786 - val_loss: 39.4811\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2885 - val_loss: 42.7872\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3583 - val_loss: 38.3335\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2449 - val_loss: 43.7220\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3106 - val_loss: 40.3593\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3083 - val_loss: 39.7379\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2451 - val_loss: 44.3625\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2709 - val_loss: 39.7076\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2743 - val_loss: 40.8703\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2809 - val_loss: 41.0378\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2468 - val_loss: 40.9035\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2494 - val_loss: 41.6814\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3089 - val_loss: 38.8282\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2434 - val_loss: 39.5795\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2213 - val_loss: 40.1868\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2383 - val_loss: 37.8225\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2280 - val_loss: 41.2844\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2243 - val_loss: 38.7913\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2326 - val_loss: 40.2133\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2217 - val_loss: 47.8460\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2686 - val_loss: 42.0114\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2292 - val_loss: 37.6714\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2188 - val_loss: 38.8332\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2336 - val_loss: 48.9366\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2109 - val_loss: 38.4989\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2299 - val_loss: 43.7007\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2526 - val_loss: 39.0225\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2447 - val_loss: 39.8844\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2143 - val_loss: 39.9050\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2840 - val_loss: 54.0186\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2225 - val_loss: 44.2101\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2485 - val_loss: 38.4023\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2423 - val_loss: 43.5853\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2983 - val_loss: 37.8412\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2243 - val_loss: 37.7066\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2228 - val_loss: 37.9005\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2852 - val_loss: 38.9695\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2219 - val_loss: 40.1455\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2549 - val_loss: 49.1122\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2754 - val_loss: 39.1266\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2001 - val_loss: 39.0162\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2251 - val_loss: 45.4468\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1751 - val_loss: 42.0450\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2612 - val_loss: 49.5321\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2294 - val_loss: 40.7723\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2271 - val_loss: 37.7640\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1922 - val_loss: 43.9025\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2184 - val_loss: 39.1922\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2438 - val_loss: 38.0384\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2036 - val_loss: 43.4228\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2470 - val_loss: 41.6216\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2302 - val_loss: 38.2604\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1612 - val_loss: 38.0204\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1569 - val_loss: 38.1676\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1948 - val_loss: 46.9882\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2162 - val_loss: 38.1620\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1819 - val_loss: 44.5769\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2041 - val_loss: 42.7424\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2060 - val_loss: 39.8807\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2184 - val_loss: 38.5614\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2228 - val_loss: 41.2406\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1772 - val_loss: 40.6856\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2163 - val_loss: 40.6797\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1616 - val_loss: 50.1422\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2084 - val_loss: 39.0537\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1631 - val_loss: 38.7505\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2153 - val_loss: 40.8560\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1576 - val_loss: 48.9461\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1180 - val_loss: 38.4237\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1762 - val_loss: 39.5772\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1301 - val_loss: 37.5095\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1890 - val_loss: 41.7288\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1765 - val_loss: 40.0363\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1495 - val_loss: 41.1903\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1794 - val_loss: 39.4260\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2088 - val_loss: 37.8066\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1578 - val_loss: 38.2703\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1479 - val_loss: 39.5485\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1423 - val_loss: 39.0050\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2256 - val_loss: 39.4884\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2411 - val_loss: 47.5329\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1777 - val_loss: 41.4730\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1125 - val_loss: 38.9879\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1842 - val_loss: 39.4844\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2431 - val_loss: 39.0003\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1647 - val_loss: 40.8699\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2104 - val_loss: 42.9750\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1492 - val_loss: 39.5815\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1646 - val_loss: 41.9863\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1243 - val_loss: 37.5522\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1841 - val_loss: 40.1363\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1753 - val_loss: 38.1114\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1626 - val_loss: 44.2677\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1996 - val_loss: 50.0111\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1792 - val_loss: 37.4996\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1227 - val_loss: 41.7018\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1476 - val_loss: 37.6371\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1354 - val_loss: 38.9747\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1332 - val_loss: 38.9292\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1439 - val_loss: 37.9725\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1589 - val_loss: 40.8197\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1564 - val_loss: 43.0303\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1059 - val_loss: 38.3343\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1421 - val_loss: 39.7265\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1469 - val_loss: 37.6434\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2302 - val_loss: 37.2025\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1517 - val_loss: 42.0347\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1413 - val_loss: 38.8151\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1698 - val_loss: 42.9901\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1112 - val_loss: 47.1104\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1446 - val_loss: 40.0966\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1470 - val_loss: 38.9651\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0879 - val_loss: 37.6021\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1395 - val_loss: 40.3416\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1698 - val_loss: 39.0810\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1216 - val_loss: 38.1927\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1091 - val_loss: 43.5456\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1407 - val_loss: 37.6770\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0691 - val_loss: 37.3902\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.1126 - val_loss: 39.5877\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1433 - val_loss: 38.5165\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1665 - val_loss: 38.2721\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1234 - val_loss: 38.8459\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0963 - val_loss: 41.1541\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c27cfe-c804-427c-91c5-c72d6b947091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -1.2781354197398882 \n",
            "MAE:  4.766879101378069 \n",
            "SD:  6.286530018583582\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cip38xZOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "c33edfa5-39a6-488f-ee59-6fd0581b8955"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8715b08-cc87-4e12-e0fb-59a6949f9754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 4)                 512       \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 569\n",
            "Trainable params: 553\n",
            "Non-trainable params: 16\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f069e24-74d8-4c94-c569-0919590ba709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 3764.2141 - val_loss: 3694.3306\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3661.9976 - val_loss: 3700.9519\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3537.0742 - val_loss: 3454.8870\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3373.5498 - val_loss: 3218.9688\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3150.0125 - val_loss: 2832.8176\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2867.6333 - val_loss: 2578.3403\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2543.5654 - val_loss: 2561.7668\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2168.9810 - val_loss: 1854.8927\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1805.5693 - val_loss: 1723.5214\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1467.1744 - val_loss: 1235.7136\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1161.1082 - val_loss: 1048.1313\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 893.3366 - val_loss: 893.0134\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 667.6469 - val_loss: 621.6503\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 484.6303 - val_loss: 491.7171\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 341.8319 - val_loss: 293.2953\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 235.3858 - val_loss: 215.6327\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 160.6846 - val_loss: 447.0969\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 111.2583 - val_loss: 97.0636\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.3626 - val_loss: 142.5459\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.0100 - val_loss: 103.1292\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 52.4187 - val_loss: 79.6362\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 48.0944 - val_loss: 133.4644\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 44.9823 - val_loss: 267.6935\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 43.9514 - val_loss: 51.7122\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 43.0772 - val_loss: 50.8450\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 42.5180 - val_loss: 54.5518\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 42.2519 - val_loss: 47.2216\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 41.6669 - val_loss: 45.0320\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 41.5448 - val_loss: 212.4702\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.7944 - val_loss: 123.4733\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 40.6862 - val_loss: 145.7832\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 40.7348 - val_loss: 44.2760\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.5097 - val_loss: 169.9100\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 40.5356 - val_loss: 53.3623\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.2122 - val_loss: 41.5537\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.3728 - val_loss: 42.3283\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 40.0157 - val_loss: 102.8017\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.5048 - val_loss: 44.3672\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 40.4656 - val_loss: 181.3304\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.7632 - val_loss: 49.0596\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.8850 - val_loss: 54.7230\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.8076 - val_loss: 62.1795\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.8968 - val_loss: 40.5330\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.2656 - val_loss: 66.3727\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.3649 - val_loss: 42.5091\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.2642 - val_loss: 48.3139\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.2776 - val_loss: 73.6533\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.1784 - val_loss: 45.7221\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.2683 - val_loss: 44.4290\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.2168 - val_loss: 46.1345\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.0549 - val_loss: 69.4025\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.9959 - val_loss: 56.0925\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.2178 - val_loss: 71.3536\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.9269 - val_loss: 115.4026\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.4094 - val_loss: 49.5448\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.8289 - val_loss: 47.5770\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.7790 - val_loss: 51.2474\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.8797 - val_loss: 44.7854\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.9390 - val_loss: 54.1081\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.7421 - val_loss: 44.7254\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.8702 - val_loss: 110.9282\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.0244 - val_loss: 41.9320\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.7389 - val_loss: 40.9248\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.8930 - val_loss: 54.1178\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.9772 - val_loss: 52.1712\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.8612 - val_loss: 88.3656\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.9168 - val_loss: 40.8808\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.6400 - val_loss: 57.3252\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.7989 - val_loss: 51.2873\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.8513 - val_loss: 76.6116\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.7499 - val_loss: 45.1476\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.9235 - val_loss: 82.9192\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.3819 - val_loss: 44.3018\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.6335 - val_loss: 39.3578\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.5102 - val_loss: 72.2894\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.4554 - val_loss: 42.9886\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.5728 - val_loss: 42.8626\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.7350 - val_loss: 41.2953\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.6178 - val_loss: 48.9007\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.6802 - val_loss: 41.1495\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.4876 - val_loss: 62.7041\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.3103 - val_loss: 61.8618\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.3612 - val_loss: 96.8730\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.4317 - val_loss: 40.8763\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.5049 - val_loss: 53.4285\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.4544 - val_loss: 50.0914\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.6099 - val_loss: 47.5339\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.2868 - val_loss: 45.2132\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.1939 - val_loss: 40.4719\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.1816 - val_loss: 67.8754\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.1565 - val_loss: 40.7962\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.1919 - val_loss: 137.9721\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.3171 - val_loss: 48.6835\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.1779 - val_loss: 39.5706\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.1452 - val_loss: 139.7676\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.0813 - val_loss: 45.5984\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.2047 - val_loss: 84.3734\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.2656 - val_loss: 69.4858\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.1079 - val_loss: 42.5228\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.0533 - val_loss: 45.7614\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.0439 - val_loss: 40.1281\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.8469 - val_loss: 41.6085\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.0350 - val_loss: 49.6953\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.9859 - val_loss: 48.1999\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.9182 - val_loss: 46.2333\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.8641 - val_loss: 63.6943\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.0213 - val_loss: 43.8142\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.9102 - val_loss: 40.9445\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.8238 - val_loss: 44.3212\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.8817 - val_loss: 40.9273\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.8608 - val_loss: 48.9556\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.6836 - val_loss: 42.9849\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.8191 - val_loss: 49.7681\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.7048 - val_loss: 40.1284\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.7237 - val_loss: 41.6952\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5555 - val_loss: 52.3172\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5350 - val_loss: 51.3681\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.6368 - val_loss: 54.4539\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.4687 - val_loss: 42.9933\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5800 - val_loss: 49.6023\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.6534 - val_loss: 40.5194\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.6899 - val_loss: 41.0861\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.5745 - val_loss: 41.2316\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.4816 - val_loss: 46.3621\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5297 - val_loss: 56.4548\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.5225 - val_loss: 40.3685\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.2963 - val_loss: 42.5672\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.3261 - val_loss: 49.9013\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.3907 - val_loss: 39.8769\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.2527 - val_loss: 43.1355\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.2092 - val_loss: 44.8958\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.1692 - val_loss: 39.1730\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.0115 - val_loss: 39.3459\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.0378 - val_loss: 44.3015\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.0854 - val_loss: 48.4943\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9727 - val_loss: 49.7970\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9044 - val_loss: 43.1265\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0278 - val_loss: 49.6136\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.8417 - val_loss: 38.5362\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8424 - val_loss: 39.6831\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7977 - val_loss: 47.2022\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.7848 - val_loss: 42.9948\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.6905 - val_loss: 72.7573\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.7020 - val_loss: 45.9934\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.7329 - val_loss: 40.7016\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.7984 - val_loss: 40.3195\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6326 - val_loss: 39.8672\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6887 - val_loss: 44.8538\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.5300 - val_loss: 38.4237\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.6376 - val_loss: 47.3339\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.4624 - val_loss: 42.2246\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.5103 - val_loss: 44.9648\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5241 - val_loss: 48.5299\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5070 - val_loss: 45.7215\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4440 - val_loss: 39.5584\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4525 - val_loss: 43.4157\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.4554 - val_loss: 41.7118\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.3796 - val_loss: 41.3442\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.2797 - val_loss: 38.9096\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.3021 - val_loss: 40.6874\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3494 - val_loss: 41.9941\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.2551 - val_loss: 39.2213\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.2270 - val_loss: 41.4960\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.1781 - val_loss: 42.2799\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.1993 - val_loss: 43.6889\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3069 - val_loss: 39.7059\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.0847 - val_loss: 41.3837\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.1384 - val_loss: 41.2187\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.1058 - val_loss: 43.4094\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.1367 - val_loss: 42.1653\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.0523 - val_loss: 40.4543\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.9890 - val_loss: 40.6217\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.0641 - val_loss: 40.8435\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.0009 - val_loss: 38.7828\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.0192 - val_loss: 38.9635\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.0258 - val_loss: 38.2663\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.9709 - val_loss: 42.4908\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.9780 - val_loss: 40.9299\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.9442 - val_loss: 41.0514\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.8852 - val_loss: 40.0175\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.8573 - val_loss: 40.5979\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.9190 - val_loss: 43.7016\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.9187 - val_loss: 47.9389\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.8764 - val_loss: 39.2896\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.8208 - val_loss: 41.6675\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7664 - val_loss: 39.5249\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.8432 - val_loss: 38.1646\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7093 - val_loss: 59.1260\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7950 - val_loss: 39.2685\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.7820 - val_loss: 38.5834\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7235 - val_loss: 45.1212\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7274 - val_loss: 38.5053\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.6799 - val_loss: 38.3285\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7483 - val_loss: 42.6158\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7724 - val_loss: 38.1677\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7214 - val_loss: 42.7499\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.6011 - val_loss: 45.7394\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.6670 - val_loss: 38.5176\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5830 - val_loss: 39.3560\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.6151 - val_loss: 39.0450\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5730 - val_loss: 42.4572\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5996 - val_loss: 39.7220\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.6004 - val_loss: 49.3902\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4750 - val_loss: 46.5958\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.5948 - val_loss: 50.7353\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5429 - val_loss: 40.2468\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4977 - val_loss: 42.6261\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.5431 - val_loss: 39.1152\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4627 - val_loss: 41.2978\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5049 - val_loss: 39.8558\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4342 - val_loss: 39.6978\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4918 - val_loss: 43.6770\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4975 - val_loss: 41.6928\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4709 - val_loss: 42.0806\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3495 - val_loss: 50.4893\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4219 - val_loss: 45.7642\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3840 - val_loss: 38.5218\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3345 - val_loss: 39.2492\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3354 - val_loss: 42.8830\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3091 - val_loss: 39.9270\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3305 - val_loss: 47.4262\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3003 - val_loss: 38.7675\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2415 - val_loss: 39.9330\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3432 - val_loss: 40.0577\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2991 - val_loss: 39.0982\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2848 - val_loss: 45.9435\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2972 - val_loss: 38.9782\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3246 - val_loss: 40.1599\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2376 - val_loss: 37.9484\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2666 - val_loss: 38.9739\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1916 - val_loss: 38.1740\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2313 - val_loss: 39.1543\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2345 - val_loss: 38.5315\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2082 - val_loss: 38.4607\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2094 - val_loss: 44.8313\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2454 - val_loss: 38.5222\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1670 - val_loss: 38.2216\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.1700 - val_loss: 39.6351\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1568 - val_loss: 37.5921\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1334 - val_loss: 40.8753\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.1670 - val_loss: 38.2618\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1255 - val_loss: 42.0800\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1125 - val_loss: 40.6004\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0744 - val_loss: 38.4284\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0790 - val_loss: 40.5859\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1159 - val_loss: 38.3022\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0211 - val_loss: 38.2866\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0578 - val_loss: 42.3849\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0597 - val_loss: 40.2119\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0186 - val_loss: 38.2909\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0682 - val_loss: 39.4007\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9936 - val_loss: 43.9444\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9981 - val_loss: 41.2640\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9908 - val_loss: 39.6548\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0631 - val_loss: 45.3496\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0417 - val_loss: 38.9772\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0282 - val_loss: 42.0395\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9736 - val_loss: 39.1201\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9889 - val_loss: 41.9753\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9166 - val_loss: 45.7635\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9261 - val_loss: 37.4827\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9354 - val_loss: 39.1437\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9274 - val_loss: 40.2201\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9354 - val_loss: 39.7479\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9499 - val_loss: 39.2349\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9402 - val_loss: 38.6702\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9030 - val_loss: 38.6511\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8644 - val_loss: 37.3949\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8577 - val_loss: 40.5161\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9214 - val_loss: 40.8675\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8925 - val_loss: 38.2675\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9033 - val_loss: 38.2417\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7914 - val_loss: 39.6409\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9061 - val_loss: 39.6906\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8658 - val_loss: 40.4868\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8944 - val_loss: 39.3032\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8038 - val_loss: 39.8957\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8254 - val_loss: 37.9022\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7778 - val_loss: 38.8631\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7248 - val_loss: 43.3460\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8195 - val_loss: 38.3601\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7478 - val_loss: 39.0350\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8011 - val_loss: 38.4693\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7376 - val_loss: 44.1472\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7473 - val_loss: 37.8408\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7030 - val_loss: 42.0708\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6750 - val_loss: 38.4678\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7262 - val_loss: 38.2590\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7303 - val_loss: 38.0405\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7352 - val_loss: 38.1802\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7045 - val_loss: 41.4215\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7404 - val_loss: 38.1641\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7008 - val_loss: 38.9286\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6569 - val_loss: 39.6947\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6875 - val_loss: 50.6150\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6420 - val_loss: 39.8454\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6583 - val_loss: 36.9364\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6711 - val_loss: 38.9825\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6611 - val_loss: 40.6551\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6650 - val_loss: 41.7373\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6080 - val_loss: 38.7470\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6172 - val_loss: 44.0935\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6368 - val_loss: 40.6568\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6667 - val_loss: 37.3409\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5990 - val_loss: 37.3475\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6063 - val_loss: 39.6404\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6011 - val_loss: 37.9615\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5546 - val_loss: 41.2728\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5779 - val_loss: 39.2175\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5593 - val_loss: 38.2675\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5914 - val_loss: 39.2206\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5557 - val_loss: 38.7450\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5973 - val_loss: 42.3406\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5695 - val_loss: 38.0863\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5064 - val_loss: 37.3453\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5274 - val_loss: 37.5087\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5295 - val_loss: 38.4398\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5473 - val_loss: 39.1102\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5357 - val_loss: 37.7803\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5015 - val_loss: 38.6231\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5093 - val_loss: 37.6925\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4708 - val_loss: 36.8056\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4863 - val_loss: 40.1867\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4561 - val_loss: 37.4381\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4700 - val_loss: 36.5599\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4504 - val_loss: 38.1225\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4285 - val_loss: 40.0020\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4606 - val_loss: 38.9593\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4695 - val_loss: 36.7648\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3862 - val_loss: 39.6239\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4500 - val_loss: 43.7858\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4242 - val_loss: 39.0693\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4448 - val_loss: 41.2015\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4182 - val_loss: 36.8575\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4375 - val_loss: 42.2409\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3971 - val_loss: 36.6560\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4542 - val_loss: 36.7287\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4353 - val_loss: 36.5557\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3652 - val_loss: 38.1541\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3864 - val_loss: 42.4968\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4289 - val_loss: 39.3440\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3769 - val_loss: 37.3647\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4023 - val_loss: 42.7229\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3654 - val_loss: 38.8076\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3995 - val_loss: 40.1104\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3435 - val_loss: 41.5817\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3561 - val_loss: 37.2418\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3060 - val_loss: 37.2238\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3851 - val_loss: 37.1813\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3260 - val_loss: 43.4376\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3356 - val_loss: 39.2401\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3314 - val_loss: 37.7319\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3427 - val_loss: 39.8590\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3706 - val_loss: 40.2657\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3707 - val_loss: 41.2330\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3162 - val_loss: 39.0257\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3235 - val_loss: 39.7183\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2776 - val_loss: 36.9732\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3098 - val_loss: 41.2492\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2983 - val_loss: 40.7293\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3274 - val_loss: 40.3423\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2841 - val_loss: 37.6292\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2765 - val_loss: 36.9020\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2904 - val_loss: 37.4160\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3214 - val_loss: 36.9642\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2477 - val_loss: 38.9063\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2740 - val_loss: 42.2919\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3038 - val_loss: 42.8442\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2373 - val_loss: 38.4975\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2528 - val_loss: 40.2815\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2544 - val_loss: 38.8210\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2377 - val_loss: 38.5632\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2690 - val_loss: 38.7263\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2228 - val_loss: 39.2014\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1682 - val_loss: 38.6542\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2522 - val_loss: 40.0530\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2273 - val_loss: 36.9986\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2179 - val_loss: 38.6717\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2298 - val_loss: 38.7102\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1756 - val_loss: 40.0435\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2316 - val_loss: 40.4552\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2020 - val_loss: 37.3693\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2121 - val_loss: 38.1519\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2681 - val_loss: 46.7219\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2381 - val_loss: 46.6072\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1910 - val_loss: 37.9210\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2492 - val_loss: 38.3511\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2298 - val_loss: 36.7454\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2349 - val_loss: 39.8316\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1966 - val_loss: 40.5957\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1624 - val_loss: 36.8572\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1569 - val_loss: 45.3476\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1728 - val_loss: 36.7580\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1434 - val_loss: 37.6497\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1701 - val_loss: 39.5058\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1747 - val_loss: 38.5078\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2100 - val_loss: 42.4377\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1362 - val_loss: 39.6772\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1319 - val_loss: 52.4019\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2081 - val_loss: 36.6075\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1655 - val_loss: 37.9211\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1060 - val_loss: 43.5350\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1869 - val_loss: 38.7482\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1368 - val_loss: 41.5079\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1381 - val_loss: 36.7929\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1814 - val_loss: 38.4477\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1689 - val_loss: 40.4250\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1767 - val_loss: 37.1298\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1377 - val_loss: 47.1602\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1372 - val_loss: 44.3912\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1218 - val_loss: 38.1888\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1619 - val_loss: 38.2892\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1509 - val_loss: 38.8879\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1284 - val_loss: 40.3222\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1692 - val_loss: 40.4801\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1760 - val_loss: 36.9115\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1004 - val_loss: 39.2673\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1452 - val_loss: 53.1035\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1552 - val_loss: 38.9781\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1252 - val_loss: 39.2441\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1571 - val_loss: 37.0179\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.1166 - val_loss: 39.7722\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.1620 - val_loss: 38.1568\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 34.1555 - val_loss: 38.2294\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1216 - val_loss: 37.7158\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0684 - val_loss: 46.3723\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1452 - val_loss: 38.8638\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1104 - val_loss: 38.3675\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0637 - val_loss: 38.7057\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0712 - val_loss: 46.3999\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.0907 - val_loss: 36.3663\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0643 - val_loss: 39.6594\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1262 - val_loss: 40.7588\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1185 - val_loss: 42.9123\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1088 - val_loss: 36.9701\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1026 - val_loss: 40.7322\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1041 - val_loss: 37.5976\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.0755 - val_loss: 37.1308\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.1111 - val_loss: 45.8072\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0941 - val_loss: 41.7359\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1401 - val_loss: 40.9571\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1115 - val_loss: 41.7223\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0628 - val_loss: 37.2199\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0838 - val_loss: 38.6392\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0480 - val_loss: 39.0743\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0737 - val_loss: 38.9961\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0975 - val_loss: 37.2673\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0809 - val_loss: 38.8575\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0507 - val_loss: 40.1500\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0282 - val_loss: 46.7604\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0700 - val_loss: 36.7428\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1280 - val_loss: 36.8428\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0496 - val_loss: 42.9927\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0617 - val_loss: 37.8478\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0642 - val_loss: 38.3607\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0818 - val_loss: 36.9782\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0228 - val_loss: 36.6881\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0520 - val_loss: 40.6866\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0765 - val_loss: 40.6961\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0086 - val_loss: 38.5537\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0865 - val_loss: 42.7508\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0464 - val_loss: 38.8082\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0280 - val_loss: 38.0178\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0390 - val_loss: 36.8434\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0837 - val_loss: 36.7232\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1070 - val_loss: 37.0959\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0392 - val_loss: 38.0584\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0584 - val_loss: 37.7015\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0620 - val_loss: 37.7888\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0435 - val_loss: 36.7969\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0507 - val_loss: 43.1614\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0010 - val_loss: 40.9098\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0305 - val_loss: 39.0171\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0609 - val_loss: 37.7653\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0941 - val_loss: 37.4922\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0463 - val_loss: 37.9250\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1033 - val_loss: 42.1791\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0220 - val_loss: 44.0527\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0618 - val_loss: 39.8885\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0241 - val_loss: 38.1031\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.0835 - val_loss: 38.8300\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0253 - val_loss: 38.6312\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0329 - val_loss: 37.1451\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0724 - val_loss: 38.1590\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0134 - val_loss: 36.4927\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0450 - val_loss: 37.4968\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0418 - val_loss: 39.2742\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0190 - val_loss: 39.2787\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 33.9992 - val_loss: 38.8369\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0277 - val_loss: 44.0039\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0458 - val_loss: 37.9044\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0566 - val_loss: 39.1520\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0692 - val_loss: 43.6964\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0294 - val_loss: 36.9619\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0022 - val_loss: 37.9991\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0109 - val_loss: 37.9265\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0367 - val_loss: 37.9532\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0385 - val_loss: 41.4701\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9949 - val_loss: 37.0010\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0915 - val_loss: 37.3641\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bbb662-7b3f-4ba6-ee81-b752dd7acd6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.9180317788534591 \n",
            "MAE:  4.4447236416404685 \n",
            "SD:  6.043288344910288\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "885dd537-4e2c-4851-8a14-cb61bfc93537"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21986264-46a8-464a-838b-c03718effb50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_15 (Dense)            (None, 4)                 512       \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 569\n",
            "Trainable params: 553\n",
            "Non-trainable params: 16\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bd81b0a-78c4-4311-bfa5-b160b4e659c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3781.5947 - val_loss: 3716.6101\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3673.5249 - val_loss: 3567.6921\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3523.2622 - val_loss: 3413.7961\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3330.7498 - val_loss: 3246.5269\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 3099.1008 - val_loss: 3040.8513\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2834.3518 - val_loss: 2967.2896\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2550.8032 - val_loss: 2076.1421\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 2261.4348 - val_loss: 1963.8567\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1973.7756 - val_loss: 1311.8853\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1692.1913 - val_loss: 2015.1591\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1415.3154 - val_loss: 931.5060\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 1163.3838 - val_loss: 484.5316\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 939.2675 - val_loss: 437.8578\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 745.5788 - val_loss: 413.8785\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 578.8978 - val_loss: 495.3008\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 441.5362 - val_loss: 260.9119\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 332.0530 - val_loss: 379.9669\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 244.6543 - val_loss: 192.0021\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 180.4693 - val_loss: 420.2924\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 133.5190 - val_loss: 353.7278\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 101.8839 - val_loss: 392.8534\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 80.6018 - val_loss: 205.4820\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 66.1829 - val_loss: 287.0855\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 57.8686 - val_loss: 82.0135\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 52.7123 - val_loss: 76.1981\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 49.9065 - val_loss: 472.7050\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 47.8220 - val_loss: 54.4507\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 47.3584 - val_loss: 59.0364\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 46.4300 - val_loss: 96.1141\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 45.8035 - val_loss: 124.0946\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 45.2541 - val_loss: 109.6451\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 44.6988 - val_loss: 271.4164\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 43.9196 - val_loss: 67.1496\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 43.7963 - val_loss: 75.5518\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 43.3504 - val_loss: 58.6884\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 43.5652 - val_loss: 49.7935\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 43.2272 - val_loss: 75.8263\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 42.6441 - val_loss: 59.1432\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 42.5669 - val_loss: 47.8015\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 42.1395 - val_loss: 42.0975\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 42.1378 - val_loss: 49.0606\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 41.9431 - val_loss: 69.0085\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 41.8624 - val_loss: 43.2595\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 41.5212 - val_loss: 45.8359\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 41.3249 - val_loss: 43.8729\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 41.0654 - val_loss: 91.0706\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 40.9768 - val_loss: 47.7012\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.8449 - val_loss: 54.8605\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 40.3088 - val_loss: 44.8080\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.2608 - val_loss: 89.9077\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.0837 - val_loss: 51.0857\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.9761 - val_loss: 48.4675\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.8768 - val_loss: 64.8183\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.6356 - val_loss: 132.6581\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.6299 - val_loss: 120.0471\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.3944 - val_loss: 50.5283\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.3844 - val_loss: 178.5940\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.3134 - val_loss: 43.9999\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.0297 - val_loss: 89.2034\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 39.1319 - val_loss: 47.8561\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.0612 - val_loss: 84.9318\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.9249 - val_loss: 51.2998\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.7000 - val_loss: 64.8177\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.6396 - val_loss: 49.3752\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.7093 - val_loss: 42.3749\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.3898 - val_loss: 44.7927\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.3886 - val_loss: 66.9919\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.3655 - val_loss: 41.9161\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.2930 - val_loss: 45.5455\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.2581 - val_loss: 63.0446\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.2430 - val_loss: 59.0588\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.2106 - val_loss: 58.0564\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.3037 - val_loss: 69.3121\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 38.0185 - val_loss: 53.0520\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.0386 - val_loss: 44.7768\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.1622 - val_loss: 95.7857\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.9246 - val_loss: 50.3020\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.9314 - val_loss: 81.1923\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.8961 - val_loss: 39.4199\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.8905 - val_loss: 44.0634\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.7796 - val_loss: 47.3845\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.6711 - val_loss: 55.2156\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.7214 - val_loss: 95.3021\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.7316 - val_loss: 41.3035\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.6134 - val_loss: 46.5096\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.4968 - val_loss: 43.0796\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5559 - val_loss: 43.5309\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5438 - val_loss: 46.2655\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.4437 - val_loss: 48.0050\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.4085 - val_loss: 41.4306\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.3963 - val_loss: 41.7893\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.3008 - val_loss: 40.8207\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.3324 - val_loss: 68.6807\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.2930 - val_loss: 42.7174\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.2458 - val_loss: 40.2339\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1735 - val_loss: 53.1914\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.1381 - val_loss: 51.0672\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3003 - val_loss: 41.6834\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.2232 - val_loss: 67.5268\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.1122 - val_loss: 41.9155\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.0341 - val_loss: 62.1948\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1018 - val_loss: 40.2558\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.9930 - val_loss: 45.9963\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 37.0302 - val_loss: 45.8626\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.9440 - val_loss: 40.7924\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0876 - val_loss: 41.2963\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.9474 - val_loss: 42.7268\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.8638 - val_loss: 39.9129\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.8799 - val_loss: 40.0572\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7481 - val_loss: 42.5183\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7512 - val_loss: 48.4087\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7042 - val_loss: 39.9133\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.7367 - val_loss: 41.6474\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6359 - val_loss: 76.0136\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6029 - val_loss: 52.7904\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5089 - val_loss: 48.5831\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4349 - val_loss: 43.6554\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3855 - val_loss: 39.3130\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3758 - val_loss: 40.1967\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.3265 - val_loss: 38.0279\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.3180 - val_loss: 42.3713\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.1870 - val_loss: 45.8649\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.2373 - val_loss: 46.4837\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.1220 - val_loss: 38.3737\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.0785 - val_loss: 37.8574\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 36.0360 - val_loss: 49.9952\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.9911 - val_loss: 43.9386\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.9178 - val_loss: 39.5370\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.9374 - val_loss: 38.6136\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.9118 - val_loss: 44.9325\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.8282 - val_loss: 39.8558\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7457 - val_loss: 41.5918\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.7513 - val_loss: 39.9513\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.8162 - val_loss: 49.2876\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.7011 - val_loss: 46.3116\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.6693 - val_loss: 40.0138\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.6431 - val_loss: 70.8631\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.6540 - val_loss: 66.2223\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.6260 - val_loss: 46.9112\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5845 - val_loss: 67.3998\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.6121 - val_loss: 38.7793\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.5629 - val_loss: 63.8399\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4745 - val_loss: 39.5161\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4531 - val_loss: 58.9131\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4457 - val_loss: 74.1855\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4348 - val_loss: 42.2285\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4568 - val_loss: 40.7892\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4267 - val_loss: 47.8156\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4112 - val_loss: 41.9663\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4194 - val_loss: 39.4115\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3316 - val_loss: 39.1610\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.4696 - val_loss: 45.9579\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3761 - val_loss: 48.9299\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4043 - val_loss: 46.1994\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3030 - val_loss: 51.4390\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2295 - val_loss: 40.3579\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3268 - val_loss: 39.8823\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3232 - val_loss: 56.8015\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2873 - val_loss: 43.3243\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.3008 - val_loss: 53.0907\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2524 - val_loss: 38.4361\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2286 - val_loss: 52.4338\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2626 - val_loss: 44.3619\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2545 - val_loss: 41.0791\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.1775 - val_loss: 41.8892\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1622 - val_loss: 44.9015\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.2179 - val_loss: 43.2510\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1461 - val_loss: 43.6077\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.1234 - val_loss: 42.7949\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2064 - val_loss: 73.4706\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1356 - val_loss: 40.2190\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1954 - val_loss: 42.7451\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.1551 - val_loss: 39.8701\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0821 - val_loss: 41.8681\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.1495 - val_loss: 41.6095\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0409 - val_loss: 38.4702\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0771 - val_loss: 56.6076\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0367 - val_loss: 41.6594\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0770 - val_loss: 39.8926\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.1111 - val_loss: 43.3355\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0296 - val_loss: 41.2732\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0291 - val_loss: 47.4854\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9860 - val_loss: 41.7255\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9527 - val_loss: 42.1646\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0020 - val_loss: 46.0094\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9657 - val_loss: 41.6822\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9722 - val_loss: 40.8635\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 35.0249 - val_loss: 45.3074\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9548 - val_loss: 44.9529\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9650 - val_loss: 40.9922\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9616 - val_loss: 38.2502\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9375 - val_loss: 39.9955\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8487 - val_loss: 39.3410\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9456 - val_loss: 41.8495\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8978 - val_loss: 38.1054\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9285 - val_loss: 39.0474\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8811 - val_loss: 41.2061\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9510 - val_loss: 44.7491\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9033 - val_loss: 42.4317\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9182 - val_loss: 40.5749\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9380 - val_loss: 41.4619\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8693 - val_loss: 42.0263\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8548 - val_loss: 42.5508\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8542 - val_loss: 39.3295\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8407 - val_loss: 51.0543\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8666 - val_loss: 38.9802\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.9457 - val_loss: 40.6453\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8618 - val_loss: 49.8423\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8114 - val_loss: 40.1936\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8074 - val_loss: 40.7770\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8333 - val_loss: 39.7670\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9276 - val_loss: 39.7593\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7382 - val_loss: 38.5478\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8613 - val_loss: 42.0489\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7814 - val_loss: 38.1666\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.8048 - val_loss: 40.3423\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7594 - val_loss: 41.6621\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7405 - val_loss: 37.9791\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7841 - val_loss: 41.1840\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7800 - val_loss: 41.0712\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7560 - val_loss: 38.3625\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7273 - val_loss: 43.1633\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7986 - val_loss: 55.3226\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7758 - val_loss: 40.9422\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6890 - val_loss: 40.7922\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7317 - val_loss: 41.5137\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6292 - val_loss: 51.0564\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7589 - val_loss: 61.2909\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6942 - val_loss: 41.8560\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6913 - val_loss: 44.8056\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7578 - val_loss: 44.7811\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7626 - val_loss: 44.4368\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6668 - val_loss: 42.8945\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7054 - val_loss: 39.7740\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7715 - val_loss: 39.3956\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.7050 - val_loss: 40.6113\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7199 - val_loss: 39.6253\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7463 - val_loss: 42.5694\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6495 - val_loss: 38.4798\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6792 - val_loss: 38.0008\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7049 - val_loss: 37.5923\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6866 - val_loss: 44.5170\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6452 - val_loss: 47.2931\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6576 - val_loss: 42.0002\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6715 - val_loss: 41.0750\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6200 - val_loss: 41.0880\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6102 - val_loss: 40.1219\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6653 - val_loss: 41.5979\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6855 - val_loss: 85.6584\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6393 - val_loss: 44.3726\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6089 - val_loss: 37.8425\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6297 - val_loss: 42.5615\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6848 - val_loss: 38.3577\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6163 - val_loss: 49.2785\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5905 - val_loss: 38.2259\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5951 - val_loss: 41.0439\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5306 - val_loss: 42.1924\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5997 - val_loss: 38.0711\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5741 - val_loss: 50.3220\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6082 - val_loss: 41.4285\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5852 - val_loss: 48.8346\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.6399 - val_loss: 40.9411\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5699 - val_loss: 39.7909\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5246 - val_loss: 38.1669\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5777 - val_loss: 44.8506\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 34.5232 - val_loss: 40.1123\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 34.6295 - val_loss: 67.6766\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5216 - val_loss: 40.8015\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5067 - val_loss: 40.5762\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5568 - val_loss: 40.7033\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5170 - val_loss: 40.5440\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5600 - val_loss: 41.1521\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5288 - val_loss: 40.1980\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5286 - val_loss: 41.6710\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4986 - val_loss: 41.1578\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5567 - val_loss: 41.3412\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5689 - val_loss: 42.8102\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5291 - val_loss: 54.1855\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4771 - val_loss: 38.5747\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5859 - val_loss: 50.9306\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5804 - val_loss: 40.5918\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5469 - val_loss: 41.5483\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5006 - val_loss: 45.2903\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5035 - val_loss: 52.9313\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4588 - val_loss: 38.2182\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4778 - val_loss: 39.0639\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5148 - val_loss: 40.0765\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4568 - val_loss: 40.7420\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4325 - val_loss: 46.4939\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4117 - val_loss: 42.7230\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4334 - val_loss: 39.0020\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4211 - val_loss: 43.1703\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5048 - val_loss: 54.3330\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5577 - val_loss: 42.2946\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4687 - val_loss: 41.0514\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5283 - val_loss: 39.5014\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4995 - val_loss: 53.5691\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4972 - val_loss: 40.4638\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3980 - val_loss: 41.9249\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4384 - val_loss: 39.0428\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4507 - val_loss: 41.8759\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4813 - val_loss: 37.8075\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4385 - val_loss: 42.0704\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4926 - val_loss: 42.3107\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4367 - val_loss: 40.3817\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4938 - val_loss: 46.0200\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4399 - val_loss: 40.9585\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4075 - val_loss: 49.1756\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4682 - val_loss: 43.6692\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4519 - val_loss: 46.0419\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4540 - val_loss: 38.7801\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3778 - val_loss: 42.4850\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4249 - val_loss: 64.2542\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4927 - val_loss: 50.7599\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4057 - val_loss: 43.5567\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4263 - val_loss: 41.9030\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.5228 - val_loss: 50.4188\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4217 - val_loss: 39.1544\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4062 - val_loss: 39.9548\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4350 - val_loss: 43.5352\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3881 - val_loss: 42.6510\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3927 - val_loss: 44.5429\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3302 - val_loss: 38.4009\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4041 - val_loss: 40.4603\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4448 - val_loss: 39.0591\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4154 - val_loss: 42.2485\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4028 - val_loss: 44.3903\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4339 - val_loss: 38.8755\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3523 - val_loss: 37.9270\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4011 - val_loss: 38.6038\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4092 - val_loss: 39.7735\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4203 - val_loss: 38.8347\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3393 - val_loss: 39.6985\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4222 - val_loss: 47.9344\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4105 - val_loss: 41.3338\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3589 - val_loss: 41.9636\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4194 - val_loss: 41.5739\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4537 - val_loss: 39.7743\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3493 - val_loss: 48.1630\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3893 - val_loss: 40.9224\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4487 - val_loss: 38.6464\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3437 - val_loss: 37.8556\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4195 - val_loss: 45.1191\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4082 - val_loss: 39.2149\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3747 - val_loss: 40.9405\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3807 - val_loss: 38.5724\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3776 - val_loss: 42.4253\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3518 - val_loss: 44.2255\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3367 - val_loss: 39.6540\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3568 - val_loss: 42.1094\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3915 - val_loss: 38.3511\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3412 - val_loss: 38.8279\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4084 - val_loss: 38.1399\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3778 - val_loss: 41.5109\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.4025 - val_loss: 39.8197\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3620 - val_loss: 45.3845\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3630 - val_loss: 37.5776\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4318 - val_loss: 42.4032\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3641 - val_loss: 38.6185\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4456 - val_loss: 38.9550\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3340 - val_loss: 40.1346\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3986 - val_loss: 40.6970\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3258 - val_loss: 41.8964\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3528 - val_loss: 39.7984\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3718 - val_loss: 44.3330\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4157 - val_loss: 39.9432\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4177 - val_loss: 41.8525\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2952 - val_loss: 38.3282\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2985 - val_loss: 43.8398\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4130 - val_loss: 40.0382\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3815 - val_loss: 38.3681\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3519 - val_loss: 39.4049\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3669 - val_loss: 50.1208\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3531 - val_loss: 39.6081\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3710 - val_loss: 57.4867\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4234 - val_loss: 48.9402\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3198 - val_loss: 46.0774\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3343 - val_loss: 40.1455\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3950 - val_loss: 38.4968\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3943 - val_loss: 40.7870\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3401 - val_loss: 47.4262\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3192 - val_loss: 39.0936\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2379 - val_loss: 39.0535\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2598 - val_loss: 44.0901\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3455 - val_loss: 47.2415\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3131 - val_loss: 38.7192\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3237 - val_loss: 46.3299\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3561 - val_loss: 40.4776\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2889 - val_loss: 39.2244\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3708 - val_loss: 49.2381\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3087 - val_loss: 38.5630\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2764 - val_loss: 39.0205\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3515 - val_loss: 40.8353\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3942 - val_loss: 38.2202\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2865 - val_loss: 43.2964\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2708 - val_loss: 40.3771\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2967 - val_loss: 39.8152\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2996 - val_loss: 40.1778\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2903 - val_loss: 47.6098\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2896 - val_loss: 39.6588\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3790 - val_loss: 40.7521\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2921 - val_loss: 50.6310\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3640 - val_loss: 42.2424\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3135 - val_loss: 39.2960\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.3046 - val_loss: 49.0787\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3495 - val_loss: 45.4136\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2912 - val_loss: 39.0887\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3145 - val_loss: 39.2007\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3309 - val_loss: 38.3653\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2936 - val_loss: 39.7738\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3031 - val_loss: 64.2704\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3116 - val_loss: 38.1736\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3321 - val_loss: 42.8375\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2588 - val_loss: 37.7539\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3405 - val_loss: 51.3567\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2857 - val_loss: 44.8079\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2752 - val_loss: 42.9534\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3481 - val_loss: 40.7909\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3282 - val_loss: 41.2766\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2997 - val_loss: 45.5794\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2814 - val_loss: 40.0940\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2781 - val_loss: 41.1236\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2901 - val_loss: 39.6650\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3015 - val_loss: 49.7747\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2913 - val_loss: 40.0975\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2539 - val_loss: 50.2255\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3219 - val_loss: 39.5417\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2929 - val_loss: 39.5006\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2665 - val_loss: 44.8388\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3405 - val_loss: 42.2506\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2851 - val_loss: 42.5169\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2970 - val_loss: 49.7265\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2641 - val_loss: 39.1272\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3015 - val_loss: 38.8170\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2738 - val_loss: 39.5203\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2666 - val_loss: 38.7038\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1845 - val_loss: 41.0487\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2395 - val_loss: 44.1674\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3082 - val_loss: 39.8553\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2768 - val_loss: 39.9173\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2257 - val_loss: 44.3530\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3009 - val_loss: 41.2865\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2793 - val_loss: 40.5674\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2932 - val_loss: 40.4616\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3037 - val_loss: 39.5361\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2474 - val_loss: 45.3766\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3025 - val_loss: 41.7568\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2620 - val_loss: 40.4403\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2333 - val_loss: 40.0701\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2735 - val_loss: 41.8406\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2873 - val_loss: 38.7566\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3274 - val_loss: 38.4573\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2930 - val_loss: 43.9340\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2205 - val_loss: 40.0416\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2196 - val_loss: 40.0594\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2937 - val_loss: 39.5727\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2480 - val_loss: 51.3301\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2553 - val_loss: 39.8216\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2417 - val_loss: 40.1794\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2367 - val_loss: 38.6335\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2231 - val_loss: 40.2406\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2066 - val_loss: 41.0094\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2725 - val_loss: 39.0340\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 4ms/step - loss: 34.2433 - val_loss: 41.9653\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2266 - val_loss: 39.6731\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2493 - val_loss: 39.7346\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2040 - val_loss: 39.7213\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2412 - val_loss: 40.5627\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2347 - val_loss: 40.5174\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2183 - val_loss: 39.1830\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1679 - val_loss: 40.5311\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1493 - val_loss: 41.9137\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2309 - val_loss: 40.6846\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1932 - val_loss: 41.5024\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2360 - val_loss: 46.4306\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2510 - val_loss: 38.5414\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2244 - val_loss: 39.7863\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2209 - val_loss: 41.9328\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2614 - val_loss: 38.2208\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3160 - val_loss: 38.1303\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2417 - val_loss: 43.0707\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2811 - val_loss: 41.0731\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2217 - val_loss: 40.2632\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3090 - val_loss: 39.0265\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2142 - val_loss: 49.4020\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2373 - val_loss: 41.0498\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2398 - val_loss: 41.8178\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1705 - val_loss: 38.7724\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3059 - val_loss: 40.7925\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2322 - val_loss: 39.2550\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2255 - val_loss: 37.7757\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2550 - val_loss: 48.7778\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2034 - val_loss: 38.8304\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2391 - val_loss: 38.5757\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2341 - val_loss: 43.5281\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1897 - val_loss: 41.5307\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1731 - val_loss: 39.6431\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1804 - val_loss: 38.4653\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1685 - val_loss: 38.8754\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2449 - val_loss: 39.0298\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367c8f9c-678a-4700-f51d-ce0d0b8feded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.6754803433639762 \n",
            "MAE:  4.651615714009073 \n",
            "SD:  6.210757359635859\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c45a645-22f4-4cc1-f7da-fc92b75262ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6284752b-1c02-4682-957c-31f2cc9237d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -0.3451946614168018 \n",
            "Ensemble_std:  6.180191907709911\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "\bBP_hv3_6(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}