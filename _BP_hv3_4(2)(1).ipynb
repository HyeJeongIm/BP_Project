{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/_BP_hv3_4(2)(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiiiBla2-j1S"
      },
      "source": [
        "# batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCoux5AOZnK",
        "outputId": "05ec15c5-4d35-4fbf-df2c-7869c9eda218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version :  3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]\n",
            "TensorFlow version :  2.3.0\n",
            "Keras version :  2.4.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러오기\n",
        "# train_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/train_shuffled_raw_v1.mat')\n",
        "# test_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/test_not_shuffled_raw_v1.mat')\n",
        "\n",
        "train_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/new/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/new/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "b570a293-9e53-473a-f3ed-a9b19951a83d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "8f731ca9-0203-46e7-87ab-30015e694029"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "8abc45d0-bd5c-49cd-8d12-1cde358bdca2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "outputs": [],
      "source": [
        "#parameter\n",
        "batch_size = 128\n",
        "epochs = 300\n",
        "lrate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN",
        "outputId": "224d4945-5ae7-44ac-e11b-92d3111e2d6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8SHBwBOZnO"
      },
      "outputs": [],
      "source": [
        "# model = model1()\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGT6-7NcOZnO",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1319/1319 [==============================] - 45s 34ms/step - loss: 10277.4111 - val_loss: 8285.3135\n",
            "Epoch 2/300\n",
            "1319/1319 [==============================] - 41s 31ms/step - loss: 3377.9248 - val_loss: 1001.0939\n",
            "Epoch 3/300\n",
            "1319/1319 [==============================] - 45s 34ms/step - loss: 300.9760 - val_loss: 150.0457\n",
            "Epoch 4/300\n",
            "1319/1319 [==============================] - 45s 34ms/step - loss: 138.6676 - val_loss: 207.0417\n",
            "Epoch 5/300\n",
            "1319/1319 [==============================] - 41s 31ms/step - loss: 125.4987 - val_loss: 123.2754\n",
            "Epoch 6/300\n",
            "1319/1319 [==============================] - 25s 19ms/step - loss: 113.1460 - val_loss: 137.7837\n",
            "Epoch 7/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 110.0886 - val_loss: 158.6056\n",
            "Epoch 8/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 102.7875 - val_loss: 147.6842\n",
            "Epoch 9/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 99.3465 - val_loss: 106.7581\n",
            "Epoch 10/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 97.6046 - val_loss: 154.9752\n",
            "Epoch 11/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 95.1753 - val_loss: 112.1205\n",
            "Epoch 12/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 93.9159 - val_loss: 109.6952\n",
            "Epoch 13/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 92.3180 - val_loss: 104.6490\n",
            "Epoch 14/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 91.6954 - val_loss: 115.9265\n",
            "Epoch 15/300\n",
            "1319/1319 [==============================] - 17s 13ms/step - loss: 89.5842 - val_loss: 161.6397\n",
            "Epoch 16/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 90.5908 - val_loss: 98.1237\n",
            "Epoch 17/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 88.3964 - val_loss: 113.5975\n",
            "Epoch 18/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 87.0989 - val_loss: 100.3606\n",
            "Epoch 19/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 85.4148 - val_loss: 141.4666\n",
            "Epoch 20/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 85.1832 - val_loss: 135.7454\n",
            "Epoch 21/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 85.8916 - val_loss: 100.2343\n",
            "Epoch 22/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 84.3128 - val_loss: 97.4600\n",
            "Epoch 23/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 84.1491 - val_loss: 107.5697\n",
            "Epoch 24/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 83.8896 - val_loss: 114.9403\n",
            "Epoch 25/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 82.5452 - val_loss: 95.0017\n",
            "Epoch 26/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 81.9363 - val_loss: 100.4551\n",
            "Epoch 27/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 81.7737 - val_loss: 141.7054\n",
            "Epoch 28/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 81.9389 - val_loss: 91.0816\n",
            "Epoch 29/300\n",
            "1319/1319 [==============================] - 19s 15ms/step - loss: 81.3157 - val_loss: 97.1230\n",
            "Epoch 30/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 80.8087 - val_loss: 103.2374\n",
            "Epoch 31/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 80.2577 - val_loss: 163.6913\n",
            "Epoch 32/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 79.8703 - val_loss: 101.1264\n",
            "Epoch 33/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 79.9113 - val_loss: 87.9372\n",
            "Epoch 34/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 78.7188 - val_loss: 89.5559\n",
            "Epoch 35/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 78.3563 - val_loss: 103.4345\n",
            "Epoch 36/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 77.8953 - val_loss: 95.8368\n",
            "Epoch 37/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 77.4157 - val_loss: 105.5784\n",
            "Epoch 38/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 77.6374 - val_loss: 95.9728\n",
            "Epoch 39/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 77.6454 - val_loss: 94.7513\n",
            "Epoch 40/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 76.6375 - val_loss: 116.0837\n",
            "Epoch 41/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 76.3721 - val_loss: 155.2231\n",
            "Epoch 42/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 75.9225 - val_loss: 96.4529\n",
            "Epoch 43/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 76.4510 - val_loss: 90.5647\n",
            "Epoch 44/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 75.4747 - val_loss: 94.7537\n",
            "Epoch 45/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 75.9405 - val_loss: 93.4546\n",
            "Epoch 46/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 76.6064 - val_loss: 92.0347\n",
            "Epoch 47/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 75.9525 - val_loss: 115.6501\n",
            "Epoch 48/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 76.5449 - val_loss: 95.6892\n",
            "Epoch 49/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 75.3684 - val_loss: 196.0389\n",
            "Epoch 50/300\n",
            "1319/1319 [==============================] - 19s 15ms/step - loss: 75.5612 - val_loss: 97.0721\n",
            "Epoch 51/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 76.1674 - val_loss: 107.5478\n",
            "Epoch 52/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 74.9737 - val_loss: 143.5860\n",
            "Epoch 53/300\n",
            "1319/1319 [==============================] - 17s 13ms/step - loss: 74.9907 - val_loss: 97.7103\n",
            "Epoch 54/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 75.2255 - val_loss: 102.3421\n",
            "Epoch 55/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 75.1087 - val_loss: 90.2864\n",
            "Epoch 56/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 74.1465 - val_loss: 105.7727\n",
            "Epoch 57/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 74.6520 - val_loss: 108.3903\n",
            "Epoch 58/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 74.0571 - val_loss: 113.2801\n",
            "Epoch 59/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 73.8149 - val_loss: 103.7194\n",
            "Epoch 60/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 73.6435 - val_loss: 93.0004\n",
            "Epoch 61/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 73.3869 - val_loss: 95.3008\n",
            "Epoch 62/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 73.4198 - val_loss: 105.0648\n",
            "Epoch 63/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 73.0650 - val_loss: 101.5010\n",
            "Epoch 64/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 73.3190 - val_loss: 210.2529\n",
            "Epoch 65/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.8725 - val_loss: 154.3044\n",
            "Epoch 66/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 73.0535 - val_loss: 94.9869\n",
            "Epoch 67/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 73.6411 - val_loss: 96.1415\n",
            "Epoch 68/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 74.0171 - val_loss: 98.4214\n",
            "Epoch 69/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 73.5688 - val_loss: 128.7034\n",
            "Epoch 70/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 73.2048 - val_loss: 136.5765\n",
            "Epoch 71/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.9050 - val_loss: 97.4767\n",
            "Epoch 72/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 73.0450 - val_loss: 91.4574\n",
            "Epoch 73/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.9028 - val_loss: 98.8591\n",
            "Epoch 74/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.4654 - val_loss: 111.9563\n",
            "Epoch 75/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.7249 - val_loss: 89.0574\n",
            "Epoch 76/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 18s 14ms/step - loss: 73.1749 - val_loss: 88.9752\n",
            "Epoch 77/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.8897 - val_loss: 90.5258\n",
            "Epoch 78/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.8489 - val_loss: 125.4957\n",
            "Epoch 79/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.4533 - val_loss: 158.8855\n",
            "Epoch 80/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.6116 - val_loss: 92.6863\n",
            "Epoch 81/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.3599 - val_loss: 93.1322\n",
            "Epoch 82/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.8698 - val_loss: 111.2091\n",
            "Epoch 83/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.5522 - val_loss: 99.3885\n",
            "Epoch 84/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.2841 - val_loss: 115.0617\n",
            "Epoch 85/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.8664 - val_loss: 98.3672\n",
            "Epoch 86/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.1140 - val_loss: 91.8265\n",
            "Epoch 87/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.2849 - val_loss: 108.0428\n",
            "Epoch 88/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 72.0438 - val_loss: 96.6916\n",
            "Epoch 89/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.9198 - val_loss: 90.3505\n",
            "Epoch 90/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.6483 - val_loss: 102.4049\n",
            "Epoch 91/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.5520 - val_loss: 89.1474\n",
            "Epoch 92/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.2184 - val_loss: 110.9760\n",
            "Epoch 93/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.2740 - val_loss: 88.4949\n",
            "Epoch 94/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.5638 - val_loss: 95.1589\n",
            "Epoch 95/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.2128 - val_loss: 89.9302\n",
            "Epoch 96/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.9952 - val_loss: 94.9153\n",
            "Epoch 97/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.3895 - val_loss: 93.4279\n",
            "Epoch 98/300\n",
            "1319/1319 [==============================] - 17s 13ms/step - loss: 71.5269 - val_loss: 87.3422\n",
            "Epoch 99/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.7659 - val_loss: 91.4894\n",
            "Epoch 100/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.1247 - val_loss: 89.6671\n",
            "Epoch 101/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.9797 - val_loss: 92.0376\n",
            "Epoch 102/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.7333 - val_loss: 98.5339\n",
            "Epoch 103/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.5650 - val_loss: 95.9175\n",
            "Epoch 104/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.3861 - val_loss: 86.1525\n",
            "Epoch 105/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.6279 - val_loss: 106.8855\n",
            "Epoch 106/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.8532 - val_loss: 93.6148\n",
            "Epoch 107/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.4037 - val_loss: 112.5847\n",
            "Epoch 108/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.9083 - val_loss: 104.1166\n",
            "Epoch 109/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.1049 - val_loss: 203.5590\n",
            "Epoch 110/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.6014 - val_loss: 97.3930\n",
            "Epoch 111/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.5398 - val_loss: 87.8766\n",
            "Epoch 112/300\n",
            "1319/1319 [==============================] - 19s 15ms/step - loss: 70.5714 - val_loss: 122.5206\n",
            "Epoch 113/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.8292 - val_loss: 102.9828\n",
            "Epoch 114/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.7848 - val_loss: 92.1152\n",
            "Epoch 115/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.7961 - val_loss: 100.5786\n",
            "Epoch 116/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.6664 - val_loss: 89.7836\n",
            "Epoch 117/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.8580 - val_loss: 116.4789\n",
            "Epoch 118/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.3355 - val_loss: 87.8020\n",
            "Epoch 119/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.4655 - val_loss: 85.7918\n",
            "Epoch 120/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.9483 - val_loss: 87.0012\n",
            "Epoch 121/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.9441 - val_loss: 98.5339\n",
            "Epoch 122/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.8957 - val_loss: 88.2855\n",
            "Epoch 123/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.8869 - val_loss: 110.9439\n",
            "Epoch 124/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.8230 - val_loss: 125.9077\n",
            "Epoch 125/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.0623 - val_loss: 154.2166\n",
            "Epoch 126/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.7957 - val_loss: 89.3484\n",
            "Epoch 127/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.4636 - val_loss: 183.1122\n",
            "Epoch 128/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.5885 - val_loss: 84.8663\n",
            "Epoch 129/300\n",
            "1319/1319 [==============================] - 20s 15ms/step - loss: 69.9747 - val_loss: 98.9035\n",
            "Epoch 130/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.1009 - val_loss: 104.1539\n",
            "Epoch 131/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 71.2697 - val_loss: 112.9762\n",
            "Epoch 132/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.7940 - val_loss: 99.4088\n",
            "Epoch 133/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.3031 - val_loss: 109.4748\n",
            "Epoch 134/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.2504 - val_loss: 109.8220\n",
            "Epoch 135/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.4845 - val_loss: 102.7357\n",
            "Epoch 136/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.2271 - val_loss: 148.0656\n",
            "Epoch 137/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.1393 - val_loss: 94.5814\n",
            "Epoch 138/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.4619 - val_loss: 90.6823\n",
            "Epoch 139/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.6305 - val_loss: 88.0222\n",
            "Epoch 140/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 70.0176 - val_loss: 92.3615\n",
            "Epoch 141/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.0863 - val_loss: 85.4950\n",
            "Epoch 142/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.2412 - val_loss: 97.1354\n",
            "Epoch 143/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 68.8318 - val_loss: 84.7844\n",
            "Epoch 144/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.1855 - val_loss: 88.1973\n",
            "Epoch 145/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.0738 - val_loss: 96.6393\n",
            "Epoch 146/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 69.5323 - val_loss: 98.0823\n",
            "Epoch 147/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 68.5274 - val_loss: 90.9076\n",
            "Epoch 148/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 68.5548 - val_loss: 91.8602\n",
            "Epoch 149/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 68.0944 - val_loss: 96.0586\n",
            "Epoch 150/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 68.7158 - val_loss: 94.2802\n",
            "Epoch 151/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 18s 14ms/step - loss: 68.1256 - val_loss: 113.5141\n",
            "Epoch 152/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 68.8416 - val_loss: 88.1411\n",
            "Epoch 153/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 68.0045 - val_loss: 89.4111\n",
            "Epoch 154/300\n",
            "1319/1319 [==============================] - 18s 14ms/step - loss: 68.8830 - val_loss: 100.7325\n",
            "Epoch 155/300\n",
            "1319/1319 [==============================] - 9s 7ms/step - loss: 68.9712 - val_loss: 185.7653\n",
            "Epoch 156/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.2671 - val_loss: 95.9560\n",
            "Epoch 157/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.4560 - val_loss: 95.9484\n",
            "Epoch 158/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.1657 - val_loss: 90.5365\n",
            "Epoch 159/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.2742 - val_loss: 90.3777\n",
            "Epoch 160/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.2348 - val_loss: 83.7009\n",
            "Epoch 161/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.3528 - val_loss: 95.3337\n",
            "Epoch 162/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.3825 - val_loss: 96.8565\n",
            "Epoch 163/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.4074 - val_loss: 84.4131\n",
            "Epoch 164/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.5802 - val_loss: 93.1607\n",
            "Epoch 165/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.3792 - val_loss: 84.8683\n",
            "Epoch 166/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.3437 - val_loss: 83.4454\n",
            "Epoch 167/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.0564 - val_loss: 89.4491\n",
            "Epoch 168/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.9308 - val_loss: 97.9418\n",
            "Epoch 169/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.8544 - val_loss: 94.2966\n",
            "Epoch 170/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.7395 - val_loss: 130.8151\n",
            "Epoch 171/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.6582 - val_loss: 82.6006\n",
            "Epoch 172/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.0022 - val_loss: 89.0725\n",
            "Epoch 173/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.0044 - val_loss: 100.8269\n",
            "Epoch 174/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.7237 - val_loss: 97.3331\n",
            "Epoch 175/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.5730 - val_loss: 86.4803\n",
            "Epoch 176/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.0288 - val_loss: 85.1700\n",
            "Epoch 177/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.5559 - val_loss: 99.0490\n",
            "Epoch 178/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.5994 - val_loss: 92.5153\n",
            "Epoch 179/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.0020 - val_loss: 83.0716\n",
            "Epoch 180/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.0527 - val_loss: 89.1918\n",
            "Epoch 181/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.2734 - val_loss: 82.4747\n",
            "Epoch 182/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.4847 - val_loss: 85.6493\n",
            "Epoch 183/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.9186 - val_loss: 193.2968\n",
            "Epoch 184/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.7422 - val_loss: 87.2095\n",
            "Epoch 185/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.4300 - val_loss: 92.9358\n",
            "Epoch 186/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.6394 - val_loss: 90.8258\n",
            "Epoch 187/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.8728 - val_loss: 119.0101\n",
            "Epoch 188/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.3202 - val_loss: 97.1032\n",
            "Epoch 189/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2363 - val_loss: 83.3022\n",
            "Epoch 190/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2310 - val_loss: 85.8786\n",
            "Epoch 191/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.6472 - val_loss: 90.7978\n",
            "Epoch 192/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.5412 - val_loss: 95.4722\n",
            "Epoch 193/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.4990 - val_loss: 84.4089\n",
            "Epoch 194/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.0597 - val_loss: 107.0068\n",
            "Epoch 195/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.3403 - val_loss: 90.6642\n",
            "Epoch 196/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.1441 - val_loss: 88.3272\n",
            "Epoch 197/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2057 - val_loss: 114.1215\n",
            "Epoch 198/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.3494 - val_loss: 114.8202\n",
            "Epoch 199/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.8197 - val_loss: 89.5103\n",
            "Epoch 200/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.8267 - val_loss: 97.6168\n",
            "Epoch 201/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.4663 - val_loss: 82.6969\n",
            "Epoch 202/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.4710 - val_loss: 84.8731\n",
            "Epoch 203/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.8838 - val_loss: 82.9901\n",
            "Epoch 204/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.6344 - val_loss: 98.2959\n",
            "Epoch 205/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.0475 - val_loss: 84.7635\n",
            "Epoch 206/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.0537 - val_loss: 89.7549\n",
            "Epoch 207/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.1927 - val_loss: 89.5835\n",
            "Epoch 208/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.0157 - val_loss: 99.7759\n",
            "Epoch 209/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2074 - val_loss: 95.3921\n",
            "Epoch 210/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.1219 - val_loss: 102.5160\n",
            "Epoch 211/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.8892 - val_loss: 101.8171\n",
            "Epoch 212/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.5641 - val_loss: 88.3048\n",
            "Epoch 213/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.4458 - val_loss: 84.2605\n",
            "Epoch 214/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.5560 - val_loss: 93.7386\n",
            "Epoch 215/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.6712 - val_loss: 92.0748\n",
            "Epoch 216/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.3197 - val_loss: 95.7294\n",
            "Epoch 217/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.5838 - val_loss: 90.7444\n",
            "Epoch 218/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.2181 - val_loss: 85.8188\n",
            "Epoch 219/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.9788 - val_loss: 85.0724\n",
            "Epoch 220/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.2089 - val_loss: 83.9645\n",
            "Epoch 221/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.0739 - val_loss: 83.9720\n",
            "Epoch 222/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.1837 - val_loss: 99.5223\n",
            "Epoch 223/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.7867 - val_loss: 81.9066\n",
            "Epoch 224/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.2155 - val_loss: 103.2433\n",
            "Epoch 225/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.0433 - val_loss: 130.8108\n",
            "Epoch 226/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.6136 - val_loss: 84.1654\n",
            "Epoch 227/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.1809 - val_loss: 91.9231\n",
            "Epoch 228/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.2355 - val_loss: 114.2272\n",
            "Epoch 229/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.6223 - val_loss: 90.7654\n",
            "Epoch 230/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.7980 - val_loss: 98.8377\n",
            "Epoch 231/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.8971 - val_loss: 89.8849\n",
            "Epoch 232/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.6412 - val_loss: 92.7377\n",
            "Epoch 233/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.8625 - val_loss: 93.4079\n",
            "Epoch 234/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.7097 - val_loss: 84.4150\n",
            "Epoch 235/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.7876 - val_loss: 94.1118\n",
            "Epoch 236/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.7180 - val_loss: 86.7471\n",
            "Epoch 237/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.5918 - val_loss: 95.9110\n",
            "Epoch 238/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3224 - val_loss: 97.0776\n",
            "Epoch 239/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.2278 - val_loss: 90.2280\n",
            "Epoch 240/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.5601 - val_loss: 122.0523\n",
            "Epoch 241/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4744 - val_loss: 87.7389\n",
            "Epoch 242/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.9174 - val_loss: 111.2570\n",
            "Epoch 243/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.7753 - val_loss: 106.6022\n",
            "Epoch 244/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4263 - val_loss: 82.1964\n",
            "Epoch 245/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4792 - val_loss: 86.0370\n",
            "Epoch 246/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.6664 - val_loss: 91.9347\n",
            "Epoch 247/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.5862 - val_loss: 87.2638\n",
            "Epoch 248/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.8128 - val_loss: 84.4362\n",
            "Epoch 249/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.5051 - val_loss: 89.9463\n",
            "Epoch 250/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.8624 - val_loss: 93.9069\n",
            "Epoch 251/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.9864 - val_loss: 84.3756\n",
            "Epoch 252/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.8799 - val_loss: 127.0912\n",
            "Epoch 253/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.5901 - val_loss: 84.5924\n",
            "Epoch 254/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4398 - val_loss: 85.4144\n",
            "Epoch 255/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4729 - val_loss: 90.0054\n",
            "Epoch 256/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.6177 - val_loss: 94.0799\n",
            "Epoch 257/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3933 - val_loss: 87.2300\n",
            "Epoch 258/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.6668 - val_loss: 84.0772\n",
            "Epoch 259/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3183 - val_loss: 85.5588\n",
            "Epoch 260/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3532 - val_loss: 96.6569\n",
            "Epoch 261/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.6630 - val_loss: 99.8052\n",
            "Epoch 262/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.1580 - val_loss: 83.9560\n",
            "Epoch 263/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.7098 - val_loss: 85.2181\n",
            "Epoch 264/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0843 - val_loss: 91.3972\n",
            "Epoch 265/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.9391 - val_loss: 107.8337\n",
            "Epoch 266/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.5576 - val_loss: 83.9338\n",
            "Epoch 267/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.9849 - val_loss: 98.1447\n",
            "Epoch 268/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.9545 - val_loss: 89.9291\n",
            "Epoch 269/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3605 - val_loss: 89.6576\n",
            "Epoch 270/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3790 - val_loss: 100.3426\n",
            "Epoch 271/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.9982 - val_loss: 88.5554\n",
            "Epoch 272/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.5307 - val_loss: 92.8196\n",
            "Epoch 273/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.7632 - val_loss: 88.1364\n",
            "Epoch 274/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0674 - val_loss: 97.2580\n",
            "Epoch 275/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.6535 - val_loss: 91.8026\n",
            "Epoch 276/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3136 - val_loss: 91.0551\n",
            "Epoch 277/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.9974 - val_loss: 85.8578\n",
            "Epoch 278/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.2788 - val_loss: 82.6472\n",
            "Epoch 279/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3180 - val_loss: 88.2827\n",
            "Epoch 280/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0655 - val_loss: 84.7118\n",
            "Epoch 281/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.1086 - val_loss: 95.5950\n",
            "Epoch 282/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.7857 - val_loss: 96.8410\n",
            "Epoch 283/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.6711 - val_loss: 91.1516\n",
            "Epoch 284/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.8426 - val_loss: 95.1838\n",
            "Epoch 285/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.2835 - val_loss: 83.9971\n",
            "Epoch 286/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.8239 - val_loss: 83.7785\n",
            "Epoch 287/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.6605 - val_loss: 91.4499\n",
            "Epoch 288/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.8740 - val_loss: 89.5210\n",
            "Epoch 289/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0310 - val_loss: 86.9948\n",
            "Epoch 290/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5349 - val_loss: 85.1188\n",
            "Epoch 291/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.8209 - val_loss: 118.3884\n",
            "Epoch 292/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.8252 - val_loss: 84.2465\n",
            "Epoch 293/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5372 - val_loss: 82.6907\n",
            "Epoch 294/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.4647 - val_loss: 82.4012\n",
            "Epoch 295/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5733 - val_loss: 87.9088\n",
            "Epoch 296/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.9384 - val_loss: 95.3813\n",
            "Epoch 297/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.9572 - val_loss: 137.9953\n",
            "Epoch 298/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5025 - val_loss: 83.2701\n",
            "Epoch 299/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.8877 - val_loss: 87.9421\n",
            "Epoch 300/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.4533 - val_loss: 98.4240\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -3.310510407337586 \n",
            "MAE:  7.611199105292496 \n",
            "SD:  9.352249177696613\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su2Sj5jZCSwH",
        "outputId": "4a85f233-7730-4d19-a30a-66ad769ace4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPRh6v-mCSwH",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 10695.3643 - val_loss: 7311.4482\n",
            "Epoch 2/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 4335.1187 - val_loss: 1480.5641\n",
            "Epoch 3/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 638.9699 - val_loss: 268.0805\n",
            "Epoch 4/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 137.3043 - val_loss: 152.8720\n",
            "Epoch 5/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 118.3697 - val_loss: 147.1579\n",
            "Epoch 6/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 113.1580 - val_loss: 223.8670\n",
            "Epoch 7/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 105.3012 - val_loss: 139.5678\n",
            "Epoch 8/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 100.6204 - val_loss: 125.6983\n",
            "Epoch 9/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 97.1879 - val_loss: 124.0032\n",
            "Epoch 10/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 95.3096 - val_loss: 118.0687\n",
            "Epoch 11/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 92.8210 - val_loss: 159.8455\n",
            "Epoch 12/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 91.0195 - val_loss: 190.5581\n",
            "Epoch 13/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 88.9835 - val_loss: 151.1251\n",
            "Epoch 14/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 87.4685 - val_loss: 114.9102\n",
            "Epoch 15/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 86.8350 - val_loss: 105.9921\n",
            "Epoch 16/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 86.3846 - val_loss: 110.3142\n",
            "Epoch 17/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 84.3149 - val_loss: 96.5403\n",
            "Epoch 18/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 83.4572 - val_loss: 107.5609\n",
            "Epoch 19/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 82.4842 - val_loss: 103.4323\n",
            "Epoch 20/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 81.5116 - val_loss: 89.9782\n",
            "Epoch 21/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 80.0237 - val_loss: 106.1271\n",
            "Epoch 22/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 79.0076 - val_loss: 104.8296\n",
            "Epoch 23/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 78.4457 - val_loss: 108.4739\n",
            "Epoch 24/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 78.4446 - val_loss: 91.1046\n",
            "Epoch 25/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 77.0682 - val_loss: 204.3407\n",
            "Epoch 26/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.7164 - val_loss: 131.5596\n",
            "Epoch 27/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.0052 - val_loss: 104.5734\n",
            "Epoch 28/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 75.9147 - val_loss: 125.8027\n",
            "Epoch 29/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 75.4880 - val_loss: 98.6254\n",
            "Epoch 30/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 74.9102 - val_loss: 117.8817\n",
            "Epoch 31/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 74.3364 - val_loss: 131.2123\n",
            "Epoch 32/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 73.9863 - val_loss: 137.4204\n",
            "Epoch 33/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 74.1733 - val_loss: 91.4568\n",
            "Epoch 34/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 73.3212 - val_loss: 121.9140\n",
            "Epoch 35/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 73.3016 - val_loss: 102.6802\n",
            "Epoch 36/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.7662 - val_loss: 98.6964\n",
            "Epoch 37/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.7255 - val_loss: 146.5369\n",
            "Epoch 38/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.4442 - val_loss: 87.4504\n",
            "Epoch 39/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.1045 - val_loss: 104.8176\n",
            "Epoch 40/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.7947 - val_loss: 100.3278\n",
            "Epoch 41/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.6908 - val_loss: 90.1365\n",
            "Epoch 42/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.5511 - val_loss: 93.7803\n",
            "Epoch 43/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.9851 - val_loss: 104.4882\n",
            "Epoch 44/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.2728 - val_loss: 102.0500\n",
            "Epoch 45/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.8451 - val_loss: 85.5052\n",
            "Epoch 46/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.7493 - val_loss: 89.8741\n",
            "Epoch 47/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.4885 - val_loss: 111.4311\n",
            "Epoch 48/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.6683 - val_loss: 100.6207\n",
            "Epoch 49/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.5306 - val_loss: 93.2814\n",
            "Epoch 50/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.9893 - val_loss: 93.3161\n",
            "Epoch 51/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.1234 - val_loss: 83.8494\n",
            "Epoch 52/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.0205 - val_loss: 92.3820\n",
            "Epoch 53/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.1876 - val_loss: 95.5388\n",
            "Epoch 54/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.4558 - val_loss: 96.5122\n",
            "Epoch 55/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.0155 - val_loss: 86.2627\n",
            "Epoch 56/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.4598 - val_loss: 90.5797\n",
            "Epoch 57/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.3225 - val_loss: 98.0881\n",
            "Epoch 58/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.5689 - val_loss: 126.7595\n",
            "Epoch 59/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.6798 - val_loss: 86.6494\n",
            "Epoch 60/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.1742 - val_loss: 103.4870\n",
            "Epoch 61/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.8472 - val_loss: 104.2428\n",
            "Epoch 62/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.6073 - val_loss: 99.5557\n",
            "Epoch 63/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.4758 - val_loss: 92.2900\n",
            "Epoch 64/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.8456 - val_loss: 100.7951\n",
            "Epoch 65/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.0165 - val_loss: 92.2889\n",
            "Epoch 66/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.1287 - val_loss: 104.2714\n",
            "Epoch 67/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.1891 - val_loss: 101.8983\n",
            "Epoch 68/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.9779 - val_loss: 94.8143\n",
            "Epoch 69/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.9680 - val_loss: 122.2664\n",
            "Epoch 70/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.2396 - val_loss: 108.7890\n",
            "Epoch 71/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.8606 - val_loss: 103.9356\n",
            "Epoch 72/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.5311 - val_loss: 83.3718\n",
            "Epoch 73/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2051 - val_loss: 98.6994\n",
            "Epoch 74/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.6206 - val_loss: 96.3248\n",
            "Epoch 75/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2384 - val_loss: 116.7469\n",
            "Epoch 76/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2183 - val_loss: 99.5564\n",
            "Epoch 77/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.3227 - val_loss: 109.8207\n",
            "Epoch 78/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.1532 - val_loss: 85.9908\n",
            "Epoch 79/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.1351 - val_loss: 87.7097\n",
            "Epoch 80/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.0105 - val_loss: 103.5579\n",
            "Epoch 81/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.8720 - val_loss: 101.5979\n",
            "Epoch 82/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.6361 - val_loss: 86.7890\n",
            "Epoch 83/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.6895 - val_loss: 87.4879\n",
            "Epoch 84/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.6807 - val_loss: 88.0069\n",
            "Epoch 85/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.7293 - val_loss: 84.2806\n",
            "Epoch 86/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.5006 - val_loss: 87.1859\n",
            "Epoch 87/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.0668 - val_loss: 83.2054\n",
            "Epoch 88/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.2472 - val_loss: 86.1823\n",
            "Epoch 89/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.3228 - val_loss: 87.2180\n",
            "Epoch 90/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.1557 - val_loss: 96.4616\n",
            "Epoch 91/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.1000 - val_loss: 91.7300\n",
            "Epoch 92/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.9588 - val_loss: 92.9115\n",
            "Epoch 93/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.2316 - val_loss: 86.2784\n",
            "Epoch 94/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.9524 - val_loss: 111.0307\n",
            "Epoch 95/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.7228 - val_loss: 83.9754\n",
            "Epoch 96/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.9268 - val_loss: 93.3965\n",
            "Epoch 97/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.7389 - val_loss: 91.9406\n",
            "Epoch 98/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.5043 - val_loss: 98.8571\n",
            "Epoch 99/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.7222 - val_loss: 105.0273\n",
            "Epoch 100/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.5581 - val_loss: 84.9382\n",
            "Epoch 101/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3169 - val_loss: 97.2689\n",
            "Epoch 102/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4838 - val_loss: 107.0004\n",
            "Epoch 103/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4160 - val_loss: 92.7425\n",
            "Epoch 104/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.2266 - val_loss: 84.5035\n",
            "Epoch 105/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3041 - val_loss: 98.6291\n",
            "Epoch 106/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0989 - val_loss: 89.2186\n",
            "Epoch 107/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.1458 - val_loss: 93.4681\n",
            "Epoch 108/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.8037 - val_loss: 89.2533\n",
            "Epoch 109/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0229 - val_loss: 102.1118\n",
            "Epoch 110/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.7471 - val_loss: 96.9710\n",
            "Epoch 111/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.7793 - val_loss: 92.1252\n",
            "Epoch 112/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.8979 - val_loss: 93.3047\n",
            "Epoch 113/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.8743 - val_loss: 98.1686\n",
            "Epoch 114/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.7064 - val_loss: 85.6979\n",
            "Epoch 115/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.6633 - val_loss: 104.5617\n",
            "Epoch 116/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5930 - val_loss: 82.8184\n",
            "Epoch 117/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.7013 - val_loss: 88.6443\n",
            "Epoch 118/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.6639 - val_loss: 94.7661\n",
            "Epoch 119/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5981 - val_loss: 89.8050\n",
            "Epoch 120/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5951 - val_loss: 93.3112\n",
            "Epoch 121/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5027 - val_loss: 88.2993\n",
            "Epoch 122/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.0621 - val_loss: 88.4919\n",
            "Epoch 123/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5484 - val_loss: 85.1764\n",
            "Epoch 124/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.2358 - val_loss: 97.2561\n",
            "Epoch 125/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5735 - val_loss: 85.7271\n",
            "Epoch 126/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.3978 - val_loss: 101.3372\n",
            "Epoch 127/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.3075 - val_loss: 98.6232\n",
            "Epoch 128/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.3269 - val_loss: 85.3638\n",
            "Epoch 129/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.3651 - val_loss: 90.4954\n",
            "Epoch 130/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.1155 - val_loss: 83.9166\n",
            "Epoch 131/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.0109 - val_loss: 85.7927\n",
            "Epoch 132/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.8527 - val_loss: 90.2451\n",
            "Epoch 133/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.2524 - val_loss: 113.5770\n",
            "Epoch 134/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.2916 - val_loss: 87.0883\n",
            "Epoch 135/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.9129 - val_loss: 89.3075\n",
            "Epoch 136/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.7028 - val_loss: 89.9957\n",
            "Epoch 137/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.1488 - val_loss: 84.3770\n",
            "Epoch 138/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.9191 - val_loss: 83.9508\n",
            "Epoch 139/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.5618 - val_loss: 101.5261\n",
            "Epoch 140/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.8940 - val_loss: 88.1752\n",
            "Epoch 141/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.9056 - val_loss: 84.6141\n",
            "Epoch 142/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.5620 - val_loss: 86.3287\n",
            "Epoch 143/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.4853 - val_loss: 85.5227\n",
            "Epoch 144/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.8020 - val_loss: 95.9366\n",
            "Epoch 145/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.5410 - val_loss: 104.3571\n",
            "Epoch 146/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.4058 - val_loss: 87.3466\n",
            "Epoch 147/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.6139 - val_loss: 85.7204\n",
            "Epoch 148/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.6500 - val_loss: 87.7164\n",
            "Epoch 149/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.3074 - val_loss: 86.1094\n",
            "Epoch 150/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.6239 - val_loss: 92.8965\n",
            "Epoch 151/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.2961 - val_loss: 128.4764\n",
            "Epoch 152/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.3140 - val_loss: 92.0611\n",
            "Epoch 153/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.3991 - val_loss: 92.3076\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 154/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.2844 - val_loss: 98.5218\n",
            "Epoch 155/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.3721 - val_loss: 89.4982\n",
            "Epoch 156/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.4966 - val_loss: 107.0582\n",
            "Epoch 157/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.1341 - val_loss: 92.7768\n",
            "Epoch 158/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.2385 - val_loss: 92.2416\n",
            "Epoch 159/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.1966 - val_loss: 88.8530\n",
            "Epoch 160/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.3146 - val_loss: 83.1041\n",
            "Epoch 161/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.2524 - val_loss: 89.1246\n",
            "Epoch 162/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.1326 - val_loss: 84.2586\n",
            "Epoch 163/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.0261 - val_loss: 96.7008\n",
            "Epoch 164/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.1804 - val_loss: 87.7450\n",
            "Epoch 165/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.8384 - val_loss: 94.8902\n",
            "Epoch 166/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.8855 - val_loss: 115.4945\n",
            "Epoch 167/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.8144 - val_loss: 87.4680\n",
            "Epoch 168/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.9129 - val_loss: 133.5432\n",
            "Epoch 169/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.8321 - val_loss: 98.0653\n",
            "Epoch 170/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.9965 - val_loss: 96.5341\n",
            "Epoch 171/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.0480 - val_loss: 94.3313\n",
            "Epoch 172/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.6713 - val_loss: 83.6513\n",
            "Epoch 173/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.0237 - val_loss: 94.2008\n",
            "Epoch 174/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.6918 - val_loss: 91.8733\n",
            "Epoch 175/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.8099 - val_loss: 98.9303\n",
            "Epoch 176/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.7701 - val_loss: 89.6414\n",
            "Epoch 177/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.4015 - val_loss: 89.3203\n",
            "Epoch 178/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.4344 - val_loss: 88.6443\n",
            "Epoch 179/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.6472 - val_loss: 90.9093\n",
            "Epoch 180/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.4643 - val_loss: 90.8512\n",
            "Epoch 181/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.5649 - val_loss: 86.7528\n",
            "Epoch 182/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.5156 - val_loss: 116.2051\n",
            "Epoch 183/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.6087 - val_loss: 111.6967\n",
            "Epoch 184/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.5330 - val_loss: 92.0110\n",
            "Epoch 185/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.4450 - val_loss: 89.8829\n",
            "Epoch 186/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.4306 - val_loss: 86.9162\n",
            "Epoch 187/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.4887 - val_loss: 83.2945\n",
            "Epoch 188/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.3893 - val_loss: 88.9842\n",
            "Epoch 189/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.2734 - val_loss: 88.9770\n",
            "Epoch 190/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.1993 - val_loss: 86.6454\n",
            "Epoch 191/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.6318 - val_loss: 85.4773\n",
            "Epoch 192/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.3498 - val_loss: 90.3592\n",
            "Epoch 193/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.5574 - val_loss: 105.8212\n",
            "Epoch 194/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.3207 - val_loss: 109.3247\n",
            "Epoch 195/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.3904 - val_loss: 90.6130\n",
            "Epoch 196/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.3853 - val_loss: 87.1055\n",
            "Epoch 197/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.2078 - val_loss: 96.1241\n",
            "Epoch 198/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.2342 - val_loss: 108.3648\n",
            "Epoch 199/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.4129 - val_loss: 103.2313\n",
            "Epoch 200/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.3843 - val_loss: 92.1698\n",
            "Epoch 201/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.2707 - val_loss: 93.9084\n",
            "Epoch 202/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.0692 - val_loss: 99.8281\n",
            "Epoch 203/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.2612 - val_loss: 93.0959\n",
            "Epoch 204/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.1467 - val_loss: 98.0607\n",
            "Epoch 205/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.2636 - val_loss: 81.9875\n",
            "Epoch 206/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.9766 - val_loss: 94.7599\n",
            "Epoch 207/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.2278 - val_loss: 82.7263\n",
            "Epoch 208/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.1774 - val_loss: 85.8352\n",
            "Epoch 209/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.0368 - val_loss: 99.6947\n",
            "Epoch 210/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.3321 - val_loss: 90.4179\n",
            "Epoch 211/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.2108 - val_loss: 99.2437\n",
            "Epoch 212/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.8571 - val_loss: 100.7895\n",
            "Epoch 213/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.0369 - val_loss: 82.4061\n",
            "Epoch 214/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.1484 - val_loss: 96.2453\n",
            "Epoch 215/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.0369 - val_loss: 95.5908\n",
            "Epoch 216/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.4233 - val_loss: 87.1005\n",
            "Epoch 217/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.6985 - val_loss: 94.2142\n",
            "Epoch 218/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.8324 - val_loss: 87.1648\n",
            "Epoch 219/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.7796 - val_loss: 86.5146\n",
            "Epoch 220/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.8979 - val_loss: 80.3155\n",
            "Epoch 221/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 62.0381 - val_loss: 82.5988\n",
            "Epoch 222/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.7493 - val_loss: 80.1796\n",
            "Epoch 223/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.7957 - val_loss: 96.2819\n",
            "Epoch 224/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.9829 - val_loss: 90.2391\n",
            "Epoch 225/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.9950 - val_loss: 100.3248\n",
            "Epoch 226/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.7062 - val_loss: 88.7883\n",
            "Epoch 227/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.8546 - val_loss: 87.1178\n",
            "Epoch 228/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.6801 - val_loss: 87.2282\n",
            "Epoch 229/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.8614 - val_loss: 88.0772\n",
            "Epoch 230/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.6562 - val_loss: 106.5144\n",
            "Epoch 231/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.8274 - val_loss: 85.3896\n",
            "Epoch 232/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.6221 - val_loss: 89.7076\n",
            "Epoch 233/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.4837 - val_loss: 82.0827\n",
            "Epoch 234/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.6431 - val_loss: 93.4488\n",
            "Epoch 235/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.7034 - val_loss: 82.6064\n",
            "Epoch 236/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.7042 - val_loss: 97.9544\n",
            "Epoch 237/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.6408 - val_loss: 88.1677\n",
            "Epoch 238/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.8010 - val_loss: 87.7337\n",
            "Epoch 239/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.7922 - val_loss: 85.6535\n",
            "Epoch 240/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.8815 - val_loss: 89.3138\n",
            "Epoch 241/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.4626 - val_loss: 84.7623\n",
            "Epoch 242/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.5971 - val_loss: 83.3761\n",
            "Epoch 243/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.4661 - val_loss: 81.7735\n",
            "Epoch 244/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.8372 - val_loss: 83.5862\n",
            "Epoch 245/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.6268 - val_loss: 85.3870\n",
            "Epoch 246/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.6571 - val_loss: 84.9673\n",
            "Epoch 247/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.6558 - val_loss: 96.2002\n",
            "Epoch 248/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.2029 - val_loss: 87.3818\n",
            "Epoch 249/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.4922 - val_loss: 93.3524\n",
            "Epoch 250/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.7802 - val_loss: 84.0160\n",
            "Epoch 251/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.4611 - val_loss: 88.1915\n",
            "Epoch 252/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.3986 - val_loss: 93.8582\n",
            "Epoch 253/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.5063 - val_loss: 97.2308\n",
            "Epoch 254/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.4485 - val_loss: 87.4561\n",
            "Epoch 255/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.4737 - val_loss: 86.1790\n",
            "Epoch 256/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.3251 - val_loss: 100.6191\n",
            "Epoch 257/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.4091 - val_loss: 83.9514\n",
            "Epoch 258/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.2585 - val_loss: 88.0212\n",
            "Epoch 259/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.3176 - val_loss: 87.7865\n",
            "Epoch 260/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.2931 - val_loss: 95.1023\n",
            "Epoch 261/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.5343 - val_loss: 94.8421\n",
            "Epoch 262/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.1550 - val_loss: 94.0780\n",
            "Epoch 263/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.3426 - val_loss: 87.4420\n",
            "Epoch 264/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.0041 - val_loss: 94.4684\n",
            "Epoch 265/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.1122 - val_loss: 83.5529\n",
            "Epoch 266/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.4058 - val_loss: 96.6718\n",
            "Epoch 267/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.1812 - val_loss: 90.9080\n",
            "Epoch 268/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.4074 - val_loss: 98.0451\n",
            "Epoch 269/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.8911 - val_loss: 94.7756\n",
            "Epoch 270/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.2395 - val_loss: 88.8197\n",
            "Epoch 271/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.0195 - val_loss: 86.9844\n",
            "Epoch 272/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.2333 - val_loss: 84.4348\n",
            "Epoch 273/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.2120 - val_loss: 86.5254\n",
            "Epoch 274/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.0448 - val_loss: 85.5572\n",
            "Epoch 275/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.0342 - val_loss: 90.7074\n",
            "Epoch 276/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.3068 - val_loss: 89.8866\n",
            "Epoch 277/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.0176 - val_loss: 86.0474\n",
            "Epoch 278/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.1967 - val_loss: 91.9608\n",
            "Epoch 279/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.0992 - val_loss: 92.7622\n",
            "Epoch 280/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.1857 - val_loss: 89.0791\n",
            "Epoch 281/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.2355 - val_loss: 86.0494\n",
            "Epoch 282/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.9412 - val_loss: 89.3939\n",
            "Epoch 283/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.4767 - val_loss: 82.8689\n",
            "Epoch 284/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.2912 - val_loss: 96.2762\n",
            "Epoch 285/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.8861 - val_loss: 86.9146\n",
            "Epoch 286/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.9909 - val_loss: 89.9884\n",
            "Epoch 287/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.9959 - val_loss: 89.2784\n",
            "Epoch 288/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.9790 - val_loss: 92.8350\n",
            "Epoch 289/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.8870 - val_loss: 90.6593\n",
            "Epoch 290/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.9559 - val_loss: 82.0836\n",
            "Epoch 291/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.9492 - val_loss: 104.1525\n",
            "Epoch 292/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.1090 - val_loss: 93.0167\n",
            "Epoch 293/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.8261 - val_loss: 86.9083\n",
            "Epoch 294/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.2508 - val_loss: 101.4847\n",
            "Epoch 295/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.9721 - val_loss: 87.9420\n",
            "Epoch 296/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.0928 - val_loss: 88.2190\n",
            "Epoch 297/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.9674 - val_loss: 78.9982\n",
            "Epoch 298/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.0947 - val_loss: 83.0821\n",
            "Epoch 299/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 60.9305 - val_loss: 100.2994\n",
            "Epoch 300/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 61.0484 - val_loss: 87.0009\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  0.9548241730708869 \n",
            "MAE:  6.879272194520557 \n",
            "SD:  9.278426303177406\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "outputId": "4a85f233-7730-4d19-a30a-66ad769ace4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_24 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXAOqd2CTsa",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 10554.3594 - val_loss: 7138.0317\n",
            "Epoch 2/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 3632.3723 - val_loss: 773.4019\n",
            "Epoch 3/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 383.1566 - val_loss: 128.2645\n",
            "Epoch 4/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 117.9918 - val_loss: 1043.1552\n",
            "Epoch 5/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 108.9534 - val_loss: 128.7829\n",
            "Epoch 6/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 102.1006 - val_loss: 201.2568\n",
            "Epoch 7/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 99.3700 - val_loss: 145.2899\n",
            "Epoch 8/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 95.7730 - val_loss: 123.4271\n",
            "Epoch 9/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 93.1045 - val_loss: 113.9752\n",
            "Epoch 10/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 91.8503 - val_loss: 103.5123\n",
            "Epoch 11/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 89.3274 - val_loss: 439.0759\n",
            "Epoch 12/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 88.4241 - val_loss: 99.2094\n",
            "Epoch 13/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 86.7276 - val_loss: 116.5126\n",
            "Epoch 14/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 85.6802 - val_loss: 117.7774\n",
            "Epoch 15/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 85.0061 - val_loss: 125.1547\n",
            "Epoch 16/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 83.7292 - val_loss: 112.2267\n",
            "Epoch 17/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 83.1975 - val_loss: 101.5895\n",
            "Epoch 18/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 82.4324 - val_loss: 181.6574\n",
            "Epoch 19/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 82.8293 - val_loss: 107.5338\n",
            "Epoch 20/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 81.7056 - val_loss: 127.0467\n",
            "Epoch 21/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 81.0161 - val_loss: 98.9314\n",
            "Epoch 22/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 80.1335 - val_loss: 92.9142\n",
            "Epoch 23/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 79.6933 - val_loss: 100.7350\n",
            "Epoch 24/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 79.4475 - val_loss: 93.6320\n",
            "Epoch 25/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 78.7736 - val_loss: 103.5897\n",
            "Epoch 26/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 78.3955 - val_loss: 119.4149\n",
            "Epoch 27/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 78.0647 - val_loss: 99.5731\n",
            "Epoch 28/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 77.7157 - val_loss: 88.9741\n",
            "Epoch 29/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 77.0108 - val_loss: 99.5434\n",
            "Epoch 30/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.7710 - val_loss: 98.2904\n",
            "Epoch 31/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.3689 - val_loss: 103.9957\n",
            "Epoch 32/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.2290 - val_loss: 103.7368\n",
            "Epoch 33/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.2885 - val_loss: 90.5932\n",
            "Epoch 34/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.2858 - val_loss: 119.8953\n",
            "Epoch 35/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.0938 - val_loss: 92.2919\n",
            "Epoch 36/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.2511 - val_loss: 96.2336\n",
            "Epoch 37/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 75.7698 - val_loss: 91.3652\n",
            "Epoch 38/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 75.9157 - val_loss: 94.7353\n",
            "Epoch 39/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.2794 - val_loss: 117.5441\n",
            "Epoch 40/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 75.8720 - val_loss: 140.5382\n",
            "Epoch 41/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 74.8706 - val_loss: 91.0057\n",
            "Epoch 42/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 74.5398 - val_loss: 158.9688\n",
            "Epoch 43/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 74.1468 - val_loss: 123.5040\n",
            "Epoch 44/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 74.6315 - val_loss: 88.1100\n",
            "Epoch 45/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 74.2175 - val_loss: 109.5624\n",
            "Epoch 46/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 74.2392 - val_loss: 130.3990\n",
            "Epoch 47/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 74.3200 - val_loss: 90.5279\n",
            "Epoch 48/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 74.0317 - val_loss: 90.9462\n",
            "Epoch 49/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 73.5116 - val_loss: 82.5712\n",
            "Epoch 50/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 73.3231 - val_loss: 92.7902\n",
            "Epoch 51/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 73.4059 - val_loss: 87.1722\n",
            "Epoch 52/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.5456 - val_loss: 93.6237\n",
            "Epoch 53/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.2998 - val_loss: 430.8959\n",
            "Epoch 54/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.3721 - val_loss: 93.5494\n",
            "Epoch 55/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.4419 - val_loss: 93.0469\n",
            "Epoch 56/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.7779 - val_loss: 103.3246\n",
            "Epoch 57/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.3705 - val_loss: 92.8054\n",
            "Epoch 58/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.3867 - val_loss: 88.7500\n",
            "Epoch 59/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.2113 - val_loss: 96.0652\n",
            "Epoch 60/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.0192 - val_loss: 97.9332\n",
            "Epoch 61/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 72.2567 - val_loss: 110.1701\n",
            "Epoch 62/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.8289 - val_loss: 101.2149\n",
            "Epoch 63/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.8829 - val_loss: 95.9384\n",
            "Epoch 64/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.9490 - val_loss: 87.9088\n",
            "Epoch 65/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.6991 - val_loss: 91.6101\n",
            "Epoch 66/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.7469 - val_loss: 95.6105\n",
            "Epoch 67/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.2672 - val_loss: 108.8229\n",
            "Epoch 68/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.4551 - val_loss: 94.8326\n",
            "Epoch 69/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.1782 - val_loss: 151.1910\n",
            "Epoch 70/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.0301 - val_loss: 113.8914\n",
            "Epoch 71/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.8664 - val_loss: 93.2367\n",
            "Epoch 72/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.8542 - val_loss: 94.9807\n",
            "Epoch 73/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.6362 - val_loss: 94.7282\n",
            "Epoch 74/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.4091 - val_loss: 90.2980\n",
            "Epoch 75/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.8544 - val_loss: 82.1112\n",
            "Epoch 76/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.2416 - val_loss: 118.0797\n",
            "Epoch 77/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 9s 6ms/step - loss: 70.5947 - val_loss: 135.1928\n",
            "Epoch 78/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.1763 - val_loss: 97.4447\n",
            "Epoch 79/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 71.1889 - val_loss: 89.9457\n",
            "Epoch 80/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.1791 - val_loss: 106.2137\n",
            "Epoch 81/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.4521 - val_loss: 105.1698\n",
            "Epoch 82/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.1781 - val_loss: 159.9357\n",
            "Epoch 83/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.7217 - val_loss: 81.6670\n",
            "Epoch 84/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.9112 - val_loss: 86.9421\n",
            "Epoch 85/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.0952 - val_loss: 98.6616\n",
            "Epoch 86/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.7772 - val_loss: 95.6842\n",
            "Epoch 87/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.5936 - val_loss: 92.2107\n",
            "Epoch 88/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 70.3727 - val_loss: 92.8929\n",
            "Epoch 89/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.8468 - val_loss: 93.2644\n",
            "Epoch 90/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.7146 - val_loss: 89.6299\n",
            "Epoch 91/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.6455 - val_loss: 84.9206\n",
            "Epoch 92/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.3160 - val_loss: 89.2949\n",
            "Epoch 93/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.7995 - val_loss: 127.7153\n",
            "Epoch 94/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.4767 - val_loss: 86.7228\n",
            "Epoch 95/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.8461 - val_loss: 92.5655\n",
            "Epoch 96/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.1481 - val_loss: 167.6531\n",
            "Epoch 97/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.7741 - val_loss: 82.8164\n",
            "Epoch 98/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 69.1013 - val_loss: 232.0878\n",
            "Epoch 99/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.9216 - val_loss: 90.2948\n",
            "Epoch 100/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.3216 - val_loss: 85.7813\n",
            "Epoch 101/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.9680 - val_loss: 90.6000\n",
            "Epoch 102/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.1334 - val_loss: 99.1345\n",
            "Epoch 103/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.6179 - val_loss: 80.0648\n",
            "Epoch 104/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.0047 - val_loss: 89.0082\n",
            "Epoch 105/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.1318 - val_loss: 100.0850\n",
            "Epoch 106/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.9035 - val_loss: 86.9997\n",
            "Epoch 107/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.0811 - val_loss: 98.3914\n",
            "Epoch 108/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.9995 - val_loss: 136.5718\n",
            "Epoch 109/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.1856 - val_loss: 97.2150\n",
            "Epoch 110/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.8809 - val_loss: 86.8040\n",
            "Epoch 111/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.1552 - val_loss: 86.8422\n",
            "Epoch 112/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.5375 - val_loss: 106.0012\n",
            "Epoch 113/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.9039 - val_loss: 104.3761\n",
            "Epoch 114/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.5844 - val_loss: 101.0795\n",
            "Epoch 115/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.7141 - val_loss: 142.3413\n",
            "Epoch 116/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.5306 - val_loss: 92.0913\n",
            "Epoch 117/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.3238 - val_loss: 89.5141\n",
            "Epoch 118/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.6372 - val_loss: 91.7896\n",
            "Epoch 119/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.5502 - val_loss: 87.7117\n",
            "Epoch 120/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.3881 - val_loss: 139.0491\n",
            "Epoch 121/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.3119 - val_loss: 100.5399\n",
            "Epoch 122/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.3589 - val_loss: 111.1057\n",
            "Epoch 123/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.4212 - val_loss: 90.1038\n",
            "Epoch 124/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.6494 - val_loss: 121.4364\n",
            "Epoch 125/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 68.5096 - val_loss: 100.6367\n",
            "Epoch 126/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.5518 - val_loss: 108.0376\n",
            "Epoch 127/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.0783 - val_loss: 89.6906\n",
            "Epoch 128/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.3886 - val_loss: 86.3385\n",
            "Epoch 129/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.1483 - val_loss: 103.6226\n",
            "Epoch 130/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2330 - val_loss: 98.2070\n",
            "Epoch 131/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.1280 - val_loss: 85.5916\n",
            "Epoch 132/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2517 - val_loss: 102.0996\n",
            "Epoch 133/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2915 - val_loss: 85.8693\n",
            "Epoch 134/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.5612 - val_loss: 105.3300\n",
            "Epoch 135/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.5001 - val_loss: 81.9371\n",
            "Epoch 136/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2777 - val_loss: 84.2027\n",
            "Epoch 137/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.4591 - val_loss: 93.9031\n",
            "Epoch 138/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.3553 - val_loss: 85.3837\n",
            "Epoch 139/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2915 - val_loss: 102.1229\n",
            "Epoch 140/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.8094 - val_loss: 86.7401\n",
            "Epoch 141/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.9339 - val_loss: 96.0019\n",
            "Epoch 142/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2536 - val_loss: 97.1520\n",
            "Epoch 143/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.9770 - val_loss: 102.4375\n",
            "Epoch 144/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.4695 - val_loss: 110.6776\n",
            "Epoch 145/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.2037 - val_loss: 82.1787\n",
            "Epoch 146/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.1588 - val_loss: 91.7368\n",
            "Epoch 147/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.1017 - val_loss: 97.3566\n",
            "Epoch 148/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.5225 - val_loss: 114.1273\n",
            "Epoch 149/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 67.0390 - val_loss: 107.2993\n",
            "Epoch 150/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.7271 - val_loss: 99.8531\n",
            "Epoch 151/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.7005 - val_loss: 96.4604\n",
            "Epoch 152/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.6925 - val_loss: 82.5220\n",
            "Epoch 153/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.7359 - val_loss: 87.1203\n",
            "Epoch 154/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.8296 - val_loss: 85.7890\n",
            "Epoch 155/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.4595 - val_loss: 83.3837\n",
            "Epoch 156/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.2357 - val_loss: 90.5580\n",
            "Epoch 157/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.3763 - val_loss: 85.8642\n",
            "Epoch 158/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.0991 - val_loss: 81.6359\n",
            "Epoch 159/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.4944 - val_loss: 83.1264\n",
            "Epoch 160/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.4315 - val_loss: 91.7148\n",
            "Epoch 161/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.0361 - val_loss: 81.2594\n",
            "Epoch 162/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.0157 - val_loss: 91.2036\n",
            "Epoch 163/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.8862 - val_loss: 98.5393\n",
            "Epoch 164/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.2153 - val_loss: 91.4702\n",
            "Epoch 165/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.6556 - val_loss: 83.7885\n",
            "Epoch 166/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.0375 - val_loss: 93.0697\n",
            "Epoch 167/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.6490 - val_loss: 104.2499\n",
            "Epoch 168/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.3769 - val_loss: 109.3561\n",
            "Epoch 169/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.5860 - val_loss: 88.7887\n",
            "Epoch 170/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.3021 - val_loss: 85.3747\n",
            "Epoch 171/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.0812 - val_loss: 89.9892\n",
            "Epoch 172/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.2760 - val_loss: 77.9849\n",
            "Epoch 173/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.9795 - val_loss: 104.7913\n",
            "Epoch 174/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.1099 - val_loss: 83.6860\n",
            "Epoch 175/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.0258 - val_loss: 81.2332\n",
            "Epoch 176/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.1548 - val_loss: 83.4649\n",
            "Epoch 177/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.8949 - val_loss: 93.7446\n",
            "Epoch 178/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.9509 - val_loss: 91.7312\n",
            "Epoch 179/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.0900 - val_loss: 81.0474\n",
            "Epoch 180/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.3125 - val_loss: 113.4194\n",
            "Epoch 181/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.1875 - val_loss: 100.5964\n",
            "Epoch 182/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.0919 - val_loss: 112.9722\n",
            "Epoch 183/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.8014 - val_loss: 84.6942\n",
            "Epoch 184/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.7603 - val_loss: 88.6599\n",
            "Epoch 185/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4268 - val_loss: 156.1340\n",
            "Epoch 186/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4872 - val_loss: 103.9097\n",
            "Epoch 187/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.7618 - val_loss: 91.2443\n",
            "Epoch 188/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4628 - val_loss: 103.4947\n",
            "Epoch 189/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4342 - val_loss: 110.4844\n",
            "Epoch 190/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4450 - val_loss: 88.2469\n",
            "Epoch 191/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.1851 - val_loss: 93.1111\n",
            "Epoch 192/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4374 - val_loss: 97.3999\n",
            "Epoch 193/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3869 - val_loss: 84.3764\n",
            "Epoch 194/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.2856 - val_loss: 81.0394\n",
            "Epoch 195/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4785 - val_loss: 95.2356\n",
            "Epoch 196/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.1167 - val_loss: 104.5688\n",
            "Epoch 197/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3285 - val_loss: 98.2096\n",
            "Epoch 198/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.2287 - val_loss: 78.6115\n",
            "Epoch 199/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3481 - val_loss: 79.8823\n",
            "Epoch 200/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.2416 - val_loss: 89.8884\n",
            "Epoch 201/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3230 - val_loss: 88.2596\n",
            "Epoch 202/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.1438 - val_loss: 96.6229\n",
            "Epoch 203/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4255 - val_loss: 85.0462\n",
            "Epoch 204/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.5848 - val_loss: 91.0042\n",
            "Epoch 205/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4043 - val_loss: 87.8019\n",
            "Epoch 206/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.2576 - val_loss: 84.9219\n",
            "Epoch 207/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.6395 - val_loss: 107.5403\n",
            "Epoch 208/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4825 - val_loss: 95.5746\n",
            "Epoch 209/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0032 - val_loss: 86.9505\n",
            "Epoch 210/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.9348 - val_loss: 119.9190\n",
            "Epoch 211/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3729 - val_loss: 84.9426\n",
            "Epoch 212/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.1528 - val_loss: 87.0642\n",
            "Epoch 213/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.1309 - val_loss: 192.8943\n",
            "Epoch 214/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0212 - val_loss: 125.5870\n",
            "Epoch 215/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.8111 - val_loss: 123.0498\n",
            "Epoch 216/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0727 - val_loss: 88.4287\n",
            "Epoch 217/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.4674 - val_loss: 83.9550\n",
            "Epoch 218/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.9577 - val_loss: 92.1761\n",
            "Epoch 219/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.2534 - val_loss: 83.9026\n",
            "Epoch 220/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.3214 - val_loss: 100.7693\n",
            "Epoch 221/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.9979 - val_loss: 89.4076\n",
            "Epoch 222/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0318 - val_loss: 90.2486\n",
            "Epoch 223/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.6108 - val_loss: 109.9649\n",
            "Epoch 224/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.2684 - val_loss: 96.6070\n",
            "Epoch 225/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0338 - val_loss: 92.8884\n",
            "Epoch 226/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.6923 - val_loss: 96.2955\n",
            "Epoch 227/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.6826 - val_loss: 83.8673\n",
            "Epoch 228/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.8370 - val_loss: 82.9737\n",
            "Epoch 229/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0562 - val_loss: 99.9995\n",
            "Epoch 230/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.6891 - val_loss: 100.3490\n",
            "Epoch 231/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.9896 - val_loss: 85.4263\n",
            "Epoch 232/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.4472 - val_loss: 151.2611\n",
            "Epoch 233/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5918 - val_loss: 84.8455\n",
            "Epoch 234/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.4450 - val_loss: 99.2200\n",
            "Epoch 235/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.4897 - val_loss: 87.8272\n",
            "Epoch 236/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.4139 - val_loss: 94.5974\n",
            "Epoch 237/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5084 - val_loss: 108.1172\n",
            "Epoch 238/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.5150 - val_loss: 93.7902\n",
            "Epoch 239/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.6436 - val_loss: 90.4071\n",
            "Epoch 240/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.4875 - val_loss: 86.5717\n",
            "Epoch 241/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.2890 - val_loss: 86.1705\n",
            "Epoch 242/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 65.0325 - val_loss: 92.8673\n",
            "Epoch 243/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.2736 - val_loss: 89.1161\n",
            "Epoch 244/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.9051 - val_loss: 118.2236\n",
            "Epoch 245/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.7993 - val_loss: 84.3975\n",
            "Epoch 246/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.2620 - val_loss: 96.2279\n",
            "Epoch 247/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.3054 - val_loss: 84.4538\n",
            "Epoch 248/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.3687 - val_loss: 90.0535\n",
            "Epoch 249/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.2232 - val_loss: 86.9150\n",
            "Epoch 250/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.1018 - val_loss: 124.6270\n",
            "Epoch 251/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.9621 - val_loss: 81.3604\n",
            "Epoch 252/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.0957 - val_loss: 98.7412\n",
            "Epoch 253/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.2953 - val_loss: 87.5901\n",
            "Epoch 254/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.2246 - val_loss: 108.5294\n",
            "Epoch 255/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.4044 - val_loss: 87.0324\n",
            "Epoch 256/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.1394 - val_loss: 104.5935\n",
            "Epoch 257/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.1812 - val_loss: 82.3326\n",
            "Epoch 258/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.1688 - val_loss: 109.2875\n",
            "Epoch 259/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.9573 - val_loss: 92.3598\n",
            "Epoch 260/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.0450 - val_loss: 128.0535\n",
            "Epoch 261/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.2311 - val_loss: 86.3907\n",
            "Epoch 262/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.9867 - val_loss: 84.6549\n",
            "Epoch 263/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.3266 - val_loss: 98.0958\n",
            "Epoch 264/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.6919 - val_loss: 83.8398\n",
            "Epoch 265/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.0892 - val_loss: 83.9051\n",
            "Epoch 266/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.8487 - val_loss: 112.4147\n",
            "Epoch 267/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 64.2873 - val_loss: 88.1165\n",
            "Epoch 268/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.7404 - val_loss: 93.1365\n",
            "Epoch 269/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.9441 - val_loss: 102.6384\n",
            "Epoch 270/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.8099 - val_loss: 107.0067\n",
            "Epoch 271/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.9487 - val_loss: 101.4688\n",
            "Epoch 272/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.5761 - val_loss: 96.7569\n",
            "Epoch 273/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.6024 - val_loss: 89.6530\n",
            "Epoch 274/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.7914 - val_loss: 87.8136\n",
            "Epoch 275/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.6058 - val_loss: 89.5423\n",
            "Epoch 276/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.5955 - val_loss: 95.7417\n",
            "Epoch 277/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.6469 - val_loss: 97.8592\n",
            "Epoch 278/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.5446 - val_loss: 96.5563\n",
            "Epoch 279/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.5839 - val_loss: 85.5598\n",
            "Epoch 280/300\n",
            "1319/1319 [==============================] - 9s 7ms/step - loss: 63.9105 - val_loss: 79.8703\n",
            "Epoch 281/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.5092 - val_loss: 80.7856\n",
            "Epoch 282/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 66.1189 - val_loss: 82.6853\n",
            "Epoch 283/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.2245 - val_loss: 85.1922\n",
            "Epoch 284/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.0726 - val_loss: 87.5809\n",
            "Epoch 285/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.3183 - val_loss: 86.7846\n",
            "Epoch 286/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.3502 - val_loss: 95.8152\n",
            "Epoch 287/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.6041 - val_loss: 87.9892\n",
            "Epoch 288/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.2356 - val_loss: 82.4193\n",
            "Epoch 289/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.6470 - val_loss: 83.7485\n",
            "Epoch 290/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.5820 - val_loss: 92.9222\n",
            "Epoch 291/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.3315 - val_loss: 89.4557\n",
            "Epoch 292/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.4918 - val_loss: 100.7453\n",
            "Epoch 293/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.2788 - val_loss: 90.2025\n",
            "Epoch 294/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.6525 - val_loss: 111.7751\n",
            "Epoch 295/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.3449 - val_loss: 87.0075\n",
            "Epoch 296/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.4751 - val_loss: 93.2220\n",
            "Epoch 297/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.4550 - val_loss: 87.6506\n",
            "Epoch 298/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.4596 - val_loss: 84.9828\n",
            "Epoch 299/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.0316 - val_loss: 85.5201\n",
            "Epoch 300/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 63.5294 - val_loss: 107.1614\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696v_fuFCTsa",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -4.426956547406521 \n",
            "MAE:  8.140570949094036 \n",
            "SD:  9.35753707410751\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdZF2osWCUQS",
        "outputId": "3d227d82-5803-469c-a599-fd657d983ef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  -2.2608809272244064 \n",
            "Ensemble_std:  9.329404184993843\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_36 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1319/1319 [==============================] - 9s 6ms/step - loss: 2790.3936 - val_loss: 1148.1448\n",
            "Epoch 2/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 442.0314 - val_loss: 79.0994\n",
            "Epoch 3/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 56.7679 - val_loss: 145.8382\n",
            "Epoch 4/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 46.4090 - val_loss: 43.2232\n",
            "Epoch 5/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 42.2761 - val_loss: 71.4177\n",
            "Epoch 6/300\n",
            "1319/1319 [==============================] - 9s 6ms/step - loss: 39.3785 - val_loss: 58.9052\n",
            "Epoch 7/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 37.0796 - val_loss: 83.4480\n",
            "Epoch 8/300\n",
            "1319/1319 [==============================] - 9s 6ms/step - loss: 35.9136 - val_loss: 54.6836\n",
            "Epoch 9/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 35.0205 - val_loss: 48.7934\n",
            "Epoch 10/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 34.3185 - val_loss: 50.5191\n",
            "Epoch 11/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 34.0092 - val_loss: 36.6854\n",
            "Epoch 12/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 33.2755 - val_loss: 42.5314\n",
            "Epoch 13/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 32.5784 - val_loss: 52.0431\n",
            "Epoch 14/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 32.2842 - val_loss: 35.9375\n",
            "Epoch 15/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 31.8391 - val_loss: 35.4461\n",
            "Epoch 16/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 31.2871 - val_loss: 37.5004\n",
            "Epoch 17/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 30.7841 - val_loss: 50.1822\n",
            "Epoch 18/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 30.6209 - val_loss: 39.6165\n",
            "Epoch 19/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 30.2986 - val_loss: 40.8689\n",
            "Epoch 20/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 30.2245 - val_loss: 41.2922\n",
            "Epoch 21/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.9230 - val_loss: 49.6476\n",
            "Epoch 22/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.7317 - val_loss: 34.1412\n",
            "Epoch 23/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.6408 - val_loss: 41.3094\n",
            "Epoch 24/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.3582 - val_loss: 39.0325\n",
            "Epoch 25/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.3060 - val_loss: 33.7220\n",
            "Epoch 26/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.1792 - val_loss: 42.3654\n",
            "Epoch 27/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.0787 - val_loss: 37.0308\n",
            "Epoch 28/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.2407 - val_loss: 43.4115\n",
            "Epoch 29/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.8999 - val_loss: 42.7885\n",
            "Epoch 30/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.6335 - val_loss: 38.7218\n",
            "Epoch 31/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.5092 - val_loss: 35.9863\n",
            "Epoch 32/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.6776 - val_loss: 34.5141\n",
            "Epoch 33/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.2422 - val_loss: 38.0134\n",
            "Epoch 34/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.3428 - val_loss: 50.6573\n",
            "Epoch 35/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.1116 - val_loss: 35.5867\n",
            "Epoch 36/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.1001 - val_loss: 43.2754\n",
            "Epoch 37/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.1318 - val_loss: 40.5598\n",
            "Epoch 38/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.9377 - val_loss: 43.0426\n",
            "Epoch 39/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.8411 - val_loss: 34.0397\n",
            "Epoch 40/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.7253 - val_loss: 36.8060\n",
            "Epoch 41/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.6759 - val_loss: 32.4365\n",
            "Epoch 42/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.7526 - val_loss: 38.3096\n",
            "Epoch 43/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.4930 - val_loss: 39.8575\n",
            "Epoch 44/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.4541 - val_loss: 36.7785\n",
            "Epoch 45/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.4922 - val_loss: 55.1112\n",
            "Epoch 46/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.4080 - val_loss: 44.6453\n",
            "Epoch 47/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.4368 - val_loss: 44.8220\n",
            "Epoch 48/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.3786 - val_loss: 34.8288\n",
            "Epoch 49/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.3506 - val_loss: 33.2529\n",
            "Epoch 50/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.1080 - val_loss: 34.3622\n",
            "Epoch 51/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.1831 - val_loss: 43.0137\n",
            "Epoch 52/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.9894 - val_loss: 34.2829\n",
            "Epoch 53/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.2251 - val_loss: 37.8500\n",
            "Epoch 54/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.7375 - val_loss: 36.4746\n",
            "Epoch 55/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.2353 - val_loss: 33.1410\n",
            "Epoch 56/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.9015 - val_loss: 37.1547\n",
            "Epoch 57/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.1600 - val_loss: 32.1803\n",
            "Epoch 58/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.0185 - val_loss: 33.6169\n",
            "Epoch 59/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.9999 - val_loss: 36.1677\n",
            "Epoch 60/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.8893 - val_loss: 45.3715\n",
            "Epoch 61/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.3005 - val_loss: 46.2463\n",
            "Epoch 62/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.8619 - val_loss: 34.3305\n",
            "Epoch 63/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.7609 - val_loss: 35.6517\n",
            "Epoch 64/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.6477 - val_loss: 39.4029\n",
            "Epoch 65/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.7182 - val_loss: 32.7732\n",
            "Epoch 66/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.6776 - val_loss: 32.9700\n",
            "Epoch 67/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.7609 - val_loss: 48.9285\n",
            "Epoch 68/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.9115 - val_loss: 33.9582\n",
            "Epoch 69/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.5927 - val_loss: 34.4429\n",
            "Epoch 70/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.5524 - val_loss: 34.9785\n",
            "Epoch 71/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.5863 - val_loss: 35.6002\n",
            "Epoch 72/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.6033 - val_loss: 34.1487\n",
            "Epoch 73/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.5377 - val_loss: 41.0537\n",
            "Epoch 74/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.1040 - val_loss: 34.9647\n",
            "Epoch 75/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.6426 - val_loss: 33.5912\n",
            "Epoch 76/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.5463 - val_loss: 36.3305\n",
            "Epoch 77/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.4432 - val_loss: 36.5242\n",
            "Epoch 78/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.2498 - val_loss: 40.8793\n",
            "Epoch 79/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.2237 - val_loss: 41.4526\n",
            "Epoch 80/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.5617 - val_loss: 36.8743\n",
            "Epoch 81/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.5297 - val_loss: 34.9320\n",
            "Epoch 82/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.1260 - val_loss: 38.1757\n",
            "Epoch 83/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.4274 - val_loss: 34.2813\n",
            "Epoch 84/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.2415 - val_loss: 40.1429\n",
            "Epoch 85/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.0810 - val_loss: 34.2192\n",
            "Epoch 86/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.1706 - val_loss: 44.9586\n",
            "Epoch 87/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.0677 - val_loss: 41.3264\n",
            "Epoch 88/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.0576 - val_loss: 44.5768\n",
            "Epoch 89/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.9537 - val_loss: 33.2079\n",
            "Epoch 90/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.2271 - val_loss: 38.2915\n",
            "Epoch 91/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.0572 - val_loss: 33.4446\n",
            "Epoch 92/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.1085 - val_loss: 40.7988\n",
            "Epoch 93/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.8317 - val_loss: 35.3321\n",
            "Epoch 94/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.9072 - val_loss: 33.1874\n",
            "Epoch 95/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.0250 - val_loss: 35.9512\n",
            "Epoch 96/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.8512 - val_loss: 40.6405\n",
            "Epoch 97/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.8967 - val_loss: 32.5707\n",
            "Epoch 98/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7271 - val_loss: 35.9555\n",
            "Epoch 99/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.2513 - val_loss: 54.9804\n",
            "Epoch 100/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.8831 - val_loss: 35.6107\n",
            "Epoch 101/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7959 - val_loss: 65.5935\n",
            "Epoch 102/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6992 - val_loss: 35.3441\n",
            "Epoch 103/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6809 - val_loss: 32.9400\n",
            "Epoch 104/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.8784 - val_loss: 32.8191\n",
            "Epoch 105/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6089 - val_loss: 46.7763\n",
            "Epoch 106/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.5455 - val_loss: 35.7812\n",
            "Epoch 107/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7303 - val_loss: 34.4176\n",
            "Epoch 108/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6236 - val_loss: 37.6772\n",
            "Epoch 109/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6987 - val_loss: 30.9788\n",
            "Epoch 110/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7046 - val_loss: 39.5450\n",
            "Epoch 111/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7991 - val_loss: 39.6196\n",
            "Epoch 112/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4611 - val_loss: 32.7551\n",
            "Epoch 113/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.5719 - val_loss: 33.2757\n",
            "Epoch 114/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.5954 - val_loss: 40.5816\n",
            "Epoch 115/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6409 - val_loss: 35.4605\n",
            "Epoch 116/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.5379 - val_loss: 36.6286\n",
            "Epoch 117/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.5701 - val_loss: 34.3505\n",
            "Epoch 118/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4919 - val_loss: 33.2152\n",
            "Epoch 119/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4292 - val_loss: 32.3504\n",
            "Epoch 120/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3938 - val_loss: 38.0908\n",
            "Epoch 121/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4577 - val_loss: 34.2208\n",
            "Epoch 122/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4282 - val_loss: 35.9416\n",
            "Epoch 123/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3111 - val_loss: 32.9289\n",
            "Epoch 124/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3766 - val_loss: 36.9363\n",
            "Epoch 125/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2671 - val_loss: 32.5106\n",
            "Epoch 126/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6894 - val_loss: 34.6308\n",
            "Epoch 127/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6935 - val_loss: 32.8090\n",
            "Epoch 128/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6472 - val_loss: 31.3556\n",
            "Epoch 129/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2197 - val_loss: 33.1073\n",
            "Epoch 130/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2701 - val_loss: 34.7163\n",
            "Epoch 131/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2448 - val_loss: 34.1085\n",
            "Epoch 132/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3288 - val_loss: 42.7662\n",
            "Epoch 133/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2176 - val_loss: 33.6191\n",
            "Epoch 134/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3670 - val_loss: 36.6053\n",
            "Epoch 135/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2025 - val_loss: 32.3813\n",
            "Epoch 136/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3982 - val_loss: 32.9206\n",
            "Epoch 137/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3516 - val_loss: 32.7117\n",
            "Epoch 138/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0793 - val_loss: 36.1745\n",
            "Epoch 139/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2882 - val_loss: 32.5362\n",
            "Epoch 140/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.1951 - val_loss: 31.8379\n",
            "Epoch 141/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2284 - val_loss: 32.0662\n",
            "Epoch 142/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3255 - val_loss: 34.0113\n",
            "Epoch 143/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2242 - val_loss: 33.7456\n",
            "Epoch 144/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.1169 - val_loss: 33.4584\n",
            "Epoch 145/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9751 - val_loss: 44.9573\n",
            "Epoch 146/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0998 - val_loss: 44.0664\n",
            "Epoch 147/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0437 - val_loss: 34.2008\n",
            "Epoch 148/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0612 - val_loss: 36.4733\n",
            "Epoch 149/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9385 - val_loss: 38.1680\n",
            "Epoch 150/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0790 - val_loss: 36.6270\n",
            "Epoch 151/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0616 - val_loss: 36.3544\n",
            "Epoch 152/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.1468 - val_loss: 38.2390\n",
            "Epoch 153/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9915 - val_loss: 34.4687\n",
            "Epoch 154/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0466 - val_loss: 33.1550\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 155/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9361 - val_loss: 46.2010\n",
            "Epoch 156/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0324 - val_loss: 35.8240\n",
            "Epoch 157/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0666 - val_loss: 32.8344\n",
            "Epoch 158/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0550 - val_loss: 39.4636\n",
            "Epoch 159/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9323 - val_loss: 33.0540\n",
            "Epoch 160/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9144 - val_loss: 36.1767\n",
            "Epoch 161/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8543 - val_loss: 36.7388\n",
            "Epoch 162/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8387 - val_loss: 34.7094\n",
            "Epoch 163/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0281 - val_loss: 39.4920\n",
            "Epoch 164/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8782 - val_loss: 31.8060\n",
            "Epoch 165/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8129 - val_loss: 33.9923\n",
            "Epoch 166/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9083 - val_loss: 32.7518\n",
            "Epoch 167/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9064 - val_loss: 34.4333\n",
            "Epoch 168/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8374 - val_loss: 35.4699\n",
            "Epoch 169/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8808 - val_loss: 31.9853\n",
            "Epoch 170/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8006 - val_loss: 41.5570\n",
            "Epoch 171/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9153 - val_loss: 34.5336\n",
            "Epoch 172/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8711 - val_loss: 31.7161\n",
            "Epoch 173/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8906 - val_loss: 33.0794\n",
            "Epoch 174/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7827 - val_loss: 42.6793\n",
            "Epoch 175/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7353 - val_loss: 33.0808\n",
            "Epoch 176/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6416 - val_loss: 33.0089\n",
            "Epoch 177/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7332 - val_loss: 35.4944\n",
            "Epoch 178/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8187 - val_loss: 36.1297\n",
            "Epoch 179/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7024 - val_loss: 31.6706\n",
            "Epoch 180/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7753 - val_loss: 41.1314\n",
            "Epoch 181/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6285 - val_loss: 37.8948\n",
            "Epoch 182/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7469 - val_loss: 34.7805\n",
            "Epoch 183/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5974 - val_loss: 39.0854\n",
            "Epoch 184/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6786 - val_loss: 32.4671\n",
            "Epoch 185/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5163 - val_loss: 30.9255\n",
            "Epoch 186/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6281 - val_loss: 36.4010\n",
            "Epoch 187/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8328 - val_loss: 37.1447\n",
            "Epoch 188/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7206 - val_loss: 32.5804\n",
            "Epoch 189/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6241 - val_loss: 31.6699\n",
            "Epoch 190/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5606 - val_loss: 31.4033\n",
            "Epoch 191/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6002 - val_loss: 32.0165\n",
            "Epoch 192/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5203 - val_loss: 40.8113\n",
            "Epoch 193/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5967 - val_loss: 34.2645\n",
            "Epoch 194/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5998 - val_loss: 31.4853\n",
            "Epoch 195/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5047 - val_loss: 33.1399\n",
            "Epoch 196/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5184 - val_loss: 41.0207\n",
            "Epoch 197/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7898 - val_loss: 45.6243\n",
            "Epoch 198/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5457 - val_loss: 33.3387\n",
            "Epoch 199/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5035 - val_loss: 36.8705\n",
            "Epoch 200/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6233 - val_loss: 37.9815\n",
            "Epoch 201/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6251 - val_loss: 41.0429\n",
            "Epoch 202/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6749 - val_loss: 35.9416\n",
            "Epoch 203/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5436 - val_loss: 31.5362\n",
            "Epoch 204/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4438 - val_loss: 37.8538\n",
            "Epoch 205/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4945 - val_loss: 36.5934\n",
            "Epoch 206/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5208 - val_loss: 36.7584\n",
            "Epoch 207/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4545 - val_loss: 75.1409\n",
            "Epoch 208/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5265 - val_loss: 34.1108\n",
            "Epoch 209/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5330 - val_loss: 32.6124\n",
            "Epoch 210/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3953 - val_loss: 34.6695\n",
            "Epoch 211/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5246 - val_loss: 44.8423\n",
            "Epoch 212/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5046 - val_loss: 32.7654\n",
            "Epoch 213/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5175 - val_loss: 31.9403\n",
            "Epoch 214/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3410 - val_loss: 37.3962\n",
            "Epoch 215/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3700 - val_loss: 49.1174\n",
            "Epoch 216/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3747 - val_loss: 35.5155\n",
            "Epoch 217/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4164 - val_loss: 34.4507\n",
            "Epoch 218/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3938 - val_loss: 36.7584\n",
            "Epoch 219/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3902 - val_loss: 32.3751\n",
            "Epoch 220/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3761 - val_loss: 31.3468\n",
            "Epoch 221/300\n",
            "1319/1319 [==============================] - 9s 7ms/step - loss: 24.4118 - val_loss: 33.0896\n",
            "Epoch 222/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3856 - val_loss: 30.7384\n",
            "Epoch 223/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2667 - val_loss: 47.7738\n",
            "Epoch 224/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2477 - val_loss: 33.9536\n",
            "Epoch 225/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2972 - val_loss: 32.4439\n",
            "Epoch 226/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3431 - val_loss: 31.6478\n",
            "Epoch 227/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3945 - val_loss: 35.3410\n",
            "Epoch 228/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2882 - val_loss: 31.2184\n",
            "Epoch 229/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3427 - val_loss: 33.8191\n",
            "Epoch 230/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2257 - val_loss: 34.2864\n",
            "Epoch 231/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2453 - val_loss: 32.7289\n",
            "Epoch 232/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2566 - val_loss: 32.6009\n",
            "Epoch 233/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1704 - val_loss: 37.4549\n",
            "Epoch 234/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2739 - val_loss: 32.8309\n",
            "Epoch 235/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2991 - val_loss: 36.8599\n",
            "Epoch 236/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2818 - val_loss: 31.7492\n",
            "Epoch 237/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2715 - val_loss: 31.5273\n",
            "Epoch 238/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1696 - val_loss: 36.8172\n",
            "Epoch 239/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1542 - val_loss: 30.7360\n",
            "Epoch 240/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1593 - val_loss: 32.1421\n",
            "Epoch 241/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2348 - val_loss: 34.6290\n",
            "Epoch 242/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1962 - val_loss: 45.7455\n",
            "Epoch 243/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1402 - val_loss: 36.2043\n",
            "Epoch 244/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1255 - val_loss: 32.1898\n",
            "Epoch 245/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2282 - val_loss: 36.3479\n",
            "Epoch 246/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1248 - val_loss: 36.0952\n",
            "Epoch 247/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1220 - val_loss: 32.0266\n",
            "Epoch 248/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0086 - val_loss: 33.2000\n",
            "Epoch 249/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1543 - val_loss: 32.7935\n",
            "Epoch 250/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9997 - val_loss: 39.6202\n",
            "Epoch 251/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0604 - val_loss: 37.4646\n",
            "Epoch 252/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1284 - val_loss: 32.3784\n",
            "Epoch 253/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1335 - val_loss: 32.1217\n",
            "Epoch 254/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0654 - val_loss: 35.6529\n",
            "Epoch 255/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0650 - val_loss: 34.3280\n",
            "Epoch 256/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0628 - val_loss: 31.6695\n",
            "Epoch 257/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0846 - val_loss: 32.1583\n",
            "Epoch 258/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0778 - val_loss: 33.9412\n",
            "Epoch 259/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0287 - val_loss: 41.7085\n",
            "Epoch 260/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0388 - val_loss: 32.8963\n",
            "Epoch 261/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8955 - val_loss: 30.7362\n",
            "Epoch 262/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0083 - val_loss: 33.0230\n",
            "Epoch 263/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0838 - val_loss: 31.5401\n",
            "Epoch 264/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0460 - val_loss: 32.7453\n",
            "Epoch 265/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0388 - val_loss: 35.5709\n",
            "Epoch 266/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0469 - val_loss: 31.8698\n",
            "Epoch 267/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9283 - val_loss: 33.0209\n",
            "Epoch 268/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0199 - val_loss: 34.6567\n",
            "Epoch 269/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9827 - val_loss: 36.7950\n",
            "Epoch 270/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0418 - val_loss: 32.9565\n",
            "Epoch 271/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9625 - val_loss: 30.5584\n",
            "Epoch 272/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8903 - val_loss: 35.7913\n",
            "Epoch 273/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9277 - val_loss: 31.0018\n",
            "Epoch 274/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9439 - val_loss: 32.4491\n",
            "Epoch 275/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9912 - val_loss: 33.2017\n",
            "Epoch 276/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8603 - val_loss: 33.5868\n",
            "Epoch 277/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9448 - val_loss: 33.7675\n",
            "Epoch 278/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8851 - val_loss: 33.0180\n",
            "Epoch 279/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9046 - val_loss: 31.5099\n",
            "Epoch 280/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9185 - val_loss: 35.9444\n",
            "Epoch 281/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8287 - val_loss: 30.4659\n",
            "Epoch 282/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8899 - val_loss: 34.6434\n",
            "Epoch 283/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8522 - val_loss: 32.0597\n",
            "Epoch 284/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8662 - val_loss: 31.2929\n",
            "Epoch 285/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9284 - val_loss: 31.8301\n",
            "Epoch 286/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8964 - val_loss: 31.5832\n",
            "Epoch 287/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8837 - val_loss: 30.4452\n",
            "Epoch 288/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8165 - val_loss: 31.1725\n",
            "Epoch 289/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8098 - val_loss: 33.2423\n",
            "Epoch 290/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9585 - val_loss: 30.6163\n",
            "Epoch 291/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8793 - val_loss: 31.0838\n",
            "Epoch 292/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8238 - val_loss: 33.7117\n",
            "Epoch 293/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8780 - val_loss: 32.0723\n",
            "Epoch 294/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8655 - val_loss: 32.9937\n",
            "Epoch 295/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7944 - val_loss: 34.8755\n",
            "Epoch 296/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8937 - val_loss: 32.8305\n",
            "Epoch 297/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9400 - val_loss: 34.4280\n",
            "Epoch 298/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8200 - val_loss: 31.2384\n",
            "Epoch 299/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7770 - val_loss: 32.8406\n",
            "Epoch 300/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6790 - val_loss: 31.5060\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  0.4900018521798971 \n",
            "MAE:  4.134804705655306 \n",
            "SD:  5.591589302393729\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cip38xZOZnV",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_48 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_54 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 2581.5879 - val_loss: 968.7616\n",
            "Epoch 2/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 285.7541 - val_loss: 98.2514\n",
            "Epoch 3/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 42.4264 - val_loss: 105.3062\n",
            "Epoch 4/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 37.6149 - val_loss: 67.7842\n",
            "Epoch 5/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 36.1210 - val_loss: 52.7568\n",
            "Epoch 6/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 35.3982 - val_loss: 68.1443\n",
            "Epoch 7/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 34.3665 - val_loss: 43.5186\n",
            "Epoch 8/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 34.2450 - val_loss: 40.2815\n",
            "Epoch 9/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 33.4362 - val_loss: 57.1942\n",
            "Epoch 10/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 33.0462 - val_loss: 41.5130\n",
            "Epoch 11/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 32.3485 - val_loss: 46.7299\n",
            "Epoch 12/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 32.1271 - val_loss: 43.0388\n",
            "Epoch 13/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 31.8818 - val_loss: 38.9838\n",
            "Epoch 14/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 31.7981 - val_loss: 42.5234\n",
            "Epoch 15/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 31.1365 - val_loss: 42.2354\n",
            "Epoch 16/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 30.7197 - val_loss: 42.3520\n",
            "Epoch 17/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 30.6189 - val_loss: 39.2115\n",
            "Epoch 18/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 30.6230 - val_loss: 47.5787\n",
            "Epoch 19/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 30.3131 - val_loss: 34.8302\n",
            "Epoch 20/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.9348 - val_loss: 49.1787\n",
            "Epoch 21/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.7097 - val_loss: 42.0896\n",
            "Epoch 22/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.8780 - val_loss: 37.6449\n",
            "Epoch 23/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.5154 - val_loss: 37.5291\n",
            "Epoch 24/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.2608 - val_loss: 42.0229\n",
            "Epoch 25/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.3692 - val_loss: 34.2703\n",
            "Epoch 26/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.2042 - val_loss: 38.8637\n",
            "Epoch 27/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.3954 - val_loss: 38.6862\n",
            "Epoch 28/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.8288 - val_loss: 37.3309\n",
            "Epoch 29/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.7932 - val_loss: 44.9436\n",
            "Epoch 30/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.8093 - val_loss: 34.8256\n",
            "Epoch 31/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.7634 - val_loss: 41.6556\n",
            "Epoch 32/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.5405 - val_loss: 41.1327\n",
            "Epoch 33/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.5299 - val_loss: 39.9161\n",
            "Epoch 34/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.7435 - val_loss: 40.7086\n",
            "Epoch 35/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.5653 - val_loss: 44.0477\n",
            "Epoch 36/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.6023 - val_loss: 37.9543\n",
            "Epoch 37/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.3580 - val_loss: 42.0002\n",
            "Epoch 38/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.3383 - val_loss: 36.0788\n",
            "Epoch 39/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.3316 - val_loss: 39.1334\n",
            "Epoch 40/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.1549 - val_loss: 41.8111\n",
            "Epoch 41/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.0303 - val_loss: 44.1068\n",
            "Epoch 42/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.9316 - val_loss: 36.8397\n",
            "Epoch 43/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.8219 - val_loss: 37.2488\n",
            "Epoch 44/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.9255 - val_loss: 34.6555\n",
            "Epoch 45/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.8184 - val_loss: 37.6221\n",
            "Epoch 46/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.7007 - val_loss: 40.2192\n",
            "Epoch 47/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.8011 - val_loss: 37.9103\n",
            "Epoch 48/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.6693 - val_loss: 41.5276\n",
            "Epoch 49/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.5656 - val_loss: 41.7464\n",
            "Epoch 50/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.4462 - val_loss: 35.7988\n",
            "Epoch 51/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.5423 - val_loss: 37.4822\n",
            "Epoch 52/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.4276 - val_loss: 40.0560\n",
            "Epoch 53/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.2873 - val_loss: 36.7654\n",
            "Epoch 54/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.1386 - val_loss: 38.9242\n",
            "Epoch 55/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.2014 - val_loss: 34.3788\n",
            "Epoch 56/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.0775 - val_loss: 41.6711\n",
            "Epoch 57/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.0951 - val_loss: 40.1734\n",
            "Epoch 58/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.1316 - val_loss: 36.4832\n",
            "Epoch 59/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.0850 - val_loss: 35.9039\n",
            "Epoch 60/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.0793 - val_loss: 36.6394\n",
            "Epoch 61/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.2645 - val_loss: 37.1142\n",
            "Epoch 62/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.0380 - val_loss: 33.8139\n",
            "Epoch 63/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.0534 - val_loss: 57.2634\n",
            "Epoch 64/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.0760 - val_loss: 34.2928\n",
            "Epoch 65/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.8332 - val_loss: 36.6735\n",
            "Epoch 66/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.8703 - val_loss: 50.8160\n",
            "Epoch 67/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.0147 - val_loss: 44.1524\n",
            "Epoch 68/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.9333 - val_loss: 37.2761\n",
            "Epoch 69/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.9317 - val_loss: 35.5178\n",
            "Epoch 70/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.8095 - val_loss: 34.3378\n",
            "Epoch 71/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.8739 - val_loss: 40.0655\n",
            "Epoch 72/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.8117 - val_loss: 36.0098\n",
            "Epoch 73/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.6174 - val_loss: 43.9887\n",
            "Epoch 74/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.7629 - val_loss: 47.4186\n",
            "Epoch 75/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.7164 - val_loss: 42.5234\n",
            "Epoch 76/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.6619 - val_loss: 36.1718\n",
            "Epoch 77/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.5945 - val_loss: 41.8356\n",
            "Epoch 78/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.6074 - val_loss: 35.7637\n",
            "Epoch 79/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.7932 - val_loss: 36.2954\n",
            "Epoch 80/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.7322 - val_loss: 40.5601\n",
            "Epoch 81/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.6272 - val_loss: 46.7087\n",
            "Epoch 82/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.6144 - val_loss: 46.4123\n",
            "Epoch 83/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.7434 - val_loss: 35.8428\n",
            "Epoch 84/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.4176 - val_loss: 39.4434\n",
            "Epoch 85/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.4400 - val_loss: 34.6159\n",
            "Epoch 86/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.4773 - val_loss: 52.5413\n",
            "Epoch 87/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.3113 - val_loss: 44.2182\n",
            "Epoch 88/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.3425 - val_loss: 38.9997\n",
            "Epoch 89/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.3045 - val_loss: 36.0842\n",
            "Epoch 90/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.3378 - val_loss: 41.3049\n",
            "Epoch 91/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.2735 - val_loss: 32.9400\n",
            "Epoch 92/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.3501 - val_loss: 32.7273\n",
            "Epoch 93/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.2249 - val_loss: 42.0826\n",
            "Epoch 94/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.3655 - val_loss: 45.4494\n",
            "Epoch 95/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.3526 - val_loss: 33.7334\n",
            "Epoch 96/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.4051 - val_loss: 37.3615\n",
            "Epoch 97/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.2166 - val_loss: 34.9876\n",
            "Epoch 98/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.2322 - val_loss: 38.0954\n",
            "Epoch 99/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.2711 - val_loss: 34.6339\n",
            "Epoch 100/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.0874 - val_loss: 37.0716\n",
            "Epoch 101/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.0385 - val_loss: 50.1063\n",
            "Epoch 102/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.1380 - val_loss: 34.8606\n",
            "Epoch 103/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.0538 - val_loss: 38.1313\n",
            "Epoch 104/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.9063 - val_loss: 35.7460\n",
            "Epoch 105/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.9753 - val_loss: 50.4695\n",
            "Epoch 106/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.1495 - val_loss: 47.3853\n",
            "Epoch 107/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.9323 - val_loss: 39.5982\n",
            "Epoch 108/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.9050 - val_loss: 36.1117\n",
            "Epoch 109/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.8934 - val_loss: 35.6297\n",
            "Epoch 110/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7202 - val_loss: 45.9227\n",
            "Epoch 111/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7383 - val_loss: 33.9943\n",
            "Epoch 112/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7115 - val_loss: 33.2164\n",
            "Epoch 113/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7922 - val_loss: 33.4034\n",
            "Epoch 114/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7143 - val_loss: 35.0715\n",
            "Epoch 115/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6455 - val_loss: 33.8576\n",
            "Epoch 116/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7892 - val_loss: 38.8268\n",
            "Epoch 117/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6105 - val_loss: 36.6560\n",
            "Epoch 118/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6484 - val_loss: 33.5932\n",
            "Epoch 119/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6836 - val_loss: 46.6938\n",
            "Epoch 120/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.5923 - val_loss: 36.1219\n",
            "Epoch 121/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6287 - val_loss: 34.8472\n",
            "Epoch 122/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6095 - val_loss: 32.7011\n",
            "Epoch 123/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6383 - val_loss: 40.4017\n",
            "Epoch 124/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6989 - val_loss: 35.6937\n",
            "Epoch 125/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3409 - val_loss: 34.0530\n",
            "Epoch 126/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.5744 - val_loss: 35.7747\n",
            "Epoch 127/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4227 - val_loss: 34.0326\n",
            "Epoch 128/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3498 - val_loss: 37.2387\n",
            "Epoch 129/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.5850 - val_loss: 36.5836\n",
            "Epoch 130/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4354 - val_loss: 33.0192\n",
            "Epoch 131/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3210 - val_loss: 33.5720\n",
            "Epoch 132/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2817 - val_loss: 37.6819\n",
            "Epoch 133/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2416 - val_loss: 35.0763\n",
            "Epoch 134/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3151 - val_loss: 35.7662\n",
            "Epoch 135/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3368 - val_loss: 38.7227\n",
            "Epoch 136/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2469 - val_loss: 35.9579\n",
            "Epoch 137/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2395 - val_loss: 33.3837\n",
            "Epoch 138/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2463 - val_loss: 43.9079\n",
            "Epoch 139/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2294 - val_loss: 32.2807\n",
            "Epoch 140/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3431 - val_loss: 33.0552\n",
            "Epoch 141/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2981 - val_loss: 33.8158\n",
            "Epoch 142/300\n",
            "1319/1319 [==============================] - 9s 6ms/step - loss: 25.1784 - val_loss: 33.0316\n",
            "Epoch 143/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0872 - val_loss: 32.8388\n",
            "Epoch 144/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0510 - val_loss: 34.5792\n",
            "Epoch 145/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.1122 - val_loss: 32.7300\n",
            "Epoch 146/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.1550 - val_loss: 35.6812\n",
            "Epoch 147/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0680 - val_loss: 36.1226\n",
            "Epoch 148/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0511 - val_loss: 37.4420\n",
            "Epoch 149/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9444 - val_loss: 40.7117\n",
            "Epoch 150/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0515 - val_loss: 33.6467\n",
            "Epoch 151/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0343 - val_loss: 32.0548\n",
            "Epoch 152/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0789 - val_loss: 32.0259\n",
            "Epoch 153/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0981 - val_loss: 36.5301\n",
            "Epoch 154/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0101 - val_loss: 35.1210\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 155/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9989 - val_loss: 34.9616\n",
            "Epoch 156/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0911 - val_loss: 32.8327\n",
            "Epoch 157/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9791 - val_loss: 37.3387\n",
            "Epoch 158/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0593 - val_loss: 33.9127\n",
            "Epoch 159/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9116 - val_loss: 32.2998\n",
            "Epoch 160/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8758 - val_loss: 38.0661\n",
            "Epoch 161/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9481 - val_loss: 42.6013\n",
            "Epoch 162/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8852 - val_loss: 38.4524\n",
            "Epoch 163/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9984 - val_loss: 33.3607\n",
            "Epoch 164/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8869 - val_loss: 35.8606\n",
            "Epoch 165/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0391 - val_loss: 39.5702\n",
            "Epoch 166/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8892 - val_loss: 33.6309\n",
            "Epoch 167/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9928 - val_loss: 31.7909\n",
            "Epoch 168/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9667 - val_loss: 36.9257\n",
            "Epoch 169/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0257 - val_loss: 34.0763\n",
            "Epoch 170/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6765 - val_loss: 39.3838\n",
            "Epoch 171/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7537 - val_loss: 40.3714\n",
            "Epoch 172/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9245 - val_loss: 33.4480\n",
            "Epoch 173/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8659 - val_loss: 40.0839\n",
            "Epoch 174/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9811 - val_loss: 39.0472\n",
            "Epoch 175/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7810 - val_loss: 34.7232\n",
            "Epoch 176/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7749 - val_loss: 32.4979\n",
            "Epoch 177/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6818 - val_loss: 36.2736\n",
            "Epoch 178/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7469 - val_loss: 33.1913\n",
            "Epoch 179/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8053 - val_loss: 35.6582\n",
            "Epoch 180/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7336 - val_loss: 41.8568\n",
            "Epoch 181/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5895 - val_loss: 40.1588\n",
            "Epoch 182/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7119 - val_loss: 35.5311\n",
            "Epoch 183/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6658 - val_loss: 34.9906\n",
            "Epoch 184/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7069 - val_loss: 32.8673\n",
            "Epoch 185/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5913 - val_loss: 32.0972\n",
            "Epoch 186/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8370 - val_loss: 40.6591\n",
            "Epoch 187/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7401 - val_loss: 44.8658\n",
            "Epoch 188/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6773 - val_loss: 34.7246\n",
            "Epoch 189/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6500 - val_loss: 34.7241\n",
            "Epoch 190/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7100 - val_loss: 35.1921\n",
            "Epoch 191/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5949 - val_loss: 35.1624\n",
            "Epoch 192/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6067 - val_loss: 32.3874\n",
            "Epoch 193/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6025 - val_loss: 36.2794\n",
            "Epoch 194/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4795 - val_loss: 35.0150\n",
            "Epoch 195/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5424 - val_loss: 31.9801\n",
            "Epoch 196/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5897 - val_loss: 36.9743\n",
            "Epoch 197/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6110 - val_loss: 32.2708\n",
            "Epoch 198/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5498 - val_loss: 35.7337\n",
            "Epoch 199/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5073 - val_loss: 33.9051\n",
            "Epoch 200/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4639 - val_loss: 35.8995\n",
            "Epoch 201/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6211 - val_loss: 34.6489\n",
            "Epoch 202/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6735 - val_loss: 31.7236\n",
            "Epoch 203/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5911 - val_loss: 32.4725\n",
            "Epoch 204/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5789 - val_loss: 37.9413\n",
            "Epoch 205/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5985 - val_loss: 34.9011\n",
            "Epoch 206/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5481 - val_loss: 40.4887\n",
            "Epoch 207/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4372 - val_loss: 35.4336\n",
            "Epoch 208/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5442 - val_loss: 31.9500\n",
            "Epoch 209/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5179 - val_loss: 31.7275\n",
            "Epoch 210/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4386 - val_loss: 34.0717\n",
            "Epoch 211/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3337 - val_loss: 33.1467\n",
            "Epoch 212/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3337 - val_loss: 35.6500\n",
            "Epoch 213/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4694 - val_loss: 34.2412\n",
            "Epoch 214/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4554 - val_loss: 36.7221\n",
            "Epoch 215/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4706 - val_loss: 33.8345\n",
            "Epoch 216/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5361 - val_loss: 34.5836\n",
            "Epoch 217/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3943 - val_loss: 35.9184\n",
            "Epoch 218/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3992 - val_loss: 41.5128\n",
            "Epoch 219/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3647 - val_loss: 34.3939\n",
            "Epoch 220/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3549 - val_loss: 33.5947\n",
            "Epoch 221/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4140 - val_loss: 35.5270\n",
            "Epoch 222/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3528 - val_loss: 32.8691\n",
            "Epoch 223/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2588 - val_loss: 34.3364\n",
            "Epoch 224/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3236 - val_loss: 34.9027\n",
            "Epoch 225/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3079 - val_loss: 35.4139\n",
            "Epoch 226/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2818 - val_loss: 33.4814\n",
            "Epoch 227/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2969 - val_loss: 37.1591\n",
            "Epoch 228/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2673 - val_loss: 33.0908\n",
            "Epoch 229/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2799 - val_loss: 33.8468\n",
            "Epoch 230/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2522 - val_loss: 35.6328\n",
            "Epoch 231/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2831 - val_loss: 37.2441\n",
            "Epoch 232/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2566 - val_loss: 37.1943\n",
            "Epoch 233/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2502 - val_loss: 33.2073\n",
            "Epoch 234/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2373 - val_loss: 33.7347\n",
            "Epoch 235/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1967 - val_loss: 33.8568\n",
            "Epoch 236/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1426 - val_loss: 34.9439\n",
            "Epoch 237/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0962 - val_loss: 32.2797\n",
            "Epoch 238/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2807 - val_loss: 37.5092\n",
            "Epoch 239/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2313 - val_loss: 33.5148\n",
            "Epoch 240/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3264 - val_loss: 34.0625\n",
            "Epoch 241/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1783 - val_loss: 36.8163\n",
            "Epoch 242/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0831 - val_loss: 35.4658\n",
            "Epoch 243/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1173 - val_loss: 35.0282\n",
            "Epoch 244/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1247 - val_loss: 41.8656\n",
            "Epoch 245/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0646 - val_loss: 35.5465\n",
            "Epoch 246/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0684 - val_loss: 33.5539\n",
            "Epoch 247/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0562 - val_loss: 34.0043\n",
            "Epoch 248/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1488 - val_loss: 32.9562\n",
            "Epoch 249/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0409 - val_loss: 36.5175\n",
            "Epoch 250/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0666 - val_loss: 35.3380\n",
            "Epoch 251/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1242 - val_loss: 33.5483\n",
            "Epoch 252/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1468 - val_loss: 35.7078\n",
            "Epoch 253/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2370 - val_loss: 37.2808\n",
            "Epoch 254/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0032 - val_loss: 31.5770\n",
            "Epoch 255/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9644 - val_loss: 32.3126\n",
            "Epoch 256/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1455 - val_loss: 37.7499\n",
            "Epoch 257/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9966 - val_loss: 38.0416\n",
            "Epoch 258/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1840 - val_loss: 32.2780\n",
            "Epoch 259/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0960 - val_loss: 44.6983\n",
            "Epoch 260/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0467 - val_loss: 32.1923\n",
            "Epoch 261/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0118 - val_loss: 32.6434\n",
            "Epoch 262/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0084 - val_loss: 37.0465\n",
            "Epoch 263/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0613 - val_loss: 34.4622\n",
            "Epoch 264/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0919 - val_loss: 40.4062\n",
            "Epoch 265/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9965 - val_loss: 34.7100\n",
            "Epoch 266/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9305 - val_loss: 32.1590\n",
            "Epoch 267/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9677 - val_loss: 35.6109\n",
            "Epoch 268/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0147 - val_loss: 32.7658\n",
            "Epoch 269/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9800 - val_loss: 33.6141\n",
            "Epoch 270/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9360 - val_loss: 32.4424\n",
            "Epoch 271/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8331 - val_loss: 34.5717\n",
            "Epoch 272/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0394 - val_loss: 32.5789\n",
            "Epoch 273/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8465 - val_loss: 34.1761\n",
            "Epoch 274/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0100 - val_loss: 40.1829\n",
            "Epoch 275/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9851 - val_loss: 32.9715\n",
            "Epoch 276/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9305 - val_loss: 33.0136\n",
            "Epoch 277/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8855 - val_loss: 32.6490\n",
            "Epoch 278/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9269 - val_loss: 33.4523\n",
            "Epoch 279/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8927 - val_loss: 33.9299\n",
            "Epoch 280/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0724 - val_loss: 31.7029\n",
            "Epoch 281/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9108 - val_loss: 38.5158\n",
            "Epoch 282/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9234 - val_loss: 31.4650\n",
            "Epoch 283/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9013 - val_loss: 33.7991\n",
            "Epoch 284/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9389 - val_loss: 33.2111\n",
            "Epoch 285/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9412 - val_loss: 35.8695\n",
            "Epoch 286/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8581 - val_loss: 62.0321\n",
            "Epoch 287/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9460 - val_loss: 49.6124\n",
            "Epoch 288/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8452 - val_loss: 35.5118\n",
            "Epoch 289/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7806 - val_loss: 32.6473\n",
            "Epoch 290/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8708 - val_loss: 43.5566\n",
            "Epoch 291/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9935 - val_loss: 33.4820\n",
            "Epoch 292/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7925 - val_loss: 33.9341\n",
            "Epoch 293/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8524 - val_loss: 34.7337\n",
            "Epoch 294/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7258 - val_loss: 32.9618\n",
            "Epoch 295/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8242 - val_loss: 40.1702\n",
            "Epoch 296/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9332 - val_loss: 32.8605\n",
            "Epoch 297/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9418 - val_loss: 34.6001\n",
            "Epoch 298/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8030 - val_loss: 44.1104\n",
            "Epoch 299/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7586 - val_loss: 34.1329\n",
            "Epoch 300/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8003 - val_loss: 38.7087\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -2.3525888776837545 \n",
            "MAE:  4.79746995685346 \n",
            "SD:  5.759693725575109\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_60 (Dense)             (None, 32)                4096      \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_55 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_57 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "activation_65 (Activation)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,833\n",
            "Trainable params: 7,481\n",
            "Non-trainable params: 352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 2979.4756 - val_loss: 1410.1434\n",
            "Epoch 2/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 581.1311 - val_loss: 63.5254\n",
            "Epoch 3/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 50.0507 - val_loss: 47.1776\n",
            "Epoch 4/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 40.0025 - val_loss: 80.4839\n",
            "Epoch 5/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 37.1075 - val_loss: 58.8378\n",
            "Epoch 6/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 35.9770 - val_loss: 46.1241\n",
            "Epoch 7/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 35.1416 - val_loss: 49.5976\n",
            "Epoch 8/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 34.2729 - val_loss: 56.0687\n",
            "Epoch 9/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 33.6953 - val_loss: 57.8835\n",
            "Epoch 10/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 32.7743 - val_loss: 38.7144\n",
            "Epoch 11/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 32.6259 - val_loss: 41.9118\n",
            "Epoch 12/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 32.1245 - val_loss: 39.9466\n",
            "Epoch 13/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 31.4087 - val_loss: 40.3120\n",
            "Epoch 14/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 31.4758 - val_loss: 36.5537\n",
            "Epoch 15/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 30.9942 - val_loss: 41.6640\n",
            "Epoch 16/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 30.8688 - val_loss: 39.4393\n",
            "Epoch 17/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 30.5235 - val_loss: 41.2333\n",
            "Epoch 18/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 30.1676 - val_loss: 37.7654\n",
            "Epoch 19/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.8854 - val_loss: 38.6687\n",
            "Epoch 20/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.6456 - val_loss: 46.1821\n",
            "Epoch 21/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.6851 - val_loss: 34.8576\n",
            "Epoch 22/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.4922 - val_loss: 45.1114\n",
            "Epoch 23/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.2707 - val_loss: 48.2339\n",
            "Epoch 24/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.2370 - val_loss: 33.4721\n",
            "Epoch 25/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.2066 - val_loss: 36.5572\n",
            "Epoch 26/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.2000 - val_loss: 49.5709\n",
            "Epoch 27/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.8741 - val_loss: 37.7925\n",
            "Epoch 28/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.8065 - val_loss: 43.0888\n",
            "Epoch 29/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.5412 - val_loss: 62.2584\n",
            "Epoch 30/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.4789 - val_loss: 40.5742\n",
            "Epoch 31/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.5095 - val_loss: 34.7338\n",
            "Epoch 32/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.3875 - val_loss: 36.9217\n",
            "Epoch 33/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.2816 - val_loss: 43.3644\n",
            "Epoch 34/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.3214 - val_loss: 41.4798\n",
            "Epoch 35/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.2423 - val_loss: 39.1256\n",
            "Epoch 36/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.2513 - val_loss: 40.5182\n",
            "Epoch 37/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.0792 - val_loss: 36.6308\n",
            "Epoch 38/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 28.0910 - val_loss: 44.6102\n",
            "Epoch 39/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.7987 - val_loss: 33.5921\n",
            "Epoch 40/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.7348 - val_loss: 31.6914\n",
            "Epoch 41/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.7153 - val_loss: 37.2789\n",
            "Epoch 42/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.5850 - val_loss: 56.0340\n",
            "Epoch 43/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.5965 - val_loss: 36.5242\n",
            "Epoch 44/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.6407 - val_loss: 36.2201\n",
            "Epoch 45/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.3106 - val_loss: 33.7038\n",
            "Epoch 46/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.4421 - val_loss: 36.4414\n",
            "Epoch 47/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.3181 - val_loss: 73.6084\n",
            "Epoch 48/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.3176 - val_loss: 35.7241\n",
            "Epoch 49/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.1770 - val_loss: 34.1570\n",
            "Epoch 50/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.0911 - val_loss: 32.8290\n",
            "Epoch 51/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 27.2015 - val_loss: 39.8286\n",
            "Epoch 52/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.9270 - val_loss: 51.2156\n",
            "Epoch 53/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.9946 - val_loss: 43.5482\n",
            "Epoch 54/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.8636 - val_loss: 46.1803\n",
            "Epoch 55/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.7253 - val_loss: 36.8425\n",
            "Epoch 56/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.9135 - val_loss: 41.0977\n",
            "Epoch 57/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.6504 - val_loss: 44.0125\n",
            "Epoch 58/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.7135 - val_loss: 33.4206\n",
            "Epoch 59/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.6794 - val_loss: 36.0705\n",
            "Epoch 60/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.5424 - val_loss: 39.9529\n",
            "Epoch 61/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.5183 - val_loss: 33.4424\n",
            "Epoch 62/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.5588 - val_loss: 41.2039\n",
            "Epoch 63/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.3735 - val_loss: 36.1195\n",
            "Epoch 64/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.4173 - val_loss: 43.3364\n",
            "Epoch 65/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.5266 - val_loss: 36.9049\n",
            "Epoch 66/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.3807 - val_loss: 46.4153\n",
            "Epoch 67/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.4883 - val_loss: 36.5136\n",
            "Epoch 68/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.2130 - val_loss: 33.2386\n",
            "Epoch 69/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.2474 - val_loss: 33.8276\n",
            "Epoch 70/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.0207 - val_loss: 33.0363\n",
            "Epoch 71/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.1863 - val_loss: 45.0885\n",
            "Epoch 72/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.9865 - val_loss: 44.0624\n",
            "Epoch 73/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.0004 - val_loss: 33.1837\n",
            "Epoch 74/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.0512 - val_loss: 46.7405\n",
            "Epoch 75/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.8481 - val_loss: 38.3323\n",
            "Epoch 76/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.8870 - val_loss: 34.9552\n",
            "Epoch 77/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.9002 - val_loss: 32.4280\n",
            "Epoch 78/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 8s 6ms/step - loss: 26.0052 - val_loss: 35.5239\n",
            "Epoch 79/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.9398 - val_loss: 32.5194\n",
            "Epoch 80/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7830 - val_loss: 39.3754\n",
            "Epoch 81/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7741 - val_loss: 38.7798\n",
            "Epoch 82/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.8514 - val_loss: 43.1006\n",
            "Epoch 83/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.8181 - val_loss: 33.8694\n",
            "Epoch 84/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6845 - val_loss: 32.4982\n",
            "Epoch 85/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.7136 - val_loss: 36.9062\n",
            "Epoch 86/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6491 - val_loss: 38.1173\n",
            "Epoch 87/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.5608 - val_loss: 36.9488\n",
            "Epoch 88/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6545 - val_loss: 32.6148\n",
            "Epoch 89/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6102 - val_loss: 36.4841\n",
            "Epoch 90/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6231 - val_loss: 33.1766\n",
            "Epoch 91/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.5757 - val_loss: 38.2181\n",
            "Epoch 92/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4080 - val_loss: 39.1210\n",
            "Epoch 93/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.5942 - val_loss: 36.1842\n",
            "Epoch 94/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4342 - val_loss: 36.1797\n",
            "Epoch 95/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4587 - val_loss: 32.5046\n",
            "Epoch 96/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4886 - val_loss: 37.8555\n",
            "Epoch 97/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.6527 - val_loss: 43.1917\n",
            "Epoch 98/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4783 - val_loss: 35.3572\n",
            "Epoch 99/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4333 - val_loss: 43.5736\n",
            "Epoch 100/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.4016 - val_loss: 40.6090\n",
            "Epoch 101/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3955 - val_loss: 41.1065\n",
            "Epoch 102/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3391 - val_loss: 36.0331\n",
            "Epoch 103/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3173 - val_loss: 36.5807\n",
            "Epoch 104/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2079 - val_loss: 34.6353\n",
            "Epoch 105/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2596 - val_loss: 43.4839\n",
            "Epoch 106/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.3087 - val_loss: 32.5699\n",
            "Epoch 107/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2668 - val_loss: 41.3036\n",
            "Epoch 108/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.2230 - val_loss: 40.0356\n",
            "Epoch 109/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0803 - val_loss: 36.5584\n",
            "Epoch 110/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.1887 - val_loss: 35.6627\n",
            "Epoch 111/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0096 - val_loss: 38.6604\n",
            "Epoch 112/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.1173 - val_loss: 39.0436\n",
            "Epoch 113/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.1325 - val_loss: 33.4304\n",
            "Epoch 114/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0608 - val_loss: 36.8277\n",
            "Epoch 115/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.1850 - val_loss: 35.0563\n",
            "Epoch 116/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0973 - val_loss: 35.2226\n",
            "Epoch 117/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0280 - val_loss: 36.7505\n",
            "Epoch 118/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.1150 - val_loss: 39.0746\n",
            "Epoch 119/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.0219 - val_loss: 36.2494\n",
            "Epoch 120/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 25.1040 - val_loss: 41.9915\n",
            "Epoch 121/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9501 - val_loss: 32.7179\n",
            "Epoch 122/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9432 - val_loss: 40.5106\n",
            "Epoch 123/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9671 - val_loss: 37.5340\n",
            "Epoch 124/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9134 - val_loss: 38.1225\n",
            "Epoch 125/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9016 - val_loss: 35.3985\n",
            "Epoch 126/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8832 - val_loss: 35.4372\n",
            "Epoch 127/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8379 - val_loss: 41.5048\n",
            "Epoch 128/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9401 - val_loss: 34.8616\n",
            "Epoch 129/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8289 - val_loss: 33.5569\n",
            "Epoch 130/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7644 - val_loss: 34.6741\n",
            "Epoch 131/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.9120 - val_loss: 33.5598\n",
            "Epoch 132/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8575 - val_loss: 52.9375\n",
            "Epoch 133/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8560 - val_loss: 33.5318\n",
            "Epoch 134/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.8991 - val_loss: 53.2167\n",
            "Epoch 135/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7456 - val_loss: 35.9645\n",
            "Epoch 136/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7323 - val_loss: 33.1012\n",
            "Epoch 137/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7844 - val_loss: 35.5114\n",
            "Epoch 138/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6954 - val_loss: 33.5585\n",
            "Epoch 139/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7111 - val_loss: 56.5132\n",
            "Epoch 140/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7904 - val_loss: 35.6680\n",
            "Epoch 141/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7170 - val_loss: 31.4075\n",
            "Epoch 142/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6491 - val_loss: 34.3411\n",
            "Epoch 143/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.7377 - val_loss: 34.3020\n",
            "Epoch 144/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6385 - val_loss: 34.0975\n",
            "Epoch 145/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5975 - val_loss: 33.1677\n",
            "Epoch 146/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5866 - val_loss: 33.2886\n",
            "Epoch 147/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.6269 - val_loss: 47.6112\n",
            "Epoch 148/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4980 - val_loss: 35.5745\n",
            "Epoch 149/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5231 - val_loss: 34.6748\n",
            "Epoch 150/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4860 - val_loss: 34.4429\n",
            "Epoch 151/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4596 - val_loss: 35.4264\n",
            "Epoch 152/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5305 - val_loss: 34.1356\n",
            "Epoch 153/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4583 - val_loss: 40.5830\n",
            "Epoch 154/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3890 - val_loss: 34.9361\n",
            "Epoch 155/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3660 - val_loss: 33.8815\n",
            "Epoch 156/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4230 - val_loss: 35.0530\n",
            "Epoch 157/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3841 - val_loss: 37.8171\n",
            "Epoch 158/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3069 - val_loss: 36.1962\n",
            "Epoch 159/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3655 - val_loss: 37.6263\n",
            "Epoch 160/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.5191 - val_loss: 32.3250\n",
            "Epoch 161/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4311 - val_loss: 50.9092\n",
            "Epoch 162/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3178 - val_loss: 33.2833\n",
            "Epoch 163/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3499 - val_loss: 33.4570\n",
            "Epoch 164/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4329 - val_loss: 34.0592\n",
            "Epoch 165/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.4202 - val_loss: 33.6906\n",
            "Epoch 166/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3186 - val_loss: 35.0072\n",
            "Epoch 167/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3895 - val_loss: 36.0823\n",
            "Epoch 168/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2510 - val_loss: 32.7770\n",
            "Epoch 169/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2222 - val_loss: 37.8759\n",
            "Epoch 170/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3599 - val_loss: 36.7928\n",
            "Epoch 171/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3252 - val_loss: 42.5585\n",
            "Epoch 172/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2337 - val_loss: 35.9618\n",
            "Epoch 173/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2952 - val_loss: 33.3941\n",
            "Epoch 174/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.3336 - val_loss: 32.0292\n",
            "Epoch 175/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2226 - val_loss: 32.4609\n",
            "Epoch 176/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1765 - val_loss: 38.0750\n",
            "Epoch 177/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2739 - val_loss: 32.9106\n",
            "Epoch 178/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2063 - val_loss: 35.4928\n",
            "Epoch 179/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1551 - val_loss: 33.5473\n",
            "Epoch 180/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2157 - val_loss: 33.5913\n",
            "Epoch 181/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1411 - val_loss: 39.2494\n",
            "Epoch 182/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2592 - val_loss: 36.4076\n",
            "Epoch 183/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1549 - val_loss: 39.6605\n",
            "Epoch 184/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1021 - val_loss: 32.5179\n",
            "Epoch 185/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1921 - val_loss: 34.2498\n",
            "Epoch 186/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.2616 - val_loss: 32.4488\n",
            "Epoch 187/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1274 - val_loss: 34.1362\n",
            "Epoch 188/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1200 - val_loss: 34.0729\n",
            "Epoch 189/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0785 - val_loss: 32.8725\n",
            "Epoch 190/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0538 - val_loss: 34.8289\n",
            "Epoch 191/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1714 - val_loss: 35.0614\n",
            "Epoch 192/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0200 - val_loss: 34.6625\n",
            "Epoch 193/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9806 - val_loss: 36.4113\n",
            "Epoch 194/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0653 - val_loss: 38.3129\n",
            "Epoch 195/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1030 - val_loss: 32.2303\n",
            "Epoch 196/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0288 - val_loss: 39.8110\n",
            "Epoch 197/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9337 - val_loss: 31.1146\n",
            "Epoch 198/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0320 - val_loss: 34.8951\n",
            "Epoch 199/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.1071 - val_loss: 50.6934\n",
            "Epoch 200/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0134 - val_loss: 36.4401\n",
            "Epoch 201/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8893 - val_loss: 36.6937\n",
            "Epoch 202/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0267 - val_loss: 36.5173\n",
            "Epoch 203/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9362 - val_loss: 39.6077\n",
            "Epoch 204/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9949 - val_loss: 33.5116\n",
            "Epoch 205/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9712 - val_loss: 31.8998\n",
            "Epoch 206/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9398 - val_loss: 38.5128\n",
            "Epoch 207/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9302 - val_loss: 41.3282\n",
            "Epoch 208/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9557 - val_loss: 34.7662\n",
            "Epoch 209/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8940 - val_loss: 35.1168\n",
            "Epoch 210/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9418 - val_loss: 33.1977\n",
            "Epoch 211/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8213 - val_loss: 35.0496\n",
            "Epoch 212/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8280 - val_loss: 33.2913\n",
            "Epoch 213/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8376 - val_loss: 38.4121\n",
            "Epoch 214/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8387 - val_loss: 36.0852\n",
            "Epoch 215/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8830 - val_loss: 37.9174\n",
            "Epoch 216/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9560 - val_loss: 32.0428\n",
            "Epoch 217/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 24.0418 - val_loss: 43.2322\n",
            "Epoch 218/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9772 - val_loss: 32.3824\n",
            "Epoch 219/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7911 - val_loss: 36.3224\n",
            "Epoch 220/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8399 - val_loss: 36.6163\n",
            "Epoch 221/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8393 - val_loss: 34.3178\n",
            "Epoch 222/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.9103 - val_loss: 36.1884\n",
            "Epoch 223/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7267 - val_loss: 35.1102\n",
            "Epoch 224/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7603 - val_loss: 35.3003\n",
            "Epoch 225/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8447 - val_loss: 33.3154\n",
            "Epoch 226/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7678 - val_loss: 32.2047\n",
            "Epoch 227/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7258 - val_loss: 31.7392\n",
            "Epoch 228/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7288 - val_loss: 31.5939\n",
            "Epoch 229/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8391 - val_loss: 36.4054\n",
            "Epoch 230/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7664 - val_loss: 38.5321\n",
            "Epoch 231/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7824 - val_loss: 38.5580\n",
            "Epoch 232/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7426 - val_loss: 37.3037\n",
            "Epoch 233/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8277 - val_loss: 33.3089\n",
            "Epoch 234/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7458 - val_loss: 35.0350\n",
            "Epoch 235/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7347 - val_loss: 33.3837\n",
            "Epoch 236/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8182 - val_loss: 38.7389\n",
            "Epoch 237/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7578 - val_loss: 33.0859\n",
            "Epoch 238/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7404 - val_loss: 31.6445\n",
            "Epoch 239/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6715 - val_loss: 46.9448\n",
            "Epoch 240/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7478 - val_loss: 32.5894\n",
            "Epoch 241/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7047 - val_loss: 31.5691\n",
            "Epoch 242/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6302 - val_loss: 31.7244\n",
            "Epoch 243/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.8431 - val_loss: 37.6332\n",
            "Epoch 244/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7814 - val_loss: 33.8195\n",
            "Epoch 245/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6801 - val_loss: 38.6731\n",
            "Epoch 246/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7008 - val_loss: 32.6283\n",
            "Epoch 247/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6753 - val_loss: 32.7543\n",
            "Epoch 248/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5779 - val_loss: 31.7973\n",
            "Epoch 249/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6700 - val_loss: 31.7348\n",
            "Epoch 250/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6528 - val_loss: 32.9836\n",
            "Epoch 251/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7186 - val_loss: 33.6546\n",
            "Epoch 252/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6402 - val_loss: 34.2338\n",
            "Epoch 253/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5846 - val_loss: 34.6987\n",
            "Epoch 254/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5842 - val_loss: 32.3425\n",
            "Epoch 255/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6549 - val_loss: 34.3483\n",
            "Epoch 256/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6472 - val_loss: 31.5176\n",
            "Epoch 257/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6626 - val_loss: 35.4074\n",
            "Epoch 258/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5924 - val_loss: 38.6178\n",
            "Epoch 259/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6357 - val_loss: 33.3986\n",
            "Epoch 260/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5988 - val_loss: 33.0853\n",
            "Epoch 261/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6413 - val_loss: 33.0752\n",
            "Epoch 262/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5599 - val_loss: 32.6942\n",
            "Epoch 263/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.7029 - val_loss: 35.3373\n",
            "Epoch 264/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6859 - val_loss: 34.6658\n",
            "Epoch 265/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.6365 - val_loss: 33.4368\n",
            "Epoch 266/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5529 - val_loss: 33.6258\n",
            "Epoch 267/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5552 - val_loss: 33.8956\n",
            "Epoch 268/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4696 - val_loss: 34.3261\n",
            "Epoch 269/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5880 - val_loss: 36.7310\n",
            "Epoch 270/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5192 - val_loss: 37.8088\n",
            "Epoch 271/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4494 - val_loss: 34.5598\n",
            "Epoch 272/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4839 - val_loss: 35.3594\n",
            "Epoch 273/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5440 - val_loss: 33.2934\n",
            "Epoch 274/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5337 - val_loss: 31.4902\n",
            "Epoch 275/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5127 - val_loss: 35.3398\n",
            "Epoch 276/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.5018 - val_loss: 33.2327\n",
            "Epoch 277/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4228 - val_loss: 34.6069\n",
            "Epoch 278/300\n",
            "1319/1319 [==============================] - 9s 6ms/step - loss: 23.4586 - val_loss: 32.8051\n",
            "Epoch 279/300\n",
            "1319/1319 [==============================] - 9s 6ms/step - loss: 23.4286 - val_loss: 35.4432\n",
            "Epoch 280/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.3543 - val_loss: 34.4663\n",
            "Epoch 281/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4312 - val_loss: 34.2202\n",
            "Epoch 282/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4931 - val_loss: 33.9414\n",
            "Epoch 283/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.3758 - val_loss: 35.4871\n",
            "Epoch 284/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4664 - val_loss: 33.2258\n",
            "Epoch 285/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.3978 - val_loss: 32.3945\n",
            "Epoch 286/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.3976 - val_loss: 33.8847\n",
            "Epoch 287/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.3165 - val_loss: 31.4993\n",
            "Epoch 288/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.3946 - val_loss: 34.0061\n",
            "Epoch 289/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4733 - val_loss: 31.6566\n",
            "Epoch 290/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4169 - val_loss: 35.7825\n",
            "Epoch 291/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4540 - val_loss: 33.6621\n",
            "Epoch 292/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4037 - val_loss: 32.5119\n",
            "Epoch 293/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4050 - val_loss: 31.3961\n",
            "Epoch 294/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.3550 - val_loss: 33.7107\n",
            "Epoch 295/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.3235 - val_loss: 33.5022\n",
            "Epoch 296/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4306 - val_loss: 36.9647\n",
            "Epoch 297/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4100 - val_loss: 32.1398\n",
            "Epoch 298/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.3988 - val_loss: 34.1333\n",
            "Epoch 299/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.3011 - val_loss: 32.5813\n",
            "Epoch 300/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 23.4029 - val_loss: 35.3740\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  1.548610192275694 \n",
            "MAE:  4.412462635192053 \n",
            "SD:  5.742459750679213\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "outputId": "62416558-79a0-46f4-b68b-a4dbac8b99b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  -0.10465894440938772 \n",
            "Ensemble_std:  5.69791425954935\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "_BP_hv3_4(2)(1).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}