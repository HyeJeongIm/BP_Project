{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/%08BP_hv3_2(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# batch_size"
      ],
      "metadata": {
        "id": "XiiiBla2-j1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsCoux5AOZnK",
        "outputId": "34e478a7-3b92-4ec4-e6f4-3a9f394ad81c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "TensorFlow version :  2.8.2\n",
            "Keras version :  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlHICkovd809",
        "outputId": "d5345d51-c0ad-4914-dd09-1f9bff301e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러z오기\n",
        "train_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ],
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "6af30aaa-58b6-48a3-ac60-aa4626307ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "e9e5160d-98ee-419c-d766-7c05538aa933"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7494274-cb05-4785-9dde-78913a143cc0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7494274-cb05-4785-9dde-78913a143cc0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a7494274-cb05-4785-9dde-78913a143cc0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a7494274-cb05-4785-9dde-78913a143cc0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "37e8dec2-04c4-4e06-9732-ec8529a4b963"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb7d170a-e52e-4381-bc42-ee837653bb5f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb7d170a-e52e-4381-bc42-ee837653bb5f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb7d170a-e52e-4381-bc42-ee837653bb5f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb7d170a-e52e-4381-bc42-ee837653bb5f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 400\n",
        "lrate = 0.001"
      ],
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8SHBwBOZnO",
        "outputId": "dacbda5e-e04d-472d-9cce-cdff4fab9b53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 8)                32        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,185\n",
            "Trainable params: 1,145\n",
            "Non-trainable params: 40\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dGT6-7NcOZnO",
        "outputId": "b1f56249-4a6a-4e5f-e6d3-8e7fc504f68d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "165/165 [==============================] - 4s 12ms/step - loss: 12215.5020 - val_loss: 11812.5557\n",
            "Epoch 2/400\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 11872.0996 - val_loss: 10909.8789\n",
            "Epoch 3/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 11470.5791 - val_loss: 11312.8564\n",
            "Epoch 4/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10978.4492 - val_loss: 10505.7158\n",
            "Epoch 5/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10390.5879 - val_loss: 10068.4619\n",
            "Epoch 6/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9724.9062 - val_loss: 9103.4502\n",
            "Epoch 7/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9001.2705 - val_loss: 9404.3262\n",
            "Epoch 8/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8226.7686 - val_loss: 8653.5664\n",
            "Epoch 9/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7435.9731 - val_loss: 7296.4673\n",
            "Epoch 10/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6618.6807 - val_loss: 7577.6699\n",
            "Epoch 11/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5805.0181 - val_loss: 4810.6011\n",
            "Epoch 12/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5036.3203 - val_loss: 6035.7915\n",
            "Epoch 13/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4298.5693 - val_loss: 3596.4387\n",
            "Epoch 14/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 3625.3042 - val_loss: 3601.2324\n",
            "Epoch 15/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 3009.2290 - val_loss: 3365.5332\n",
            "Epoch 16/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2457.1809 - val_loss: 3137.8596\n",
            "Epoch 17/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1969.7544 - val_loss: 3062.9280\n",
            "Epoch 18/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1553.7695 - val_loss: 1398.2655\n",
            "Epoch 19/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1202.8790 - val_loss: 1718.2438\n",
            "Epoch 20/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 914.5540 - val_loss: 635.6813\n",
            "Epoch 21/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 682.4555 - val_loss: 892.4516\n",
            "Epoch 22/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 509.9243 - val_loss: 672.4413\n",
            "Epoch 23/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 378.2737 - val_loss: 304.5829\n",
            "Epoch 24/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 282.3904 - val_loss: 149.8466\n",
            "Epoch 25/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 222.0100 - val_loss: 303.6646\n",
            "Epoch 26/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 177.3793 - val_loss: 493.3416\n",
            "Epoch 27/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 150.4921 - val_loss: 144.5449\n",
            "Epoch 28/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 135.1289 - val_loss: 218.7350\n",
            "Epoch 29/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 125.4409 - val_loss: 156.7441\n",
            "Epoch 30/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 119.3128 - val_loss: 205.1563\n",
            "Epoch 31/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 115.5785 - val_loss: 142.2411\n",
            "Epoch 32/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 113.6615 - val_loss: 158.2539\n",
            "Epoch 33/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.6438 - val_loss: 176.4047\n",
            "Epoch 34/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 112.3940 - val_loss: 147.0996\n",
            "Epoch 35/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 110.9857 - val_loss: 180.9888\n",
            "Epoch 36/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.5170 - val_loss: 165.2823\n",
            "Epoch 37/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.2767 - val_loss: 188.4605\n",
            "Epoch 38/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.2352 - val_loss: 164.5264\n",
            "Epoch 39/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.6415 - val_loss: 220.8731\n",
            "Epoch 40/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.4600 - val_loss: 146.6354\n",
            "Epoch 41/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.3718 - val_loss: 165.5077\n",
            "Epoch 42/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.1121 - val_loss: 120.2470\n",
            "Epoch 43/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.4636 - val_loss: 203.4308\n",
            "Epoch 44/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.2001 - val_loss: 128.6447\n",
            "Epoch 45/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.5731 - val_loss: 222.2302\n",
            "Epoch 46/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0662 - val_loss: 136.9196\n",
            "Epoch 47/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.5309 - val_loss: 166.5240\n",
            "Epoch 48/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.1062 - val_loss: 194.0025\n",
            "Epoch 49/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.1881 - val_loss: 148.3681\n",
            "Epoch 50/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2989 - val_loss: 172.3893\n",
            "Epoch 51/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.9450 - val_loss: 186.4097\n",
            "Epoch 52/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1874 - val_loss: 203.2366\n",
            "Epoch 53/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.1315 - val_loss: 143.0473\n",
            "Epoch 54/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.0031 - val_loss: 142.9233\n",
            "Epoch 55/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3532 - val_loss: 136.0694\n",
            "Epoch 56/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.9936 - val_loss: 118.8126\n",
            "Epoch 57/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.5799 - val_loss: 181.3501\n",
            "Epoch 58/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4890 - val_loss: 130.1169\n",
            "Epoch 59/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4930 - val_loss: 150.3763\n",
            "Epoch 60/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2076 - val_loss: 187.1822\n",
            "Epoch 61/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.6626 - val_loss: 118.4908\n",
            "Epoch 62/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.5037 - val_loss: 118.2952\n",
            "Epoch 63/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.4825 - val_loss: 112.5218\n",
            "Epoch 64/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.3106 - val_loss: 109.9134\n",
            "Epoch 65/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8655 - val_loss: 139.7265\n",
            "Epoch 66/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.4285 - val_loss: 162.9150\n",
            "Epoch 67/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.8490 - val_loss: 119.1849\n",
            "Epoch 68/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 99.3767 - val_loss: 151.8695\n",
            "Epoch 69/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 99.4781 - val_loss: 160.3134\n",
            "Epoch 70/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 99.0334 - val_loss: 130.1427\n",
            "Epoch 71/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8234 - val_loss: 122.4315\n",
            "Epoch 72/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.8117 - val_loss: 136.0640\n",
            "Epoch 73/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.3163 - val_loss: 126.4397\n",
            "Epoch 74/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.8570 - val_loss: 113.5724\n",
            "Epoch 75/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.9833 - val_loss: 176.0632\n",
            "Epoch 76/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7958 - val_loss: 170.5626\n",
            "Epoch 77/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.0936 - val_loss: 116.2514\n",
            "Epoch 78/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.0198 - val_loss: 111.7396\n",
            "Epoch 79/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.4437 - val_loss: 240.4536\n",
            "Epoch 80/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.4754 - val_loss: 126.5000\n",
            "Epoch 81/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.3927 - val_loss: 111.8500\n",
            "Epoch 82/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.3260 - val_loss: 110.9962\n",
            "Epoch 83/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.3639 - val_loss: 129.7668\n",
            "Epoch 84/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.2094 - val_loss: 113.7114\n",
            "Epoch 85/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.2487 - val_loss: 176.5879\n",
            "Epoch 86/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.9709 - val_loss: 129.4130\n",
            "Epoch 87/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.0892 - val_loss: 111.4928\n",
            "Epoch 88/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.8946 - val_loss: 154.1647\n",
            "Epoch 89/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.7331 - val_loss: 119.9492\n",
            "Epoch 90/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.7335 - val_loss: 124.8102\n",
            "Epoch 91/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.7298 - val_loss: 121.8098\n",
            "Epoch 92/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.7320 - val_loss: 133.1027\n",
            "Epoch 93/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.5748 - val_loss: 128.0637\n",
            "Epoch 94/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.2959 - val_loss: 111.4489\n",
            "Epoch 95/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.6298 - val_loss: 113.4780\n",
            "Epoch 96/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.2804 - val_loss: 138.6989\n",
            "Epoch 97/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.2559 - val_loss: 131.7719\n",
            "Epoch 98/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.2894 - val_loss: 106.4443\n",
            "Epoch 99/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.1892 - val_loss: 108.9175\n",
            "Epoch 100/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.0478 - val_loss: 129.1305\n",
            "Epoch 101/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.0296 - val_loss: 126.1782\n",
            "Epoch 102/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.4456 - val_loss: 110.6458\n",
            "Epoch 103/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.5325 - val_loss: 107.3501\n",
            "Epoch 104/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.5588 - val_loss: 173.0540\n",
            "Epoch 105/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.5628 - val_loss: 150.8315\n",
            "Epoch 106/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.1826 - val_loss: 105.2946\n",
            "Epoch 107/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.9089 - val_loss: 117.7948\n",
            "Epoch 108/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.4447 - val_loss: 104.6511\n",
            "Epoch 109/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.5242 - val_loss: 146.5603\n",
            "Epoch 110/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.3923 - val_loss: 106.4103\n",
            "Epoch 111/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.4969 - val_loss: 120.2522\n",
            "Epoch 112/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.2566 - val_loss: 133.9617\n",
            "Epoch 113/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.1895 - val_loss: 108.0106\n",
            "Epoch 114/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.4167 - val_loss: 163.6015\n",
            "Epoch 115/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.5420 - val_loss: 107.3235\n",
            "Epoch 116/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.3716 - val_loss: 113.3942\n",
            "Epoch 117/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.3160 - val_loss: 117.4305\n",
            "Epoch 118/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.4232 - val_loss: 122.5098\n",
            "Epoch 119/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.6819 - val_loss: 143.5855\n",
            "Epoch 120/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.4484 - val_loss: 126.8293\n",
            "Epoch 121/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.2353 - val_loss: 110.3356\n",
            "Epoch 122/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.2592 - val_loss: 119.6172\n",
            "Epoch 123/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.0751 - val_loss: 132.9110\n",
            "Epoch 124/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.1036 - val_loss: 137.3007\n",
            "Epoch 125/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.8742 - val_loss: 106.1935\n",
            "Epoch 126/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.0929 - val_loss: 105.5293\n",
            "Epoch 127/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.5474 - val_loss: 132.4205\n",
            "Epoch 128/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.7492 - val_loss: 148.3966\n",
            "Epoch 129/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.7512 - val_loss: 125.1519\n",
            "Epoch 130/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.5490 - val_loss: 112.9089\n",
            "Epoch 131/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4402 - val_loss: 141.9638\n",
            "Epoch 132/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.0382 - val_loss: 124.4168\n",
            "Epoch 133/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.5733 - val_loss: 120.8865\n",
            "Epoch 134/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.2867 - val_loss: 117.3321\n",
            "Epoch 135/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.2159 - val_loss: 131.2659\n",
            "Epoch 136/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4364 - val_loss: 101.8368\n",
            "Epoch 137/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.2400 - val_loss: 122.0299\n",
            "Epoch 138/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1917 - val_loss: 147.7610\n",
            "Epoch 139/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.5466 - val_loss: 113.9960\n",
            "Epoch 140/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.4027 - val_loss: 111.0372\n",
            "Epoch 141/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2556 - val_loss: 165.1818\n",
            "Epoch 142/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1619 - val_loss: 117.9361\n",
            "Epoch 143/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1713 - val_loss: 146.2918\n",
            "Epoch 144/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.9588 - val_loss: 135.4845\n",
            "Epoch 145/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.3561 - val_loss: 110.3005\n",
            "Epoch 146/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4603 - val_loss: 127.6376\n",
            "Epoch 147/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9513 - val_loss: 110.5127\n",
            "Epoch 148/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9210 - val_loss: 117.5709\n",
            "Epoch 149/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.7441 - val_loss: 113.2702\n",
            "Epoch 150/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.7816 - val_loss: 113.3069\n",
            "Epoch 151/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6645 - val_loss: 122.9406\n",
            "Epoch 152/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.7181 - val_loss: 113.4655\n",
            "Epoch 153/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.8432 - val_loss: 111.3642\n",
            "Epoch 154/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.7752 - val_loss: 109.2901\n",
            "Epoch 155/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.7325 - val_loss: 121.8222\n",
            "Epoch 156/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7338 - val_loss: 107.2385\n",
            "Epoch 157/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.5568 - val_loss: 106.6650\n",
            "Epoch 158/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.7292 - val_loss: 153.4052\n",
            "Epoch 159/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5216 - val_loss: 119.0901\n",
            "Epoch 160/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.4395 - val_loss: 116.2516\n",
            "Epoch 161/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.4386 - val_loss: 109.3745\n",
            "Epoch 162/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.4498 - val_loss: 113.9121\n",
            "Epoch 163/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.5082 - val_loss: 119.5509\n",
            "Epoch 164/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.3552 - val_loss: 120.1035\n",
            "Epoch 165/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.1114 - val_loss: 100.3426\n",
            "Epoch 166/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.1867 - val_loss: 106.7861\n",
            "Epoch 167/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.3672 - val_loss: 127.7751\n",
            "Epoch 168/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.1127 - val_loss: 158.1464\n",
            "Epoch 169/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.1168 - val_loss: 108.1213\n",
            "Epoch 170/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7256 - val_loss: 114.4150\n",
            "Epoch 171/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.9003 - val_loss: 108.0807\n",
            "Epoch 172/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7223 - val_loss: 103.5295\n",
            "Epoch 173/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.8166 - val_loss: 107.4820\n",
            "Epoch 174/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.8540 - val_loss: 106.6362\n",
            "Epoch 175/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.8888 - val_loss: 118.6548\n",
            "Epoch 176/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.8556 - val_loss: 109.9878\n",
            "Epoch 177/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.8928 - val_loss: 121.7630\n",
            "Epoch 178/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.2298 - val_loss: 126.2810\n",
            "Epoch 179/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.8313 - val_loss: 102.2698\n",
            "Epoch 180/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7451 - val_loss: 122.0483\n",
            "Epoch 181/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.6914 - val_loss: 103.9965\n",
            "Epoch 182/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7441 - val_loss: 108.2293\n",
            "Epoch 183/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.6133 - val_loss: 107.4731\n",
            "Epoch 184/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.6028 - val_loss: 132.3260\n",
            "Epoch 185/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.3211 - val_loss: 111.5370\n",
            "Epoch 186/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.3994 - val_loss: 105.2402\n",
            "Epoch 187/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.4025 - val_loss: 115.0109\n",
            "Epoch 188/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.2956 - val_loss: 111.4548\n",
            "Epoch 189/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.2210 - val_loss: 128.4818\n",
            "Epoch 190/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.2565 - val_loss: 104.4251\n",
            "Epoch 191/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.2329 - val_loss: 107.1499\n",
            "Epoch 192/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.3349 - val_loss: 113.3836\n",
            "Epoch 193/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.2762 - val_loss: 116.2806\n",
            "Epoch 194/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.3246 - val_loss: 110.2396\n",
            "Epoch 195/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.9724 - val_loss: 103.1685\n",
            "Epoch 196/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.2600 - val_loss: 115.3641\n",
            "Epoch 197/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.9497 - val_loss: 104.7226\n",
            "Epoch 198/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.8107 - val_loss: 142.7819\n",
            "Epoch 199/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.8996 - val_loss: 123.2173\n",
            "Epoch 200/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.9991 - val_loss: 110.7160\n",
            "Epoch 201/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.0650 - val_loss: 107.2372\n",
            "Epoch 202/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.5873 - val_loss: 104.6023\n",
            "Epoch 203/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7862 - val_loss: 115.4535\n",
            "Epoch 204/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.8819 - val_loss: 165.5223\n",
            "Epoch 205/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.6741 - val_loss: 113.6815\n",
            "Epoch 206/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.6403 - val_loss: 107.9330\n",
            "Epoch 207/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.5169 - val_loss: 104.3048\n",
            "Epoch 208/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.5549 - val_loss: 118.3757\n",
            "Epoch 209/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.5387 - val_loss: 141.3362\n",
            "Epoch 210/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7879 - val_loss: 111.0424\n",
            "Epoch 211/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.5645 - val_loss: 105.8246\n",
            "Epoch 212/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.5627 - val_loss: 110.5315\n",
            "Epoch 213/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.6902 - val_loss: 134.8257\n",
            "Epoch 214/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.5126 - val_loss: 108.6014\n",
            "Epoch 215/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.4999 - val_loss: 115.5977\n",
            "Epoch 216/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.2846 - val_loss: 105.4691\n",
            "Epoch 217/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.4630 - val_loss: 110.4573\n",
            "Epoch 218/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.3528 - val_loss: 108.7688\n",
            "Epoch 219/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.4972 - val_loss: 109.2132\n",
            "Epoch 220/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.0857 - val_loss: 119.6930\n",
            "Epoch 221/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.0810 - val_loss: 118.2859\n",
            "Epoch 222/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.9818 - val_loss: 116.1690\n",
            "Epoch 223/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.2788 - val_loss: 107.2252\n",
            "Epoch 224/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.0720 - val_loss: 107.3453\n",
            "Epoch 225/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.2926 - val_loss: 142.4437\n",
            "Epoch 226/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.0637 - val_loss: 112.0451\n",
            "Epoch 227/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.0770 - val_loss: 193.3884\n",
            "Epoch 228/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.0463 - val_loss: 107.6925\n",
            "Epoch 229/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.8382 - val_loss: 127.9576\n",
            "Epoch 230/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.1095 - val_loss: 112.5519\n",
            "Epoch 231/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.8844 - val_loss: 143.9594\n",
            "Epoch 232/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8869 - val_loss: 107.3324\n",
            "Epoch 233/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8027 - val_loss: 118.0131\n",
            "Epoch 234/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.7757 - val_loss: 124.7886\n",
            "Epoch 235/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.6957 - val_loss: 111.4367\n",
            "Epoch 236/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.9357 - val_loss: 105.0208\n",
            "Epoch 237/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8685 - val_loss: 116.6934\n",
            "Epoch 238/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.6385 - val_loss: 104.0502\n",
            "Epoch 239/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8196 - val_loss: 109.0190\n",
            "Epoch 240/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.7960 - val_loss: 113.9407\n",
            "Epoch 241/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5123 - val_loss: 108.6404\n",
            "Epoch 242/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5243 - val_loss: 106.2535\n",
            "Epoch 243/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5805 - val_loss: 100.7264\n",
            "Epoch 244/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.6574 - val_loss: 113.4340\n",
            "Epoch 245/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5575 - val_loss: 110.6648\n",
            "Epoch 246/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.4510 - val_loss: 113.8944\n",
            "Epoch 247/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.6297 - val_loss: 111.7518\n",
            "Epoch 248/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.7509 - val_loss: 108.0654\n",
            "Epoch 249/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4855 - val_loss: 106.5269\n",
            "Epoch 250/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.6706 - val_loss: 121.3114\n",
            "Epoch 251/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.6741 - val_loss: 108.8187\n",
            "Epoch 252/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5411 - val_loss: 123.8967\n",
            "Epoch 253/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5127 - val_loss: 102.9962\n",
            "Epoch 254/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4698 - val_loss: 117.6510\n",
            "Epoch 255/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.3583 - val_loss: 134.7570\n",
            "Epoch 256/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5295 - val_loss: 106.6347\n",
            "Epoch 257/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.4628 - val_loss: 139.6368\n",
            "Epoch 258/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5600 - val_loss: 106.7102\n",
            "Epoch 259/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.2457 - val_loss: 127.3655\n",
            "Epoch 260/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2641 - val_loss: 135.3478\n",
            "Epoch 261/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.4453 - val_loss: 113.3736\n",
            "Epoch 262/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.3995 - val_loss: 113.1089\n",
            "Epoch 263/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3530 - val_loss: 102.5705\n",
            "Epoch 264/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.5386 - val_loss: 121.4408\n",
            "Epoch 265/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.4505 - val_loss: 113.6620\n",
            "Epoch 266/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.4452 - val_loss: 103.6412\n",
            "Epoch 267/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3993 - val_loss: 103.9052\n",
            "Epoch 268/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1517 - val_loss: 105.5490\n",
            "Epoch 269/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2405 - val_loss: 102.8132\n",
            "Epoch 270/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.0706 - val_loss: 108.1685\n",
            "Epoch 271/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3484 - val_loss: 117.8711\n",
            "Epoch 272/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1777 - val_loss: 148.4395\n",
            "Epoch 273/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.2466 - val_loss: 102.6188\n",
            "Epoch 274/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.1100 - val_loss: 103.0998\n",
            "Epoch 275/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.0464 - val_loss: 113.5049\n",
            "Epoch 276/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.1217 - val_loss: 114.1614\n",
            "Epoch 277/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0532 - val_loss: 106.5581\n",
            "Epoch 278/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.0395 - val_loss: 123.4114\n",
            "Epoch 279/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1899 - val_loss: 103.2919\n",
            "Epoch 280/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0580 - val_loss: 123.4675\n",
            "Epoch 281/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.1928 - val_loss: 108.3812\n",
            "Epoch 282/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0274 - val_loss: 109.4279\n",
            "Epoch 283/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.9576 - val_loss: 106.0236\n",
            "Epoch 284/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1178 - val_loss: 120.9501\n",
            "Epoch 285/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.0014 - val_loss: 124.5403\n",
            "Epoch 286/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9953 - val_loss: 125.0539\n",
            "Epoch 287/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.1793 - val_loss: 115.5679\n",
            "Epoch 288/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.8253 - val_loss: 117.0127\n",
            "Epoch 289/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8968 - val_loss: 115.2292\n",
            "Epoch 290/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8648 - val_loss: 101.4457\n",
            "Epoch 291/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.7693 - val_loss: 101.8337\n",
            "Epoch 292/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.9471 - val_loss: 111.7211\n",
            "Epoch 293/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.7528 - val_loss: 113.7992\n",
            "Epoch 294/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.8585 - val_loss: 119.5236\n",
            "Epoch 295/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.8834 - val_loss: 104.8608\n",
            "Epoch 296/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.7690 - val_loss: 109.9003\n",
            "Epoch 297/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.8987 - val_loss: 112.6406\n",
            "Epoch 298/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.0861 - val_loss: 114.9822\n",
            "Epoch 299/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.8103 - val_loss: 112.2981\n",
            "Epoch 300/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.8515 - val_loss: 119.7579\n",
            "Epoch 301/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8401 - val_loss: 112.5109\n",
            "Epoch 302/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.8349 - val_loss: 105.2139\n",
            "Epoch 303/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.6792 - val_loss: 115.4599\n",
            "Epoch 304/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.9171 - val_loss: 152.2519\n",
            "Epoch 305/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.6241 - val_loss: 121.1201\n",
            "Epoch 306/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8572 - val_loss: 111.6525\n",
            "Epoch 307/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.5735 - val_loss: 119.7534\n",
            "Epoch 308/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.5327 - val_loss: 104.1867\n",
            "Epoch 309/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.8518 - val_loss: 129.6896\n",
            "Epoch 310/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.8740 - val_loss: 112.6525\n",
            "Epoch 311/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.6886 - val_loss: 111.3410\n",
            "Epoch 312/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.4041 - val_loss: 112.7514\n",
            "Epoch 313/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.5154 - val_loss: 118.4973\n",
            "Epoch 314/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.4414 - val_loss: 112.5136\n",
            "Epoch 315/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.5841 - val_loss: 117.8934\n",
            "Epoch 316/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.6377 - val_loss: 113.2910\n",
            "Epoch 317/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 89.4943 - val_loss: 134.1145\n",
            "Epoch 318/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 89.6170 - val_loss: 104.2017\n",
            "Epoch 319/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.4378 - val_loss: 103.5196\n",
            "Epoch 320/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.4400 - val_loss: 164.0441\n",
            "Epoch 321/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.5423 - val_loss: 119.0543\n",
            "Epoch 322/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.4652 - val_loss: 112.3328\n",
            "Epoch 323/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.5762 - val_loss: 116.7313\n",
            "Epoch 324/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.4286 - val_loss: 104.9849\n",
            "Epoch 325/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.3793 - val_loss: 103.8871\n",
            "Epoch 326/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.5226 - val_loss: 106.2915\n",
            "Epoch 327/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.3492 - val_loss: 106.7365\n",
            "Epoch 328/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.1650 - val_loss: 154.4361\n",
            "Epoch 329/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.0021 - val_loss: 107.6422\n",
            "Epoch 330/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.9426 - val_loss: 109.4982\n",
            "Epoch 331/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.5876 - val_loss: 101.8329\n",
            "Epoch 332/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.6406 - val_loss: 114.1701\n",
            "Epoch 333/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.6648 - val_loss: 157.1937\n",
            "Epoch 334/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.5328 - val_loss: 124.0261\n",
            "Epoch 335/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.3847 - val_loss: 112.0050\n",
            "Epoch 336/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.4213 - val_loss: 105.7475\n",
            "Epoch 337/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.3787 - val_loss: 103.5857\n",
            "Epoch 338/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.5272 - val_loss: 123.5438\n",
            "Epoch 339/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.3064 - val_loss: 119.0704\n",
            "Epoch 340/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.1822 - val_loss: 103.5678\n",
            "Epoch 341/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.2684 - val_loss: 125.9487\n",
            "Epoch 342/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.4696 - val_loss: 106.6926\n",
            "Epoch 343/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.2849 - val_loss: 128.8201\n",
            "Epoch 344/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.2280 - val_loss: 115.2300\n",
            "Epoch 345/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.1515 - val_loss: 144.7784\n",
            "Epoch 346/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.1806 - val_loss: 118.1867\n",
            "Epoch 347/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.3210 - val_loss: 101.9533\n",
            "Epoch 348/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.9559 - val_loss: 101.5422\n",
            "Epoch 349/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.9314 - val_loss: 108.8561\n",
            "Epoch 350/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.4260 - val_loss: 109.6618\n",
            "Epoch 351/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.1602 - val_loss: 102.4119\n",
            "Epoch 352/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0082 - val_loss: 145.0512\n",
            "Epoch 353/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.1114 - val_loss: 110.1873\n",
            "Epoch 354/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8804 - val_loss: 109.4906\n",
            "Epoch 355/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.0351 - val_loss: 99.0664\n",
            "Epoch 356/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.1122 - val_loss: 146.2428\n",
            "Epoch 357/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.9297 - val_loss: 100.6462\n",
            "Epoch 358/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.0165 - val_loss: 99.3686\n",
            "Epoch 359/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.9652 - val_loss: 108.7894\n",
            "Epoch 360/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.9314 - val_loss: 119.4679\n",
            "Epoch 361/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0697 - val_loss: 119.1368\n",
            "Epoch 362/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.0018 - val_loss: 111.8022\n",
            "Epoch 363/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0992 - val_loss: 105.7294\n",
            "Epoch 364/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.9820 - val_loss: 105.6578\n",
            "Epoch 365/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.9771 - val_loss: 111.9547\n",
            "Epoch 366/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8350 - val_loss: 106.6195\n",
            "Epoch 367/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.7410 - val_loss: 103.8730\n",
            "Epoch 368/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.9036 - val_loss: 114.5393\n",
            "Epoch 369/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8800 - val_loss: 106.6427\n",
            "Epoch 370/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.8000 - val_loss: 108.0606\n",
            "Epoch 371/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8337 - val_loss: 99.3633\n",
            "Epoch 372/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.8054 - val_loss: 106.6256\n",
            "Epoch 373/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8331 - val_loss: 104.2031\n",
            "Epoch 374/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8135 - val_loss: 110.2383\n",
            "Epoch 375/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.5834 - val_loss: 118.4231\n",
            "Epoch 376/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6254 - val_loss: 126.8683\n",
            "Epoch 377/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.6804 - val_loss: 111.2621\n",
            "Epoch 378/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.7252 - val_loss: 101.7197\n",
            "Epoch 379/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.6732 - val_loss: 110.3738\n",
            "Epoch 380/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6080 - val_loss: 129.9571\n",
            "Epoch 381/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.5585 - val_loss: 101.7650\n",
            "Epoch 382/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.7473 - val_loss: 126.4594\n",
            "Epoch 383/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6374 - val_loss: 106.1568\n",
            "Epoch 384/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6293 - val_loss: 126.6966\n",
            "Epoch 385/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.5036 - val_loss: 103.8090\n",
            "Epoch 386/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.6478 - val_loss: 100.7667\n",
            "Epoch 387/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.6338 - val_loss: 117.3809\n",
            "Epoch 388/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.6266 - val_loss: 110.5261\n",
            "Epoch 389/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.5457 - val_loss: 135.4776\n",
            "Epoch 390/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.5208 - val_loss: 98.5192\n",
            "Epoch 391/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.3602 - val_loss: 111.5383\n",
            "Epoch 392/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.4037 - val_loss: 104.9155\n",
            "Epoch 393/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.2436 - val_loss: 107.7899\n",
            "Epoch 394/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.4507 - val_loss: 98.4365\n",
            "Epoch 395/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.2724 - val_loss: 103.8469\n",
            "Epoch 396/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.3127 - val_loss: 108.6768\n",
            "Epoch 397/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.3516 - val_loss: 137.3816\n",
            "Epoch 398/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.3604 - val_loss: 105.6491\n",
            "Epoch 399/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.3506 - val_loss: 102.5943\n",
            "Epoch 400/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.1713 - val_loss: 105.9004\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "e63f4c8b-6f56-4a50-eabc-ee2c44bad17c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.6646462844211515 \n",
            "MAE:  7.747621529664681 \n",
            "SD:  10.26930476113225\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "d4867a2e-8079-45e9-e8a1-7c6616b21666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "325c9fb1-36e7-4d59-a0e9-2260c230d591",
        "id": "su2Sj5jZCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,185\n",
            "Trainable params: 1,145\n",
            "Non-trainable params: 40\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "outputId": "e4786468-fa79-45cf-c58e-c617364f8f2c",
        "id": "kPRh6v-mCSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 12556.8018 - val_loss: 12454.7773\n",
            "Epoch 2/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 12338.7822 - val_loss: 12115.8389\n",
            "Epoch 3/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 12170.9902 - val_loss: 11874.4736\n",
            "Epoch 4/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11992.6504 - val_loss: 11577.7920\n",
            "Epoch 5/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11789.2090 - val_loss: 11395.2998\n",
            "Epoch 6/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11561.5771 - val_loss: 11001.9541\n",
            "Epoch 7/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11312.1934 - val_loss: 11127.6299\n",
            "Epoch 8/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 11041.5771 - val_loss: 10571.8848\n",
            "Epoch 9/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 10752.0752 - val_loss: 10042.3125\n",
            "Epoch 10/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10447.0342 - val_loss: 9944.2334\n",
            "Epoch 11/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10127.0225 - val_loss: 9269.2129\n",
            "Epoch 12/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9796.4072 - val_loss: 9402.0000\n",
            "Epoch 13/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9453.5586 - val_loss: 8758.7803\n",
            "Epoch 14/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9103.5664 - val_loss: 9975.9062\n",
            "Epoch 15/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8743.0869 - val_loss: 8349.1377\n",
            "Epoch 16/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8380.3701 - val_loss: 8208.3506\n",
            "Epoch 17/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8012.2236 - val_loss: 7279.0039\n",
            "Epoch 18/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7643.5454 - val_loss: 7161.1445\n",
            "Epoch 19/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7278.9912 - val_loss: 7422.6016\n",
            "Epoch 20/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6915.4312 - val_loss: 7800.9336\n",
            "Epoch 21/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6554.6587 - val_loss: 5417.4609\n",
            "Epoch 22/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6201.5430 - val_loss: 4659.8145\n",
            "Epoch 23/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5847.6216 - val_loss: 5841.8613\n",
            "Epoch 24/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5499.8994 - val_loss: 5290.0752\n",
            "Epoch 25/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5160.6387 - val_loss: 4673.0762\n",
            "Epoch 26/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4832.6729 - val_loss: 4117.6636\n",
            "Epoch 27/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4513.4058 - val_loss: 3806.8870\n",
            "Epoch 28/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4209.4062 - val_loss: 3878.0469\n",
            "Epoch 29/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3912.3184 - val_loss: 4777.9106\n",
            "Epoch 30/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3622.4805 - val_loss: 1865.2423\n",
            "Epoch 31/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3348.8545 - val_loss: 3221.8589\n",
            "Epoch 32/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3087.1431 - val_loss: 2007.9337\n",
            "Epoch 33/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2844.6929 - val_loss: 2058.6890\n",
            "Epoch 34/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2605.9702 - val_loss: 3109.0232\n",
            "Epoch 35/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2385.3523 - val_loss: 1288.1732\n",
            "Epoch 36/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2167.8955 - val_loss: 2589.5386\n",
            "Epoch 37/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1973.8455 - val_loss: 1859.6200\n",
            "Epoch 38/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1788.0662 - val_loss: 2822.8345\n",
            "Epoch 39/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1614.6472 - val_loss: 3405.1714\n",
            "Epoch 40/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1458.0421 - val_loss: 3048.7039\n",
            "Epoch 41/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1311.0344 - val_loss: 1295.3280\n",
            "Epoch 42/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1171.2667 - val_loss: 1021.2946\n",
            "Epoch 43/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1050.2673 - val_loss: 3034.1111\n",
            "Epoch 44/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 938.4489 - val_loss: 333.0501\n",
            "Epoch 45/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 837.3785 - val_loss: 924.9631\n",
            "Epoch 46/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 744.1558 - val_loss: 391.2059\n",
            "Epoch 47/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 658.6084 - val_loss: 747.6932\n",
            "Epoch 48/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 587.0582 - val_loss: 1464.1174\n",
            "Epoch 49/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 521.9624 - val_loss: 465.7297\n",
            "Epoch 50/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 458.0785 - val_loss: 286.0776\n",
            "Epoch 51/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 403.6054 - val_loss: 290.3867\n",
            "Epoch 52/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 360.3064 - val_loss: 643.7307\n",
            "Epoch 53/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 321.0931 - val_loss: 197.0514\n",
            "Epoch 54/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 288.4986 - val_loss: 1173.5903\n",
            "Epoch 55/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 267.4159 - val_loss: 197.8666\n",
            "Epoch 56/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 236.2832 - val_loss: 200.1079\n",
            "Epoch 57/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 219.5820 - val_loss: 437.6372\n",
            "Epoch 58/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 208.0687 - val_loss: 249.6048\n",
            "Epoch 59/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 205.9559 - val_loss: 218.5181\n",
            "Epoch 60/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 201.7135 - val_loss: 729.4586\n",
            "Epoch 61/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 184.2372 - val_loss: 163.5848\n",
            "Epoch 62/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 189.6866 - val_loss: 131.3366\n",
            "Epoch 63/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 173.3618 - val_loss: 247.9666\n",
            "Epoch 64/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 171.6069 - val_loss: 651.2881\n",
            "Epoch 65/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 198.4423 - val_loss: 1285.1229\n",
            "Epoch 66/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 217.6674 - val_loss: 187.5004\n",
            "Epoch 67/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 168.7350 - val_loss: 159.9537\n",
            "Epoch 68/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 188.7971 - val_loss: 721.1416\n",
            "Epoch 69/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 167.7554 - val_loss: 134.8980\n",
            "Epoch 70/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 183.3276 - val_loss: 320.4856\n",
            "Epoch 71/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 154.3368 - val_loss: 460.1548\n",
            "Epoch 72/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 118.9108 - val_loss: 134.0605\n",
            "Epoch 73/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.2843 - val_loss: 422.8262\n",
            "Epoch 74/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 107.0119 - val_loss: 198.1823\n",
            "Epoch 75/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.4037 - val_loss: 287.0007\n",
            "Epoch 76/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.7916 - val_loss: 184.9580\n",
            "Epoch 77/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 105.3449 - val_loss: 366.0981\n",
            "Epoch 78/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0409 - val_loss: 493.1844\n",
            "Epoch 79/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.7005 - val_loss: 157.4386\n",
            "Epoch 80/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2306 - val_loss: 158.8426\n",
            "Epoch 81/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.0138 - val_loss: 409.6451\n",
            "Epoch 82/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.5352 - val_loss: 1516.7395\n",
            "Epoch 83/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5714 - val_loss: 2659.7559\n",
            "Epoch 84/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.1404 - val_loss: 125.7816\n",
            "Epoch 85/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.7913 - val_loss: 175.2169\n",
            "Epoch 86/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2430 - val_loss: 130.8190\n",
            "Epoch 87/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0918 - val_loss: 120.2783\n",
            "Epoch 88/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 100.3616 - val_loss: 481.2291\n",
            "Epoch 89/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.0604 - val_loss: 541.4678\n",
            "Epoch 90/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.7022 - val_loss: 752.9357\n",
            "Epoch 91/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1782 - val_loss: 113.4255\n",
            "Epoch 92/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9580 - val_loss: 111.8137\n",
            "Epoch 93/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.2015 - val_loss: 728.9663\n",
            "Epoch 94/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.5887 - val_loss: 105.4516\n",
            "Epoch 95/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.1095 - val_loss: 142.0371\n",
            "Epoch 96/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.9459 - val_loss: 658.8105\n",
            "Epoch 97/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.4110 - val_loss: 581.2642\n",
            "Epoch 98/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.7895 - val_loss: 218.3665\n",
            "Epoch 99/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.6807 - val_loss: 172.0198\n",
            "Epoch 100/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.7047 - val_loss: 777.1320\n",
            "Epoch 101/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.8730 - val_loss: 349.5484\n",
            "Epoch 102/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.6546 - val_loss: 128.8189\n",
            "Epoch 103/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1843 - val_loss: 105.7654\n",
            "Epoch 104/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.8286 - val_loss: 177.2104\n",
            "Epoch 105/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.6589 - val_loss: 219.0552\n",
            "Epoch 106/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.2471 - val_loss: 108.8266\n",
            "Epoch 107/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.1300 - val_loss: 154.9225\n",
            "Epoch 108/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.9723 - val_loss: 346.7576\n",
            "Epoch 109/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7337 - val_loss: 368.7996\n",
            "Epoch 110/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.9688 - val_loss: 793.5511\n",
            "Epoch 111/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7768 - val_loss: 100.9404\n",
            "Epoch 112/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.3410 - val_loss: 105.4395\n",
            "Epoch 113/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.0063 - val_loss: 115.9159\n",
            "Epoch 114/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.7148 - val_loss: 215.8627\n",
            "Epoch 115/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.6506 - val_loss: 1659.0597\n",
            "Epoch 116/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.2947 - val_loss: 355.8921\n",
            "Epoch 117/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.0414 - val_loss: 415.0940\n",
            "Epoch 118/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.1443 - val_loss: 401.9236\n",
            "Epoch 119/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8405 - val_loss: 256.9422\n",
            "Epoch 120/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.6300 - val_loss: 606.1177\n",
            "Epoch 121/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.3425 - val_loss: 138.1222\n",
            "Epoch 122/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3159 - val_loss: 611.5342\n",
            "Epoch 123/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.1181 - val_loss: 154.6234\n",
            "Epoch 124/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.1296 - val_loss: 1283.8143\n",
            "Epoch 125/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.9979 - val_loss: 110.7502\n",
            "Epoch 126/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.6448 - val_loss: 1254.4491\n",
            "Epoch 127/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.7617 - val_loss: 108.8061\n",
            "Epoch 128/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.4406 - val_loss: 101.8642\n",
            "Epoch 129/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.3793 - val_loss: 105.6648\n",
            "Epoch 130/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.2672 - val_loss: 107.6982\n",
            "Epoch 131/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.2804 - val_loss: 178.7883\n",
            "Epoch 132/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.0731 - val_loss: 538.8265\n",
            "Epoch 133/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.9664 - val_loss: 638.8003\n",
            "Epoch 134/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.8930 - val_loss: 128.3640\n",
            "Epoch 135/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.7172 - val_loss: 102.3129\n",
            "Epoch 136/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.6351 - val_loss: 2197.9192\n",
            "Epoch 137/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.5033 - val_loss: 293.0403\n",
            "Epoch 138/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.4067 - val_loss: 543.0010\n",
            "Epoch 139/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.3042 - val_loss: 99.4010\n",
            "Epoch 140/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0474 - val_loss: 108.1801\n",
            "Epoch 141/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.0896 - val_loss: 102.0749\n",
            "Epoch 142/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.1961 - val_loss: 282.7533\n",
            "Epoch 143/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8902 - val_loss: 279.8210\n",
            "Epoch 144/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.7997 - val_loss: 2212.5759\n",
            "Epoch 145/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6806 - val_loss: 359.0006\n",
            "Epoch 146/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.5172 - val_loss: 207.5968\n",
            "Epoch 147/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.3570 - val_loss: 586.9983\n",
            "Epoch 148/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.4206 - val_loss: 182.0070\n",
            "Epoch 149/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.1863 - val_loss: 343.3032\n",
            "Epoch 150/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.0830 - val_loss: 155.3843\n",
            "Epoch 151/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.6608 - val_loss: 143.3472\n",
            "Epoch 152/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.5936 - val_loss: 405.7190\n",
            "Epoch 153/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.5783 - val_loss: 112.9002\n",
            "Epoch 154/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.2634 - val_loss: 161.5564\n",
            "Epoch 155/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.0808 - val_loss: 102.2008\n",
            "Epoch 156/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.9233 - val_loss: 101.4400\n",
            "Epoch 157/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.6197 - val_loss: 104.2859\n",
            "Epoch 158/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.7290 - val_loss: 214.2601\n",
            "Epoch 159/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.5511 - val_loss: 167.7856\n",
            "Epoch 160/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.4488 - val_loss: 104.3524\n",
            "Epoch 161/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.2952 - val_loss: 103.1920\n",
            "Epoch 162/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.2320 - val_loss: 530.4964\n",
            "Epoch 163/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.1889 - val_loss: 277.3173\n",
            "Epoch 164/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.1890 - val_loss: 105.9723\n",
            "Epoch 165/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.2558 - val_loss: 94.6355\n",
            "Epoch 166/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8827 - val_loss: 243.1914\n",
            "Epoch 167/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.9448 - val_loss: 545.2643\n",
            "Epoch 168/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.9545 - val_loss: 1766.5264\n",
            "Epoch 169/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8955 - val_loss: 546.5020\n",
            "Epoch 170/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.7670 - val_loss: 366.4944\n",
            "Epoch 171/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.6651 - val_loss: 223.7419\n",
            "Epoch 172/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.7510 - val_loss: 391.8668\n",
            "Epoch 173/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5485 - val_loss: 238.8332\n",
            "Epoch 174/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.5828 - val_loss: 197.0516\n",
            "Epoch 175/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.4979 - val_loss: 363.0014\n",
            "Epoch 176/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4340 - val_loss: 221.3029\n",
            "Epoch 177/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.3104 - val_loss: 437.6449\n",
            "Epoch 178/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.2232 - val_loss: 611.5340\n",
            "Epoch 179/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4997 - val_loss: 115.9263\n",
            "Epoch 180/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.2145 - val_loss: 181.3851\n",
            "Epoch 181/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.3485 - val_loss: 575.7515\n",
            "Epoch 182/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.0973 - val_loss: 387.3005\n",
            "Epoch 183/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.0550 - val_loss: 905.2822\n",
            "Epoch 184/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.0513 - val_loss: 929.9225\n",
            "Epoch 185/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.9193 - val_loss: 342.0960\n",
            "Epoch 186/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.8253 - val_loss: 173.3024\n",
            "Epoch 187/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.8392 - val_loss: 236.0933\n",
            "Epoch 188/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.0701 - val_loss: 340.5315\n",
            "Epoch 189/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.7983 - val_loss: 240.2495\n",
            "Epoch 190/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.7647 - val_loss: 255.3660\n",
            "Epoch 191/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.7342 - val_loss: 759.2547\n",
            "Epoch 192/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.6216 - val_loss: 215.7796\n",
            "Epoch 193/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.6972 - val_loss: 148.2940\n",
            "Epoch 194/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.5535 - val_loss: 516.2128\n",
            "Epoch 195/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.4844 - val_loss: 174.3405\n",
            "Epoch 196/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.4576 - val_loss: 1133.2118\n",
            "Epoch 197/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.7094 - val_loss: 111.6086\n",
            "Epoch 198/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.4643 - val_loss: 999.9948\n",
            "Epoch 199/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 84.3563 - val_loss: 96.9880\n",
            "Epoch 200/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 84.3732 - val_loss: 380.9497\n",
            "Epoch 201/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.4078 - val_loss: 826.5137\n",
            "Epoch 202/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.2554 - val_loss: 1135.8680\n",
            "Epoch 203/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.2851 - val_loss: 338.1996\n",
            "Epoch 204/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1988 - val_loss: 94.9295\n",
            "Epoch 205/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.1398 - val_loss: 770.9216\n",
            "Epoch 206/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1655 - val_loss: 647.9951\n",
            "Epoch 207/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1698 - val_loss: 172.6886\n",
            "Epoch 208/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.9156 - val_loss: 740.4052\n",
            "Epoch 209/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.0038 - val_loss: 500.2579\n",
            "Epoch 210/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.8327 - val_loss: 959.5504\n",
            "Epoch 211/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.0070 - val_loss: 666.0123\n",
            "Epoch 212/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.8036 - val_loss: 723.3019\n",
            "Epoch 213/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.7162 - val_loss: 660.1831\n",
            "Epoch 214/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.5991 - val_loss: 241.6556\n",
            "Epoch 215/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.6386 - val_loss: 521.2537\n",
            "Epoch 216/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.4419 - val_loss: 777.7418\n",
            "Epoch 217/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.3341 - val_loss: 336.0313\n",
            "Epoch 218/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.3470 - val_loss: 1064.8828\n",
            "Epoch 219/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.8135 - val_loss: 300.5280\n",
            "Epoch 220/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.8336 - val_loss: 210.7872\n",
            "Epoch 221/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.2808 - val_loss: 134.6181\n",
            "Epoch 222/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.2398 - val_loss: 226.5650\n",
            "Epoch 223/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.7582 - val_loss: 95.6154\n",
            "Epoch 224/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.6486 - val_loss: 1129.0632\n",
            "Epoch 225/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.4609 - val_loss: 270.2424\n",
            "Epoch 226/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.2972 - val_loss: 116.1357\n",
            "Epoch 227/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.2014 - val_loss: 489.9364\n",
            "Epoch 228/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.2307 - val_loss: 162.6418\n",
            "Epoch 229/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0612 - val_loss: 121.0369\n",
            "Epoch 230/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.1146 - val_loss: 152.6721\n",
            "Epoch 231/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.3026 - val_loss: 596.2863\n",
            "Epoch 232/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.4122 - val_loss: 819.1562\n",
            "Epoch 233/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0064 - val_loss: 116.9596\n",
            "Epoch 234/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.0195 - val_loss: 176.5851\n",
            "Epoch 235/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.1460 - val_loss: 635.4885\n",
            "Epoch 236/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.9088 - val_loss: 229.9348\n",
            "Epoch 237/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0340 - val_loss: 93.8008\n",
            "Epoch 238/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0530 - val_loss: 264.1419\n",
            "Epoch 239/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.8679 - val_loss: 670.2047\n",
            "Epoch 240/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1198 - val_loss: 96.1772\n",
            "Epoch 241/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.9643 - val_loss: 701.9726\n",
            "Epoch 242/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.8770 - val_loss: 419.1567\n",
            "Epoch 243/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0088 - val_loss: 104.2976\n",
            "Epoch 244/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.0787 - val_loss: 290.1286\n",
            "Epoch 245/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.8427 - val_loss: 95.3770\n",
            "Epoch 246/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.9034 - val_loss: 152.5851\n",
            "Epoch 247/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.9218 - val_loss: 281.0818\n",
            "Epoch 248/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.1058 - val_loss: 357.9403\n",
            "Epoch 249/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0507 - val_loss: 679.2050\n",
            "Epoch 250/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.2406 - val_loss: 546.8984\n",
            "Epoch 251/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.7651 - val_loss: 330.2155\n",
            "Epoch 252/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.7351 - val_loss: 297.3367\n",
            "Epoch 253/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.8695 - val_loss: 458.4212\n",
            "Epoch 254/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1646 - val_loss: 485.2816\n",
            "Epoch 255/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.8863 - val_loss: 140.5145\n",
            "Epoch 256/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.7209 - val_loss: 132.2676\n",
            "Epoch 257/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.8100 - val_loss: 193.1225\n",
            "Epoch 258/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.8340 - val_loss: 223.1140\n",
            "Epoch 259/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.8505 - val_loss: 94.6182\n",
            "Epoch 260/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.7511 - val_loss: 100.6440\n",
            "Epoch 261/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.9343 - val_loss: 756.2103\n",
            "Epoch 262/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.6615 - val_loss: 331.2631\n",
            "Epoch 263/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.7986 - val_loss: 912.3523\n",
            "Epoch 264/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.6615 - val_loss: 236.4243\n",
            "Epoch 265/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.7634 - val_loss: 1910.9373\n",
            "Epoch 266/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.7481 - val_loss: 961.2394\n",
            "Epoch 267/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.7504 - val_loss: 200.1150\n",
            "Epoch 268/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.6177 - val_loss: 95.9933\n",
            "Epoch 269/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.5407 - val_loss: 773.9944\n",
            "Epoch 270/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.6288 - val_loss: 1041.3427\n",
            "Epoch 271/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.6900 - val_loss: 398.1928\n",
            "Epoch 272/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.6157 - val_loss: 918.2301\n",
            "Epoch 273/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.4613 - val_loss: 1574.8020\n",
            "Epoch 274/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.4498 - val_loss: 263.8985\n",
            "Epoch 275/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.7173 - val_loss: 762.4174\n",
            "Epoch 276/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.4218 - val_loss: 136.1033\n",
            "Epoch 277/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.4814 - val_loss: 120.8090\n",
            "Epoch 278/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.2458 - val_loss: 148.7967\n",
            "Epoch 279/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.4864 - val_loss: 181.0037\n",
            "Epoch 280/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.3904 - val_loss: 148.5396\n",
            "Epoch 281/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.4436 - val_loss: 1044.7721\n",
            "Epoch 282/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.4355 - val_loss: 199.9348\n",
            "Epoch 283/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.4566 - val_loss: 112.0086\n",
            "Epoch 284/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.2210 - val_loss: 831.2972\n",
            "Epoch 285/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.3897 - val_loss: 497.3548\n",
            "Epoch 286/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.4186 - val_loss: 146.6370\n",
            "Epoch 287/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.4654 - val_loss: 194.3312\n",
            "Epoch 288/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.1720 - val_loss: 437.9661\n",
            "Epoch 289/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.4413 - val_loss: 1313.9326\n",
            "Epoch 290/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.3491 - val_loss: 808.8213\n",
            "Epoch 291/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.3184 - val_loss: 1264.4806\n",
            "Epoch 292/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.3488 - val_loss: 391.5250\n",
            "Epoch 293/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.3812 - val_loss: 1260.5903\n",
            "Epoch 294/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.1264 - val_loss: 366.0508\n",
            "Epoch 295/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.1620 - val_loss: 1414.6343\n",
            "Epoch 296/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.3346 - val_loss: 687.2528\n",
            "Epoch 297/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.3118 - val_loss: 765.3897\n",
            "Epoch 298/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.2260 - val_loss: 213.2473\n",
            "Epoch 299/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.3091 - val_loss: 158.4385\n",
            "Epoch 300/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.1558 - val_loss: 98.2629\n",
            "Epoch 301/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.4115 - val_loss: 206.1112\n",
            "Epoch 302/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.2224 - val_loss: 720.8805\n",
            "Epoch 303/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.1742 - val_loss: 95.2629\n",
            "Epoch 304/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.2959 - val_loss: 102.4298\n",
            "Epoch 305/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.0192 - val_loss: 93.5334\n",
            "Epoch 306/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.1589 - val_loss: 831.7247\n",
            "Epoch 307/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.3172 - val_loss: 1175.2798\n",
            "Epoch 308/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.2693 - val_loss: 718.3239\n",
            "Epoch 309/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.9374 - val_loss: 100.3593\n",
            "Epoch 310/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.1909 - val_loss: 496.6372\n",
            "Epoch 311/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.0792 - val_loss: 650.8782\n",
            "Epoch 312/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.0868 - val_loss: 1746.8882\n",
            "Epoch 313/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.1522 - val_loss: 154.2155\n",
            "Epoch 314/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9829 - val_loss: 166.5513\n",
            "Epoch 315/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.0084 - val_loss: 109.2441\n",
            "Epoch 316/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.0676 - val_loss: 1088.8851\n",
            "Epoch 317/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.8869 - val_loss: 95.8993\n",
            "Epoch 318/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.0326 - val_loss: 95.3820\n",
            "Epoch 319/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.0860 - val_loss: 2212.6492\n",
            "Epoch 320/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.0221 - val_loss: 271.5161\n",
            "Epoch 321/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.0657 - val_loss: 134.1680\n",
            "Epoch 322/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.1917 - val_loss: 260.2393\n",
            "Epoch 323/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9097 - val_loss: 223.3429\n",
            "Epoch 324/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.1192 - val_loss: 203.3045\n",
            "Epoch 325/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.6769 - val_loss: 219.6234\n",
            "Epoch 326/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.3038 - val_loss: 545.7075\n",
            "Epoch 327/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.0147 - val_loss: 193.4517\n",
            "Epoch 328/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.0216 - val_loss: 975.6660\n",
            "Epoch 329/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.9969 - val_loss: 585.6392\n",
            "Epoch 330/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.0149 - val_loss: 2511.8352\n",
            "Epoch 331/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.0410 - val_loss: 448.4554\n",
            "Epoch 332/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.8713 - val_loss: 117.2064\n",
            "Epoch 333/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.8958 - val_loss: 181.5025\n",
            "Epoch 334/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.9375 - val_loss: 147.9891\n",
            "Epoch 335/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.9073 - val_loss: 521.8058\n",
            "Epoch 336/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9488 - val_loss: 219.8457\n",
            "Epoch 337/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.9108 - val_loss: 202.9360\n",
            "Epoch 338/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.9028 - val_loss: 232.1684\n",
            "Epoch 339/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7872 - val_loss: 183.5227\n",
            "Epoch 340/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.8665 - val_loss: 129.3828\n",
            "Epoch 341/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9355 - val_loss: 104.3908\n",
            "Epoch 342/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9502 - val_loss: 427.5236\n",
            "Epoch 343/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.9555 - val_loss: 842.4502\n",
            "Epoch 344/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9960 - val_loss: 105.5230\n",
            "Epoch 345/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9857 - val_loss: 1384.7615\n",
            "Epoch 346/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9425 - val_loss: 128.4946\n",
            "Epoch 347/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.8153 - val_loss: 728.7201\n",
            "Epoch 348/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.7734 - val_loss: 195.9905\n",
            "Epoch 349/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.7672 - val_loss: 376.0368\n",
            "Epoch 350/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.7306 - val_loss: 1324.3698\n",
            "Epoch 351/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.0427 - val_loss: 131.5647\n",
            "Epoch 352/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7748 - val_loss: 918.9368\n",
            "Epoch 353/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.8204 - val_loss: 102.5264\n",
            "Epoch 354/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.8759 - val_loss: 102.4134\n",
            "Epoch 355/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.9338 - val_loss: 1074.2037\n",
            "Epoch 356/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.9534 - val_loss: 295.1488\n",
            "Epoch 357/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7812 - val_loss: 194.9774\n",
            "Epoch 358/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7306 - val_loss: 163.9749\n",
            "Epoch 359/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.6871 - val_loss: 100.8416\n",
            "Epoch 360/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7991 - val_loss: 328.3908\n",
            "Epoch 361/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.8976 - val_loss: 105.3623\n",
            "Epoch 362/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.6670 - val_loss: 113.3806\n",
            "Epoch 363/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9230 - val_loss: 633.0738\n",
            "Epoch 364/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.0830 - val_loss: 1365.0233\n",
            "Epoch 365/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.6862 - val_loss: 156.6124\n",
            "Epoch 366/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.8222 - val_loss: 267.6738\n",
            "Epoch 367/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7364 - val_loss: 118.5419\n",
            "Epoch 368/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7331 - val_loss: 93.7570\n",
            "Epoch 369/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.9951 - val_loss: 195.7335\n",
            "Epoch 370/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.9702 - val_loss: 136.7820\n",
            "Epoch 371/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7937 - val_loss: 103.4502\n",
            "Epoch 372/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.8009 - val_loss: 121.3729\n",
            "Epoch 373/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7172 - val_loss: 401.7515\n",
            "Epoch 374/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7187 - val_loss: 282.6408\n",
            "Epoch 375/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.9466 - val_loss: 452.5497\n",
            "Epoch 376/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.7660 - val_loss: 907.2465\n",
            "Epoch 377/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.8281 - val_loss: 363.3560\n",
            "Epoch 378/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7517 - val_loss: 560.1508\n",
            "Epoch 379/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.6875 - val_loss: 372.7243\n",
            "Epoch 380/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.5549 - val_loss: 167.6740\n",
            "Epoch 381/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.6746 - val_loss: 99.0565\n",
            "Epoch 382/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.6340 - val_loss: 948.2109\n",
            "Epoch 383/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.5216 - val_loss: 1122.6260\n",
            "Epoch 384/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.5628 - val_loss: 102.4663\n",
            "Epoch 385/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.4532 - val_loss: 1342.7915\n",
            "Epoch 386/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.6117 - val_loss: 100.6191\n",
            "Epoch 387/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7176 - val_loss: 100.8870\n",
            "Epoch 388/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.6635 - val_loss: 122.7672\n",
            "Epoch 389/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.6361 - val_loss: 204.7092\n",
            "Epoch 390/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.5486 - val_loss: 1129.4435\n",
            "Epoch 391/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.7187 - val_loss: 93.9192\n",
            "Epoch 392/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.4403 - val_loss: 853.6837\n",
            "Epoch 393/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.4650 - val_loss: 104.7181\n",
            "Epoch 394/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.6779 - val_loss: 835.0151\n",
            "Epoch 395/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.6142 - val_loss: 238.3385\n",
            "Epoch 396/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.5642 - val_loss: 457.2863\n",
            "Epoch 397/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.5589 - val_loss: 104.3283\n",
            "Epoch 398/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.5953 - val_loss: 750.1031\n",
            "Epoch 399/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.8597 - val_loss: 160.5423\n",
            "Epoch 400/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4102 - val_loss: 113.9522\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "397a04e3-8688-4ff1-bf2e-d96c7e59c563",
        "id": "lYDcggm8CSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -4.085688019036053 \n",
            "MAE:  8.183134073790542 \n",
            "SD:  9.862017907986475\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "81140260-fccf-4c60-98b0-694727594573",
        "id": "VpKjAxdPCSwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "98d4abf3-d753-4b0f-e154-2e642162a672",
        "id": "U0vAhaD0CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,185\n",
            "Trainable params: 1,145\n",
            "Non-trainable params: 40\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "outputId": "bb361293-3f36-4cf0-e0c7-c885ea1666ad",
        "id": "dcXAOqd2CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 12493.0508 - val_loss: 12352.4395\n",
            "Epoch 2/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 12277.7471 - val_loss: 12115.2949\n",
            "Epoch 3/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 12062.0312 - val_loss: 11898.3701\n",
            "Epoch 4/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11818.6240 - val_loss: 11633.2246\n",
            "Epoch 5/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11532.6162 - val_loss: 11346.3047\n",
            "Epoch 6/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11206.9932 - val_loss: 10997.0254\n",
            "Epoch 7/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 10844.3408 - val_loss: 10600.8359\n",
            "Epoch 8/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 10449.8047 - val_loss: 10198.3770\n",
            "Epoch 9/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10027.7256 - val_loss: 9747.5605\n",
            "Epoch 10/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 9582.5439 - val_loss: 9257.7090\n",
            "Epoch 11/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9117.9131 - val_loss: 8936.7256\n",
            "Epoch 12/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8637.3857 - val_loss: 8269.6709\n",
            "Epoch 13/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 8025.8525 - val_loss: 8042.7729\n",
            "Epoch 14/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7147.7480 - val_loss: 7194.3604\n",
            "Epoch 15/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6493.4736 - val_loss: 5411.5337\n",
            "Epoch 16/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 5889.6592 - val_loss: 4933.8110\n",
            "Epoch 17/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 5316.9741 - val_loss: 5342.2144\n",
            "Epoch 18/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4766.6494 - val_loss: 3733.8877\n",
            "Epoch 19/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4248.8677 - val_loss: 2703.4819\n",
            "Epoch 20/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3758.2390 - val_loss: 2327.0293\n",
            "Epoch 21/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3302.6284 - val_loss: 2465.7920\n",
            "Epoch 22/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2879.1409 - val_loss: 2730.2717\n",
            "Epoch 23/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2488.6958 - val_loss: 1983.6500\n",
            "Epoch 24/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2130.4805 - val_loss: 1318.6655\n",
            "Epoch 25/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1809.4781 - val_loss: 973.5735\n",
            "Epoch 26/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1516.0708 - val_loss: 1151.3691\n",
            "Epoch 27/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1257.2487 - val_loss: 1344.3308\n",
            "Epoch 28/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1033.2668 - val_loss: 1285.0243\n",
            "Epoch 29/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 840.6418 - val_loss: 1007.5222\n",
            "Epoch 30/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 676.3434 - val_loss: 742.1771\n",
            "Epoch 31/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 539.9042 - val_loss: 537.3574\n",
            "Epoch 32/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 432.3811 - val_loss: 397.2315\n",
            "Epoch 33/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 340.6370 - val_loss: 762.8563\n",
            "Epoch 34/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 272.4765 - val_loss: 784.4812\n",
            "Epoch 35/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 220.7398 - val_loss: 453.6106\n",
            "Epoch 36/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 183.8531 - val_loss: 344.1262\n",
            "Epoch 37/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 156.3507 - val_loss: 173.7784\n",
            "Epoch 38/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 138.6964 - val_loss: 163.2557\n",
            "Epoch 39/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 127.7277 - val_loss: 250.2345\n",
            "Epoch 40/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 119.6477 - val_loss: 357.8995\n",
            "Epoch 41/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 116.2455 - val_loss: 131.1270\n",
            "Epoch 42/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 113.1819 - val_loss: 250.0964\n",
            "Epoch 43/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 112.6030 - val_loss: 160.5030\n",
            "Epoch 44/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.8127 - val_loss: 160.5100\n",
            "Epoch 45/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.6191 - val_loss: 211.6680\n",
            "Epoch 46/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 108.8395 - val_loss: 204.7130\n",
            "Epoch 47/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 108.2825 - val_loss: 181.7526\n",
            "Epoch 48/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.3714 - val_loss: 1080.2560\n",
            "Epoch 49/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 107.9648 - val_loss: 144.6080\n",
            "Epoch 50/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 106.8687 - val_loss: 115.3304\n",
            "Epoch 51/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.7599 - val_loss: 348.6108\n",
            "Epoch 52/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 106.1721 - val_loss: 216.1304\n",
            "Epoch 53/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.9556 - val_loss: 294.2776\n",
            "Epoch 54/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 104.0984 - val_loss: 159.4169\n",
            "Epoch 55/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.8904 - val_loss: 120.8996\n",
            "Epoch 56/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.6470 - val_loss: 127.8093\n",
            "Epoch 57/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 102.9966 - val_loss: 124.0465\n",
            "Epoch 58/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 103.2043 - val_loss: 194.1063\n",
            "Epoch 59/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.8100 - val_loss: 973.1024\n",
            "Epoch 60/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1777 - val_loss: 145.4551\n",
            "Epoch 61/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.8563 - val_loss: 335.6816\n",
            "Epoch 62/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.2044 - val_loss: 176.9990\n",
            "Epoch 63/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.2121 - val_loss: 345.6705\n",
            "Epoch 64/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.4264 - val_loss: 144.0659\n",
            "Epoch 65/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.1353 - val_loss: 240.4462\n",
            "Epoch 66/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.0355 - val_loss: 110.7401\n",
            "Epoch 67/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.1033 - val_loss: 159.7658\n",
            "Epoch 68/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.9220 - val_loss: 343.1957\n",
            "Epoch 69/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 103.4787 - val_loss: 174.7304\n",
            "Epoch 70/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.4937 - val_loss: 127.6848\n",
            "Epoch 71/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.3533 - val_loss: 129.1637\n",
            "Epoch 72/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.6320 - val_loss: 138.5868\n",
            "Epoch 73/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 101.5327 - val_loss: 119.4870\n",
            "Epoch 74/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 100.4023 - val_loss: 119.5496\n",
            "Epoch 75/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 99.9924 - val_loss: 140.8503\n",
            "Epoch 76/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 101.0970 - val_loss: 116.2290\n",
            "Epoch 77/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.2182 - val_loss: 138.4789\n",
            "Epoch 78/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.3542 - val_loss: 111.1381\n",
            "Epoch 79/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.3187 - val_loss: 191.1129\n",
            "Epoch 80/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 99.3443 - val_loss: 189.6233\n",
            "Epoch 81/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.1140 - val_loss: 161.7966\n",
            "Epoch 82/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9408 - val_loss: 153.6837\n",
            "Epoch 83/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.2186 - val_loss: 161.7766\n",
            "Epoch 84/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.6124 - val_loss: 160.3092\n",
            "Epoch 85/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 99.0740 - val_loss: 108.5809\n",
            "Epoch 86/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9333 - val_loss: 249.6033\n",
            "Epoch 87/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7556 - val_loss: 109.2564\n",
            "Epoch 88/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.4349 - val_loss: 164.3915\n",
            "Epoch 89/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.2095 - val_loss: 317.5430\n",
            "Epoch 90/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.8575 - val_loss: 655.2834\n",
            "Epoch 91/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.9971 - val_loss: 118.8233\n",
            "Epoch 92/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.1575 - val_loss: 148.9418\n",
            "Epoch 93/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 98.5114 - val_loss: 272.5490\n",
            "Epoch 94/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.8332 - val_loss: 124.1311\n",
            "Epoch 95/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.8075 - val_loss: 132.4580\n",
            "Epoch 96/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.8071 - val_loss: 148.3768\n",
            "Epoch 97/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.6633 - val_loss: 192.4142\n",
            "Epoch 98/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.7825 - val_loss: 118.1878\n",
            "Epoch 99/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.4810 - val_loss: 106.1167\n",
            "Epoch 100/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.4681 - val_loss: 160.4963\n",
            "Epoch 101/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.7255 - val_loss: 127.3486\n",
            "Epoch 102/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.7292 - val_loss: 172.6773\n",
            "Epoch 103/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.7625 - val_loss: 140.6402\n",
            "Epoch 104/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.5685 - val_loss: 191.7544\n",
            "Epoch 105/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.5624 - val_loss: 106.4781\n",
            "Epoch 106/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.1371 - val_loss: 107.6897\n",
            "Epoch 107/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.1301 - val_loss: 483.9066\n",
            "Epoch 108/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.0989 - val_loss: 455.3778\n",
            "Epoch 109/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.0828 - val_loss: 191.4482\n",
            "Epoch 110/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.1681 - val_loss: 114.7689\n",
            "Epoch 111/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 97.0624 - val_loss: 142.3127\n",
            "Epoch 112/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.8591 - val_loss: 116.7334\n",
            "Epoch 113/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.7721 - val_loss: 121.2388\n",
            "Epoch 114/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.7816 - val_loss: 269.8223\n",
            "Epoch 115/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.5099 - val_loss: 164.2741\n",
            "Epoch 116/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.7758 - val_loss: 183.0036\n",
            "Epoch 117/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.4083 - val_loss: 261.6602\n",
            "Epoch 118/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.5882 - val_loss: 274.0880\n",
            "Epoch 119/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.7515 - val_loss: 145.5536\n",
            "Epoch 120/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.8197 - val_loss: 273.8549\n",
            "Epoch 121/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.4422 - val_loss: 151.4859\n",
            "Epoch 122/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.1693 - val_loss: 121.9617\n",
            "Epoch 123/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.7302 - val_loss: 248.6261\n",
            "Epoch 124/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.0526 - val_loss: 164.2705\n",
            "Epoch 125/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.7025 - val_loss: 199.1499\n",
            "Epoch 126/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.6397 - val_loss: 115.8617\n",
            "Epoch 127/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.3592 - val_loss: 192.0358\n",
            "Epoch 128/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.2396 - val_loss: 123.5423\n",
            "Epoch 129/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.1712 - val_loss: 141.0643\n",
            "Epoch 130/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.3114 - val_loss: 423.4194\n",
            "Epoch 131/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.7305 - val_loss: 152.2162\n",
            "Epoch 132/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.1963 - val_loss: 112.2305\n",
            "Epoch 133/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.9561 - val_loss: 238.6084\n",
            "Epoch 134/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.9853 - val_loss: 216.9982\n",
            "Epoch 135/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.8449 - val_loss: 161.2230\n",
            "Epoch 136/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.9232 - val_loss: 125.6112\n",
            "Epoch 137/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.9984 - val_loss: 111.5449\n",
            "Epoch 138/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.3079 - val_loss: 155.5011\n",
            "Epoch 139/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.9587 - val_loss: 120.6518\n",
            "Epoch 140/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.8216 - val_loss: 114.5787\n",
            "Epoch 141/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.6921 - val_loss: 108.3724\n",
            "Epoch 142/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.9197 - val_loss: 135.4179\n",
            "Epoch 143/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.6347 - val_loss: 170.0302\n",
            "Epoch 144/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.3868 - val_loss: 146.2507\n",
            "Epoch 145/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.4244 - val_loss: 196.6358\n",
            "Epoch 146/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.6849 - val_loss: 122.8518\n",
            "Epoch 147/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.9622 - val_loss: 126.0629\n",
            "Epoch 148/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.6651 - val_loss: 286.4337\n",
            "Epoch 149/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.8068 - val_loss: 107.1484\n",
            "Epoch 150/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.3313 - val_loss: 180.3712\n",
            "Epoch 151/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.5766 - val_loss: 239.9015\n",
            "Epoch 152/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.3826 - val_loss: 104.7029\n",
            "Epoch 153/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.2329 - val_loss: 148.1850\n",
            "Epoch 154/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.9143 - val_loss: 106.0329\n",
            "Epoch 155/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.0576 - val_loss: 117.1575\n",
            "Epoch 156/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.3725 - val_loss: 107.2133\n",
            "Epoch 157/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.0089 - val_loss: 118.0636\n",
            "Epoch 158/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.0498 - val_loss: 109.4970\n",
            "Epoch 159/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.9198 - val_loss: 115.4738\n",
            "Epoch 160/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.2416 - val_loss: 108.0997\n",
            "Epoch 161/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.8546 - val_loss: 141.2186\n",
            "Epoch 162/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.9914 - val_loss: 159.8259\n",
            "Epoch 163/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.0632 - val_loss: 112.6842\n",
            "Epoch 164/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.8313 - val_loss: 123.1377\n",
            "Epoch 165/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.1121 - val_loss: 147.4113\n",
            "Epoch 166/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.9774 - val_loss: 165.4045\n",
            "Epoch 167/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.8303 - val_loss: 113.9341\n",
            "Epoch 168/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 95.1976 - val_loss: 114.9699\n",
            "Epoch 169/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.8605 - val_loss: 156.8422\n",
            "Epoch 170/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.8036 - val_loss: 111.6058\n",
            "Epoch 171/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.8812 - val_loss: 144.1691\n",
            "Epoch 172/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.8927 - val_loss: 245.2208\n",
            "Epoch 173/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.4756 - val_loss: 105.2539\n",
            "Epoch 174/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.8140 - val_loss: 173.2749\n",
            "Epoch 175/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.6510 - val_loss: 111.6347\n",
            "Epoch 176/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4891 - val_loss: 162.3766\n",
            "Epoch 177/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.9796 - val_loss: 154.4925\n",
            "Epoch 178/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.7859 - val_loss: 108.3279\n",
            "Epoch 179/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.8097 - val_loss: 123.9853\n",
            "Epoch 180/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.4786 - val_loss: 135.2070\n",
            "Epoch 181/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.5087 - val_loss: 142.2439\n",
            "Epoch 182/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4628 - val_loss: 241.1382\n",
            "Epoch 183/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.6468 - val_loss: 194.5145\n",
            "Epoch 184/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.4395 - val_loss: 109.8805\n",
            "Epoch 185/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2882 - val_loss: 209.3058\n",
            "Epoch 186/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4800 - val_loss: 117.3369\n",
            "Epoch 187/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.2260 - val_loss: 129.1229\n",
            "Epoch 188/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4917 - val_loss: 105.0288\n",
            "Epoch 189/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2854 - val_loss: 125.5412\n",
            "Epoch 190/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.2295 - val_loss: 111.7470\n",
            "Epoch 191/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.3076 - val_loss: 202.1338\n",
            "Epoch 192/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.4639 - val_loss: 115.0926\n",
            "Epoch 193/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1690 - val_loss: 162.4826\n",
            "Epoch 194/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.5229 - val_loss: 133.7063\n",
            "Epoch 195/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1974 - val_loss: 107.8172\n",
            "Epoch 196/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.0977 - val_loss: 137.9036\n",
            "Epoch 197/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1623 - val_loss: 134.9609\n",
            "Epoch 198/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.8563 - val_loss: 102.7106\n",
            "Epoch 199/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 94.1365 - val_loss: 164.7544\n",
            "Epoch 200/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.8022 - val_loss: 127.7051\n",
            "Epoch 201/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.3200 - val_loss: 200.1080\n",
            "Epoch 202/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9688 - val_loss: 112.5050\n",
            "Epoch 203/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.5437 - val_loss: 117.9397\n",
            "Epoch 204/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.7240 - val_loss: 124.5531\n",
            "Epoch 205/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.4920 - val_loss: 104.5284\n",
            "Epoch 206/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.5925 - val_loss: 104.0480\n",
            "Epoch 207/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.2229 - val_loss: 148.9122\n",
            "Epoch 208/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.9523 - val_loss: 122.2177\n",
            "Epoch 209/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.6463 - val_loss: 118.1008\n",
            "Epoch 210/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.3938 - val_loss: 155.2856\n",
            "Epoch 211/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.5080 - val_loss: 122.5051\n",
            "Epoch 212/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.3259 - val_loss: 112.2407\n",
            "Epoch 213/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.3930 - val_loss: 187.6221\n",
            "Epoch 214/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.1027 - val_loss: 117.5895\n",
            "Epoch 215/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.4031 - val_loss: 107.4240\n",
            "Epoch 216/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.1730 - val_loss: 102.1799\n",
            "Epoch 217/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.0731 - val_loss: 113.9315\n",
            "Epoch 218/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.0382 - val_loss: 112.8359\n",
            "Epoch 219/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.1055 - val_loss: 127.0811\n",
            "Epoch 220/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.0034 - val_loss: 201.6182\n",
            "Epoch 221/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.0665 - val_loss: 111.4185\n",
            "Epoch 222/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.9108 - val_loss: 186.4147\n",
            "Epoch 223/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.9878 - val_loss: 111.0441\n",
            "Epoch 224/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.8801 - val_loss: 144.2442\n",
            "Epoch 225/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 93.0748 - val_loss: 148.3092\n",
            "Epoch 226/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.0055 - val_loss: 115.9136\n",
            "Epoch 227/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.1214 - val_loss: 118.4926\n",
            "Epoch 228/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.6998 - val_loss: 108.4427\n",
            "Epoch 229/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7657 - val_loss: 101.1326\n",
            "Epoch 230/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7725 - val_loss: 113.7858\n",
            "Epoch 231/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.8498 - val_loss: 102.0297\n",
            "Epoch 232/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.3239 - val_loss: 104.1680\n",
            "Epoch 233/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.6114 - val_loss: 112.4978\n",
            "Epoch 234/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7280 - val_loss: 105.8762\n",
            "Epoch 235/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.5218 - val_loss: 118.7473\n",
            "Epoch 236/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.5475 - val_loss: 124.2877\n",
            "Epoch 237/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.5658 - val_loss: 144.6058\n",
            "Epoch 238/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 92.6205 - val_loss: 130.0553\n",
            "Epoch 239/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.4186 - val_loss: 113.3398\n",
            "Epoch 240/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.4853 - val_loss: 106.2140\n",
            "Epoch 241/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.3762 - val_loss: 131.5904\n",
            "Epoch 242/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.1514 - val_loss: 390.5204\n",
            "Epoch 243/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.4010 - val_loss: 154.1950\n",
            "Epoch 244/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.0994 - val_loss: 103.8809\n",
            "Epoch 245/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.8952 - val_loss: 110.7969\n",
            "Epoch 246/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.4306 - val_loss: 102.3845\n",
            "Epoch 247/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.2412 - val_loss: 141.0725\n",
            "Epoch 248/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.2178 - val_loss: 140.1609\n",
            "Epoch 249/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8823 - val_loss: 124.2239\n",
            "Epoch 250/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8173 - val_loss: 106.6140\n",
            "Epoch 251/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.9252 - val_loss: 103.4999\n",
            "Epoch 252/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.8421 - val_loss: 123.4378\n",
            "Epoch 253/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.6661 - val_loss: 141.0174\n",
            "Epoch 254/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.8185 - val_loss: 99.7867\n",
            "Epoch 255/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.0957 - val_loss: 121.7892\n",
            "Epoch 256/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.2857 - val_loss: 114.5607\n",
            "Epoch 257/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.2115 - val_loss: 99.9206\n",
            "Epoch 258/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.2597 - val_loss: 197.2110\n",
            "Epoch 259/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 90.3068 - val_loss: 98.3473\n",
            "Epoch 260/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8103 - val_loss: 99.0670\n",
            "Epoch 261/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.0023 - val_loss: 99.3009\n",
            "Epoch 262/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.8440 - val_loss: 98.8586\n",
            "Epoch 263/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.6037 - val_loss: 116.2615\n",
            "Epoch 264/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.6749 - val_loss: 105.0918\n",
            "Epoch 265/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.8123 - val_loss: 99.5667\n",
            "Epoch 266/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.4726 - val_loss: 119.4217\n",
            "Epoch 267/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.6232 - val_loss: 120.2019\n",
            "Epoch 268/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.7262 - val_loss: 125.0752\n",
            "Epoch 269/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.2484 - val_loss: 105.3400\n",
            "Epoch 270/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.4191 - val_loss: 100.6011\n",
            "Epoch 271/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.3084 - val_loss: 132.3243\n",
            "Epoch 272/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.2739 - val_loss: 102.5885\n",
            "Epoch 273/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.1191 - val_loss: 101.9036\n",
            "Epoch 274/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.1622 - val_loss: 118.8916\n",
            "Epoch 275/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.9953 - val_loss: 110.9760\n",
            "Epoch 276/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.0641 - val_loss: 104.9405\n",
            "Epoch 277/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.8579 - val_loss: 101.8683\n",
            "Epoch 278/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.2495 - val_loss: 136.2605\n",
            "Epoch 279/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.9677 - val_loss: 139.8840\n",
            "Epoch 280/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.1130 - val_loss: 97.2290\n",
            "Epoch 281/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 89.0301 - val_loss: 108.8355\n",
            "Epoch 282/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.6535 - val_loss: 108.9224\n",
            "Epoch 283/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.5867 - val_loss: 207.4041\n",
            "Epoch 284/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.7878 - val_loss: 101.3873\n",
            "Epoch 285/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.8760 - val_loss: 98.2549\n",
            "Epoch 286/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.4994 - val_loss: 106.5159\n",
            "Epoch 287/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.5700 - val_loss: 99.5794\n",
            "Epoch 288/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.4115 - val_loss: 102.3187\n",
            "Epoch 289/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.2999 - val_loss: 116.0424\n",
            "Epoch 290/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.4058 - val_loss: 224.2336\n",
            "Epoch 291/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 88.4731 - val_loss: 124.4479\n",
            "Epoch 292/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.3831 - val_loss: 102.4831\n",
            "Epoch 293/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.2229 - val_loss: 100.3402\n",
            "Epoch 294/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0951 - val_loss: 99.9175\n",
            "Epoch 295/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.1763 - val_loss: 100.8538\n",
            "Epoch 296/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0616 - val_loss: 134.0484\n",
            "Epoch 297/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.1000 - val_loss: 99.2791\n",
            "Epoch 298/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.2608 - val_loss: 104.1923\n",
            "Epoch 299/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.9468 - val_loss: 98.7060\n",
            "Epoch 300/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.9371 - val_loss: 114.1564\n",
            "Epoch 301/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0111 - val_loss: 98.7982\n",
            "Epoch 302/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.9926 - val_loss: 119.4578\n",
            "Epoch 303/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.8924 - val_loss: 116.5363\n",
            "Epoch 304/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.6417 - val_loss: 105.8594\n",
            "Epoch 305/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6672 - val_loss: 104.7579\n",
            "Epoch 306/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.7191 - val_loss: 95.6921\n",
            "Epoch 307/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.6081 - val_loss: 153.2517\n",
            "Epoch 308/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.6499 - val_loss: 100.6176\n",
            "Epoch 309/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.4219 - val_loss: 103.6453\n",
            "Epoch 310/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.3203 - val_loss: 99.2677\n",
            "Epoch 311/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.3972 - val_loss: 105.1831\n",
            "Epoch 312/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.1863 - val_loss: 120.5988\n",
            "Epoch 313/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.3866 - val_loss: 102.4933\n",
            "Epoch 314/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.2273 - val_loss: 96.7826\n",
            "Epoch 315/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.1122 - val_loss: 102.8667\n",
            "Epoch 316/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.2876 - val_loss: 127.1440\n",
            "Epoch 317/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.3097 - val_loss: 121.5675\n",
            "Epoch 318/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.2070 - val_loss: 114.6490\n",
            "Epoch 319/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.9545 - val_loss: 95.4252\n",
            "Epoch 320/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.0378 - val_loss: 108.0907\n",
            "Epoch 321/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 87.0377 - val_loss: 106.2391\n",
            "Epoch 322/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 86.9876 - val_loss: 99.6358\n",
            "Epoch 323/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.9029 - val_loss: 152.8968\n",
            "Epoch 324/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.9446 - val_loss: 100.8311\n",
            "Epoch 325/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.7175 - val_loss: 97.3159\n",
            "Epoch 326/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.9416 - val_loss: 97.6795\n",
            "Epoch 327/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.7495 - val_loss: 109.8377\n",
            "Epoch 328/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.6068 - val_loss: 97.2092\n",
            "Epoch 329/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.5821 - val_loss: 100.7344\n",
            "Epoch 330/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.5362 - val_loss: 102.3058\n",
            "Epoch 331/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.5402 - val_loss: 128.3094\n",
            "Epoch 332/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.4816 - val_loss: 98.8950\n",
            "Epoch 333/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.3302 - val_loss: 95.0959\n",
            "Epoch 334/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.3820 - val_loss: 106.7843\n",
            "Epoch 335/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.3437 - val_loss: 99.2967\n",
            "Epoch 336/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.5123 - val_loss: 105.8002\n",
            "Epoch 337/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.4404 - val_loss: 108.5049\n",
            "Epoch 338/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.3799 - val_loss: 110.0183\n",
            "Epoch 339/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.1973 - val_loss: 107.4894\n",
            "Epoch 340/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.2491 - val_loss: 100.8137\n",
            "Epoch 341/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.3056 - val_loss: 101.4449\n",
            "Epoch 342/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.1314 - val_loss: 95.5393\n",
            "Epoch 343/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.1399 - val_loss: 99.6086\n",
            "Epoch 344/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.1581 - val_loss: 115.6542\n",
            "Epoch 345/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.3254 - val_loss: 97.8515\n",
            "Epoch 346/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.0217 - val_loss: 98.8634\n",
            "Epoch 347/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8122 - val_loss: 103.8069\n",
            "Epoch 348/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8388 - val_loss: 122.0466\n",
            "Epoch 349/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.9814 - val_loss: 108.3440\n",
            "Epoch 350/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8337 - val_loss: 106.4513\n",
            "Epoch 351/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.9455 - val_loss: 132.9869\n",
            "Epoch 352/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.9035 - val_loss: 143.6539\n",
            "Epoch 353/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.8528 - val_loss: 105.7263\n",
            "Epoch 354/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.6489 - val_loss: 99.9253\n",
            "Epoch 355/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.7506 - val_loss: 97.0269\n",
            "Epoch 356/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.8831 - val_loss: 130.8572\n",
            "Epoch 357/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.7227 - val_loss: 99.6653\n",
            "Epoch 358/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.8109 - val_loss: 103.0942\n",
            "Epoch 359/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8264 - val_loss: 99.8539\n",
            "Epoch 360/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.7690 - val_loss: 97.2108\n",
            "Epoch 361/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.6544 - val_loss: 103.6965\n",
            "Epoch 362/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.6625 - val_loss: 109.1050\n",
            "Epoch 363/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.7748 - val_loss: 99.6935\n",
            "Epoch 364/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.4709 - val_loss: 131.9968\n",
            "Epoch 365/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.7119 - val_loss: 123.8279\n",
            "Epoch 366/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5113 - val_loss: 100.6101\n",
            "Epoch 367/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4632 - val_loss: 114.9683\n",
            "Epoch 368/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.5234 - val_loss: 104.6970\n",
            "Epoch 369/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5460 - val_loss: 108.9708\n",
            "Epoch 370/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.4997 - val_loss: 109.3530\n",
            "Epoch 371/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.5836 - val_loss: 94.1792\n",
            "Epoch 372/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.3015 - val_loss: 110.9271\n",
            "Epoch 373/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.3900 - val_loss: 97.9518\n",
            "Epoch 374/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.2806 - val_loss: 107.5867\n",
            "Epoch 375/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.5898 - val_loss: 101.5505\n",
            "Epoch 376/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.6012 - val_loss: 101.3294\n",
            "Epoch 377/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.3946 - val_loss: 109.1154\n",
            "Epoch 378/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.4929 - val_loss: 114.8717\n",
            "Epoch 379/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.2624 - val_loss: 108.1478\n",
            "Epoch 380/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.3052 - val_loss: 101.4247\n",
            "Epoch 381/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.4245 - val_loss: 99.6312\n",
            "Epoch 382/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.3215 - val_loss: 105.1660\n",
            "Epoch 383/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4632 - val_loss: 130.3343\n",
            "Epoch 384/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.3350 - val_loss: 100.8764\n",
            "Epoch 385/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.2859 - val_loss: 108.5574\n",
            "Epoch 386/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.2079 - val_loss: 121.5743\n",
            "Epoch 387/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.1591 - val_loss: 101.2110\n",
            "Epoch 388/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.2906 - val_loss: 118.5789\n",
            "Epoch 389/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.4102 - val_loss: 112.8811\n",
            "Epoch 390/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.2655 - val_loss: 100.2073\n",
            "Epoch 391/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.1356 - val_loss: 101.5145\n",
            "Epoch 392/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.0154 - val_loss: 108.5188\n",
            "Epoch 393/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.0680 - val_loss: 105.8402\n",
            "Epoch 394/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.0853 - val_loss: 95.5829\n",
            "Epoch 395/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.0693 - val_loss: 103.0926\n",
            "Epoch 396/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.2551 - val_loss: 102.0833\n",
            "Epoch 397/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1550 - val_loss: 105.5261\n",
            "Epoch 398/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.2829 - val_loss: 101.3897\n",
            "Epoch 399/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.9931 - val_loss: 107.5573\n",
            "Epoch 400/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.2635 - val_loss: 106.0050\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696v_fuFCTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "043ba4bd-aad2-43cf-fbb2-6b06718aa0c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -2.7266383171184874 \n",
            "MAE:  7.85254583438778 \n",
            "SD:  9.92826222089961\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "ea2ea6d2-6092-4700-aefa-4a7614fcab2a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "mdZF2osWCUQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edcd740d-316e-4e1f-b5ee-b96a2214dc56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -2.0492266839111295 \n",
            "Ensemble_std:  10.019861630006112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65dee55e-5d74-4ab7-f421-96beac6eb4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_15 (Dense)            (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,185\n",
            "Trainable params: 1,145\n",
            "Non-trainable params: 40\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "outputId": "49aadc4c-8d52-4d92-ba88-a74309b22574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 3705.2053 - val_loss: 3728.5337\n",
            "Epoch 2/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3564.9216 - val_loss: 3522.8149\n",
            "Epoch 3/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3376.6614 - val_loss: 3319.2031\n",
            "Epoch 4/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3142.6946 - val_loss: 2902.7998\n",
            "Epoch 5/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2872.7385 - val_loss: 2749.1282\n",
            "Epoch 6/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2578.0188 - val_loss: 2334.1694\n",
            "Epoch 7/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2270.0613 - val_loss: 2232.5537\n",
            "Epoch 8/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1904.0618 - val_loss: 1814.2632\n",
            "Epoch 9/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1542.9641 - val_loss: 1374.5748\n",
            "Epoch 10/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1226.7915 - val_loss: 988.3771\n",
            "Epoch 11/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 948.5015 - val_loss: 782.2000\n",
            "Epoch 12/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 712.5844 - val_loss: 457.8222\n",
            "Epoch 13/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 519.9642 - val_loss: 454.7666\n",
            "Epoch 14/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 367.8738 - val_loss: 279.9102\n",
            "Epoch 15/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 253.1882 - val_loss: 163.4087\n",
            "Epoch 16/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 171.1641 - val_loss: 172.1645\n",
            "Epoch 17/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 115.6906 - val_loss: 126.0010\n",
            "Epoch 18/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.3194 - val_loss: 85.0327\n",
            "Epoch 19/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 59.2358 - val_loss: 79.1105\n",
            "Epoch 20/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 48.0829 - val_loss: 52.9643\n",
            "Epoch 21/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 42.1252 - val_loss: 85.8779\n",
            "Epoch 22/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.5558 - val_loss: 61.7072\n",
            "Epoch 23/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.4511 - val_loss: 49.9131\n",
            "Epoch 24/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3767 - val_loss: 45.1649\n",
            "Epoch 25/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0976 - val_loss: 40.6412\n",
            "Epoch 26/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8416 - val_loss: 41.8419\n",
            "Epoch 27/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7402 - val_loss: 89.4840\n",
            "Epoch 28/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6758 - val_loss: 86.0688\n",
            "Epoch 29/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6244 - val_loss: 49.5414\n",
            "Epoch 30/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6574 - val_loss: 39.8628\n",
            "Epoch 31/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.2634 - val_loss: 39.3555\n",
            "Epoch 32/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.2250 - val_loss: 44.0176\n",
            "Epoch 33/400\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.2395 - val_loss: 41.6077\n",
            "Epoch 34/400\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.2331 - val_loss: 51.8057\n",
            "Epoch 35/400\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 36.1186 - val_loss: 71.1125\n",
            "Epoch 36/400\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.9973 - val_loss: 40.9162\n",
            "Epoch 37/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.0013 - val_loss: 53.3264\n",
            "Epoch 38/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.9461 - val_loss: 46.1851\n",
            "Epoch 39/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.9372 - val_loss: 40.5585\n",
            "Epoch 40/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.8341 - val_loss: 44.6285\n",
            "Epoch 41/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7464 - val_loss: 44.8577\n",
            "Epoch 42/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.8092 - val_loss: 55.9647\n",
            "Epoch 43/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.8323 - val_loss: 46.2949\n",
            "Epoch 44/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6241 - val_loss: 57.0588\n",
            "Epoch 45/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.6078 - val_loss: 49.2632\n",
            "Epoch 46/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5356 - val_loss: 61.5008\n",
            "Epoch 47/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5559 - val_loss: 43.2052\n",
            "Epoch 48/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.6862 - val_loss: 51.3391\n",
            "Epoch 49/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5461 - val_loss: 54.7130\n",
            "Epoch 50/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4905 - val_loss: 77.4044\n",
            "Epoch 51/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3350 - val_loss: 44.2775\n",
            "Epoch 52/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.3569 - val_loss: 44.0471\n",
            "Epoch 53/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3835 - val_loss: 49.4479\n",
            "Epoch 54/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4098 - val_loss: 46.9288\n",
            "Epoch 55/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3123 - val_loss: 46.1668\n",
            "Epoch 56/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2633 - val_loss: 41.5973\n",
            "Epoch 57/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2790 - val_loss: 46.4047\n",
            "Epoch 58/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2838 - val_loss: 46.6540\n",
            "Epoch 59/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2166 - val_loss: 54.0327\n",
            "Epoch 60/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.1841 - val_loss: 40.7246\n",
            "Epoch 61/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.1635 - val_loss: 43.3362\n",
            "Epoch 62/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0446 - val_loss: 40.7941\n",
            "Epoch 63/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0975 - val_loss: 43.6047\n",
            "Epoch 64/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.1782 - val_loss: 44.4258\n",
            "Epoch 65/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.0451 - val_loss: 81.2260\n",
            "Epoch 66/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.0661 - val_loss: 44.6163\n",
            "Epoch 67/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8979 - val_loss: 45.1893\n",
            "Epoch 68/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.9264 - val_loss: 53.4542\n",
            "Epoch 69/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9136 - val_loss: 42.8702\n",
            "Epoch 70/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8263 - val_loss: 41.6559\n",
            "Epoch 71/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.0226 - val_loss: 45.0435\n",
            "Epoch 72/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7823 - val_loss: 41.0108\n",
            "Epoch 73/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8562 - val_loss: 54.5561\n",
            "Epoch 74/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7872 - val_loss: 52.4303\n",
            "Epoch 75/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8018 - val_loss: 48.3768\n",
            "Epoch 76/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6996 - val_loss: 42.9547\n",
            "Epoch 77/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7481 - val_loss: 42.2828\n",
            "Epoch 78/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7160 - val_loss: 45.4171\n",
            "Epoch 79/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5911 - val_loss: 53.4409\n",
            "Epoch 80/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6409 - val_loss: 42.3711\n",
            "Epoch 81/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6053 - val_loss: 64.7234\n",
            "Epoch 82/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5605 - val_loss: 45.2523\n",
            "Epoch 83/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6129 - val_loss: 43.3887\n",
            "Epoch 84/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5038 - val_loss: 45.3920\n",
            "Epoch 85/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5418 - val_loss: 38.2657\n",
            "Epoch 86/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6129 - val_loss: 63.0286\n",
            "Epoch 87/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4111 - val_loss: 45.7629\n",
            "Epoch 88/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.3221 - val_loss: 44.0423\n",
            "Epoch 89/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3886 - val_loss: 38.7461\n",
            "Epoch 90/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4062 - val_loss: 44.0722\n",
            "Epoch 91/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3170 - val_loss: 44.4218\n",
            "Epoch 92/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.3891 - val_loss: 48.7365\n",
            "Epoch 93/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.2924 - val_loss: 48.6036\n",
            "Epoch 94/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.3709 - val_loss: 47.5085\n",
            "Epoch 95/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4437 - val_loss: 58.2889\n",
            "Epoch 96/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.3025 - val_loss: 42.4548\n",
            "Epoch 97/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2345 - val_loss: 54.0015\n",
            "Epoch 98/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2348 - val_loss: 47.4867\n",
            "Epoch 99/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0978 - val_loss: 69.6476\n",
            "Epoch 100/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1412 - val_loss: 41.5078\n",
            "Epoch 101/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.1482 - val_loss: 53.9847\n",
            "Epoch 102/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.1064 - val_loss: 42.9720\n",
            "Epoch 103/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0729 - val_loss: 44.8525\n",
            "Epoch 104/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.1051 - val_loss: 52.8589\n",
            "Epoch 105/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9922 - val_loss: 56.7956\n",
            "Epoch 106/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9611 - val_loss: 40.0938\n",
            "Epoch 107/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0012 - val_loss: 46.6914\n",
            "Epoch 108/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9637 - val_loss: 43.8805\n",
            "Epoch 109/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8691 - val_loss: 46.4310\n",
            "Epoch 110/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9690 - val_loss: 44.1789\n",
            "Epoch 111/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8985 - val_loss: 40.3112\n",
            "Epoch 112/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9084 - val_loss: 60.6540\n",
            "Epoch 113/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9316 - val_loss: 40.3194\n",
            "Epoch 114/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8779 - val_loss: 46.8570\n",
            "Epoch 115/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8592 - val_loss: 46.3449\n",
            "Epoch 116/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8044 - val_loss: 41.9755\n",
            "Epoch 117/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8882 - val_loss: 44.5116\n",
            "Epoch 118/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7905 - val_loss: 51.0001\n",
            "Epoch 119/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8556 - val_loss: 67.6568\n",
            "Epoch 120/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 33.7568 - val_loss: 55.6105\n",
            "Epoch 121/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8243 - val_loss: 50.3727\n",
            "Epoch 122/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8146 - val_loss: 58.6478\n",
            "Epoch 123/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8077 - val_loss: 48.7580\n",
            "Epoch 124/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7312 - val_loss: 44.3955\n",
            "Epoch 125/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7647 - val_loss: 48.3082\n",
            "Epoch 126/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.6529 - val_loss: 49.0252\n",
            "Epoch 127/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.6352 - val_loss: 48.4406\n",
            "Epoch 128/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5587 - val_loss: 40.5810\n",
            "Epoch 129/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6434 - val_loss: 39.4914\n",
            "Epoch 130/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.6555 - val_loss: 54.1528\n",
            "Epoch 131/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7304 - val_loss: 47.9659\n",
            "Epoch 132/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.5577 - val_loss: 47.8383\n",
            "Epoch 133/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7028 - val_loss: 43.0056\n",
            "Epoch 134/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.5126 - val_loss: 52.4731\n",
            "Epoch 135/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.5023 - val_loss: 38.4645\n",
            "Epoch 136/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4956 - val_loss: 52.7067\n",
            "Epoch 137/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4926 - val_loss: 39.2477\n",
            "Epoch 138/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4298 - val_loss: 68.3569\n",
            "Epoch 139/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.4324 - val_loss: 45.2719\n",
            "Epoch 140/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.3772 - val_loss: 44.9461\n",
            "Epoch 141/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3660 - val_loss: 40.6480\n",
            "Epoch 142/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.3405 - val_loss: 40.1697\n",
            "Epoch 143/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.3961 - val_loss: 40.2615\n",
            "Epoch 144/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3785 - val_loss: 56.7085\n",
            "Epoch 145/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3627 - val_loss: 42.3150\n",
            "Epoch 146/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.3576 - val_loss: 48.9995\n",
            "Epoch 147/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4102 - val_loss: 55.5783\n",
            "Epoch 148/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.5032 - val_loss: 43.8523\n",
            "Epoch 149/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3372 - val_loss: 41.0835\n",
            "Epoch 150/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3430 - val_loss: 39.0773\n",
            "Epoch 151/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.3707 - val_loss: 43.6809\n",
            "Epoch 152/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3391 - val_loss: 46.1583\n",
            "Epoch 153/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.3740 - val_loss: 41.5655\n",
            "Epoch 154/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3773 - val_loss: 40.5252\n",
            "Epoch 155/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2804 - val_loss: 43.0490\n",
            "Epoch 156/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2659 - val_loss: 45.2759\n",
            "Epoch 157/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.2535 - val_loss: 55.9663\n",
            "Epoch 158/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.2198 - val_loss: 54.9906\n",
            "Epoch 159/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2162 - val_loss: 42.3661\n",
            "Epoch 160/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.4468 - val_loss: 42.3130\n",
            "Epoch 161/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4444 - val_loss: 44.3689\n",
            "Epoch 162/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.2564 - val_loss: 40.6803\n",
            "Epoch 163/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.1774 - val_loss: 40.9055\n",
            "Epoch 164/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.2254 - val_loss: 38.6611\n",
            "Epoch 165/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.1974 - val_loss: 39.9679\n",
            "Epoch 166/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2314 - val_loss: 45.0293\n",
            "Epoch 167/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1263 - val_loss: 43.5704\n",
            "Epoch 168/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.1160 - val_loss: 51.1892\n",
            "Epoch 169/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.2839 - val_loss: 38.9126\n",
            "Epoch 170/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1851 - val_loss: 39.1350\n",
            "Epoch 171/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.2042 - val_loss: 48.3829\n",
            "Epoch 172/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1795 - val_loss: 42.2351\n",
            "Epoch 173/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1060 - val_loss: 38.5785\n",
            "Epoch 174/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.0877 - val_loss: 43.1098\n",
            "Epoch 175/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.1792 - val_loss: 48.0826\n",
            "Epoch 176/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.0382 - val_loss: 41.4078\n",
            "Epoch 177/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0813 - val_loss: 53.5129\n",
            "Epoch 178/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1805 - val_loss: 50.6479\n",
            "Epoch 179/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.1834 - val_loss: 43.0778\n",
            "Epoch 180/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.1570 - val_loss: 44.3746\n",
            "Epoch 181/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1874 - val_loss: 50.2118\n",
            "Epoch 182/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0963 - val_loss: 51.4317\n",
            "Epoch 183/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.1194 - val_loss: 40.4585\n",
            "Epoch 184/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.0988 - val_loss: 59.1387\n",
            "Epoch 185/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.0412 - val_loss: 41.9997\n",
            "Epoch 186/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1389 - val_loss: 55.5018\n",
            "Epoch 187/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.0340 - val_loss: 39.2718\n",
            "Epoch 188/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0571 - val_loss: 42.4290\n",
            "Epoch 189/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.0041 - val_loss: 42.4235\n",
            "Epoch 190/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0328 - val_loss: 40.8420\n",
            "Epoch 191/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9827 - val_loss: 39.9627\n",
            "Epoch 192/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1102 - val_loss: 41.3847\n",
            "Epoch 193/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9513 - val_loss: 39.4232\n",
            "Epoch 194/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1409 - val_loss: 52.5701\n",
            "Epoch 195/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9607 - val_loss: 39.3248\n",
            "Epoch 196/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0774 - val_loss: 40.8149\n",
            "Epoch 197/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9913 - val_loss: 46.4061\n",
            "Epoch 198/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0003 - val_loss: 45.1783\n",
            "Epoch 199/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9753 - val_loss: 41.9720\n",
            "Epoch 200/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1269 - val_loss: 40.4793\n",
            "Epoch 201/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9633 - val_loss: 48.7486\n",
            "Epoch 202/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9052 - val_loss: 42.0643\n",
            "Epoch 203/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9816 - val_loss: 46.4694\n",
            "Epoch 204/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0020 - val_loss: 44.7577\n",
            "Epoch 205/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9149 - val_loss: 47.0460\n",
            "Epoch 206/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9810 - val_loss: 43.3202\n",
            "Epoch 207/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8748 - val_loss: 46.9659\n",
            "Epoch 208/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9467 - val_loss: 39.9364\n",
            "Epoch 209/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9435 - val_loss: 39.3635\n",
            "Epoch 210/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9604 - val_loss: 38.2980\n",
            "Epoch 211/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8479 - val_loss: 41.1378\n",
            "Epoch 212/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9043 - val_loss: 42.0621\n",
            "Epoch 213/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8717 - val_loss: 38.6507\n",
            "Epoch 214/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8397 - val_loss: 39.5987\n",
            "Epoch 215/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8377 - val_loss: 46.1044\n",
            "Epoch 216/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8684 - val_loss: 46.4199\n",
            "Epoch 217/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9340 - val_loss: 60.1372\n",
            "Epoch 218/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9401 - val_loss: 52.1686\n",
            "Epoch 219/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8428 - val_loss: 40.8244\n",
            "Epoch 220/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9138 - val_loss: 41.2194\n",
            "Epoch 221/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9021 - val_loss: 38.4848\n",
            "Epoch 222/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8310 - val_loss: 37.6682\n",
            "Epoch 223/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7654 - val_loss: 47.6984\n",
            "Epoch 224/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8911 - val_loss: 38.7717\n",
            "Epoch 225/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7740 - val_loss: 39.6217\n",
            "Epoch 226/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9200 - val_loss: 44.4072\n",
            "Epoch 227/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8302 - val_loss: 39.9827\n",
            "Epoch 228/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7821 - val_loss: 42.7518\n",
            "Epoch 229/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8759 - val_loss: 38.4413\n",
            "Epoch 230/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8961 - val_loss: 47.8163\n",
            "Epoch 231/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8177 - val_loss: 37.4956\n",
            "Epoch 232/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8071 - val_loss: 46.8075\n",
            "Epoch 233/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7460 - val_loss: 39.0235\n",
            "Epoch 234/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7892 - val_loss: 43.0865\n",
            "Epoch 235/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7927 - val_loss: 39.3051\n",
            "Epoch 236/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7856 - val_loss: 42.1475\n",
            "Epoch 237/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7724 - val_loss: 37.3484\n",
            "Epoch 238/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7356 - val_loss: 36.8019\n",
            "Epoch 239/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7962 - val_loss: 40.0664\n",
            "Epoch 240/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7289 - val_loss: 47.0787\n",
            "Epoch 241/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8305 - val_loss: 38.4585\n",
            "Epoch 242/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.6901 - val_loss: 41.1699\n",
            "Epoch 243/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7420 - val_loss: 40.9618\n",
            "Epoch 244/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7331 - val_loss: 45.0290\n",
            "Epoch 245/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7319 - val_loss: 37.2117\n",
            "Epoch 246/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7245 - val_loss: 47.8764\n",
            "Epoch 247/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7283 - val_loss: 36.8970\n",
            "Epoch 248/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.6921 - val_loss: 38.9498\n",
            "Epoch 249/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7969 - val_loss: 50.6891\n",
            "Epoch 250/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7241 - val_loss: 42.3748\n",
            "Epoch 251/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7159 - val_loss: 39.3457\n",
            "Epoch 252/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7573 - val_loss: 39.1589\n",
            "Epoch 253/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7058 - val_loss: 38.7984\n",
            "Epoch 254/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5964 - val_loss: 41.3788\n",
            "Epoch 255/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7548 - val_loss: 42.0009\n",
            "Epoch 256/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 32.7169 - val_loss: 37.9633\n",
            "Epoch 257/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 32.6482 - val_loss: 38.3205\n",
            "Epoch 258/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.6604 - val_loss: 39.0479\n",
            "Epoch 259/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.6978 - val_loss: 48.6104\n",
            "Epoch 260/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6697 - val_loss: 37.6432\n",
            "Epoch 261/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6213 - val_loss: 38.9232\n",
            "Epoch 262/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7151 - val_loss: 38.0700\n",
            "Epoch 263/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7047 - val_loss: 41.1747\n",
            "Epoch 264/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7036 - val_loss: 42.3736\n",
            "Epoch 265/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.6849 - val_loss: 48.7103\n",
            "Epoch 266/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6036 - val_loss: 37.8232\n",
            "Epoch 267/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6205 - val_loss: 44.6529\n",
            "Epoch 268/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6062 - val_loss: 37.8898\n",
            "Epoch 269/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6044 - val_loss: 39.3019\n",
            "Epoch 270/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5644 - val_loss: 36.6490\n",
            "Epoch 271/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.6557 - val_loss: 38.9399\n",
            "Epoch 272/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6120 - val_loss: 41.2806\n",
            "Epoch 273/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5973 - val_loss: 41.5769\n",
            "Epoch 274/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6009 - val_loss: 43.1327\n",
            "Epoch 275/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6422 - val_loss: 40.9368\n",
            "Epoch 276/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6058 - val_loss: 37.3701\n",
            "Epoch 277/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5999 - val_loss: 42.8346\n",
            "Epoch 278/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6435 - val_loss: 37.7136\n",
            "Epoch 279/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6201 - val_loss: 44.0644\n",
            "Epoch 280/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5410 - val_loss: 38.1783\n",
            "Epoch 281/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4558 - val_loss: 39.6057\n",
            "Epoch 282/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5402 - val_loss: 36.0962\n",
            "Epoch 283/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5492 - val_loss: 36.4046\n",
            "Epoch 284/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5081 - val_loss: 40.0380\n",
            "Epoch 285/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5516 - val_loss: 37.7414\n",
            "Epoch 286/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5309 - val_loss: 37.6265\n",
            "Epoch 287/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5850 - val_loss: 46.8947\n",
            "Epoch 288/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5509 - val_loss: 50.8469\n",
            "Epoch 289/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5335 - val_loss: 48.0483\n",
            "Epoch 290/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4942 - val_loss: 39.0497\n",
            "Epoch 291/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5172 - val_loss: 43.3340\n",
            "Epoch 292/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5648 - val_loss: 40.0813\n",
            "Epoch 293/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5233 - val_loss: 48.0866\n",
            "Epoch 294/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5795 - val_loss: 39.6254\n",
            "Epoch 295/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5091 - val_loss: 42.9057\n",
            "Epoch 296/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5321 - val_loss: 38.2949\n",
            "Epoch 297/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5331 - val_loss: 38.0695\n",
            "Epoch 298/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4356 - val_loss: 49.5021\n",
            "Epoch 299/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5391 - val_loss: 37.4756\n",
            "Epoch 300/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4900 - val_loss: 37.2398\n",
            "Epoch 301/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4693 - val_loss: 41.4527\n",
            "Epoch 302/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4410 - val_loss: 39.7450\n",
            "Epoch 303/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4193 - val_loss: 40.2141\n",
            "Epoch 304/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4031 - val_loss: 40.5819\n",
            "Epoch 305/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4149 - val_loss: 38.8290\n",
            "Epoch 306/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4774 - val_loss: 37.3329\n",
            "Epoch 307/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4598 - val_loss: 40.1809\n",
            "Epoch 308/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4358 - val_loss: 36.3015\n",
            "Epoch 309/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4269 - val_loss: 39.7870\n",
            "Epoch 310/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4118 - val_loss: 52.8532\n",
            "Epoch 311/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5379 - val_loss: 37.7495\n",
            "Epoch 312/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3399 - val_loss: 46.5108\n",
            "Epoch 313/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4144 - val_loss: 36.6051\n",
            "Epoch 314/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3858 - val_loss: 41.3834\n",
            "Epoch 315/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3902 - val_loss: 43.0513\n",
            "Epoch 316/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3405 - val_loss: 38.8161\n",
            "Epoch 317/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4005 - val_loss: 36.5371\n",
            "Epoch 318/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4119 - val_loss: 39.5978\n",
            "Epoch 319/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3535 - val_loss: 38.5518\n",
            "Epoch 320/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3485 - val_loss: 35.8831\n",
            "Epoch 321/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3812 - val_loss: 41.6979\n",
            "Epoch 322/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3887 - val_loss: 43.2691\n",
            "Epoch 323/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2615 - val_loss: 38.9800\n",
            "Epoch 324/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3414 - val_loss: 38.3016\n",
            "Epoch 325/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3209 - val_loss: 37.1183\n",
            "Epoch 326/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2828 - val_loss: 45.3948\n",
            "Epoch 327/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3293 - val_loss: 37.7956\n",
            "Epoch 328/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2610 - val_loss: 49.9183\n",
            "Epoch 329/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2846 - val_loss: 39.4237\n",
            "Epoch 330/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3641 - val_loss: 43.6578\n",
            "Epoch 331/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3896 - val_loss: 40.7548\n",
            "Epoch 332/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3995 - val_loss: 42.8253\n",
            "Epoch 333/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3472 - val_loss: 44.8959\n",
            "Epoch 334/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3848 - val_loss: 39.0957\n",
            "Epoch 335/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4026 - val_loss: 38.3558\n",
            "Epoch 336/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3415 - val_loss: 39.7880\n",
            "Epoch 337/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2828 - val_loss: 40.2507\n",
            "Epoch 338/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3697 - val_loss: 39.5990\n",
            "Epoch 339/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2715 - val_loss: 38.2025\n",
            "Epoch 340/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2482 - val_loss: 36.7697\n",
            "Epoch 341/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2596 - val_loss: 40.5737\n",
            "Epoch 342/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1859 - val_loss: 37.8741\n",
            "Epoch 343/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1706 - val_loss: 37.6266\n",
            "Epoch 344/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2243 - val_loss: 37.0609\n",
            "Epoch 345/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2903 - val_loss: 40.7094\n",
            "Epoch 346/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1760 - val_loss: 37.4420\n",
            "Epoch 347/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1451 - val_loss: 36.5218\n",
            "Epoch 348/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1387 - val_loss: 37.8232\n",
            "Epoch 349/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1490 - val_loss: 38.6410\n",
            "Epoch 350/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1686 - val_loss: 37.5024\n",
            "Epoch 351/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0717 - val_loss: 40.3761\n",
            "Epoch 352/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1081 - val_loss: 45.8169\n",
            "Epoch 353/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1410 - val_loss: 36.6242\n",
            "Epoch 354/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0906 - val_loss: 38.0736\n",
            "Epoch 355/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0551 - val_loss: 39.9460\n",
            "Epoch 356/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0632 - val_loss: 36.4883\n",
            "Epoch 357/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0753 - val_loss: 43.8362\n",
            "Epoch 358/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9837 - val_loss: 38.5729\n",
            "Epoch 359/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0177 - val_loss: 44.8718\n",
            "Epoch 360/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9685 - val_loss: 48.7535\n",
            "Epoch 361/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9596 - val_loss: 35.8352\n",
            "Epoch 362/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9752 - val_loss: 38.3149\n",
            "Epoch 363/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9988 - val_loss: 41.2652\n",
            "Epoch 364/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9235 - val_loss: 40.5599\n",
            "Epoch 365/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0050 - val_loss: 38.8582\n",
            "Epoch 366/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9481 - val_loss: 39.1385\n",
            "Epoch 367/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9154 - val_loss: 39.2027\n",
            "Epoch 368/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8869 - val_loss: 37.5643\n",
            "Epoch 369/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9472 - val_loss: 36.4432\n",
            "Epoch 370/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8554 - val_loss: 35.9944\n",
            "Epoch 371/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9418 - val_loss: 41.5823\n",
            "Epoch 372/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8869 - val_loss: 41.2100\n",
            "Epoch 373/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8647 - val_loss: 38.7229\n",
            "Epoch 374/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9008 - val_loss: 44.3801\n",
            "Epoch 375/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9179 - val_loss: 35.9721\n",
            "Epoch 376/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8640 - val_loss: 37.0841\n",
            "Epoch 377/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8182 - val_loss: 35.8907\n",
            "Epoch 378/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9017 - val_loss: 36.4065\n",
            "Epoch 379/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8882 - val_loss: 41.3736\n",
            "Epoch 380/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8703 - val_loss: 35.8026\n",
            "Epoch 381/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8285 - val_loss: 42.7950\n",
            "Epoch 382/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8363 - val_loss: 36.6976\n",
            "Epoch 383/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8383 - val_loss: 50.8494\n",
            "Epoch 384/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8068 - val_loss: 49.1316\n",
            "Epoch 385/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7883 - val_loss: 42.6686\n",
            "Epoch 386/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8565 - val_loss: 36.5055\n",
            "Epoch 387/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8131 - val_loss: 48.5809\n",
            "Epoch 388/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7994 - val_loss: 38.7920\n",
            "Epoch 389/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7684 - val_loss: 41.1384\n",
            "Epoch 390/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7554 - val_loss: 36.7472\n",
            "Epoch 391/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 31.9221 - val_loss: 42.8393\n",
            "Epoch 392/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 31.7447 - val_loss: 46.0057\n",
            "Epoch 393/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8252 - val_loss: 38.5684\n",
            "Epoch 394/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7698 - val_loss: 36.0121\n",
            "Epoch 395/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7804 - val_loss: 37.5978\n",
            "Epoch 396/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7827 - val_loss: 36.7723\n",
            "Epoch 397/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8107 - val_loss: 45.5110\n",
            "Epoch 398/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7883 - val_loss: 36.1915\n",
            "Epoch 399/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8169 - val_loss: 36.9167\n",
            "Epoch 400/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7491 - val_loss: 38.6007\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e12dcb0d-167c-4403-c133-19fce2e00df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -1.329629969181517 \n",
            "MAE:  4.684796549654936 \n",
            "SD:  6.068999256845191\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cip38xZOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "54e5af52-628b-4fa9-852f-14160813b088"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e7800a-d516-494b-fdb2-0414e93ab764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_20 (Dense)            (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,185\n",
            "Trainable params: 1,145\n",
            "Non-trainable params: 40\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "741b0273-b1e3-4bae-8437-d0f8aaa4e46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 3693.4041 - val_loss: 3717.3682\n",
            "Epoch 2/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3537.9231 - val_loss: 3572.2598\n",
            "Epoch 3/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3363.5559 - val_loss: 3310.8743\n",
            "Epoch 4/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3158.3320 - val_loss: 3096.2217\n",
            "Epoch 5/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2919.6489 - val_loss: 2928.6677\n",
            "Epoch 6/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2650.9810 - val_loss: 2479.7222\n",
            "Epoch 7/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2358.1323 - val_loss: 1924.9749\n",
            "Epoch 8/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2058.6252 - val_loss: 1848.4935\n",
            "Epoch 9/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1770.7562 - val_loss: 1746.8643\n",
            "Epoch 10/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1496.1085 - val_loss: 1597.2845\n",
            "Epoch 11/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1239.4573 - val_loss: 1352.2515\n",
            "Epoch 12/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1005.0286 - val_loss: 640.0618\n",
            "Epoch 13/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 798.6013 - val_loss: 926.1422\n",
            "Epoch 14/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 618.1884 - val_loss: 517.0792\n",
            "Epoch 15/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 468.2659 - val_loss: 670.4166\n",
            "Epoch 16/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 346.5030 - val_loss: 350.5293\n",
            "Epoch 17/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 251.3444 - val_loss: 137.7096\n",
            "Epoch 18/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 180.0884 - val_loss: 216.1286\n",
            "Epoch 19/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 128.7653 - val_loss: 96.0884\n",
            "Epoch 20/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.1560 - val_loss: 105.2061\n",
            "Epoch 21/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9837 - val_loss: 104.1647\n",
            "Epoch 22/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 58.5813 - val_loss: 73.8061\n",
            "Epoch 23/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 50.7250 - val_loss: 100.5902\n",
            "Epoch 24/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 45.7895 - val_loss: 55.4318\n",
            "Epoch 25/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 43.8388 - val_loss: 66.8487\n",
            "Epoch 26/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 42.3898 - val_loss: 281.6251\n",
            "Epoch 27/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 41.4313 - val_loss: 52.6680\n",
            "Epoch 28/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.4986 - val_loss: 60.9146\n",
            "Epoch 29/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.1374 - val_loss: 100.4134\n",
            "Epoch 30/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.4862 - val_loss: 142.3959\n",
            "Epoch 31/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.7455 - val_loss: 70.7652\n",
            "Epoch 32/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.4899 - val_loss: 62.2727\n",
            "Epoch 33/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.3638 - val_loss: 60.4242\n",
            "Epoch 34/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.9254 - val_loss: 43.9063\n",
            "Epoch 35/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.7329 - val_loss: 48.0072\n",
            "Epoch 36/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.8341 - val_loss: 73.6074\n",
            "Epoch 37/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.6357 - val_loss: 90.2166\n",
            "Epoch 38/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.1033 - val_loss: 78.8406\n",
            "Epoch 39/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 40.0420 - val_loss: 102.4927\n",
            "Epoch 40/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.8075 - val_loss: 75.6594\n",
            "Epoch 41/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 39.1469 - val_loss: 63.2384\n",
            "Epoch 42/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.5071 - val_loss: 137.2003\n",
            "Epoch 43/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.0283 - val_loss: 78.1491\n",
            "Epoch 44/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.8036 - val_loss: 49.5249\n",
            "Epoch 45/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.7849 - val_loss: 77.2746\n",
            "Epoch 46/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.5963 - val_loss: 56.8507\n",
            "Epoch 47/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.3706 - val_loss: 68.2943\n",
            "Epoch 48/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.6872 - val_loss: 208.9976\n",
            "Epoch 49/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.0303 - val_loss: 59.0402\n",
            "Epoch 50/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.7077 - val_loss: 43.8368\n",
            "Epoch 51/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.2891 - val_loss: 50.5556\n",
            "Epoch 52/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.0607 - val_loss: 50.7642\n",
            "Epoch 53/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.6715 - val_loss: 41.1014\n",
            "Epoch 54/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.7524 - val_loss: 120.9766\n",
            "Epoch 55/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.6082 - val_loss: 43.8979\n",
            "Epoch 56/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.4663 - val_loss: 95.7567\n",
            "Epoch 57/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.4674 - val_loss: 42.6852\n",
            "Epoch 58/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 37.4997 - val_loss: 40.3032\n",
            "Epoch 59/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3039 - val_loss: 86.1412\n",
            "Epoch 60/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.4122 - val_loss: 52.1292\n",
            "Epoch 61/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5661 - val_loss: 58.7381\n",
            "Epoch 62/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1754 - val_loss: 65.8445\n",
            "Epoch 63/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1013 - val_loss: 55.8475\n",
            "Epoch 64/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.1201 - val_loss: 49.5449\n",
            "Epoch 65/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0750 - val_loss: 51.9788\n",
            "Epoch 66/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.0293 - val_loss: 44.4548\n",
            "Epoch 67/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9466 - val_loss: 43.7082\n",
            "Epoch 68/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8011 - val_loss: 63.5279\n",
            "Epoch 69/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8971 - val_loss: 42.4070\n",
            "Epoch 70/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6580 - val_loss: 72.6426\n",
            "Epoch 71/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.5909 - val_loss: 67.1389\n",
            "Epoch 72/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4798 - val_loss: 43.1167\n",
            "Epoch 73/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.4800 - val_loss: 48.0197\n",
            "Epoch 74/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3690 - val_loss: 51.3698\n",
            "Epoch 75/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.2907 - val_loss: 67.0275\n",
            "Epoch 76/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.3609 - val_loss: 55.1835\n",
            "Epoch 77/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.2671 - val_loss: 41.3590\n",
            "Epoch 78/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.1275 - val_loss: 39.1918\n",
            "Epoch 79/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.9892 - val_loss: 49.1515\n",
            "Epoch 80/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.9790 - val_loss: 52.0450\n",
            "Epoch 81/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.1477 - val_loss: 38.8140\n",
            "Epoch 82/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.8841 - val_loss: 40.8452\n",
            "Epoch 83/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.9442 - val_loss: 59.7214\n",
            "Epoch 84/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7730 - val_loss: 46.0783\n",
            "Epoch 85/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7054 - val_loss: 42.7756\n",
            "Epoch 86/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5439 - val_loss: 42.4749\n",
            "Epoch 87/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5310 - val_loss: 41.7939\n",
            "Epoch 88/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.5727 - val_loss: 37.6821\n",
            "Epoch 89/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4942 - val_loss: 40.2257\n",
            "Epoch 90/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2931 - val_loss: 65.2478\n",
            "Epoch 91/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.3142 - val_loss: 48.9260\n",
            "Epoch 92/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2953 - val_loss: 46.2685\n",
            "Epoch 93/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.2205 - val_loss: 38.2133\n",
            "Epoch 94/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.1259 - val_loss: 37.8065\n",
            "Epoch 95/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.2131 - val_loss: 68.9683\n",
            "Epoch 96/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0926 - val_loss: 51.9085\n",
            "Epoch 97/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.0672 - val_loss: 61.3145\n",
            "Epoch 98/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9715 - val_loss: 43.8338\n",
            "Epoch 99/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9415 - val_loss: 45.8248\n",
            "Epoch 100/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9005 - val_loss: 65.7826\n",
            "Epoch 101/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8106 - val_loss: 41.4243\n",
            "Epoch 102/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.7879 - val_loss: 53.9603\n",
            "Epoch 103/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6876 - val_loss: 48.3572\n",
            "Epoch 104/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6739 - val_loss: 50.6530\n",
            "Epoch 105/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5844 - val_loss: 38.4837\n",
            "Epoch 106/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5358 - val_loss: 38.6403\n",
            "Epoch 107/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.4960 - val_loss: 42.6502\n",
            "Epoch 108/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4482 - val_loss: 42.0760\n",
            "Epoch 109/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.3646 - val_loss: 67.3302\n",
            "Epoch 110/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.2836 - val_loss: 39.5119\n",
            "Epoch 111/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2513 - val_loss: 38.0878\n",
            "Epoch 112/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.1382 - val_loss: 45.1671\n",
            "Epoch 113/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0886 - val_loss: 39.4472\n",
            "Epoch 114/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9690 - val_loss: 41.3755\n",
            "Epoch 115/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9193 - val_loss: 39.5350\n",
            "Epoch 116/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8849 - val_loss: 40.9823\n",
            "Epoch 117/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9008 - val_loss: 41.3182\n",
            "Epoch 118/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7920 - val_loss: 36.6748\n",
            "Epoch 119/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7529 - val_loss: 35.9577\n",
            "Epoch 120/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.6477 - val_loss: 41.8322\n",
            "Epoch 121/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6433 - val_loss: 44.9129\n",
            "Epoch 122/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6377 - val_loss: 39.1684\n",
            "Epoch 123/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6026 - val_loss: 43.1101\n",
            "Epoch 124/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.5487 - val_loss: 37.6677\n",
            "Epoch 125/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5184 - val_loss: 38.6420\n",
            "Epoch 126/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5118 - val_loss: 36.2420\n",
            "Epoch 127/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.4059 - val_loss: 41.9900\n",
            "Epoch 128/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3985 - val_loss: 43.0171\n",
            "Epoch 129/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2923 - val_loss: 36.4165\n",
            "Epoch 130/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3067 - val_loss: 37.3347\n",
            "Epoch 131/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2543 - val_loss: 39.7411\n",
            "Epoch 132/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.2284 - val_loss: 39.9729\n",
            "Epoch 133/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2331 - val_loss: 37.4937\n",
            "Epoch 134/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.2119 - val_loss: 36.6312\n",
            "Epoch 135/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1472 - val_loss: 37.3869\n",
            "Epoch 136/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.1842 - val_loss: 36.4355\n",
            "Epoch 137/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.1253 - val_loss: 45.6124\n",
            "Epoch 138/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1846 - val_loss: 39.5804\n",
            "Epoch 139/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.1132 - val_loss: 36.5401\n",
            "Epoch 140/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0576 - val_loss: 39.7011\n",
            "Epoch 141/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.0114 - val_loss: 38.9319\n",
            "Epoch 142/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9853 - val_loss: 37.7625\n",
            "Epoch 143/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9930 - val_loss: 39.6822\n",
            "Epoch 144/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9411 - val_loss: 40.1017\n",
            "Epoch 145/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9782 - val_loss: 38.4089\n",
            "Epoch 146/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9299 - val_loss: 38.4154\n",
            "Epoch 147/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8944 - val_loss: 43.3690\n",
            "Epoch 148/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8684 - val_loss: 44.7237\n",
            "Epoch 149/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9244 - val_loss: 39.4187\n",
            "Epoch 150/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8689 - val_loss: 41.4350\n",
            "Epoch 151/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7718 - val_loss: 37.0179\n",
            "Epoch 152/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8013 - val_loss: 37.1863\n",
            "Epoch 153/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7582 - val_loss: 37.9902\n",
            "Epoch 154/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7327 - val_loss: 37.2985\n",
            "Epoch 155/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7282 - val_loss: 40.3837\n",
            "Epoch 156/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6978 - val_loss: 40.3733\n",
            "Epoch 157/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6850 - val_loss: 41.6440\n",
            "Epoch 158/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7673 - val_loss: 37.6742\n",
            "Epoch 159/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7255 - val_loss: 39.5671\n",
            "Epoch 160/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6401 - val_loss: 37.3095\n",
            "Epoch 161/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6256 - val_loss: 37.1865\n",
            "Epoch 162/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5700 - val_loss: 38.3069\n",
            "Epoch 163/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5574 - val_loss: 43.3545\n",
            "Epoch 164/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5788 - val_loss: 42.3437\n",
            "Epoch 165/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5361 - val_loss: 40.9843\n",
            "Epoch 166/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5233 - val_loss: 37.6721\n",
            "Epoch 167/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5312 - val_loss: 38.7766\n",
            "Epoch 168/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4569 - val_loss: 38.4117\n",
            "Epoch 169/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4771 - val_loss: 36.2637\n",
            "Epoch 170/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4608 - val_loss: 38.7372\n",
            "Epoch 171/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 32.4923 - val_loss: 36.1146\n",
            "Epoch 172/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 32.3854 - val_loss: 36.5444\n",
            "Epoch 173/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4142 - val_loss: 37.5646\n",
            "Epoch 174/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4065 - val_loss: 46.6559\n",
            "Epoch 175/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4267 - val_loss: 38.7096\n",
            "Epoch 176/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2873 - val_loss: 35.4130\n",
            "Epoch 177/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3342 - val_loss: 39.6512\n",
            "Epoch 178/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2203 - val_loss: 37.5023\n",
            "Epoch 179/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2204 - val_loss: 39.5915\n",
            "Epoch 180/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1937 - val_loss: 36.4251\n",
            "Epoch 181/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1663 - val_loss: 38.9469\n",
            "Epoch 182/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0943 - val_loss: 35.8812\n",
            "Epoch 183/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0875 - val_loss: 39.3247\n",
            "Epoch 184/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0209 - val_loss: 41.3196\n",
            "Epoch 185/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9569 - val_loss: 40.2860\n",
            "Epoch 186/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8983 - val_loss: 36.6666\n",
            "Epoch 187/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9393 - val_loss: 38.8458\n",
            "Epoch 188/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8229 - val_loss: 36.6967\n",
            "Epoch 189/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8438 - val_loss: 38.6475\n",
            "Epoch 190/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7838 - val_loss: 37.5421\n",
            "Epoch 191/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7801 - val_loss: 37.2597\n",
            "Epoch 192/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7000 - val_loss: 36.3252\n",
            "Epoch 193/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7798 - val_loss: 37.5459\n",
            "Epoch 194/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7277 - val_loss: 38.0603\n",
            "Epoch 195/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6819 - val_loss: 36.1270\n",
            "Epoch 196/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.6784 - val_loss: 38.1013\n",
            "Epoch 197/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6256 - val_loss: 38.0351\n",
            "Epoch 198/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5945 - val_loss: 37.8971\n",
            "Epoch 199/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5954 - val_loss: 36.9821\n",
            "Epoch 200/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.5517 - val_loss: 36.2859\n",
            "Epoch 201/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.5685 - val_loss: 37.6118\n",
            "Epoch 202/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5378 - val_loss: 40.4387\n",
            "Epoch 203/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.6193 - val_loss: 36.9600\n",
            "Epoch 204/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5268 - val_loss: 37.4584\n",
            "Epoch 205/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.5278 - val_loss: 42.4045\n",
            "Epoch 206/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.4824 - val_loss: 35.5921\n",
            "Epoch 207/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.4741 - val_loss: 39.1876\n",
            "Epoch 208/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4799 - val_loss: 35.7194\n",
            "Epoch 209/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.4272 - val_loss: 35.3985\n",
            "Epoch 210/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.4724 - val_loss: 37.5534\n",
            "Epoch 211/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4527 - val_loss: 37.4117\n",
            "Epoch 212/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.3502 - val_loss: 35.4809\n",
            "Epoch 213/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.3985 - val_loss: 34.7888\n",
            "Epoch 214/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4045 - val_loss: 37.2925\n",
            "Epoch 215/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3989 - val_loss: 53.8081\n",
            "Epoch 216/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4102 - val_loss: 36.2881\n",
            "Epoch 217/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3167 - val_loss: 36.1301\n",
            "Epoch 218/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.3656 - val_loss: 42.9227\n",
            "Epoch 219/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.3420 - val_loss: 35.6320\n",
            "Epoch 220/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3311 - val_loss: 38.9457\n",
            "Epoch 221/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3113 - val_loss: 37.8557\n",
            "Epoch 222/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.3286 - val_loss: 35.8844\n",
            "Epoch 223/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2892 - val_loss: 36.8796\n",
            "Epoch 224/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2906 - val_loss: 40.1776\n",
            "Epoch 225/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3047 - val_loss: 36.0647\n",
            "Epoch 226/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2648 - val_loss: 35.6625\n",
            "Epoch 227/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2769 - val_loss: 36.9936\n",
            "Epoch 228/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2603 - val_loss: 35.8705\n",
            "Epoch 229/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2611 - val_loss: 38.1385\n",
            "Epoch 230/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2835 - val_loss: 35.1613\n",
            "Epoch 231/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2619 - val_loss: 41.8679\n",
            "Epoch 232/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2823 - val_loss: 39.5222\n",
            "Epoch 233/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2032 - val_loss: 36.7380\n",
            "Epoch 234/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2098 - val_loss: 34.9281\n",
            "Epoch 235/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2415 - val_loss: 40.0445\n",
            "Epoch 236/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1890 - val_loss: 35.2220\n",
            "Epoch 237/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1496 - val_loss: 39.7753\n",
            "Epoch 238/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2220 - val_loss: 42.2755\n",
            "Epoch 239/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1679 - val_loss: 37.3986\n",
            "Epoch 240/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1535 - val_loss: 37.1294\n",
            "Epoch 241/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.2328 - val_loss: 37.3467\n",
            "Epoch 242/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1277 - val_loss: 39.2334\n",
            "Epoch 243/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1673 - val_loss: 43.6309\n",
            "Epoch 244/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1328 - val_loss: 40.6123\n",
            "Epoch 245/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1392 - val_loss: 38.6990\n",
            "Epoch 246/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1570 - val_loss: 38.2486\n",
            "Epoch 247/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1184 - val_loss: 40.2094\n",
            "Epoch 248/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0724 - val_loss: 38.3415\n",
            "Epoch 249/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.0959 - val_loss: 36.1989\n",
            "Epoch 250/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.1054 - val_loss: 35.8427\n",
            "Epoch 251/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0594 - val_loss: 43.5410\n",
            "Epoch 252/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.0834 - val_loss: 35.8800\n",
            "Epoch 253/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0417 - val_loss: 37.9853\n",
            "Epoch 254/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.0354 - val_loss: 36.2551\n",
            "Epoch 255/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.0301 - val_loss: 42.7494\n",
            "Epoch 256/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0246 - val_loss: 40.5023\n",
            "Epoch 257/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0249 - val_loss: 34.8549\n",
            "Epoch 258/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0376 - val_loss: 39.0566\n",
            "Epoch 259/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0514 - val_loss: 35.3581\n",
            "Epoch 260/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.0029 - val_loss: 35.8846\n",
            "Epoch 261/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9863 - val_loss: 34.7992\n",
            "Epoch 262/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9622 - val_loss: 36.4455\n",
            "Epoch 263/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0104 - val_loss: 37.3961\n",
            "Epoch 264/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0042 - val_loss: 40.2905\n",
            "Epoch 265/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.0023 - val_loss: 38.4212\n",
            "Epoch 266/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9643 - val_loss: 37.8652\n",
            "Epoch 267/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.9264 - val_loss: 38.1256\n",
            "Epoch 268/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.9815 - val_loss: 34.8907\n",
            "Epoch 269/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9472 - val_loss: 35.2069\n",
            "Epoch 270/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9395 - val_loss: 39.3615\n",
            "Epoch 271/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0033 - val_loss: 37.0524\n",
            "Epoch 272/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9784 - val_loss: 37.8911\n",
            "Epoch 273/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9292 - val_loss: 35.3456\n",
            "Epoch 274/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8509 - val_loss: 37.2220\n",
            "Epoch 275/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8984 - val_loss: 35.2059\n",
            "Epoch 276/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8767 - val_loss: 52.9252\n",
            "Epoch 277/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8882 - val_loss: 39.8309\n",
            "Epoch 278/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8019 - val_loss: 36.9880\n",
            "Epoch 279/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8576 - val_loss: 37.3846\n",
            "Epoch 280/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8636 - val_loss: 35.9587\n",
            "Epoch 281/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8682 - val_loss: 35.6788\n",
            "Epoch 282/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8751 - val_loss: 37.1040\n",
            "Epoch 283/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8508 - val_loss: 36.1118\n",
            "Epoch 284/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8208 - val_loss: 36.0052\n",
            "Epoch 285/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8339 - val_loss: 43.8389\n",
            "Epoch 286/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8399 - val_loss: 36.2057\n",
            "Epoch 287/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8365 - val_loss: 34.6574\n",
            "Epoch 288/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7498 - val_loss: 35.8829\n",
            "Epoch 289/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8130 - val_loss: 39.2465\n",
            "Epoch 290/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8465 - val_loss: 37.6965\n",
            "Epoch 291/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7712 - val_loss: 35.4418\n",
            "Epoch 292/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7856 - val_loss: 38.3947\n",
            "Epoch 293/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7313 - val_loss: 35.7132\n",
            "Epoch 294/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7775 - val_loss: 36.2619\n",
            "Epoch 295/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7811 - val_loss: 42.3999\n",
            "Epoch 296/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7906 - val_loss: 41.2319\n",
            "Epoch 297/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7385 - val_loss: 39.4438\n",
            "Epoch 298/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7467 - val_loss: 35.2262\n",
            "Epoch 299/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7699 - val_loss: 43.3615\n",
            "Epoch 300/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7147 - val_loss: 40.6720\n",
            "Epoch 301/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7386 - val_loss: 35.3142\n",
            "Epoch 302/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7285 - val_loss: 40.1475\n",
            "Epoch 303/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7023 - val_loss: 36.4431\n",
            "Epoch 304/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7505 - val_loss: 42.8946\n",
            "Epoch 305/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7718 - val_loss: 42.9605\n",
            "Epoch 306/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7366 - val_loss: 35.7866\n",
            "Epoch 307/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6769 - val_loss: 37.4161\n",
            "Epoch 308/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6858 - val_loss: 40.4165\n",
            "Epoch 309/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6712 - val_loss: 36.3798\n",
            "Epoch 310/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6829 - val_loss: 42.8224\n",
            "Epoch 311/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7540 - val_loss: 35.8691\n",
            "Epoch 312/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7248 - val_loss: 35.8710\n",
            "Epoch 313/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6340 - val_loss: 39.4906\n",
            "Epoch 314/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7064 - val_loss: 34.5197\n",
            "Epoch 315/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6785 - val_loss: 36.5205\n",
            "Epoch 316/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7294 - val_loss: 35.9370\n",
            "Epoch 317/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6125 - val_loss: 39.0329\n",
            "Epoch 318/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6861 - val_loss: 35.6894\n",
            "Epoch 319/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6624 - val_loss: 37.9095\n",
            "Epoch 320/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6424 - val_loss: 37.5376\n",
            "Epoch 321/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.5966 - val_loss: 36.7116\n",
            "Epoch 322/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6352 - val_loss: 36.0353\n",
            "Epoch 323/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6136 - val_loss: 37.7261\n",
            "Epoch 324/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6191 - val_loss: 34.9087\n",
            "Epoch 325/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6322 - val_loss: 36.9446\n",
            "Epoch 326/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.5927 - val_loss: 35.3677\n",
            "Epoch 327/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.5977 - val_loss: 36.5246\n",
            "Epoch 328/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.5649 - val_loss: 37.0266\n",
            "Epoch 329/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6471 - val_loss: 40.8040\n",
            "Epoch 330/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.5923 - val_loss: 34.9852\n",
            "Epoch 331/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6136 - val_loss: 40.0973\n",
            "Epoch 332/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.5639 - val_loss: 36.7776\n",
            "Epoch 333/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5782 - val_loss: 36.0312\n",
            "Epoch 334/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5428 - val_loss: 34.7324\n",
            "Epoch 335/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5471 - val_loss: 35.2550\n",
            "Epoch 336/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5005 - val_loss: 40.6834\n",
            "Epoch 337/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.5520 - val_loss: 38.0180\n",
            "Epoch 338/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5167 - val_loss: 36.9123\n",
            "Epoch 339/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4774 - val_loss: 35.6793\n",
            "Epoch 340/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5025 - val_loss: 36.1015\n",
            "Epoch 341/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.5451 - val_loss: 38.6111\n",
            "Epoch 342/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4914 - val_loss: 41.4847\n",
            "Epoch 343/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4846 - val_loss: 35.5322\n",
            "Epoch 344/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4827 - val_loss: 36.5871\n",
            "Epoch 345/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.4983 - val_loss: 36.2786\n",
            "Epoch 346/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5223 - val_loss: 34.6841\n",
            "Epoch 347/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4488 - val_loss: 42.4356\n",
            "Epoch 348/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4664 - val_loss: 36.3968\n",
            "Epoch 349/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.5151 - val_loss: 44.5789\n",
            "Epoch 350/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4899 - val_loss: 35.5669\n",
            "Epoch 351/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4860 - val_loss: 36.7226\n",
            "Epoch 352/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4365 - val_loss: 44.5829\n",
            "Epoch 353/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4357 - val_loss: 37.1536\n",
            "Epoch 354/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4792 - val_loss: 36.6119\n",
            "Epoch 355/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.4179 - val_loss: 37.8639\n",
            "Epoch 356/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4162 - val_loss: 36.1246\n",
            "Epoch 357/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3955 - val_loss: 38.2706\n",
            "Epoch 358/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4188 - val_loss: 38.2913\n",
            "Epoch 359/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4063 - val_loss: 37.4999\n",
            "Epoch 360/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4007 - val_loss: 43.9194\n",
            "Epoch 361/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.4150 - val_loss: 34.8057\n",
            "Epoch 362/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.3380 - val_loss: 34.6907\n",
            "Epoch 363/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.3929 - val_loss: 41.5720\n",
            "Epoch 364/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.3249 - val_loss: 35.9012\n",
            "Epoch 365/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.3114 - val_loss: 34.3949\n",
            "Epoch 366/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.3344 - val_loss: 35.2511\n",
            "Epoch 367/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.3421 - val_loss: 43.5507\n",
            "Epoch 368/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.3173 - val_loss: 34.3147\n",
            "Epoch 369/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2884 - val_loss: 35.7063\n",
            "Epoch 370/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2724 - val_loss: 37.9901\n",
            "Epoch 371/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2800 - val_loss: 36.1244\n",
            "Epoch 372/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2913 - val_loss: 40.2395\n",
            "Epoch 373/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2526 - val_loss: 36.3634\n",
            "Epoch 374/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2619 - val_loss: 35.2589\n",
            "Epoch 375/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2679 - val_loss: 36.0739\n",
            "Epoch 376/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2383 - val_loss: 34.1949\n",
            "Epoch 377/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2579 - val_loss: 36.0896\n",
            "Epoch 378/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2992 - val_loss: 35.4667\n",
            "Epoch 379/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2026 - val_loss: 42.9636\n",
            "Epoch 380/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2692 - val_loss: 35.2296\n",
            "Epoch 381/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1935 - val_loss: 38.0853\n",
            "Epoch 382/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1907 - val_loss: 38.0657\n",
            "Epoch 383/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1895 - val_loss: 35.2466\n",
            "Epoch 384/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2118 - val_loss: 36.7704\n",
            "Epoch 385/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1997 - val_loss: 37.8006\n",
            "Epoch 386/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1718 - val_loss: 39.7224\n",
            "Epoch 387/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1705 - val_loss: 35.4309\n",
            "Epoch 388/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1302 - val_loss: 34.9155\n",
            "Epoch 389/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1829 - val_loss: 35.9701\n",
            "Epoch 390/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1864 - val_loss: 38.5586\n",
            "Epoch 391/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1508 - val_loss: 35.6652\n",
            "Epoch 392/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1272 - val_loss: 39.8595\n",
            "Epoch 393/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1068 - val_loss: 39.2239\n",
            "Epoch 394/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.0885 - val_loss: 38.1471\n",
            "Epoch 395/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.0887 - val_loss: 34.8644\n",
            "Epoch 396/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1263 - val_loss: 35.9884\n",
            "Epoch 397/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.0007 - val_loss: 34.3417\n",
            "Epoch 398/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.0514 - val_loss: 36.9422\n",
            "Epoch 399/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.0436 - val_loss: 34.9586\n",
            "Epoch 400/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0215 - val_loss: 35.9018\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefcabc6-b9f4-4c0f-8882-8d5f08c97016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.34238954478731165 \n",
            "MAE:  4.43402256373717 \n",
            "SD:  5.982016765219863\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "12bae9c2-f1b5-43d6-dadc-e233b8089262"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(8, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1fcf92-f9ed-45b6-d670-56fef747340a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_25 (Dense)            (None, 8)                 1024      \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,185\n",
            "Trainable params: 1,145\n",
            "Non-trainable params: 40\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7dc35ca-ff0a-44b6-f299-83dc835af58a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "165/165 [==============================] - 2s 7ms/step - loss: 3768.7017 - val_loss: 3724.4385\n",
            "Epoch 2/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 3646.3057 - val_loss: 3652.7288\n",
            "Epoch 3/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3532.5359 - val_loss: 3404.9629\n",
            "Epoch 4/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3399.4983 - val_loss: 3269.3289\n",
            "Epoch 5/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3251.3606 - val_loss: 3168.5120\n",
            "Epoch 6/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 3090.4431 - val_loss: 3212.9858\n",
            "Epoch 7/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2884.2986 - val_loss: 2624.1692\n",
            "Epoch 8/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2644.9990 - val_loss: 2765.2004\n",
            "Epoch 9/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2419.8347 - val_loss: 2362.1265\n",
            "Epoch 10/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2198.9119 - val_loss: 1984.5277\n",
            "Epoch 11/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1982.4230 - val_loss: 2000.3973\n",
            "Epoch 12/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1774.2090 - val_loss: 1303.7645\n",
            "Epoch 13/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1574.3751 - val_loss: 1682.1781\n",
            "Epoch 14/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1385.6450 - val_loss: 1564.3077\n",
            "Epoch 15/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1206.5592 - val_loss: 607.7720\n",
            "Epoch 16/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1042.4521 - val_loss: 785.4406\n",
            "Epoch 17/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 890.1010 - val_loss: 744.9360\n",
            "Epoch 18/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 747.7208 - val_loss: 1782.0640\n",
            "Epoch 19/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 576.4728 - val_loss: 757.9544\n",
            "Epoch 20/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 454.9309 - val_loss: 701.6744\n",
            "Epoch 21/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 357.6186 - val_loss: 503.7888\n",
            "Epoch 22/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 277.7262 - val_loss: 318.5110\n",
            "Epoch 23/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 213.7585 - val_loss: 275.5748\n",
            "Epoch 24/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 163.5685 - val_loss: 211.3033\n",
            "Epoch 25/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 125.0917 - val_loss: 99.7970\n",
            "Epoch 26/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 96.2686 - val_loss: 137.2112\n",
            "Epoch 27/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.7186 - val_loss: 114.7931\n",
            "Epoch 28/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 61.5693 - val_loss: 81.5480\n",
            "Epoch 29/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 51.6304 - val_loss: 51.7065\n",
            "Epoch 30/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 45.8394 - val_loss: 68.9327\n",
            "Epoch 31/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 41.9418 - val_loss: 65.1288\n",
            "Epoch 32/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 39.9806 - val_loss: 51.7254\n",
            "Epoch 33/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 38.6958 - val_loss: 58.8075\n",
            "Epoch 34/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 38.0363 - val_loss: 44.8633\n",
            "Epoch 35/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5966 - val_loss: 74.2185\n",
            "Epoch 36/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.2866 - val_loss: 47.1072\n",
            "Epoch 37/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.2876 - val_loss: 45.5168\n",
            "Epoch 38/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9548 - val_loss: 65.5046\n",
            "Epoch 39/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.9209 - val_loss: 54.4107\n",
            "Epoch 40/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.9071 - val_loss: 47.0005\n",
            "Epoch 41/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.7479 - val_loss: 45.1875\n",
            "Epoch 42/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.8089 - val_loss: 74.8973\n",
            "Epoch 43/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.7177 - val_loss: 43.6783\n",
            "Epoch 44/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 36.4898 - val_loss: 46.6728\n",
            "Epoch 45/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.4886 - val_loss: 51.6743\n",
            "Epoch 46/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 36.3724 - val_loss: 58.2912\n",
            "Epoch 47/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.2644 - val_loss: 41.7606\n",
            "Epoch 48/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.1810 - val_loss: 72.2977\n",
            "Epoch 49/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.0021 - val_loss: 56.3236\n",
            "Epoch 50/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.0315 - val_loss: 42.4319\n",
            "Epoch 51/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 36.0233 - val_loss: 99.6648\n",
            "Epoch 52/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.7817 - val_loss: 60.4881\n",
            "Epoch 53/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.7008 - val_loss: 42.1518\n",
            "Epoch 54/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.8132 - val_loss: 58.1162\n",
            "Epoch 55/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5676 - val_loss: 39.7274\n",
            "Epoch 56/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.5750 - val_loss: 57.9335\n",
            "Epoch 57/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.6480 - val_loss: 38.6363\n",
            "Epoch 58/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4677 - val_loss: 55.5870\n",
            "Epoch 59/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4518 - val_loss: 73.1034\n",
            "Epoch 60/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.4697 - val_loss: 43.4360\n",
            "Epoch 61/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.0757 - val_loss: 46.8435\n",
            "Epoch 62/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.1729 - val_loss: 40.9679\n",
            "Epoch 63/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.1370 - val_loss: 93.1677\n",
            "Epoch 64/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0130 - val_loss: 63.1251\n",
            "Epoch 65/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.8811 - val_loss: 53.6154\n",
            "Epoch 66/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.8339 - val_loss: 44.4757\n",
            "Epoch 67/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6933 - val_loss: 52.8727\n",
            "Epoch 68/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.5882 - val_loss: 61.8816\n",
            "Epoch 69/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.6000 - val_loss: 54.8059\n",
            "Epoch 70/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.4255 - val_loss: 47.2828\n",
            "Epoch 71/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.6420 - val_loss: 46.6630\n",
            "Epoch 72/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3467 - val_loss: 39.2632\n",
            "Epoch 73/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.3689 - val_loss: 56.7583\n",
            "Epoch 74/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2508 - val_loss: 44.4606\n",
            "Epoch 75/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.2536 - val_loss: 46.9078\n",
            "Epoch 76/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.3764 - val_loss: 41.8780\n",
            "Epoch 77/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.2305 - val_loss: 65.4248\n",
            "Epoch 78/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0355 - val_loss: 51.0682\n",
            "Epoch 79/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9772 - val_loss: 40.7341\n",
            "Epoch 80/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0328 - val_loss: 40.1584\n",
            "Epoch 81/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0929 - val_loss: 39.0569\n",
            "Epoch 82/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9571 - val_loss: 53.7637\n",
            "Epoch 83/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9844 - val_loss: 65.7278\n",
            "Epoch 84/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.8667 - val_loss: 39.9137\n",
            "Epoch 85/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8259 - val_loss: 62.3337\n",
            "Epoch 86/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8440 - val_loss: 48.1402\n",
            "Epoch 87/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.9266 - val_loss: 40.2624\n",
            "Epoch 88/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.8003 - val_loss: 42.9209\n",
            "Epoch 89/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7328 - val_loss: 60.0222\n",
            "Epoch 90/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.6469 - val_loss: 39.5027\n",
            "Epoch 91/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7406 - val_loss: 46.1028\n",
            "Epoch 92/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6948 - val_loss: 41.1182\n",
            "Epoch 93/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.7789 - val_loss: 43.7477\n",
            "Epoch 94/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.6500 - val_loss: 38.2234\n",
            "Epoch 95/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.7764 - val_loss: 47.1471\n",
            "Epoch 96/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6967 - val_loss: 43.6972\n",
            "Epoch 97/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4900 - val_loss: 96.5550\n",
            "Epoch 98/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.5427 - val_loss: 71.4019\n",
            "Epoch 99/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.5166 - val_loss: 39.7911\n",
            "Epoch 100/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.5701 - val_loss: 67.1927\n",
            "Epoch 101/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5186 - val_loss: 49.0997\n",
            "Epoch 102/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5039 - val_loss: 43.9245\n",
            "Epoch 103/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4498 - val_loss: 53.8163\n",
            "Epoch 104/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.5994 - val_loss: 51.3882\n",
            "Epoch 105/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4992 - val_loss: 51.0220\n",
            "Epoch 106/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3514 - val_loss: 48.6218\n",
            "Epoch 107/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3612 - val_loss: 39.5837\n",
            "Epoch 108/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.5694 - val_loss: 73.6750\n",
            "Epoch 109/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.3106 - val_loss: 106.5911\n",
            "Epoch 110/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.4123 - val_loss: 49.3329\n",
            "Epoch 111/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3050 - val_loss: 39.3834\n",
            "Epoch 112/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.4602 - val_loss: 50.3675\n",
            "Epoch 113/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3612 - val_loss: 44.2596\n",
            "Epoch 114/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.2552 - val_loss: 53.2897\n",
            "Epoch 115/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.2767 - val_loss: 49.9710\n",
            "Epoch 116/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3980 - val_loss: 48.0981\n",
            "Epoch 117/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2629 - val_loss: 46.6372\n",
            "Epoch 118/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.2868 - val_loss: 38.1969\n",
            "Epoch 119/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1894 - val_loss: 41.3678\n",
            "Epoch 120/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3287 - val_loss: 40.6575\n",
            "Epoch 121/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1481 - val_loss: 59.6548\n",
            "Epoch 122/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.2280 - val_loss: 46.0796\n",
            "Epoch 123/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.1575 - val_loss: 49.0775\n",
            "Epoch 124/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9368 - val_loss: 90.1830\n",
            "Epoch 125/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0377 - val_loss: 61.6448\n",
            "Epoch 126/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8728 - val_loss: 48.9252\n",
            "Epoch 127/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9902 - val_loss: 37.9179\n",
            "Epoch 128/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9880 - val_loss: 91.8283\n",
            "Epoch 129/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9796 - val_loss: 48.3224\n",
            "Epoch 130/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.0124 - val_loss: 41.3898\n",
            "Epoch 131/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8266 - val_loss: 68.3856\n",
            "Epoch 132/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9029 - val_loss: 41.9324\n",
            "Epoch 133/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7888 - val_loss: 40.4549\n",
            "Epoch 134/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8293 - val_loss: 41.9929\n",
            "Epoch 135/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8803 - val_loss: 41.5534\n",
            "Epoch 136/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.9430 - val_loss: 39.3484\n",
            "Epoch 137/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8623 - val_loss: 40.1312\n",
            "Epoch 138/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7575 - val_loss: 43.2923\n",
            "Epoch 139/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6981 - val_loss: 97.2636\n",
            "Epoch 140/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8051 - val_loss: 52.2755\n",
            "Epoch 141/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.8615 - val_loss: 46.7032\n",
            "Epoch 142/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7756 - val_loss: 47.4694\n",
            "Epoch 143/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7947 - val_loss: 58.3675\n",
            "Epoch 144/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7478 - val_loss: 45.9084\n",
            "Epoch 145/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7003 - val_loss: 58.4454\n",
            "Epoch 146/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7991 - val_loss: 52.7886\n",
            "Epoch 147/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6211 - val_loss: 46.5217\n",
            "Epoch 148/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6614 - val_loss: 46.2578\n",
            "Epoch 149/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.7518 - val_loss: 57.1882\n",
            "Epoch 150/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6982 - val_loss: 50.7564\n",
            "Epoch 151/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7026 - val_loss: 39.9889\n",
            "Epoch 152/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5705 - val_loss: 46.6654\n",
            "Epoch 153/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5452 - val_loss: 37.8957\n",
            "Epoch 154/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6226 - val_loss: 70.7633\n",
            "Epoch 155/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5985 - val_loss: 39.5214\n",
            "Epoch 156/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5572 - val_loss: 49.9941\n",
            "Epoch 157/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.6595 - val_loss: 42.3484\n",
            "Epoch 158/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6671 - val_loss: 39.6228\n",
            "Epoch 159/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.6428 - val_loss: 42.4080\n",
            "Epoch 160/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6411 - val_loss: 83.8843\n",
            "Epoch 161/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5340 - val_loss: 43.2739\n",
            "Epoch 162/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4988 - val_loss: 70.9503\n",
            "Epoch 163/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5286 - val_loss: 41.2972\n",
            "Epoch 164/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6179 - val_loss: 49.2671\n",
            "Epoch 165/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.6224 - val_loss: 61.4468\n",
            "Epoch 166/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5098 - val_loss: 38.3373\n",
            "Epoch 167/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5315 - val_loss: 60.0087\n",
            "Epoch 168/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5530 - val_loss: 48.7255\n",
            "Epoch 169/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5710 - val_loss: 40.0090\n",
            "Epoch 170/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5414 - val_loss: 44.4956\n",
            "Epoch 171/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5255 - val_loss: 74.5435\n",
            "Epoch 172/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5373 - val_loss: 71.4741\n",
            "Epoch 173/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6292 - val_loss: 51.7959\n",
            "Epoch 174/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4640 - val_loss: 68.0628\n",
            "Epoch 175/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4436 - val_loss: 46.8287\n",
            "Epoch 176/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4650 - val_loss: 48.1177\n",
            "Epoch 177/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4655 - val_loss: 41.0109\n",
            "Epoch 178/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4190 - val_loss: 38.2016\n",
            "Epoch 179/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4281 - val_loss: 40.0915\n",
            "Epoch 180/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4253 - val_loss: 44.9059\n",
            "Epoch 181/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4518 - val_loss: 37.8300\n",
            "Epoch 182/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4215 - val_loss: 38.9796\n",
            "Epoch 183/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.5294 - val_loss: 37.5498\n",
            "Epoch 184/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4387 - val_loss: 41.4936\n",
            "Epoch 185/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3581 - val_loss: 40.4147\n",
            "Epoch 186/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3478 - val_loss: 38.6448\n",
            "Epoch 187/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4123 - val_loss: 38.2657\n",
            "Epoch 188/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3258 - val_loss: 37.3442\n",
            "Epoch 189/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3848 - val_loss: 43.4904\n",
            "Epoch 190/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3399 - val_loss: 62.6306\n",
            "Epoch 191/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3355 - val_loss: 56.3918\n",
            "Epoch 192/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.4329 - val_loss: 37.7593\n",
            "Epoch 193/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3979 - val_loss: 43.9034\n",
            "Epoch 194/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3543 - val_loss: 50.9045\n",
            "Epoch 195/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2822 - val_loss: 40.7081\n",
            "Epoch 196/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3640 - val_loss: 39.4237\n",
            "Epoch 197/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3641 - val_loss: 42.7737\n",
            "Epoch 198/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4102 - val_loss: 44.7207\n",
            "Epoch 199/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3753 - val_loss: 41.0836\n",
            "Epoch 200/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3802 - val_loss: 44.0646\n",
            "Epoch 201/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3898 - val_loss: 42.2477\n",
            "Epoch 202/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3237 - val_loss: 38.6565\n",
            "Epoch 203/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2373 - val_loss: 40.5487\n",
            "Epoch 204/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2745 - val_loss: 52.9272\n",
            "Epoch 205/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2623 - val_loss: 46.4349\n",
            "Epoch 206/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2400 - val_loss: 57.6363\n",
            "Epoch 207/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3299 - val_loss: 38.3497\n",
            "Epoch 208/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3512 - val_loss: 40.4313\n",
            "Epoch 209/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2974 - val_loss: 46.0691\n",
            "Epoch 210/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3899 - val_loss: 46.8566\n",
            "Epoch 211/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3149 - val_loss: 38.6253\n",
            "Epoch 212/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3223 - val_loss: 38.3601\n",
            "Epoch 213/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3342 - val_loss: 44.8453\n",
            "Epoch 214/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2768 - val_loss: 42.5794\n",
            "Epoch 215/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3747 - val_loss: 45.4237\n",
            "Epoch 216/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3479 - val_loss: 42.8513\n",
            "Epoch 217/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2332 - val_loss: 61.8147\n",
            "Epoch 218/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2857 - val_loss: 38.2632\n",
            "Epoch 219/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2641 - val_loss: 55.3649\n",
            "Epoch 220/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2473 - val_loss: 37.6165\n",
            "Epoch 221/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2164 - val_loss: 50.5336\n",
            "Epoch 222/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2692 - val_loss: 36.5416\n",
            "Epoch 223/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3471 - val_loss: 38.5738\n",
            "Epoch 224/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2805 - val_loss: 49.6589\n",
            "Epoch 225/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1860 - val_loss: 63.1239\n",
            "Epoch 226/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.3174 - val_loss: 45.9406\n",
            "Epoch 227/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2179 - val_loss: 59.6517\n",
            "Epoch 228/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2205 - val_loss: 41.4598\n",
            "Epoch 229/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2348 - val_loss: 48.2422\n",
            "Epoch 230/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2359 - val_loss: 44.9429\n",
            "Epoch 231/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2536 - val_loss: 43.2857\n",
            "Epoch 232/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2266 - val_loss: 37.4247\n",
            "Epoch 233/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2611 - val_loss: 38.5722\n",
            "Epoch 234/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2320 - val_loss: 38.6042\n",
            "Epoch 235/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2796 - val_loss: 58.0434\n",
            "Epoch 236/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1764 - val_loss: 46.7889\n",
            "Epoch 237/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2014 - val_loss: 49.5238\n",
            "Epoch 238/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2440 - val_loss: 51.4271\n",
            "Epoch 239/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2596 - val_loss: 38.9050\n",
            "Epoch 240/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2548 - val_loss: 64.2339\n",
            "Epoch 241/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1882 - val_loss: 40.0905\n",
            "Epoch 242/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2136 - val_loss: 46.8075\n",
            "Epoch 243/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.2667 - val_loss: 41.0125\n",
            "Epoch 244/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1791 - val_loss: 39.9310\n",
            "Epoch 245/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1678 - val_loss: 39.0967\n",
            "Epoch 246/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1705 - val_loss: 40.0406\n",
            "Epoch 247/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1645 - val_loss: 48.9179\n",
            "Epoch 248/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1948 - val_loss: 60.0612\n",
            "Epoch 249/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2552 - val_loss: 39.9588\n",
            "Epoch 250/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1654 - val_loss: 45.2985\n",
            "Epoch 251/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1546 - val_loss: 59.7599\n",
            "Epoch 252/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2359 - val_loss: 50.5398\n",
            "Epoch 253/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1650 - val_loss: 40.2743\n",
            "Epoch 254/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1172 - val_loss: 40.5052\n",
            "Epoch 255/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1496 - val_loss: 44.9007\n",
            "Epoch 256/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2132 - val_loss: 37.5359\n",
            "Epoch 257/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1777 - val_loss: 45.8859\n",
            "Epoch 258/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2215 - val_loss: 40.1911\n",
            "Epoch 259/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1704 - val_loss: 66.4610\n",
            "Epoch 260/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2523 - val_loss: 40.2345\n",
            "Epoch 261/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0933 - val_loss: 37.8085\n",
            "Epoch 262/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1622 - val_loss: 39.0851\n",
            "Epoch 263/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1532 - val_loss: 41.3414\n",
            "Epoch 264/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1874 - val_loss: 58.3164\n",
            "Epoch 265/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1911 - val_loss: 44.6553\n",
            "Epoch 266/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1798 - val_loss: 37.4385\n",
            "Epoch 267/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.2634 - val_loss: 39.1723\n",
            "Epoch 268/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1448 - val_loss: 56.2221\n",
            "Epoch 269/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1160 - val_loss: 41.7708\n",
            "Epoch 270/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1595 - val_loss: 41.0008\n",
            "Epoch 271/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1331 - val_loss: 38.7583\n",
            "Epoch 272/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1959 - val_loss: 45.6786\n",
            "Epoch 273/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1593 - val_loss: 51.1096\n",
            "Epoch 274/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1192 - val_loss: 40.9836\n",
            "Epoch 275/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1536 - val_loss: 41.8596\n",
            "Epoch 276/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1289 - val_loss: 39.1923\n",
            "Epoch 277/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0505 - val_loss: 38.4779\n",
            "Epoch 278/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0476 - val_loss: 41.7487\n",
            "Epoch 279/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1591 - val_loss: 37.2806\n",
            "Epoch 280/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1045 - val_loss: 37.6534\n",
            "Epoch 281/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0713 - val_loss: 36.8072\n",
            "Epoch 282/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1982 - val_loss: 39.6295\n",
            "Epoch 283/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0709 - val_loss: 51.3653\n",
            "Epoch 284/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0137 - val_loss: 38.3404\n",
            "Epoch 285/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1878 - val_loss: 40.3945\n",
            "Epoch 286/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0703 - val_loss: 36.8528\n",
            "Epoch 287/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0893 - val_loss: 39.3749\n",
            "Epoch 288/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0927 - val_loss: 42.3891\n",
            "Epoch 289/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1024 - val_loss: 36.6957\n",
            "Epoch 290/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1078 - val_loss: 45.4018\n",
            "Epoch 291/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1105 - val_loss: 37.5028\n",
            "Epoch 292/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1115 - val_loss: 37.0138\n",
            "Epoch 293/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9616 - val_loss: 46.2174\n",
            "Epoch 294/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0849 - val_loss: 48.0555\n",
            "Epoch 295/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0754 - val_loss: 53.9676\n",
            "Epoch 296/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.1078 - val_loss: 40.1828\n",
            "Epoch 297/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1372 - val_loss: 51.4084\n",
            "Epoch 298/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0966 - val_loss: 42.7191\n",
            "Epoch 299/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0795 - val_loss: 39.0306\n",
            "Epoch 300/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9668 - val_loss: 55.9195\n",
            "Epoch 301/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9896 - val_loss: 50.7090\n",
            "Epoch 302/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0462 - val_loss: 38.1434\n",
            "Epoch 303/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0427 - val_loss: 41.6770\n",
            "Epoch 304/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9455 - val_loss: 36.8344\n",
            "Epoch 305/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0263 - val_loss: 43.7830\n",
            "Epoch 306/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0587 - val_loss: 37.1630\n",
            "Epoch 307/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0231 - val_loss: 37.9041\n",
            "Epoch 308/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9981 - val_loss: 46.7381\n",
            "Epoch 309/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0746 - val_loss: 49.7691\n",
            "Epoch 310/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9885 - val_loss: 54.3612\n",
            "Epoch 311/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0566 - val_loss: 43.8171\n",
            "Epoch 312/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0663 - val_loss: 62.7004\n",
            "Epoch 313/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0353 - val_loss: 44.3234\n",
            "Epoch 314/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0438 - val_loss: 39.5167\n",
            "Epoch 315/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9889 - val_loss: 51.6523\n",
            "Epoch 316/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0181 - val_loss: 41.6911\n",
            "Epoch 317/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0647 - val_loss: 38.0184\n",
            "Epoch 318/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0294 - val_loss: 37.2794\n",
            "Epoch 319/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0221 - val_loss: 40.4654\n",
            "Epoch 320/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9266 - val_loss: 37.5179\n",
            "Epoch 321/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9824 - val_loss: 37.8205\n",
            "Epoch 322/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9659 - val_loss: 47.0191\n",
            "Epoch 323/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0167 - val_loss: 37.5316\n",
            "Epoch 324/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9605 - val_loss: 38.5817\n",
            "Epoch 325/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0125 - val_loss: 46.1080\n",
            "Epoch 326/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0300 - val_loss: 39.5961\n",
            "Epoch 327/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0015 - val_loss: 45.6710\n",
            "Epoch 328/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0422 - val_loss: 42.9247\n",
            "Epoch 329/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9187 - val_loss: 37.6344\n",
            "Epoch 330/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9732 - val_loss: 51.7206\n",
            "Epoch 331/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0135 - val_loss: 36.8786\n",
            "Epoch 332/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9359 - val_loss: 46.3414\n",
            "Epoch 333/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9842 - val_loss: 39.6284\n",
            "Epoch 334/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9363 - val_loss: 41.2909\n",
            "Epoch 335/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9783 - val_loss: 46.6892\n",
            "Epoch 336/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9489 - val_loss: 37.8404\n",
            "Epoch 337/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0164 - val_loss: 42.0978\n",
            "Epoch 338/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9195 - val_loss: 37.4484\n",
            "Epoch 339/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0006 - val_loss: 54.6914\n",
            "Epoch 340/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8994 - val_loss: 36.6839\n",
            "Epoch 341/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9152 - val_loss: 57.1866\n",
            "Epoch 342/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8153 - val_loss: 59.4036\n",
            "Epoch 343/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9213 - val_loss: 36.5401\n",
            "Epoch 344/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9529 - val_loss: 37.1186\n",
            "Epoch 345/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9200 - val_loss: 48.9998\n",
            "Epoch 346/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 31.9360 - val_loss: 36.6564\n",
            "Epoch 347/400\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 31.9220 - val_loss: 40.1161\n",
            "Epoch 348/400\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 31.8611 - val_loss: 39.9703\n",
            "Epoch 349/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8732 - val_loss: 35.9526\n",
            "Epoch 350/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8392 - val_loss: 50.3029\n",
            "Epoch 351/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9084 - val_loss: 42.6206\n",
            "Epoch 352/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9092 - val_loss: 40.5111\n",
            "Epoch 353/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9928 - val_loss: 38.9554\n",
            "Epoch 354/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9237 - val_loss: 42.6989\n",
            "Epoch 355/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8850 - val_loss: 39.9311\n",
            "Epoch 356/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9384 - val_loss: 40.2737\n",
            "Epoch 357/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8813 - val_loss: 36.4687\n",
            "Epoch 358/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8672 - val_loss: 46.1127\n",
            "Epoch 359/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7970 - val_loss: 41.1047\n",
            "Epoch 360/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9006 - val_loss: 43.9886\n",
            "Epoch 361/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8583 - val_loss: 42.8987\n",
            "Epoch 362/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8804 - val_loss: 40.0464\n",
            "Epoch 363/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8559 - val_loss: 37.6472\n",
            "Epoch 364/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7964 - val_loss: 43.3121\n",
            "Epoch 365/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8990 - val_loss: 40.0356\n",
            "Epoch 366/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9311 - val_loss: 39.6447\n",
            "Epoch 367/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8733 - val_loss: 41.0634\n",
            "Epoch 368/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8050 - val_loss: 42.8379\n",
            "Epoch 369/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8725 - val_loss: 37.4930\n",
            "Epoch 370/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8293 - val_loss: 39.1499\n",
            "Epoch 371/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8138 - val_loss: 37.7616\n",
            "Epoch 372/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8292 - val_loss: 38.6554\n",
            "Epoch 373/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8648 - val_loss: 37.7468\n",
            "Epoch 374/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8718 - val_loss: 41.6818\n",
            "Epoch 375/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7929 - val_loss: 43.8135\n",
            "Epoch 376/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8325 - val_loss: 62.5535\n",
            "Epoch 377/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8505 - val_loss: 52.3416\n",
            "Epoch 378/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8351 - val_loss: 51.5830\n",
            "Epoch 379/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8936 - val_loss: 38.3963\n",
            "Epoch 380/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8320 - val_loss: 43.6191\n",
            "Epoch 381/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8213 - val_loss: 38.5128\n",
            "Epoch 382/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8117 - val_loss: 43.8415\n",
            "Epoch 383/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8061 - val_loss: 42.8135\n",
            "Epoch 384/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7857 - val_loss: 43.6266\n",
            "Epoch 385/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8570 - val_loss: 39.8977\n",
            "Epoch 386/400\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8835 - val_loss: 36.0737\n",
            "Epoch 387/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8955 - val_loss: 51.9649\n",
            "Epoch 388/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7870 - val_loss: 45.2897\n",
            "Epoch 389/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8069 - val_loss: 44.0355\n",
            "Epoch 390/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8719 - val_loss: 36.8329\n",
            "Epoch 391/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8106 - val_loss: 36.8567\n",
            "Epoch 392/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7783 - val_loss: 36.7199\n",
            "Epoch 393/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7958 - val_loss: 39.6765\n",
            "Epoch 394/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8924 - val_loss: 48.4838\n",
            "Epoch 395/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8119 - val_loss: 47.5200\n",
            "Epoch 396/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7611 - val_loss: 41.6452\n",
            "Epoch 397/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7905 - val_loss: 43.9652\n",
            "Epoch 398/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9380 - val_loss: 37.8212\n",
            "Epoch 399/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8384 - val_loss: 35.9355\n",
            "Epoch 400/400\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.8032 - val_loss: 40.3985\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36895f86-19c0-4a6c-befe-609197d76ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.21569813420706238 \n",
            "MAE:  4.770278615342466 \n",
            "SD:  6.352321433784806\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c65668a4-b526-4362-cd94-d873622851d7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ],
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e721455b-78cd-40df-d3cc-435b8720768e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -0.6292392160586303 \n",
            "Ensemble_std:  6.13444581861662\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "\bBP_hv3_2(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}