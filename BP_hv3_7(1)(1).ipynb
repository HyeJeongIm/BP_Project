{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/BP_hv3_7(1)(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiiiBla2-j1S"
      },
      "source": [
        "# batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCoux5AOZnK",
        "outputId": "088160bf-92c1-4499-c2ab-16deba0cd296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "TensorFlow version :  2.8.2\n",
            "Keras version :  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4eSW5JPDwYZ",
        "outputId": "7d0c1844-269c-4eff-a098-ce151a2411e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러z오기\n",
        "train_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "f97a03fc-c2c8-45c6-bf66-5010f04e0836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "6d96a464-14eb-4958-b330-51ff5e586c02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22cc98e9-e654-406a-8d29-d2464b9c685e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22cc98e9-e654-406a-8d29-d2464b9c685e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22cc98e9-e654-406a-8d29-d2464b9c685e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22cc98e9-e654-406a-8d29-d2464b9c685e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "9073a0a5-52fd-41e9-a2ac-31edceea624d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88a1e88d-9f90-406c-996d-cb4df862470b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88a1e88d-9f90-406c-996d-cb4df862470b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88a1e88d-9f90-406c-996d-cb4df862470b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88a1e88d-9f90-406c-996d-cb4df862470b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "outputs": [],
      "source": [
        "#parameter\n",
        "batch_size = 128\n",
        "epochs = 300\n",
        "lrate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ptBRJtSOZnN",
        "outputId": "43e5a726-f80b-4c3b-f201-e755a518abaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 16)               64        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,465\n",
            "Trainable params: 2,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EI8SHBwBOZnO"
      },
      "outputs": [],
      "source": [
        "# model = model1()\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dGT6-7NcOZnO",
        "outputId": "be71a167-20d3-4717-92f9-f6dcdd28a590",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1319/1319 [==============================] - 9s 4ms/step - loss: 9118.9639 - val_loss: 4686.9678\n",
            "Epoch 2/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 1113.0507 - val_loss: 130.2253\n",
            "Epoch 3/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 114.7275 - val_loss: 141.4787\n",
            "Epoch 4/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 105.5567 - val_loss: 177.7238\n",
            "Epoch 5/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 101.6277 - val_loss: 155.8662\n",
            "Epoch 6/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 99.3515 - val_loss: 288.2574\n",
            "Epoch 7/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 97.5864 - val_loss: 196.8924\n",
            "Epoch 8/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 96.0549 - val_loss: 171.1152\n",
            "Epoch 9/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 94.6807 - val_loss: 115.4709\n",
            "Epoch 10/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 93.6308 - val_loss: 142.2794\n",
            "Epoch 11/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 92.6623 - val_loss: 114.4263\n",
            "Epoch 12/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 91.6181 - val_loss: 139.1580\n",
            "Epoch 13/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 90.9215 - val_loss: 214.4219\n",
            "Epoch 14/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 90.3658 - val_loss: 123.9520\n",
            "Epoch 15/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 89.6117 - val_loss: 122.0830\n",
            "Epoch 16/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 89.0334 - val_loss: 153.7928\n",
            "Epoch 17/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 88.7243 - val_loss: 111.7407\n",
            "Epoch 18/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 87.6940 - val_loss: 119.1556\n",
            "Epoch 19/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 87.6769 - val_loss: 117.9528\n",
            "Epoch 20/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 87.2118 - val_loss: 123.1831\n",
            "Epoch 21/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 86.8592 - val_loss: 148.8715\n",
            "Epoch 22/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 86.5206 - val_loss: 155.2026\n",
            "Epoch 23/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 86.0203 - val_loss: 160.8097\n",
            "Epoch 24/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 85.8482 - val_loss: 99.4967\n",
            "Epoch 25/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 85.1873 - val_loss: 104.6021\n",
            "Epoch 26/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 85.0925 - val_loss: 117.8548\n",
            "Epoch 27/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 84.7841 - val_loss: 137.2142\n",
            "Epoch 28/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 84.5101 - val_loss: 140.9020\n",
            "Epoch 29/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 84.1555 - val_loss: 152.0794\n",
            "Epoch 30/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 83.9231 - val_loss: 104.4747\n",
            "Epoch 31/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 83.7101 - val_loss: 92.5850\n",
            "Epoch 32/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 83.6825 - val_loss: 153.5242\n",
            "Epoch 33/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 83.2550 - val_loss: 98.6958\n",
            "Epoch 34/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 82.9523 - val_loss: 106.9189\n",
            "Epoch 35/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 82.9999 - val_loss: 99.0350\n",
            "Epoch 36/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 82.8621 - val_loss: 99.8230\n",
            "Epoch 37/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 82.6785 - val_loss: 106.6214\n",
            "Epoch 38/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 82.5419 - val_loss: 106.4562\n",
            "Epoch 39/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 82.5226 - val_loss: 101.6433\n",
            "Epoch 40/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 82.3963 - val_loss: 93.4304\n",
            "Epoch 41/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 82.1472 - val_loss: 115.2227\n",
            "Epoch 42/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 82.4231 - val_loss: 94.6490\n",
            "Epoch 43/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 82.0624 - val_loss: 103.7396\n",
            "Epoch 44/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 82.2071 - val_loss: 108.3257\n",
            "Epoch 45/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 81.7637 - val_loss: 164.4967\n",
            "Epoch 46/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 81.6957 - val_loss: 128.6334\n",
            "Epoch 47/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 81.7549 - val_loss: 95.5116\n",
            "Epoch 48/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 81.6930 - val_loss: 153.8956\n",
            "Epoch 49/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 81.3854 - val_loss: 94.7679\n",
            "Epoch 50/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 81.3609 - val_loss: 101.0112\n",
            "Epoch 51/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 81.4454 - val_loss: 115.1796\n",
            "Epoch 52/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 81.3025 - val_loss: 98.5794\n",
            "Epoch 53/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 81.3244 - val_loss: 124.6544\n",
            "Epoch 54/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 81.2495 - val_loss: 107.1448\n",
            "Epoch 55/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 81.1751 - val_loss: 99.2917\n",
            "Epoch 56/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 81.0152 - val_loss: 109.3886\n",
            "Epoch 57/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 80.8676 - val_loss: 136.6444\n",
            "Epoch 58/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 80.8216 - val_loss: 94.8787\n",
            "Epoch 59/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 80.6379 - val_loss: 122.3570\n",
            "Epoch 60/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 80.8992 - val_loss: 96.6187\n",
            "Epoch 61/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 80.5943 - val_loss: 137.6456\n",
            "Epoch 62/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 80.5221 - val_loss: 103.2119\n",
            "Epoch 63/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 80.4327 - val_loss: 94.9284\n",
            "Epoch 64/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 80.5167 - val_loss: 189.5551\n",
            "Epoch 65/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 80.2799 - val_loss: 231.5839\n",
            "Epoch 66/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 80.5252 - val_loss: 123.0230\n",
            "Epoch 67/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 80.2262 - val_loss: 118.6756\n",
            "Epoch 68/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 80.1665 - val_loss: 102.7469\n",
            "Epoch 69/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 80.0763 - val_loss: 111.8582\n",
            "Epoch 70/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.8236 - val_loss: 110.4174\n",
            "Epoch 71/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 80.0093 - val_loss: 110.7397\n",
            "Epoch 72/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.9288 - val_loss: 94.3966\n",
            "Epoch 73/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.7675 - val_loss: 97.3126\n",
            "Epoch 74/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.6415 - val_loss: 98.3947\n",
            "Epoch 75/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.7594 - val_loss: 169.9986\n",
            "Epoch 76/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.3474 - val_loss: 104.3834\n",
            "Epoch 77/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.3919 - val_loss: 118.0970\n",
            "Epoch 78/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.8655 - val_loss: 92.9195\n",
            "Epoch 79/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.4981 - val_loss: 106.6533\n",
            "Epoch 80/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.3006 - val_loss: 96.2339\n",
            "Epoch 81/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.2698 - val_loss: 107.1984\n",
            "Epoch 82/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.1710 - val_loss: 97.6567\n",
            "Epoch 83/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.2735 - val_loss: 98.0277\n",
            "Epoch 84/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 79.0491 - val_loss: 108.6850\n",
            "Epoch 85/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.9378 - val_loss: 138.1698\n",
            "Epoch 86/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 78.7440 - val_loss: 94.3493\n",
            "Epoch 87/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.7011 - val_loss: 110.3870\n",
            "Epoch 88/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.7528 - val_loss: 90.2029\n",
            "Epoch 89/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.8852 - val_loss: 107.8651\n",
            "Epoch 90/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.8138 - val_loss: 136.7457\n",
            "Epoch 91/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.5083 - val_loss: 91.9222\n",
            "Epoch 92/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.7585 - val_loss: 118.2132\n",
            "Epoch 93/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.5468 - val_loss: 90.6394\n",
            "Epoch 94/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.6973 - val_loss: 97.7356\n",
            "Epoch 95/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.4490 - val_loss: 102.8529\n",
            "Epoch 96/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.4838 - val_loss: 91.7304\n",
            "Epoch 97/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.4032 - val_loss: 94.8710\n",
            "Epoch 98/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 78.5956 - val_loss: 94.4082\n",
            "Epoch 99/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.2468 - val_loss: 96.1220\n",
            "Epoch 100/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.3865 - val_loss: 95.9228\n",
            "Epoch 101/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.2623 - val_loss: 98.1416\n",
            "Epoch 102/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.0773 - val_loss: 170.1640\n",
            "Epoch 103/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.1581 - val_loss: 92.5557\n",
            "Epoch 104/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.1908 - val_loss: 103.1601\n",
            "Epoch 105/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.1605 - val_loss: 111.6829\n",
            "Epoch 106/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.1572 - val_loss: 121.0896\n",
            "Epoch 107/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.1253 - val_loss: 90.3127\n",
            "Epoch 108/300\n",
            "1319/1319 [==============================] - 5s 3ms/step - loss: 78.0324 - val_loss: 99.3325\n",
            "Epoch 109/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.8763 - val_loss: 100.0764\n",
            "Epoch 110/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.5338 - val_loss: 90.7048\n",
            "Epoch 111/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.9593 - val_loss: 89.0387\n",
            "Epoch 112/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 78.1250 - val_loss: 91.9195\n",
            "Epoch 113/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.7198 - val_loss: 94.5471\n",
            "Epoch 114/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.8603 - val_loss: 102.2531\n",
            "Epoch 115/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.7842 - val_loss: 92.9291\n",
            "Epoch 116/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.8248 - val_loss: 88.5417\n",
            "Epoch 117/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.8359 - val_loss: 94.5106\n",
            "Epoch 118/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.5657 - val_loss: 89.4390\n",
            "Epoch 119/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.4978 - val_loss: 93.1718\n",
            "Epoch 120/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.6423 - val_loss: 108.9943\n",
            "Epoch 121/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.6368 - val_loss: 90.9196\n",
            "Epoch 122/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.4339 - val_loss: 95.6287\n",
            "Epoch 123/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.5264 - val_loss: 89.4398\n",
            "Epoch 124/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.6739 - val_loss: 97.1313\n",
            "Epoch 125/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.4631 - val_loss: 109.6103\n",
            "Epoch 126/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.5456 - val_loss: 98.7067\n",
            "Epoch 127/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.4636 - val_loss: 88.8202\n",
            "Epoch 128/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.5262 - val_loss: 113.4179\n",
            "Epoch 129/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.6556 - val_loss: 113.9313\n",
            "Epoch 130/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.6221 - val_loss: 132.2097\n",
            "Epoch 131/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.3945 - val_loss: 94.0478\n",
            "Epoch 132/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.8013 - val_loss: 91.9180\n",
            "Epoch 133/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.9992 - val_loss: 110.0366\n",
            "Epoch 134/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.3769 - val_loss: 88.0631\n",
            "Epoch 135/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.4965 - val_loss: 94.6122\n",
            "Epoch 136/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.1616 - val_loss: 90.9299\n",
            "Epoch 137/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.1517 - val_loss: 97.7279\n",
            "Epoch 138/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.1532 - val_loss: 88.8139\n",
            "Epoch 139/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.3629 - val_loss: 95.1204\n",
            "Epoch 140/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.2327 - val_loss: 89.6782\n",
            "Epoch 141/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.3555 - val_loss: 91.8292\n",
            "Epoch 142/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.2770 - val_loss: 93.6069\n",
            "Epoch 143/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.1115 - val_loss: 91.2121\n",
            "Epoch 144/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.2304 - val_loss: 105.3405\n",
            "Epoch 145/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.2759 - val_loss: 102.1801\n",
            "Epoch 146/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.0395 - val_loss: 92.8294\n",
            "Epoch 147/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.0392 - val_loss: 92.1760\n",
            "Epoch 148/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.9637 - val_loss: 91.3384\n",
            "Epoch 149/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.8267 - val_loss: 92.4235\n",
            "Epoch 150/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.8592 - val_loss: 93.1027\n",
            "Epoch 151/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.7044 - val_loss: 134.7556\n",
            "Epoch 152/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.7788 - val_loss: 96.3100\n",
            "Epoch 153/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.2389 - val_loss: 89.3787\n",
            "Epoch 154/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.9732 - val_loss: 112.0477\n",
            "Epoch 155/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.9416 - val_loss: 92.3508\n",
            "Epoch 156/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 77.1437 - val_loss: 96.4306\n",
            "Epoch 157/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.7075 - val_loss: 98.2387\n",
            "Epoch 158/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.7013 - val_loss: 127.1918\n",
            "Epoch 159/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.9134 - val_loss: 96.1449\n",
            "Epoch 160/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.6317 - val_loss: 92.7266\n",
            "Epoch 161/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.5520 - val_loss: 87.6366\n",
            "Epoch 162/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.8045 - val_loss: 91.5912\n",
            "Epoch 163/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.7266 - val_loss: 89.0098\n",
            "Epoch 164/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.7854 - val_loss: 92.5731\n",
            "Epoch 165/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.7218 - val_loss: 94.3697\n",
            "Epoch 166/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.8511 - val_loss: 105.6026\n",
            "Epoch 167/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.6313 - val_loss: 98.9451\n",
            "Epoch 168/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.7779 - val_loss: 95.8005\n",
            "Epoch 169/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.5480 - val_loss: 92.2454\n",
            "Epoch 170/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3599 - val_loss: 96.9045\n",
            "Epoch 171/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.8644 - val_loss: 95.7242\n",
            "Epoch 172/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.4324 - val_loss: 93.8156\n",
            "Epoch 173/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.6356 - val_loss: 96.0146\n",
            "Epoch 174/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.6610 - val_loss: 156.9317\n",
            "Epoch 175/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.5067 - val_loss: 99.5318\n",
            "Epoch 176/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.5111 - val_loss: 95.4105\n",
            "Epoch 177/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.8615 - val_loss: 98.6331\n",
            "Epoch 178/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.5029 - val_loss: 129.1368\n",
            "Epoch 179/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.4004 - val_loss: 108.3682\n",
            "Epoch 180/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3036 - val_loss: 93.4585\n",
            "Epoch 181/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.6456 - val_loss: 105.9724\n",
            "Epoch 182/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.4576 - val_loss: 108.3028\n",
            "Epoch 183/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.5485 - val_loss: 89.6222\n",
            "Epoch 184/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3420 - val_loss: 87.2476\n",
            "Epoch 185/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3625 - val_loss: 92.9143\n",
            "Epoch 186/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.4416 - val_loss: 90.4743\n",
            "Epoch 187/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3497 - val_loss: 99.3935\n",
            "Epoch 188/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3318 - val_loss: 95.7235\n",
            "Epoch 189/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3935 - val_loss: 98.6079\n",
            "Epoch 190/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.2110 - val_loss: 92.5225\n",
            "Epoch 191/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.4899 - val_loss: 88.5458\n",
            "Epoch 192/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.6159 - val_loss: 92.4898\n",
            "Epoch 193/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.4016 - val_loss: 104.9207\n",
            "Epoch 194/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.1647 - val_loss: 93.5376\n",
            "Epoch 195/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 76.2174 - val_loss: 92.8270\n",
            "Epoch 196/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3190 - val_loss: 98.0524\n",
            "Epoch 197/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3127 - val_loss: 90.4005\n",
            "Epoch 198/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0645 - val_loss: 88.2428\n",
            "Epoch 199/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.2358 - val_loss: 93.1348\n",
            "Epoch 200/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3283 - val_loss: 107.5976\n",
            "Epoch 201/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.9785 - val_loss: 100.0549\n",
            "Epoch 202/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3425 - val_loss: 94.5644\n",
            "Epoch 203/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3701 - val_loss: 94.8568\n",
            "Epoch 204/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.1622 - val_loss: 96.6005\n",
            "Epoch 205/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.1237 - val_loss: 99.3327\n",
            "Epoch 206/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3679 - val_loss: 93.0747\n",
            "Epoch 207/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3848 - val_loss: 86.1882\n",
            "Epoch 208/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.4092 - val_loss: 96.9141\n",
            "Epoch 209/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.9807 - val_loss: 88.0785\n",
            "Epoch 210/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.2183 - val_loss: 104.3176\n",
            "Epoch 211/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.3245 - val_loss: 93.4146\n",
            "Epoch 212/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.9962 - val_loss: 104.1667\n",
            "Epoch 213/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.2241 - val_loss: 99.8393\n",
            "Epoch 214/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.9241 - val_loss: 114.6660\n",
            "Epoch 215/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.1355 - val_loss: 89.6143\n",
            "Epoch 216/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0570 - val_loss: 134.1043\n",
            "Epoch 217/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0243 - val_loss: 91.5256\n",
            "Epoch 218/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.2833 - val_loss: 90.7645\n",
            "Epoch 219/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.7854 - val_loss: 112.7121\n",
            "Epoch 220/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 76.0847 - val_loss: 91.9709\n",
            "Epoch 221/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.9961 - val_loss: 88.5456\n",
            "Epoch 222/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 76.1035 - val_loss: 96.0156\n",
            "Epoch 223/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.1346 - val_loss: 88.9216\n",
            "Epoch 224/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0902 - val_loss: 90.7461\n",
            "Epoch 225/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0573 - val_loss: 90.7560\n",
            "Epoch 226/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0284 - val_loss: 89.6858\n",
            "Epoch 227/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.1700 - val_loss: 110.8100\n",
            "Epoch 228/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0217 - val_loss: 87.5364\n",
            "Epoch 229/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.2472 - val_loss: 100.1078\n",
            "Epoch 230/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.9697 - val_loss: 91.5391\n",
            "Epoch 231/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0438 - val_loss: 87.4502\n",
            "Epoch 232/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.8535 - val_loss: 94.6342\n",
            "Epoch 233/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.8969 - val_loss: 101.2907\n",
            "Epoch 234/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.1476 - val_loss: 95.4652\n",
            "Epoch 235/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.1366 - val_loss: 102.3632\n",
            "Epoch 236/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0305 - val_loss: 89.9248\n",
            "Epoch 237/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0564 - val_loss: 92.3235\n",
            "Epoch 238/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.8559 - val_loss: 106.6120\n",
            "Epoch 239/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.9640 - val_loss: 85.6355\n",
            "Epoch 240/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0620 - val_loss: 98.0708\n",
            "Epoch 241/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0964 - val_loss: 127.8634\n",
            "Epoch 242/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.9793 - val_loss: 94.3409\n",
            "Epoch 243/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0510 - val_loss: 86.8918\n",
            "Epoch 244/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0121 - val_loss: 116.0786\n",
            "Epoch 245/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.9876 - val_loss: 88.5857\n",
            "Epoch 246/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.8696 - val_loss: 101.1265\n",
            "Epoch 247/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.7506 - val_loss: 95.6582\n",
            "Epoch 248/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.9319 - val_loss: 130.5307\n",
            "Epoch 249/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.8941 - val_loss: 90.9022\n",
            "Epoch 250/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.9735 - val_loss: 108.7183\n",
            "Epoch 251/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0599 - val_loss: 98.7017\n",
            "Epoch 252/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.6680 - val_loss: 87.0652\n",
            "Epoch 253/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.7705 - val_loss: 88.9758\n",
            "Epoch 254/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.7514 - val_loss: 100.3850\n",
            "Epoch 255/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0298 - val_loss: 105.2922\n",
            "Epoch 256/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.8685 - val_loss: 89.6315\n",
            "Epoch 257/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.1031 - val_loss: 103.0845\n",
            "Epoch 258/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.8528 - val_loss: 97.7149\n",
            "Epoch 259/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.8370 - val_loss: 111.2001\n",
            "Epoch 260/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 76.0780 - val_loss: 104.6587\n",
            "Epoch 261/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.7383 - val_loss: 87.7063\n",
            "Epoch 262/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4812 - val_loss: 96.6750\n",
            "Epoch 263/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.9498 - val_loss: 96.7635\n",
            "Epoch 264/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 75.8317 - val_loss: 87.8623\n",
            "Epoch 265/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.8674 - val_loss: 93.7138\n",
            "Epoch 266/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.6994 - val_loss: 89.3617\n",
            "Epoch 267/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.6870 - val_loss: 102.0501\n",
            "Epoch 268/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.9049 - val_loss: 86.9510\n",
            "Epoch 269/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.9957 - val_loss: 97.4214\n",
            "Epoch 270/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.8301 - val_loss: 116.7694\n",
            "Epoch 271/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.7528 - val_loss: 109.8648\n",
            "Epoch 272/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.7663 - val_loss: 89.7382\n",
            "Epoch 273/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.9384 - val_loss: 90.8862\n",
            "Epoch 274/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.6068 - val_loss: 141.8944\n",
            "Epoch 275/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.7550 - val_loss: 89.5013\n",
            "Epoch 276/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.5620 - val_loss: 90.8395\n",
            "Epoch 277/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.6412 - val_loss: 91.5839\n",
            "Epoch 278/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.6632 - val_loss: 88.8073\n",
            "Epoch 279/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.5163 - val_loss: 134.4163\n",
            "Epoch 280/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.6476 - val_loss: 91.6810\n",
            "Epoch 281/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.7461 - val_loss: 103.8512\n",
            "Epoch 282/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.6227 - val_loss: 104.8466\n",
            "Epoch 283/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.7430 - val_loss: 94.5495\n",
            "Epoch 284/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.6316 - val_loss: 86.3720\n",
            "Epoch 285/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.6270 - val_loss: 91.0625\n",
            "Epoch 286/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 75.7070 - val_loss: 95.4064\n",
            "Epoch 287/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 75.4722 - val_loss: 89.4359\n",
            "Epoch 288/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.3318 - val_loss: 111.0790\n",
            "Epoch 289/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.6622 - val_loss: 87.1652\n",
            "Epoch 290/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.5512 - val_loss: 92.7262\n",
            "Epoch 291/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4902 - val_loss: 93.9511\n",
            "Epoch 292/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.6814 - val_loss: 102.5523\n",
            "Epoch 293/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.3443 - val_loss: 102.8134\n",
            "Epoch 294/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.4129 - val_loss: 94.4920\n",
            "Epoch 295/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4050 - val_loss: 86.9637\n",
            "Epoch 296/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4242 - val_loss: 86.2151\n",
            "Epoch 297/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.4916 - val_loss: 90.0994\n",
            "Epoch 298/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4345 - val_loss: 88.5795\n",
            "Epoch 299/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.3874 - val_loss: 91.0466\n",
            "Epoch 300/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.5709 - val_loss: 96.8156\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "075430b7-b039-42e6-a401-a942ff493e7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  3.2391034353334986 \n",
            "MAE:  7.521831446196286 \n",
            "SD:  9.29106230843575\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "145bfbc3-feb4-4da6-99be-9cbeb85c61bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "su2Sj5jZCSwH",
        "outputId": "3a9ebcca-af04-40ff-8bc9-2dc2f4865c2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,465\n",
            "Trainable params: 2,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kPRh6v-mCSwH",
        "outputId": "638f16f4-982b-4ef4-f9d7-92b089b2227c",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 8094.6074 - val_loss: 3142.1948\n",
            "Epoch 2/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 697.9283 - val_loss: 190.3388\n",
            "Epoch 3/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 117.5527 - val_loss: 136.6776\n",
            "Epoch 4/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 106.8367 - val_loss: 194.1002\n",
            "Epoch 5/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 102.2402 - val_loss: 250.9733\n",
            "Epoch 6/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 99.3144 - val_loss: 213.1382\n",
            "Epoch 7/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 97.0159 - val_loss: 111.8978\n",
            "Epoch 8/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 95.2493 - val_loss: 172.9477\n",
            "Epoch 9/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 93.8484 - val_loss: 129.0473\n",
            "Epoch 10/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 92.3526 - val_loss: 103.2104\n",
            "Epoch 11/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 91.2424 - val_loss: 158.9622\n",
            "Epoch 12/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 90.1697 - val_loss: 118.5393\n",
            "Epoch 13/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 89.4803 - val_loss: 120.4800\n",
            "Epoch 14/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 88.8878 - val_loss: 135.3543\n",
            "Epoch 15/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 88.3290 - val_loss: 112.3130\n",
            "Epoch 16/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 87.9755 - val_loss: 142.6561\n",
            "Epoch 17/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 87.6607 - val_loss: 102.2051\n",
            "Epoch 18/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 87.2048 - val_loss: 113.1260\n",
            "Epoch 19/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 86.8988 - val_loss: 97.8270\n",
            "Epoch 20/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 86.6453 - val_loss: 110.5290\n",
            "Epoch 21/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 86.0815 - val_loss: 97.0813\n",
            "Epoch 22/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 86.0030 - val_loss: 181.3320\n",
            "Epoch 23/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 85.5080 - val_loss: 100.3642\n",
            "Epoch 24/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 85.2753 - val_loss: 102.2733\n",
            "Epoch 25/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 85.3727 - val_loss: 95.7362\n",
            "Epoch 26/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 84.8730 - val_loss: 111.3917\n",
            "Epoch 27/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 84.7365 - val_loss: 109.8687\n",
            "Epoch 28/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 84.3604 - val_loss: 123.8576\n",
            "Epoch 29/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 84.2143 - val_loss: 95.0414\n",
            "Epoch 30/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 84.0492 - val_loss: 100.6258\n",
            "Epoch 31/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 83.8263 - val_loss: 117.1553\n",
            "Epoch 32/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 83.7801 - val_loss: 108.6895\n",
            "Epoch 33/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 83.4730 - val_loss: 128.4334\n",
            "Epoch 34/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 83.4586 - val_loss: 95.6087\n",
            "Epoch 35/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 83.3874 - val_loss: 122.3443\n",
            "Epoch 36/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 83.1928 - val_loss: 113.4047\n",
            "Epoch 37/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 83.0011 - val_loss: 107.2714\n",
            "Epoch 38/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 82.8830 - val_loss: 228.2878\n",
            "Epoch 39/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 82.4939 - val_loss: 103.0647\n",
            "Epoch 40/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 82.5418 - val_loss: 167.6817\n",
            "Epoch 41/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 82.4114 - val_loss: 99.3359\n",
            "Epoch 42/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 82.2706 - val_loss: 107.5468\n",
            "Epoch 43/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 81.9139 - val_loss: 104.6747\n",
            "Epoch 44/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 81.9718 - val_loss: 105.5286\n",
            "Epoch 45/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 81.9421 - val_loss: 92.2147\n",
            "Epoch 46/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 81.7215 - val_loss: 99.2360\n",
            "Epoch 47/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.5706 - val_loss: 118.0395\n",
            "Epoch 48/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.6247 - val_loss: 125.7166\n",
            "Epoch 49/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 81.6299 - val_loss: 115.8368\n",
            "Epoch 50/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.5237 - val_loss: 94.1635\n",
            "Epoch 51/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 81.3314 - val_loss: 104.5229\n",
            "Epoch 52/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 80.9078 - val_loss: 111.7188\n",
            "Epoch 53/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.1001 - val_loss: 169.4582\n",
            "Epoch 54/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 81.0948 - val_loss: 103.8952\n",
            "Epoch 55/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 80.8329 - val_loss: 146.7945\n",
            "Epoch 56/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.9380 - val_loss: 109.2069\n",
            "Epoch 57/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 80.7845 - val_loss: 91.2314\n",
            "Epoch 58/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 80.6067 - val_loss: 126.8249\n",
            "Epoch 59/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.5216 - val_loss: 90.3063\n",
            "Epoch 60/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 80.4379 - val_loss: 91.4514\n",
            "Epoch 61/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.4035 - val_loss: 98.2729\n",
            "Epoch 62/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.1895 - val_loss: 99.9623\n",
            "Epoch 63/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.3790 - val_loss: 120.8252\n",
            "Epoch 64/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.2010 - val_loss: 120.3125\n",
            "Epoch 65/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.0976 - val_loss: 90.5115\n",
            "Epoch 66/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.0137 - val_loss: 129.3612\n",
            "Epoch 67/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 79.7754 - val_loss: 93.0115\n",
            "Epoch 68/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.6799 - val_loss: 104.3538\n",
            "Epoch 69/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 79.8879 - val_loss: 89.3158\n",
            "Epoch 70/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 79.6936 - val_loss: 157.3027\n",
            "Epoch 71/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 79.8812 - val_loss: 94.3131\n",
            "Epoch 72/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 79.5536 - val_loss: 113.9437\n",
            "Epoch 73/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 79.5807 - val_loss: 106.7964\n",
            "Epoch 74/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.3049 - val_loss: 93.2604\n",
            "Epoch 75/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 79.4149 - val_loss: 92.9534\n",
            "Epoch 76/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.2776 - val_loss: 115.2601\n",
            "Epoch 77/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.1507 - val_loss: 104.0534\n",
            "Epoch 78/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.2244 - val_loss: 188.6290\n",
            "Epoch 79/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 79.1084 - val_loss: 101.0734\n",
            "Epoch 80/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.9783 - val_loss: 101.7003\n",
            "Epoch 81/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.0219 - val_loss: 119.6361\n",
            "Epoch 82/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 79.0749 - val_loss: 92.0325\n",
            "Epoch 83/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.6610 - val_loss: 130.8472\n",
            "Epoch 84/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.9359 - val_loss: 90.7308\n",
            "Epoch 85/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.8342 - val_loss: 107.5563\n",
            "Epoch 86/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 78.8436 - val_loss: 92.0226\n",
            "Epoch 87/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.5416 - val_loss: 91.4075\n",
            "Epoch 88/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.5826 - val_loss: 112.5144\n",
            "Epoch 89/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.8161 - val_loss: 112.3316\n",
            "Epoch 90/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.4908 - val_loss: 108.8675\n",
            "Epoch 91/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.5608 - val_loss: 90.2384\n",
            "Epoch 92/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.6363 - val_loss: 97.8083\n",
            "Epoch 93/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.6195 - val_loss: 89.0483\n",
            "Epoch 94/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.3895 - val_loss: 91.5810\n",
            "Epoch 95/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.5109 - val_loss: 89.6785\n",
            "Epoch 96/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.2563 - val_loss: 99.9296\n",
            "Epoch 97/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.3364 - val_loss: 89.9873\n",
            "Epoch 98/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.3223 - val_loss: 105.8188\n",
            "Epoch 99/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.4036 - val_loss: 85.5895\n",
            "Epoch 100/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.1723 - val_loss: 95.9836\n",
            "Epoch 101/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.3386 - val_loss: 90.0074\n",
            "Epoch 102/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.1625 - val_loss: 89.1128\n",
            "Epoch 103/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.9694 - val_loss: 90.4324\n",
            "Epoch 104/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.1429 - val_loss: 114.6228\n",
            "Epoch 105/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.1590 - val_loss: 91.2588\n",
            "Epoch 106/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.8358 - val_loss: 96.7752\n",
            "Epoch 107/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.8370 - val_loss: 102.3485\n",
            "Epoch 108/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.9287 - val_loss: 94.2696\n",
            "Epoch 109/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.8121 - val_loss: 91.7932\n",
            "Epoch 110/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.0475 - val_loss: 130.7049\n",
            "Epoch 111/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.8658 - val_loss: 96.9988\n",
            "Epoch 112/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.2555 - val_loss: 104.1059\n",
            "Epoch 113/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.8619 - val_loss: 109.9449\n",
            "Epoch 114/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.6962 - val_loss: 101.5591\n",
            "Epoch 115/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.7053 - val_loss: 147.8083\n",
            "Epoch 116/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.6309 - val_loss: 86.2902\n",
            "Epoch 117/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5501 - val_loss: 86.6059\n",
            "Epoch 118/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 77.7355 - val_loss: 96.4525\n",
            "Epoch 119/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.7101 - val_loss: 91.1319\n",
            "Epoch 120/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.7300 - val_loss: 107.8506\n",
            "Epoch 121/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.4353 - val_loss: 86.6706\n",
            "Epoch 122/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5683 - val_loss: 119.3995\n",
            "Epoch 123/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.6591 - val_loss: 158.2104\n",
            "Epoch 124/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.3337 - val_loss: 88.8254\n",
            "Epoch 125/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.4665 - val_loss: 103.8175\n",
            "Epoch 126/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1887 - val_loss: 85.1002\n",
            "Epoch 127/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.4049 - val_loss: 97.3583\n",
            "Epoch 128/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.5663 - val_loss: 142.8413\n",
            "Epoch 129/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.4790 - val_loss: 85.8503\n",
            "Epoch 130/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.4414 - val_loss: 131.3711\n",
            "Epoch 131/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.4545 - val_loss: 95.6964\n",
            "Epoch 132/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 77.4135 - val_loss: 109.9069\n",
            "Epoch 133/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2357 - val_loss: 100.7189\n",
            "Epoch 134/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1473 - val_loss: 98.6711\n",
            "Epoch 135/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.1904 - val_loss: 103.1333\n",
            "Epoch 136/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.0558 - val_loss: 89.2955\n",
            "Epoch 137/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.2126 - val_loss: 94.7604\n",
            "Epoch 138/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 77.1093 - val_loss: 92.6141\n",
            "Epoch 139/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1176 - val_loss: 89.7445\n",
            "Epoch 140/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.0137 - val_loss: 87.2681\n",
            "Epoch 141/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1772 - val_loss: 115.4397\n",
            "Epoch 142/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.0128 - val_loss: 97.1383\n",
            "Epoch 143/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.9295 - val_loss: 90.3515\n",
            "Epoch 144/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.2666 - val_loss: 88.2312\n",
            "Epoch 145/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.9202 - val_loss: 96.3278\n",
            "Epoch 146/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.0337 - val_loss: 92.3684\n",
            "Epoch 147/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.7262 - val_loss: 87.3837\n",
            "Epoch 148/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 76.9401 - val_loss: 92.1273\n",
            "Epoch 149/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.0773 - val_loss: 99.7770\n",
            "Epoch 150/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.9753 - val_loss: 99.7719\n",
            "Epoch 151/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 76.9447 - val_loss: 88.2719\n",
            "Epoch 152/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.0112 - val_loss: 105.2567\n",
            "Epoch 153/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.9231 - val_loss: 101.5197\n",
            "Epoch 154/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.8313 - val_loss: 92.4905\n",
            "Epoch 155/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.8049 - val_loss: 87.4340\n",
            "Epoch 156/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 76.6457 - val_loss: 91.9440\n",
            "Epoch 157/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.8912 - val_loss: 95.7853\n",
            "Epoch 158/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.6904 - val_loss: 90.8439\n",
            "Epoch 159/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.8707 - val_loss: 93.8392\n",
            "Epoch 160/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.7908 - val_loss: 85.4832\n",
            "Epoch 161/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.7654 - val_loss: 92.9244\n",
            "Epoch 162/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.7806 - val_loss: 92.1455\n",
            "Epoch 163/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.7037 - val_loss: 101.1045\n",
            "Epoch 164/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 76.6741 - val_loss: 99.9553\n",
            "Epoch 165/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.4898 - val_loss: 94.6848\n",
            "Epoch 166/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.6956 - val_loss: 102.8932\n",
            "Epoch 167/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.6070 - val_loss: 97.4745\n",
            "Epoch 168/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.6810 - val_loss: 92.5005\n",
            "Epoch 169/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 76.5023 - val_loss: 90.3365\n",
            "Epoch 170/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 76.7729 - val_loss: 119.3228\n",
            "Epoch 171/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.6676 - val_loss: 92.4658\n",
            "Epoch 172/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.6751 - val_loss: 99.1564\n",
            "Epoch 173/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.7863 - val_loss: 89.5149\n",
            "Epoch 174/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 76.5180 - val_loss: 90.0329\n",
            "Epoch 175/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.4888 - val_loss: 101.7452\n",
            "Epoch 176/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.4710 - val_loss: 91.6037\n",
            "Epoch 177/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 76.4591 - val_loss: 92.1440\n",
            "Epoch 178/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.5837 - val_loss: 90.8278\n",
            "Epoch 179/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.4262 - val_loss: 91.0801\n",
            "Epoch 180/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.3823 - val_loss: 89.1710\n",
            "Epoch 181/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.4124 - val_loss: 91.1247\n",
            "Epoch 182/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.4247 - val_loss: 89.3009\n",
            "Epoch 183/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.5000 - val_loss: 104.9636\n",
            "Epoch 184/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.6075 - val_loss: 86.0109\n",
            "Epoch 185/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.4452 - val_loss: 85.9075\n",
            "Epoch 186/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.6966 - val_loss: 126.9140\n",
            "Epoch 187/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.4100 - val_loss: 123.9894\n",
            "Epoch 188/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 76.1064 - val_loss: 85.6028\n",
            "Epoch 189/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.2984 - val_loss: 87.1864\n",
            "Epoch 190/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.5050 - val_loss: 133.7560\n",
            "Epoch 191/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.3054 - val_loss: 106.4046\n",
            "Epoch 192/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.3540 - val_loss: 109.8960\n",
            "Epoch 193/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.5181 - val_loss: 96.2269\n",
            "Epoch 194/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.5890 - val_loss: 91.1726\n",
            "Epoch 195/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.4906 - val_loss: 91.9194\n",
            "Epoch 196/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.4329 - val_loss: 152.1512\n",
            "Epoch 197/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.4656 - val_loss: 94.8430\n",
            "Epoch 198/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.3908 - val_loss: 85.6945\n",
            "Epoch 199/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.3193 - val_loss: 93.9323\n",
            "Epoch 200/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.2599 - val_loss: 91.0087\n",
            "Epoch 201/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.2299 - val_loss: 84.4414\n",
            "Epoch 202/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.2067 - val_loss: 101.9023\n",
            "Epoch 203/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.0867 - val_loss: 105.4070\n",
            "Epoch 204/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.2720 - val_loss: 95.5407\n",
            "Epoch 205/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.3004 - val_loss: 86.4312\n",
            "Epoch 206/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.2209 - val_loss: 92.4282\n",
            "Epoch 207/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 76.2167 - val_loss: 87.9435\n",
            "Epoch 208/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.0818 - val_loss: 95.0652\n",
            "Epoch 209/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.3766 - val_loss: 98.9649\n",
            "Epoch 210/300\n",
            "1319/1319 [==============================] - 9s 7ms/step - loss: 76.2368 - val_loss: 112.8485\n",
            "Epoch 211/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.2949 - val_loss: 106.7478\n",
            "Epoch 212/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.0851 - val_loss: 92.7881\n",
            "Epoch 213/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.1495 - val_loss: 110.8534\n",
            "Epoch 214/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 76.1130 - val_loss: 101.9283\n",
            "Epoch 215/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.9820 - val_loss: 94.5136\n",
            "Epoch 216/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.8794 - val_loss: 99.8853\n",
            "Epoch 217/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.2378 - val_loss: 94.9981\n",
            "Epoch 218/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 75.9524 - val_loss: 91.4296\n",
            "Epoch 219/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 76.0689 - val_loss: 86.0643\n",
            "Epoch 220/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.0815 - val_loss: 124.1087\n",
            "Epoch 221/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.0140 - val_loss: 89.0978\n",
            "Epoch 222/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.9696 - val_loss: 92.2698\n",
            "Epoch 223/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.9726 - val_loss: 90.6307\n",
            "Epoch 224/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.0598 - val_loss: 90.6097\n",
            "Epoch 225/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.1187 - val_loss: 91.6285\n",
            "Epoch 226/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.2091 - val_loss: 88.4133\n",
            "Epoch 227/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.8574 - val_loss: 105.9655\n",
            "Epoch 228/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.9245 - val_loss: 89.6191\n",
            "Epoch 229/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 76.0027 - val_loss: 99.3269\n",
            "Epoch 230/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.9330 - val_loss: 93.2768\n",
            "Epoch 231/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.8988 - val_loss: 92.6591\n",
            "Epoch 232/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.0200 - val_loss: 91.8950\n",
            "Epoch 233/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.1433 - val_loss: 112.5127\n",
            "Epoch 234/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 75.9314 - val_loss: 94.1828\n",
            "Epoch 235/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.7336 - val_loss: 122.3579\n",
            "Epoch 236/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.9629 - val_loss: 96.8911\n",
            "Epoch 237/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.8030 - val_loss: 89.8506\n",
            "Epoch 238/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.9727 - val_loss: 97.9312\n",
            "Epoch 239/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.9954 - val_loss: 110.8389\n",
            "Epoch 240/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.0677 - val_loss: 112.6978\n",
            "Epoch 241/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.9368 - val_loss: 102.5983\n",
            "Epoch 242/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.9585 - val_loss: 85.6785\n",
            "Epoch 243/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.9827 - val_loss: 97.7850\n",
            "Epoch 244/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.6717 - val_loss: 86.4268\n",
            "Epoch 245/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.8804 - val_loss: 92.0262\n",
            "Epoch 246/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.6562 - val_loss: 86.1846\n",
            "Epoch 247/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.8045 - val_loss: 98.3929\n",
            "Epoch 248/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.6955 - val_loss: 172.7780\n",
            "Epoch 249/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.8027 - val_loss: 90.0499\n",
            "Epoch 250/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.8013 - val_loss: 89.1800\n",
            "Epoch 251/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.6923 - val_loss: 88.2615\n",
            "Epoch 252/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.6910 - val_loss: 127.9018\n",
            "Epoch 253/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.8494 - val_loss: 88.1661\n",
            "Epoch 254/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.7317 - val_loss: 86.5919\n",
            "Epoch 255/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.5475 - val_loss: 96.1507\n",
            "Epoch 256/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.6756 - val_loss: 88.0940\n",
            "Epoch 257/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.7742 - val_loss: 87.3160\n",
            "Epoch 258/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.8114 - val_loss: 84.9372\n",
            "Epoch 259/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.9576 - val_loss: 102.4933\n",
            "Epoch 260/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.0399 - val_loss: 94.0839\n",
            "Epoch 261/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.6460 - val_loss: 89.6964\n",
            "Epoch 262/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.6945 - val_loss: 112.7199\n",
            "Epoch 263/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.7954 - val_loss: 88.9233\n",
            "Epoch 264/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.7609 - val_loss: 88.1677\n",
            "Epoch 265/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.5585 - val_loss: 87.2912\n",
            "Epoch 266/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.4953 - val_loss: 130.5726\n",
            "Epoch 267/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.5583 - val_loss: 88.4588\n",
            "Epoch 268/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.6944 - val_loss: 97.0344\n",
            "Epoch 269/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.7779 - val_loss: 96.8645\n",
            "Epoch 270/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.5382 - val_loss: 87.7760\n",
            "Epoch 271/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.5365 - val_loss: 85.0508\n",
            "Epoch 272/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 75.8948 - val_loss: 85.3186\n",
            "Epoch 273/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 75.5904 - val_loss: 102.6229\n",
            "Epoch 274/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.7252 - val_loss: 92.6389\n",
            "Epoch 275/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.5409 - val_loss: 91.8594\n",
            "Epoch 276/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.5941 - val_loss: 85.7957\n",
            "Epoch 277/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4903 - val_loss: 91.9861\n",
            "Epoch 278/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.5535 - val_loss: 94.9461\n",
            "Epoch 279/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.5856 - val_loss: 89.9067\n",
            "Epoch 280/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.5340 - val_loss: 88.1972\n",
            "Epoch 281/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.6660 - val_loss: 87.0069\n",
            "Epoch 282/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.6107 - val_loss: 103.0262\n",
            "Epoch 283/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4342 - val_loss: 90.3896\n",
            "Epoch 284/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.6333 - val_loss: 109.1129\n",
            "Epoch 285/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4555 - val_loss: 89.1238\n",
            "Epoch 286/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.5096 - val_loss: 99.9829\n",
            "Epoch 287/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.6797 - val_loss: 84.0909\n",
            "Epoch 288/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.6313 - val_loss: 85.8287\n",
            "Epoch 289/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.6936 - val_loss: 88.4830\n",
            "Epoch 290/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4986 - val_loss: 97.5329\n",
            "Epoch 291/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.5675 - val_loss: 93.7442\n",
            "Epoch 292/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.5972 - val_loss: 99.3583\n",
            "Epoch 293/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 75.6621 - val_loss: 84.3329\n",
            "Epoch 294/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.3945 - val_loss: 83.3403\n",
            "Epoch 295/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4556 - val_loss: 83.9776\n",
            "Epoch 296/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4184 - val_loss: 96.0005\n",
            "Epoch 297/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4695 - val_loss: 93.1676\n",
            "Epoch 298/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.4648 - val_loss: 90.3000\n",
            "Epoch 299/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.5659 - val_loss: 84.4980\n",
            "Epoch 300/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 75.5743 - val_loss: 98.8744\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "outputId": "79c664bb-48a4-4ac3-ec10-163f815d6c9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -3.2005011676796227 \n",
            "MAE:  7.645907672925668 \n",
            "SD:  9.4144108704464\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "outputId": "026bfa42-b742-4001-8f77-4803281476a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "outputId": "2159a6d1-4332-4106-86bc-337d306a89f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,465\n",
            "Trainable params: 2,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dcXAOqd2CTsa",
        "outputId": "2a8d1ba9-5e19-4ff6-f169-8ec6dffb9843",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 8709.8828 - val_loss: 3415.3940\n",
            "Epoch 2/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 1219.6869 - val_loss: 132.0130\n",
            "Epoch 3/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 119.3073 - val_loss: 151.6936\n",
            "Epoch 4/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 103.9409 - val_loss: 156.3703\n",
            "Epoch 5/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 99.8963 - val_loss: 127.1506\n",
            "Epoch 6/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 97.8048 - val_loss: 164.1357\n",
            "Epoch 7/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 96.1556 - val_loss: 154.1682\n",
            "Epoch 8/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 94.3305 - val_loss: 119.5150\n",
            "Epoch 9/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 92.3559 - val_loss: 125.0211\n",
            "Epoch 10/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 91.7616 - val_loss: 108.4343\n",
            "Epoch 11/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 90.7983 - val_loss: 120.1491\n",
            "Epoch 12/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 89.8220 - val_loss: 110.0504\n",
            "Epoch 13/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 89.3063 - val_loss: 122.9115\n",
            "Epoch 14/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 88.9827 - val_loss: 115.1093\n",
            "Epoch 15/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 88.4351 - val_loss: 120.8770\n",
            "Epoch 16/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 88.0124 - val_loss: 126.0042\n",
            "Epoch 17/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 87.2155 - val_loss: 107.6650\n",
            "Epoch 18/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 86.6821 - val_loss: 112.1532\n",
            "Epoch 19/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 86.6869 - val_loss: 129.5882\n",
            "Epoch 20/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 86.2972 - val_loss: 162.7457\n",
            "Epoch 21/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 85.6545 - val_loss: 104.8210\n",
            "Epoch 22/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 85.2781 - val_loss: 107.0393\n",
            "Epoch 23/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 85.0530 - val_loss: 154.2182\n",
            "Epoch 24/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 84.6441 - val_loss: 97.5268\n",
            "Epoch 25/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 84.4805 - val_loss: 98.3064\n",
            "Epoch 26/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 84.1390 - val_loss: 124.1218\n",
            "Epoch 27/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 84.0072 - val_loss: 187.5471\n",
            "Epoch 28/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 83.7255 - val_loss: 102.4773\n",
            "Epoch 29/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 83.6668 - val_loss: 107.7640\n",
            "Epoch 30/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 83.3217 - val_loss: 106.5184\n",
            "Epoch 31/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 83.2883 - val_loss: 204.9350\n",
            "Epoch 32/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 82.8708 - val_loss: 102.6288\n",
            "Epoch 33/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 83.0973 - val_loss: 107.4769\n",
            "Epoch 34/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 82.9007 - val_loss: 106.2375\n",
            "Epoch 35/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 82.8384 - val_loss: 105.4410\n",
            "Epoch 36/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 82.4671 - val_loss: 201.3177\n",
            "Epoch 37/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 82.4817 - val_loss: 106.0804\n",
            "Epoch 38/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 82.4528 - val_loss: 116.7949\n",
            "Epoch 39/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 82.1873 - val_loss: 109.9951\n",
            "Epoch 40/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 81.9793 - val_loss: 97.2758\n",
            "Epoch 41/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 81.7663 - val_loss: 106.1404\n",
            "Epoch 42/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 81.8774 - val_loss: 92.8002\n",
            "Epoch 43/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 81.6933 - val_loss: 102.7759\n",
            "Epoch 44/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.6686 - val_loss: 98.7377\n",
            "Epoch 45/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.7499 - val_loss: 101.8974\n",
            "Epoch 46/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.5162 - val_loss: 106.3326\n",
            "Epoch 47/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.4523 - val_loss: 110.2771\n",
            "Epoch 48/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.2455 - val_loss: 103.4367\n",
            "Epoch 49/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 81.3872 - val_loss: 101.9250\n",
            "Epoch 50/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 81.3964 - val_loss: 110.9543\n",
            "Epoch 51/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.2360 - val_loss: 115.8789\n",
            "Epoch 52/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.4140 - val_loss: 119.1116\n",
            "Epoch 53/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 80.9461 - val_loss: 105.1636\n",
            "Epoch 54/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 81.1468 - val_loss: 111.6151\n",
            "Epoch 55/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.7555 - val_loss: 100.5133\n",
            "Epoch 56/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.1812 - val_loss: 109.0724\n",
            "Epoch 57/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.9608 - val_loss: 107.2826\n",
            "Epoch 58/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 81.0733 - val_loss: 110.3343\n",
            "Epoch 59/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 81.0150 - val_loss: 100.6622\n",
            "Epoch 60/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 80.7395 - val_loss: 121.7235\n",
            "Epoch 61/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 80.3694 - val_loss: 95.1393\n",
            "Epoch 62/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 80.3956 - val_loss: 93.0781\n",
            "Epoch 63/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.5821 - val_loss: 94.3009\n",
            "Epoch 64/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 80.5570 - val_loss: 114.6978\n",
            "Epoch 65/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 80.2302 - val_loss: 94.6725\n",
            "Epoch 66/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.4182 - val_loss: 96.7657\n",
            "Epoch 67/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 80.1947 - val_loss: 99.8091\n",
            "Epoch 68/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.5863 - val_loss: 91.9840\n",
            "Epoch 69/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.1214 - val_loss: 101.8398\n",
            "Epoch 70/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.0683 - val_loss: 139.8851\n",
            "Epoch 71/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 80.2212 - val_loss: 145.4534\n",
            "Epoch 72/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 79.9377 - val_loss: 97.8639\n",
            "Epoch 73/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.2036 - val_loss: 97.9389\n",
            "Epoch 74/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 79.7884 - val_loss: 100.9685\n",
            "Epoch 75/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.2125 - val_loss: 146.1593\n",
            "Epoch 76/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.9336 - val_loss: 111.7449\n",
            "Epoch 77/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 80.0017 - val_loss: 105.1348\n",
            "Epoch 78/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 79.8449 - val_loss: 131.3442\n",
            "Epoch 79/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 79.5967 - val_loss: 109.0216\n",
            "Epoch 80/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.7581 - val_loss: 120.9136\n",
            "Epoch 81/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.8457 - val_loss: 97.4815\n",
            "Epoch 82/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.6227 - val_loss: 95.7182\n",
            "Epoch 83/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.6485 - val_loss: 94.6562\n",
            "Epoch 84/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.8792 - val_loss: 116.8216\n",
            "Epoch 85/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 79.4951 - val_loss: 107.8531\n",
            "Epoch 86/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.5316 - val_loss: 107.1491\n",
            "Epoch 87/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 79.5406 - val_loss: 103.1419\n",
            "Epoch 88/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.4788 - val_loss: 114.2050\n",
            "Epoch 89/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 79.7181 - val_loss: 124.5373\n",
            "Epoch 90/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.1555 - val_loss: 104.8242\n",
            "Epoch 91/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.3990 - val_loss: 189.1297\n",
            "Epoch 92/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.4261 - val_loss: 100.6938\n",
            "Epoch 93/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 79.3524 - val_loss: 95.5114\n",
            "Epoch 94/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.4343 - val_loss: 96.1460\n",
            "Epoch 95/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 79.0362 - val_loss: 94.3383\n",
            "Epoch 96/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 79.3630 - val_loss: 91.0421\n",
            "Epoch 97/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 79.3013 - val_loss: 127.3552\n",
            "Epoch 98/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.5375 - val_loss: 135.3839\n",
            "Epoch 99/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.2775 - val_loss: 104.3076\n",
            "Epoch 100/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 79.1400 - val_loss: 95.9207\n",
            "Epoch 101/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.3429 - val_loss: 111.8872\n",
            "Epoch 102/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 79.2439 - val_loss: 101.4193\n",
            "Epoch 103/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.3148 - val_loss: 149.3929\n",
            "Epoch 104/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.1793 - val_loss: 95.3811\n",
            "Epoch 105/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.2792 - val_loss: 96.1075\n",
            "Epoch 106/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.9076 - val_loss: 98.7595\n",
            "Epoch 107/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 79.0580 - val_loss: 100.6856\n",
            "Epoch 108/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.7138 - val_loss: 104.8671\n",
            "Epoch 109/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.0901 - val_loss: 91.3335\n",
            "Epoch 110/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.9272 - val_loss: 88.6538\n",
            "Epoch 111/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 79.0269 - val_loss: 97.2209\n",
            "Epoch 112/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.9312 - val_loss: 92.3206\n",
            "Epoch 113/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.7908 - val_loss: 109.7471\n",
            "Epoch 114/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.9000 - val_loss: 120.4858\n",
            "Epoch 115/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.6195 - val_loss: 91.3209\n",
            "Epoch 116/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.5609 - val_loss: 92.9062\n",
            "Epoch 117/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.8373 - val_loss: 89.1326\n",
            "Epoch 118/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.7579 - val_loss: 97.6192\n",
            "Epoch 119/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.7022 - val_loss: 94.2783\n",
            "Epoch 120/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.4983 - val_loss: 91.8582\n",
            "Epoch 121/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.5154 - val_loss: 90.1702\n",
            "Epoch 122/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.4392 - val_loss: 107.5701\n",
            "Epoch 123/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.7699 - val_loss: 113.4368\n",
            "Epoch 124/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.6857 - val_loss: 98.6120\n",
            "Epoch 125/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.6668 - val_loss: 108.3040\n",
            "Epoch 126/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 78.6669 - val_loss: 92.5862\n",
            "Epoch 127/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.4806 - val_loss: 110.7354\n",
            "Epoch 128/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.4572 - val_loss: 86.1681\n",
            "Epoch 129/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.4201 - val_loss: 99.2958\n",
            "Epoch 130/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.4681 - val_loss: 100.4675\n",
            "Epoch 131/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.6068 - val_loss: 98.3506\n",
            "Epoch 132/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.5906 - val_loss: 94.2801\n",
            "Epoch 133/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.5120 - val_loss: 122.3612\n",
            "Epoch 134/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.4892 - val_loss: 116.9721\n",
            "Epoch 135/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.3691 - val_loss: 90.6188\n",
            "Epoch 136/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.2522 - val_loss: 98.6434\n",
            "Epoch 137/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.5289 - val_loss: 109.3933\n",
            "Epoch 138/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.1131 - val_loss: 99.1994\n",
            "Epoch 139/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.5212 - val_loss: 106.2484\n",
            "Epoch 140/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.2533 - val_loss: 89.0367\n",
            "Epoch 141/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.2158 - val_loss: 88.8301\n",
            "Epoch 142/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.2231 - val_loss: 90.9298\n",
            "Epoch 143/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.4919 - val_loss: 127.0731\n",
            "Epoch 144/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.1375 - val_loss: 129.5849\n",
            "Epoch 145/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.2493 - val_loss: 103.0879\n",
            "Epoch 146/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.1090 - val_loss: 91.6975\n",
            "Epoch 147/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.0538 - val_loss: 112.8017\n",
            "Epoch 148/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.1606 - val_loss: 94.6917\n",
            "Epoch 149/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.1892 - val_loss: 88.8672\n",
            "Epoch 150/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.8548 - val_loss: 110.9693\n",
            "Epoch 151/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.0734 - val_loss: 104.7687\n",
            "Epoch 152/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 78.0089 - val_loss: 92.0002\n",
            "Epoch 153/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.1266 - val_loss: 94.6934\n",
            "Epoch 154/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.3686 - val_loss: 91.8355\n",
            "Epoch 155/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.0130 - val_loss: 92.9496\n",
            "Epoch 156/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.8122 - val_loss: 100.9653\n",
            "Epoch 157/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.9896 - val_loss: 96.3236\n",
            "Epoch 158/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.0320 - val_loss: 91.4793\n",
            "Epoch 159/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.9608 - val_loss: 88.9287\n",
            "Epoch 160/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.1723 - val_loss: 138.0067\n",
            "Epoch 161/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.9971 - val_loss: 98.7775\n",
            "Epoch 162/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.0571 - val_loss: 104.5911\n",
            "Epoch 163/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.7987 - val_loss: 92.0736\n",
            "Epoch 164/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.0604 - val_loss: 96.4970\n",
            "Epoch 165/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.7646 - val_loss: 93.1565\n",
            "Epoch 166/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.8064 - val_loss: 142.8959\n",
            "Epoch 167/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 78.1116 - val_loss: 92.0105\n",
            "Epoch 168/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.9734 - val_loss: 102.7204\n",
            "Epoch 169/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.9967 - val_loss: 99.3136\n",
            "Epoch 170/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.8077 - val_loss: 119.5379\n",
            "Epoch 171/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.8184 - val_loss: 103.4702\n",
            "Epoch 172/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.8603 - val_loss: 113.9285\n",
            "Epoch 173/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.9024 - val_loss: 95.5149\n",
            "Epoch 174/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.7269 - val_loss: 114.6540\n",
            "Epoch 175/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.8026 - val_loss: 92.1566\n",
            "Epoch 176/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.7175 - val_loss: 89.4759\n",
            "Epoch 177/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.8814 - val_loss: 107.8166\n",
            "Epoch 178/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.6472 - val_loss: 94.2930\n",
            "Epoch 179/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5674 - val_loss: 94.0845\n",
            "Epoch 180/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.8642 - val_loss: 104.1428\n",
            "Epoch 181/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.6876 - val_loss: 94.7856\n",
            "Epoch 182/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5562 - val_loss: 94.7717\n",
            "Epoch 183/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 77.9199 - val_loss: 157.2698\n",
            "Epoch 184/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 78.1470 - val_loss: 93.8579\n",
            "Epoch 185/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5883 - val_loss: 98.2242\n",
            "Epoch 186/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.7927 - val_loss: 100.7820\n",
            "Epoch 187/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.7734 - val_loss: 102.1213\n",
            "Epoch 188/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.4766 - val_loss: 98.5134\n",
            "Epoch 189/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5134 - val_loss: 98.0940\n",
            "Epoch 190/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5509 - val_loss: 107.3399\n",
            "Epoch 191/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.5821 - val_loss: 92.0757\n",
            "Epoch 192/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.8242 - val_loss: 129.8961\n",
            "Epoch 193/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5814 - val_loss: 99.5525\n",
            "Epoch 194/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.6705 - val_loss: 85.8622\n",
            "Epoch 195/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.6436 - val_loss: 97.5202\n",
            "Epoch 196/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.6152 - val_loss: 103.0839\n",
            "Epoch 197/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5802 - val_loss: 115.5293\n",
            "Epoch 198/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5507 - val_loss: 98.0318\n",
            "Epoch 199/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.6106 - val_loss: 98.7305\n",
            "Epoch 200/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5844 - val_loss: 96.8194\n",
            "Epoch 201/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.6218 - val_loss: 87.2573\n",
            "Epoch 202/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.9132 - val_loss: 91.0031\n",
            "Epoch 203/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.6424 - val_loss: 99.6826\n",
            "Epoch 204/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.6225 - val_loss: 100.8668\n",
            "Epoch 205/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.6194 - val_loss: 103.9211\n",
            "Epoch 206/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.4957 - val_loss: 97.4632\n",
            "Epoch 207/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 77.3558 - val_loss: 106.2028\n",
            "Epoch 208/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.5798 - val_loss: 88.6659\n",
            "Epoch 209/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.3093 - val_loss: 119.6715\n",
            "Epoch 210/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5168 - val_loss: 88.1790\n",
            "Epoch 211/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5431 - val_loss: 125.8227\n",
            "Epoch 212/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.4282 - val_loss: 104.0676\n",
            "Epoch 213/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.4976 - val_loss: 90.8772\n",
            "Epoch 214/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.3170 - val_loss: 92.0872\n",
            "Epoch 215/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.4547 - val_loss: 96.7589\n",
            "Epoch 216/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.4001 - val_loss: 93.8768\n",
            "Epoch 217/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.3921 - val_loss: 103.2223\n",
            "Epoch 218/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.3915 - val_loss: 87.5063\n",
            "Epoch 219/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.6031 - val_loss: 118.9133\n",
            "Epoch 220/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2807 - val_loss: 90.9416\n",
            "Epoch 221/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.3565 - val_loss: 101.0280\n",
            "Epoch 222/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.6090 - val_loss: 109.5997\n",
            "Epoch 223/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2757 - val_loss: 92.3131\n",
            "Epoch 224/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2424 - val_loss: 92.7845\n",
            "Epoch 225/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.4727 - val_loss: 93.9252\n",
            "Epoch 226/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.4178 - val_loss: 108.4042\n",
            "Epoch 227/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1697 - val_loss: 86.3060\n",
            "Epoch 228/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2530 - val_loss: 95.5494\n",
            "Epoch 229/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.3614 - val_loss: 96.3239\n",
            "Epoch 230/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.3327 - val_loss: 93.3555\n",
            "Epoch 231/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1765 - val_loss: 86.8074\n",
            "Epoch 232/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.5070 - val_loss: 97.3475\n",
            "Epoch 233/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1714 - val_loss: 99.3820\n",
            "Epoch 234/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.3780 - val_loss: 89.1715\n",
            "Epoch 235/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2542 - val_loss: 93.8058\n",
            "Epoch 236/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.3860 - val_loss: 90.7740\n",
            "Epoch 237/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.6579 - val_loss: 92.8484\n",
            "Epoch 238/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 77.1176 - val_loss: 103.8188\n",
            "Epoch 239/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2722 - val_loss: 101.0423\n",
            "Epoch 240/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1484 - val_loss: 91.3130\n",
            "Epoch 241/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.3889 - val_loss: 101.0250\n",
            "Epoch 242/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2778 - val_loss: 91.2751\n",
            "Epoch 243/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1199 - val_loss: 137.8712\n",
            "Epoch 244/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.0712 - val_loss: 100.0543\n",
            "Epoch 245/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1308 - val_loss: 90.1318\n",
            "Epoch 246/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.0548 - val_loss: 119.5718\n",
            "Epoch 247/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2201 - val_loss: 92.9072\n",
            "Epoch 248/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.0436 - val_loss: 95.7491\n",
            "Epoch 249/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.0504 - val_loss: 95.6759\n",
            "Epoch 250/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 77.1227 - val_loss: 97.6257\n",
            "Epoch 251/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.3146 - val_loss: 117.2108\n",
            "Epoch 252/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.0002 - val_loss: 96.1139\n",
            "Epoch 253/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.0227 - val_loss: 101.9946\n",
            "Epoch 254/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.1977 - val_loss: 91.9223\n",
            "Epoch 255/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.0774 - val_loss: 102.5041\n",
            "Epoch 256/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.0970 - val_loss: 94.1022\n",
            "Epoch 257/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.0581 - val_loss: 100.1212\n",
            "Epoch 258/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1438 - val_loss: 90.0245\n",
            "Epoch 259/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.9215 - val_loss: 87.8090\n",
            "Epoch 260/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2095 - val_loss: 97.6131\n",
            "Epoch 261/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.0963 - val_loss: 92.8841\n",
            "Epoch 262/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.9782 - val_loss: 107.6394\n",
            "Epoch 263/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.8784 - val_loss: 105.9478\n",
            "Epoch 264/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.9909 - val_loss: 93.2346\n",
            "Epoch 265/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.1061 - val_loss: 107.2473\n",
            "Epoch 266/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.9585 - val_loss: 99.8374\n",
            "Epoch 267/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1938 - val_loss: 91.4888\n",
            "Epoch 268/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.3042 - val_loss: 88.7062\n",
            "Epoch 269/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.1186 - val_loss: 102.5438\n",
            "Epoch 270/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.0631 - val_loss: 98.1573\n",
            "Epoch 271/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2245 - val_loss: 96.7257\n",
            "Epoch 272/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2597 - val_loss: 95.3427\n",
            "Epoch 273/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1549 - val_loss: 101.6926\n",
            "Epoch 274/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.8952 - val_loss: 89.0796\n",
            "Epoch 275/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.9778 - val_loss: 90.1032\n",
            "Epoch 276/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.8945 - val_loss: 92.5574\n",
            "Epoch 277/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1234 - val_loss: 102.1731\n",
            "Epoch 278/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.4128 - val_loss: 108.1098\n",
            "Epoch 279/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.9306 - val_loss: 102.4885\n",
            "Epoch 280/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.0406 - val_loss: 90.2738\n",
            "Epoch 281/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.9826 - val_loss: 89.7359\n",
            "Epoch 282/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1349 - val_loss: 89.4366\n",
            "Epoch 283/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1041 - val_loss: 86.8031\n",
            "Epoch 284/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 77.1952 - val_loss: 95.8499\n",
            "Epoch 285/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.1199 - val_loss: 98.1646\n",
            "Epoch 286/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.9171 - val_loss: 91.4692\n",
            "Epoch 287/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.9041 - val_loss: 154.1758\n",
            "Epoch 288/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1092 - val_loss: 86.9967\n",
            "Epoch 289/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.9620 - val_loss: 124.8609\n",
            "Epoch 290/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.2395 - val_loss: 94.5864\n",
            "Epoch 291/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 77.0894 - val_loss: 109.5914\n",
            "Epoch 292/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.8991 - val_loss: 91.8819\n",
            "Epoch 293/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1643 - val_loss: 89.8849\n",
            "Epoch 294/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.9223 - val_loss: 88.9223\n",
            "Epoch 295/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.8628 - val_loss: 92.6154\n",
            "Epoch 296/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 77.1276 - val_loss: 99.6063\n",
            "Epoch 297/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.6551 - val_loss: 93.6268\n",
            "Epoch 298/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 76.9698 - val_loss: 97.1865\n",
            "Epoch 299/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.8197 - val_loss: 93.6793\n",
            "Epoch 300/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 76.8896 - val_loss: 93.7485\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "696v_fuFCTsa",
        "outputId": "60b7411b-960d-475f-a5d2-0a696d31ddbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  1.8541634356778889 \n",
            "MAE:  7.205197036531984 \n",
            "SD:  9.503184345413828\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "outputId": "42f1669a-eb74-4ad0-d8fe-ea2a6651b825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mdZF2osWCUQS",
        "outputId": "fe0a2d17-6b25-4b9b-dd7d-89fd616920e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  0.6309219011105883 \n",
            "Ensemble_std:  9.402885841431994\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "outputId": "6b25469f-1174-4811-b9ac-0bb30005d1d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_9 (Dense)             (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,465\n",
            "Trainable params: 2,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "outputId": "db91354b-31a7-4ea0-fac7-8c22dbe8e770",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 2725.7178 - val_loss: 678.0864\n",
            "Epoch 2/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 241.7119 - val_loss: 69.5144\n",
            "Epoch 3/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 40.6631 - val_loss: 57.5147\n",
            "Epoch 4/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 37.7491 - val_loss: 52.1373\n",
            "Epoch 5/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 36.1225 - val_loss: 75.3379\n",
            "Epoch 6/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 35.4723 - val_loss: 51.5021\n",
            "Epoch 7/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 34.8629 - val_loss: 92.1830\n",
            "Epoch 8/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 34.3396 - val_loss: 45.5683\n",
            "Epoch 9/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 33.9619 - val_loss: 36.8689\n",
            "Epoch 10/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 33.6244 - val_loss: 43.7716\n",
            "Epoch 11/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 33.2675 - val_loss: 69.4501\n",
            "Epoch 12/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 32.9375 - val_loss: 41.5800\n",
            "Epoch 13/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 32.5906 - val_loss: 52.3482\n",
            "Epoch 14/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 32.4279 - val_loss: 38.4579\n",
            "Epoch 15/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 32.2150 - val_loss: 67.4193\n",
            "Epoch 16/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 32.0634 - val_loss: 47.2547\n",
            "Epoch 17/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 31.7294 - val_loss: 35.3094\n",
            "Epoch 18/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 31.7263 - val_loss: 36.9274\n",
            "Epoch 19/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 31.4561 - val_loss: 46.2696\n",
            "Epoch 20/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 31.3109 - val_loss: 33.8123\n",
            "Epoch 21/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 31.0750 - val_loss: 37.4346\n",
            "Epoch 22/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 31.1167 - val_loss: 39.9137\n",
            "Epoch 23/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 31.0006 - val_loss: 34.1650\n",
            "Epoch 24/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 30.9230 - val_loss: 38.0383\n",
            "Epoch 25/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 30.7442 - val_loss: 34.3593\n",
            "Epoch 26/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 30.8064 - val_loss: 43.9223\n",
            "Epoch 27/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 30.7305 - val_loss: 37.1797\n",
            "Epoch 28/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 30.6138 - val_loss: 38.7454\n",
            "Epoch 29/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 30.5277 - val_loss: 34.7892\n",
            "Epoch 30/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 30.5721 - val_loss: 47.6174\n",
            "Epoch 31/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 30.4309 - val_loss: 36.8954\n",
            "Epoch 32/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 30.3680 - val_loss: 34.0950\n",
            "Epoch 33/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 30.4032 - val_loss: 35.3144\n",
            "Epoch 34/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 30.2897 - val_loss: 51.6589\n",
            "Epoch 35/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 30.2601 - val_loss: 47.0884\n",
            "Epoch 36/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 30.2363 - val_loss: 34.5937\n",
            "Epoch 37/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 30.1759 - val_loss: 35.4823\n",
            "Epoch 38/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 30.1978 - val_loss: 42.8621\n",
            "Epoch 39/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 30.1703 - val_loss: 36.6880\n",
            "Epoch 40/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 30.0090 - val_loss: 34.3244\n",
            "Epoch 41/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 30.0462 - val_loss: 36.0277\n",
            "Epoch 42/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.9998 - val_loss: 36.3052\n",
            "Epoch 43/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 29.9667 - val_loss: 35.7313\n",
            "Epoch 44/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 29.9866 - val_loss: 34.7054\n",
            "Epoch 45/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.9120 - val_loss: 60.9054\n",
            "Epoch 46/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.9062 - val_loss: 35.9657\n",
            "Epoch 47/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 29.8770 - val_loss: 41.4800\n",
            "Epoch 48/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 29.8272 - val_loss: 33.7438\n",
            "Epoch 49/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.9340 - val_loss: 32.8499\n",
            "Epoch 50/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.7412 - val_loss: 33.9534\n",
            "Epoch 51/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.7080 - val_loss: 37.9725\n",
            "Epoch 52/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.7144 - val_loss: 40.1746\n",
            "Epoch 53/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.6772 - val_loss: 45.8922\n",
            "Epoch 54/300\n",
            "1319/1319 [==============================] - 8s 6ms/step - loss: 29.6394 - val_loss: 39.2322\n",
            "Epoch 55/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 29.6271 - val_loss: 41.7753\n",
            "Epoch 56/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.7158 - val_loss: 42.9564\n",
            "Epoch 57/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.6235 - val_loss: 41.3138\n",
            "Epoch 58/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.5135 - val_loss: 35.9668\n",
            "Epoch 59/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.6512 - val_loss: 39.6906\n",
            "Epoch 60/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.5825 - val_loss: 34.5856\n",
            "Epoch 61/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.4223 - val_loss: 44.1215\n",
            "Epoch 62/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.5737 - val_loss: 34.2447\n",
            "Epoch 63/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.4904 - val_loss: 40.5628\n",
            "Epoch 64/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 29.5248 - val_loss: 38.6904\n",
            "Epoch 65/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 29.3995 - val_loss: 36.4185\n",
            "Epoch 66/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.4506 - val_loss: 48.0307\n",
            "Epoch 67/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.5263 - val_loss: 35.5451\n",
            "Epoch 68/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.3801 - val_loss: 34.2767\n",
            "Epoch 69/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.3409 - val_loss: 33.0602\n",
            "Epoch 70/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.4994 - val_loss: 37.2275\n",
            "Epoch 71/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.5355 - val_loss: 39.1273\n",
            "Epoch 72/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.4003 - val_loss: 36.4385\n",
            "Epoch 73/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.4000 - val_loss: 35.7449\n",
            "Epoch 74/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.4104 - val_loss: 44.5145\n",
            "Epoch 75/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 29.3733 - val_loss: 42.1505\n",
            "Epoch 76/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.3141 - val_loss: 35.9035\n",
            "Epoch 77/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.3486 - val_loss: 34.9899\n",
            "Epoch 78/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.3757 - val_loss: 34.3696\n",
            "Epoch 79/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.3034 - val_loss: 39.1225\n",
            "Epoch 80/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.2982 - val_loss: 35.0341\n",
            "Epoch 81/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.2497 - val_loss: 47.5105\n",
            "Epoch 82/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.3657 - val_loss: 33.7133\n",
            "Epoch 83/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.2297 - val_loss: 37.7133\n",
            "Epoch 84/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.2977 - val_loss: 35.6854\n",
            "Epoch 85/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.2554 - val_loss: 35.0090\n",
            "Epoch 86/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.1900 - val_loss: 46.0894\n",
            "Epoch 87/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.2611 - val_loss: 34.2136\n",
            "Epoch 88/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 29.3055 - val_loss: 35.6379\n",
            "Epoch 89/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.2628 - val_loss: 43.2753\n",
            "Epoch 90/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 29.1570 - val_loss: 34.3236\n",
            "Epoch 91/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.2031 - val_loss: 34.8351\n",
            "Epoch 92/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 29.1616 - val_loss: 34.6564\n",
            "Epoch 93/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0554 - val_loss: 37.8679\n",
            "Epoch 94/300\n",
            "1319/1319 [==============================] - 7s 6ms/step - loss: 29.2331 - val_loss: 38.6804\n",
            "Epoch 95/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.1497 - val_loss: 32.7332\n",
            "Epoch 96/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.1997 - val_loss: 34.5809\n",
            "Epoch 97/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.1524 - val_loss: 44.5979\n",
            "Epoch 98/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0732 - val_loss: 39.9000\n",
            "Epoch 99/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.0856 - val_loss: 34.7767\n",
            "Epoch 100/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.1630 - val_loss: 34.5985\n",
            "Epoch 101/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0949 - val_loss: 34.6860\n",
            "Epoch 102/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0446 - val_loss: 34.2736\n",
            "Epoch 103/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.1638 - val_loss: 33.5406\n",
            "Epoch 104/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.2434 - val_loss: 36.8591\n",
            "Epoch 105/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 29.1319 - val_loss: 37.8127\n",
            "Epoch 106/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0966 - val_loss: 47.6653\n",
            "Epoch 107/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0232 - val_loss: 40.6737\n",
            "Epoch 108/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.9534 - val_loss: 40.6332\n",
            "Epoch 109/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.9712 - val_loss: 32.1554\n",
            "Epoch 110/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 29.1064 - val_loss: 36.5536\n",
            "Epoch 111/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0106 - val_loss: 35.5347\n",
            "Epoch 112/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0388 - val_loss: 40.4894\n",
            "Epoch 113/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0391 - val_loss: 36.9627\n",
            "Epoch 114/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0283 - val_loss: 34.6778\n",
            "Epoch 115/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.9802 - val_loss: 36.0335\n",
            "Epoch 116/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 28.9447 - val_loss: 34.1345\n",
            "Epoch 117/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0547 - val_loss: 45.8262\n",
            "Epoch 118/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.0477 - val_loss: 47.4439\n",
            "Epoch 119/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.9912 - val_loss: 36.4880\n",
            "Epoch 120/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 28.9301 - val_loss: 35.3753\n",
            "Epoch 121/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 28.9669 - val_loss: 34.5524\n",
            "Epoch 122/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.9746 - val_loss: 39.1905\n",
            "Epoch 123/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 28.9214 - val_loss: 40.9509\n",
            "Epoch 124/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0191 - val_loss: 37.6188\n",
            "Epoch 125/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0269 - val_loss: 37.0595\n",
            "Epoch 126/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.0034 - val_loss: 33.9168\n",
            "Epoch 127/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 28.8785 - val_loss: 38.9624\n",
            "Epoch 128/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.9280 - val_loss: 38.3693\n",
            "Epoch 129/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.8628 - val_loss: 35.0932\n",
            "Epoch 130/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.9211 - val_loss: 37.7533\n",
            "Epoch 131/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 28.9088 - val_loss: 39.8273\n",
            "Epoch 132/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.9122 - val_loss: 32.3422\n",
            "Epoch 133/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.8667 - val_loss: 43.4021\n",
            "Epoch 134/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.9253 - val_loss: 39.4605\n",
            "Epoch 135/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.8524 - val_loss: 55.7087\n",
            "Epoch 136/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.9223 - val_loss: 33.9592\n",
            "Epoch 137/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.7652 - val_loss: 39.2526\n",
            "Epoch 138/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 28.8850 - val_loss: 33.0135\n",
            "Epoch 139/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.8306 - val_loss: 32.9587\n",
            "Epoch 140/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 28.8598 - val_loss: 40.2563\n",
            "Epoch 141/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.8770 - val_loss: 39.1599\n",
            "Epoch 142/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.8181 - val_loss: 37.4532\n",
            "Epoch 143/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.8919 - val_loss: 34.2622\n",
            "Epoch 144/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 28.7990 - val_loss: 36.5329\n",
            "Epoch 145/300\n",
            "1319/1319 [==============================] - 7s 5ms/step - loss: 28.8590 - val_loss: 34.2359\n",
            "Epoch 146/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.8192 - val_loss: 34.8211\n",
            "Epoch 147/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.8109 - val_loss: 32.4500\n",
            "Epoch 148/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 28.8169 - val_loss: 38.1794\n",
            "Epoch 149/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8133 - val_loss: 33.7697\n",
            "Epoch 150/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7652 - val_loss: 34.0104\n",
            "Epoch 151/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7237 - val_loss: 37.3894\n",
            "Epoch 152/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9065 - val_loss: 40.6109\n",
            "Epoch 153/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8472 - val_loss: 33.4644\n",
            "Epoch 154/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8382 - val_loss: 38.0558\n",
            "Epoch 155/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8080 - val_loss: 34.3211\n",
            "Epoch 156/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7785 - val_loss: 38.5777\n",
            "Epoch 157/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7718 - val_loss: 33.2323\n",
            "Epoch 158/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7434 - val_loss: 33.8825\n",
            "Epoch 159/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7904 - val_loss: 34.2296\n",
            "Epoch 160/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8288 - val_loss: 36.2831\n",
            "Epoch 161/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7695 - val_loss: 36.8856\n",
            "Epoch 162/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7653 - val_loss: 33.2764\n",
            "Epoch 163/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7813 - val_loss: 33.4998\n",
            "Epoch 164/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7503 - val_loss: 34.5023\n",
            "Epoch 165/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7060 - val_loss: 42.1683\n",
            "Epoch 166/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7614 - val_loss: 43.8206\n",
            "Epoch 167/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7524 - val_loss: 43.3740\n",
            "Epoch 168/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7390 - val_loss: 37.0422\n",
            "Epoch 169/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7787 - val_loss: 37.0438\n",
            "Epoch 170/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7778 - val_loss: 33.1166\n",
            "Epoch 171/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6969 - val_loss: 37.0845\n",
            "Epoch 172/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6500 - val_loss: 36.1720\n",
            "Epoch 173/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7905 - val_loss: 40.8274\n",
            "Epoch 174/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7298 - val_loss: 34.4321\n",
            "Epoch 175/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6877 - val_loss: 35.9624\n",
            "Epoch 176/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7271 - val_loss: 34.7154\n",
            "Epoch 177/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7734 - val_loss: 33.5326\n",
            "Epoch 178/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7024 - val_loss: 33.8316\n",
            "Epoch 179/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7511 - val_loss: 33.2893\n",
            "Epoch 180/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7625 - val_loss: 33.9305\n",
            "Epoch 181/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7759 - val_loss: 34.0993\n",
            "Epoch 182/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6553 - val_loss: 47.9198\n",
            "Epoch 183/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7507 - val_loss: 32.4330\n",
            "Epoch 184/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6690 - val_loss: 35.3401\n",
            "Epoch 185/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6659 - val_loss: 34.6809\n",
            "Epoch 186/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7549 - val_loss: 36.0584\n",
            "Epoch 187/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6692 - val_loss: 35.0639\n",
            "Epoch 188/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6274 - val_loss: 45.8607\n",
            "Epoch 189/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6693 - val_loss: 34.5967\n",
            "Epoch 190/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6440 - val_loss: 33.3472\n",
            "Epoch 191/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7260 - val_loss: 35.5559\n",
            "Epoch 192/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7463 - val_loss: 32.2490\n",
            "Epoch 193/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6193 - val_loss: 37.9575\n",
            "Epoch 194/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6951 - val_loss: 32.3541\n",
            "Epoch 195/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 28.6309 - val_loss: 33.3993\n",
            "Epoch 196/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6985 - val_loss: 36.3990\n",
            "Epoch 197/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6849 - val_loss: 37.1449\n",
            "Epoch 198/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6552 - val_loss: 35.1207\n",
            "Epoch 199/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7034 - val_loss: 36.9465\n",
            "Epoch 200/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6963 - val_loss: 36.3822\n",
            "Epoch 201/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6793 - val_loss: 33.9503\n",
            "Epoch 202/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6704 - val_loss: 35.5709\n",
            "Epoch 203/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6376 - val_loss: 35.2279\n",
            "Epoch 204/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6247 - val_loss: 36.2914\n",
            "Epoch 205/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6404 - val_loss: 34.0687\n",
            "Epoch 206/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6588 - val_loss: 33.9289\n",
            "Epoch 207/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7130 - val_loss: 35.9508\n",
            "Epoch 208/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6961 - val_loss: 41.5213\n",
            "Epoch 209/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7141 - val_loss: 36.1735\n",
            "Epoch 210/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6663 - val_loss: 33.5591\n",
            "Epoch 211/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6594 - val_loss: 38.3972\n",
            "Epoch 212/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5857 - val_loss: 32.8800\n",
            "Epoch 213/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7179 - val_loss: 32.5117\n",
            "Epoch 214/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5694 - val_loss: 33.2498\n",
            "Epoch 215/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5901 - val_loss: 35.1830\n",
            "Epoch 216/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7026 - val_loss: 34.4983\n",
            "Epoch 217/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5540 - val_loss: 35.4653\n",
            "Epoch 218/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6284 - val_loss: 47.4233\n",
            "Epoch 219/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5731 - val_loss: 37.8750\n",
            "Epoch 220/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5940 - val_loss: 37.4136\n",
            "Epoch 221/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6683 - val_loss: 34.3562\n",
            "Epoch 222/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5666 - val_loss: 34.9171\n",
            "Epoch 223/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6250 - val_loss: 38.7585\n",
            "Epoch 224/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6300 - val_loss: 37.0245\n",
            "Epoch 225/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5966 - val_loss: 38.9811\n",
            "Epoch 226/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6935 - val_loss: 34.6706\n",
            "Epoch 227/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6032 - val_loss: 32.7745\n",
            "Epoch 228/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5591 - val_loss: 38.5698\n",
            "Epoch 229/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5809 - val_loss: 37.4065\n",
            "Epoch 230/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6301 - val_loss: 34.2625\n",
            "Epoch 231/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5464 - val_loss: 38.2400\n",
            "Epoch 232/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6534 - val_loss: 35.8186\n",
            "Epoch 233/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5439 - val_loss: 33.8447\n",
            "Epoch 234/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5936 - val_loss: 34.2501\n",
            "Epoch 235/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5936 - val_loss: 40.1852\n",
            "Epoch 236/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5652 - val_loss: 40.6536\n",
            "Epoch 237/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5593 - val_loss: 35.2293\n",
            "Epoch 238/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5607 - val_loss: 41.7562\n",
            "Epoch 239/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6410 - val_loss: 32.5844\n",
            "Epoch 240/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6239 - val_loss: 34.6708\n",
            "Epoch 241/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5885 - val_loss: 33.9894\n",
            "Epoch 242/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5766 - val_loss: 38.5966\n",
            "Epoch 243/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.5366 - val_loss: 33.6800\n",
            "Epoch 244/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5704 - val_loss: 43.6389\n",
            "Epoch 245/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5271 - val_loss: 34.5463\n",
            "Epoch 246/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5607 - val_loss: 38.4609\n",
            "Epoch 247/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5511 - val_loss: 34.5807\n",
            "Epoch 248/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5591 - val_loss: 35.6147\n",
            "Epoch 249/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5325 - val_loss: 39.1709\n",
            "Epoch 250/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5933 - val_loss: 33.7573\n",
            "Epoch 251/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6051 - val_loss: 34.9599\n",
            "Epoch 252/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5838 - val_loss: 33.2416\n",
            "Epoch 253/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5600 - val_loss: 34.0897\n",
            "Epoch 254/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5616 - val_loss: 35.0764\n",
            "Epoch 255/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5321 - val_loss: 38.2448\n",
            "Epoch 256/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5590 - val_loss: 33.4157\n",
            "Epoch 257/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5152 - val_loss: 35.5412\n",
            "Epoch 258/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4950 - val_loss: 39.5703\n",
            "Epoch 259/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5218 - val_loss: 34.7869\n",
            "Epoch 260/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4693 - val_loss: 35.3224\n",
            "Epoch 261/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5709 - val_loss: 33.4510\n",
            "Epoch 262/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5071 - val_loss: 33.6128\n",
            "Epoch 263/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5026 - val_loss: 36.5251\n",
            "Epoch 264/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5583 - val_loss: 33.5632\n",
            "Epoch 265/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5046 - val_loss: 33.2633\n",
            "Epoch 266/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4957 - val_loss: 34.8334\n",
            "Epoch 267/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4631 - val_loss: 38.4425\n",
            "Epoch 268/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6081 - val_loss: 34.4713\n",
            "Epoch 269/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4914 - val_loss: 33.9746\n",
            "Epoch 270/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6129 - val_loss: 33.7759\n",
            "Epoch 271/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5637 - val_loss: 37.6696\n",
            "Epoch 272/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5782 - val_loss: 34.0425\n",
            "Epoch 273/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5353 - val_loss: 33.9019\n",
            "Epoch 274/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4631 - val_loss: 35.4006\n",
            "Epoch 275/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6041 - val_loss: 35.3076\n",
            "Epoch 276/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4731 - val_loss: 34.5355\n",
            "Epoch 277/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5392 - val_loss: 34.2889\n",
            "Epoch 278/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4407 - val_loss: 33.1659\n",
            "Epoch 279/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5135 - val_loss: 33.4935\n",
            "Epoch 280/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4412 - val_loss: 35.1876\n",
            "Epoch 281/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4648 - val_loss: 32.7510\n",
            "Epoch 282/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4720 - val_loss: 32.9062\n",
            "Epoch 283/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4991 - val_loss: 33.5789\n",
            "Epoch 284/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4935 - val_loss: 33.6944\n",
            "Epoch 285/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4237 - val_loss: 37.8700\n",
            "Epoch 286/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5604 - val_loss: 34.3682\n",
            "Epoch 287/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6078 - val_loss: 36.8747\n",
            "Epoch 288/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4896 - val_loss: 34.1400\n",
            "Epoch 289/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5799 - val_loss: 34.0930\n",
            "Epoch 290/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4814 - val_loss: 33.5145\n",
            "Epoch 291/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4556 - val_loss: 34.2644\n",
            "Epoch 292/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4994 - val_loss: 47.7749\n",
            "Epoch 293/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5050 - val_loss: 35.2395\n",
            "Epoch 294/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.3719 - val_loss: 33.5395\n",
            "Epoch 295/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5328 - val_loss: 34.4281\n",
            "Epoch 296/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4912 - val_loss: 38.7025\n",
            "Epoch 297/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4229 - val_loss: 35.3952\n",
            "Epoch 298/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5015 - val_loss: 33.3804\n",
            "Epoch 299/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5000 - val_loss: 35.2149\n",
            "Epoch 300/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4307 - val_loss: 36.8170\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "outputId": "c3d76621-8749-4d0c-db40-85826a4bafbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -1.6438420158527027 \n",
            "MAE:  4.56360218242742 \n",
            "SD:  5.8407855680161544\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0cip38xZOZnV",
        "outputId": "e1e10771-d081-4967-c44a-97a88b345173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAFBCAYAAAAsfIegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5wU5f0H8M/3Or2JQIAIKIriSREIiC1ixYLGAohG7D9L1JgYG7ZETZQY/RmNaNSfqFgQayIqiigSjYh4gPQi6J3A0Y5+cHf7/f3xnYfZvdtr3O7tzPl5v177mtnZKc/OznzmmWdnZ0VVQUREiZOW6gIQETU0DFYiogRjsBIRJRiDlYgowRisREQJxmAlIkqwpAWriOSIyEwRmSMi80XkHm94VxH5UkSWicirIpLlDc/2ni/zXu+SrLIRESVTMmusuwAcp6q9APQGcLKIDATwAICHVfUAAJsAXOqNfymATd7wh73xiIhCJ2nBqmab9zTTeyiA4wBM8oaPB3Cm1z/Mew7v9SEiIskqHxFRsiS1jVVE0kUkD0AhgA8BLAdQpKql3ij5ADp6/R0B/AAA3uubAbRJZvmIiJIhI5kzV9UyAL1FpCWANwH0qOs8ReQKAFcAQDaaHo70/XFo76S+DSL6ifn666/Xq2rbvZ2+XhJJVYtEZBqAQQBaikiGVyvtBKDAG60AQGcA+SKSAaAFgA1x5vUUgKcAoI1001bNpmLWLFZsiShxRGRVXaZP5lUBbb2aKkSkEYATACwEMA3AOd5oFwF42+t/x3sO7/WPtUZ3iOFNZIgoWJJZY+0AYLyIpMMCfKKq/ltEFgB4RUTuBfANgGe88Z8B8IKILAOwEcCIJJaNiChpkhasqjoXQJ84w1cAGBBneDGAc5NVHiKi+sJvfYjqUUlJCfLz81FcXJzqohCAnJwcdOrUCZmZmQmdL4OVqB7l5+ejWbNm6NKlC3iZdmqpKjZs2ID8/Hx07do1ofPmvQKI6lFxcTHatGnDUA0AEUGbNm2ScvbAYCWqZwzV4EjWZ8FgJSJKsNAHqyqP/kQNQdOmTSt9beXKlTj00EPrsTR1E/pg5Q8EiChoGkCwElFtrFy5Ej169MDo0aNx4IEHYtSoUfjoo48wePBgdO/eHTNnzsSnn36K3r17o3fv3ujTpw+2bt0KABg7diz69++Pww47DHfddVely7jlllvw+OOP73l+9913469//Su2bduGIUOGoG/fvsjNzcXbb79d6TwqU1xcjIsvvhi5ubno06cPpk2bBgCYP38+BgwYgN69e+Owww7D0qVLsX37dpx66qno1asXDj30ULz66qu1Xt7e4OVWRKlyww1AXl5i59m7N/DII9WOtmzZMrz22mt49tln0b9/f7z00kuYMWMG3nnnHdx///0oKyvD448/jsGDB2Pbtm3IycnBlClTsHTpUsycOROqijPOOAPTp0/H0UcfXWH+w4cPxw033IBrrrkGADBx4kR88MEHyMnJwZtvvonmzZtj/fr1GDhwIM4444xafYn0+OOPQ0Qwb948LFq0CCeeeCKWLFmCcePG4frrr8eoUaOwe/dulJWVYfLkyfjZz36Gd999FwCwefPmGi+nLlhjJfoJ6tq1K3Jzc5GWloaePXtiyJAhEBHk5uZi5cqVGDx4MG688UY8+uijKCoqQkZGBqZMmYIpU6agT58+6Nu3LxYtWoSlS5fGnX+fPn1QWFiIH3/8EXPmzEGrVq3QuXNnqCpuu+02HHbYYTj++ONRUFCAtWvX1qrsM2bMwAUXXAAA6NGjB/bbbz8sWbIEgwYNwv33348HHngAq1atQqNGjZCbm4sPP/wQN998Mz777DO0aNGizuuuJlhjJUqVGtQskyU7O3tPf1pa2p7naWlpKC0txS233IJTTz0VkydPxuDBg/HBBx9AVXHrrbfiyiuvrNEyzj33XEyaNAlr1qzB8OHDAQATJkzAunXr8PXXXyMzMxNdunRJ2HWk559/Pn7xi1/g3XffxdChQ/Hkk0/iuOOOw+zZszF58mSMGTMGQ4YMwZ133pmQ5VWFwUpEFSxfvhy5ubnIzc3FV199hUWLFuGkk07CHXfcgVGjRqFp06YoKChAZmYm9t1337jzGD58OC6//HKsX78en376KQA7Fd93332RmZmJadOmYdWq2t+d76ijjsKECRNw3HHHYcmSJfj+++9x0EEHYcWKFejWrRuuu+46fP/995g7dy569OiB1q1b44ILLkDLli3x9NNP12m91BSDlYgqeOSRRzBt2rQ9TQWnnHIKsrOzsXDhQgwaNAiAXR714osvVhqsPXv2xNatW9GxY0d06NABADBq1CicfvrpyM3NRb9+/dCjR+3vfX/11VfjqquuQm5uLjIyMvDcc88hOzsbEydOxAsvvIDMzEy0b98et912G7766ivcdNNNSEtLQ2ZmJp544om9Xym1IDW65WlAtZFu2rLFTCwv2ifVRSGqkYULF+Lggw9OdTEoSrzPRES+VtV+ezvPcH95xd8GEFEAsSmAiPbahg0bMGTIkArDp06dijZtav+XSfPmzcOFF14YMyw7OxtffvnlXpcxFUIerIIQt2QQhV6bNm2Ql8BrcXNzcxM6v1QJd1MAf85KRAEU8mAlIgoeBisRUYIxWImIEozBSkRJUdX9VRs6BisRUYKF/nIrorBK1V0DV65ciZNPPhkDBw7E559/jv79++Piiy/GXXfdhcLCQkyYMAE7d+7E9ddfD8D+F2r69Olo1qwZxo4di4kTJ2LXrl0466yzcM8991RbJlXFH/7wB7z33nsQEYwZMwbDhw/H6tWrMXz4cGzZsgWlpaV44okncMQRR+DSSy/FrFmzICK45JJL8Nvf/jYRq6ZehTxYiWhvJPt+rNHeeOMN5OXlYc6cOVi/fj369++Po48+Gi+99BJOOukk3H777SgrK8OOHTuQl5eHgoICfPvttwCAoqKi+lgdCcdgJUqRFN41cM/9WAHEvR/riBEjcOONN2LUqFH41a9+hU6dOsXcjxUAtm3bhqVLl1YbrDNmzMDIkSORnp6Odu3a4ZhjjsFXX32F/v3745JLLkFJSQnOPPNM9O7dG926dcOKFSvwm9/8BqeeeipOPPHEpK+LZAh9Gyt/eUVUezW5H+vTTz+NnTt3YvDgwVi0aNGe+7Hm5eUhLy8Py5Ytw6WXXrrXZTj66KMxffp0dOzYEaNHj8bzzz+PVq1aYc6cOTj22GMxbtw4XHbZZXV+r6kQ6mAV/vKKKCnc/Vhvvvlm9O/ff8/9WJ999lls27YNAFBQUIDCwsJq53XUUUfh1VdfRVlZGdatW4fp06djwIABWLVqFdq1a4fLL78cl112GWbPno3169cjEong7LPPxr333ovZs2cn+60mRcibAvjlFVEyJOJ+rM5ZZ52FL774Ar169YKI4MEHH0T79u0xfvx4jB07FpmZmWjatCmef/55FBQU4OKLL0YkEgEA/PnPf076e02GUN+PdZ+0/bVZ0y/x3Rbej5XCgfdjDR7ej5WIKARC3hRARKmU6PuxNhQMViLaa4m+H2tDwaYAonoW5u81GppkfRYMVqJ6lJOTgw0bNjBcA0BVsWHDBuTk5CR83mwKIKpHnTp1Qn5+PtatW5fqohDsQNepU6eEzzdpwSoinQE8D6Ad7D9UnlLV/xWRuwFcDsBtWbep6mRvmlsBXAqgDMB1qvpBsspHlAqZmZno2rVrqotBSZbMGmspgN+p6mwRaQbgaxH50HvtYVX9a/TIInIIgBEAegL4GYCPRORAVS2raiE8oyKioElaG6uqrlbV2V7/VgALAXSsYpJhAF5R1V2q+h2AZQAGJKt8RETJUi9fXolIFwB9ALg/B79WROaKyLMi0sob1hHAD1GT5aPqICYiCqSkB6uINAXwOoAbVHULgCcA7A+gN4DVAB6q5fyuEJFZIjJLeRMWIgqgpAariGTCQnWCqr4BAKq6VlXLVDUC4J/wT/cLAHSOmryTNyyGqj6lqv1UtZ/wJixEFEBJC1YREQDPAFioqn+LGt4harSzAHzr9b8DYISIZItIVwDdAcxMVvmIiJIlmVcFDAZwIYB5IuJ+83YbgJEi0ht2CdZKAFcCgKrOF5GJABbArii4prorAoiIgijctw1M31+bNv4SK7fytoFElDi8bSARUcAwWImIEiz0warKKwOIKFjCH6ypLgARUTnhD1bWWIkoYEIdrAL+9oqIgifUwQoAEdZYiShgQh+syp+1ElHAhD9YWWMlooAJdbAKgAhrrEQUMKEOVkD5DwJEFDghD1a2sRJR8IQ+WHlVABEFTaiDVcAaKxEFT6iDlW2sRBREIQ9W1liJKHjCHazCNlYiCp5QByvbWIkoiEIdrADYxkpEgRPyYFVEwv4WiKjBCX0qscZKREET6mBlGysRBVGogxXgVQFEFDyhD1bWWIkoaBisREQJFupgFSgiGuq3QEQNEFOJiCjBGkSw8pIrIgqSUAera11lsBJRkIQ6WF2yRiKpLQYRUbRwB6uHNVYiCpKQB6slKoOViIIk1MHq2ljZFEBEQRLqYHVYYyWiIGGwEhElWIMIVjYFEFGQhDpYeR0rEQVR0oJVRDqLyDQRWSAi80Xkem94axH5UESWet1W3nARkUdFZJmIzBWRvtUvhVcFEFHwJLPGWgrgd6p6CICBAK4RkUMA3AJgqqp2BzDVew4ApwDo7j2uAPBETRfEpgAiCpKkBauqrlbV2V7/VgALAXQEMAzAeG+08QDO9PqHAXhezX8BtBSRDjVbVkKLTkRUJ/XSxioiXQD0AfAlgHaqutp7aQ2Adl5/RwA/RE2W7w2rfL5el8FKREGS9GAVkaYAXgdwg6puiX5NVRWuobTm87tCRGaJyKyIRrz5JKq0RER1l9RgFZFMWKhOUNU3vMFr3Sm+1y30hhcA6Bw1eSdvWAxVfUpV+6lqv7Q0q7OyjZWIgiSZVwUIgGcALFTVv0W99A6Ai7z+iwC8HTX8197VAQMBbI5qMqgSa6xEFCQZSZz3YAAXApgnInnesNsA/AXARBG5FMAqAOd5r00GMBTAMgA7AFxc3QLYxkpEQZS0YFXVGUCl//Q3JM74CuCavVkWmwKIKEhC/csrhzVWIgqSUAcrmwKIKIhCHawOmwKIKEhCHqy8VwARBU+4g9VrC2CwElGQhDpY+dcsRBREoQ5WhzVWIgoSBisRUYKFOliFX14RUQCFOlgdtrESUZA0iGBljZWIgoTBSkSUYKEOVl5uRURBFOpgdVhjJaIgCXew8pdXRBRA4Q5W73IrNgUQUZCEOlh520AiCqJQB6vDYCWiIGkQwcqmACIKkgYRrKyxElGQhDpY2cZKREEU6mB1GKxEFCQhD1ZebkVEwRPuYOUPBIgogEIdrGxjJaIgCnWwOmwKIKIgaRDByhorEQVJqIOVTQFEFEShDlaHTQFEFCQhD1b+mSARBU/Ig9UwWIkoSGoUrCLSRETSvP4DReQMEclMbtFqUi7rsimAiIKkpjXW6QByRKQjgCkALgTwXLIKVVussRJRkNQ0WEVVdwD4FYB/qOq5AHomr1i1w2AloiCpcbCKyCAAowC86w1LT06Rao/BSkRBUtNgvQHArQDeVNX5ItINwLTkFatm+PfXRBRENQpWVf1UVc9Q1Qe8L7HWq+p1VU0jIs+KSKGIfBs17G4RKRCRPO8xNOq1W0VkmYgsFpGTavMmWGMloiCp6VUBL4lIcxFpAuBbAAtE5KZqJnsOwMlxhj+sqr29x2Rv/ocAGAFrtz0ZwD9EpAZNDbyOlYiCp6ZNAYeo6hYAZwJ4D0BX2JUBlVLV6QA21nD+wwC8oqq7VPU7AMsADKjhtGwKIKJAqWmwZnrXrZ4J4B1VLYGrLtbetSIy12sqaOUN6wjgh6hx8r1hVRLej5WIAqimwfokgJUAmgCYLiL7AdiyF8t7AsD+AHoDWA3godrOQESuEJFZIjKrtLQUAIOViIKlpl9ePaqqHVV1qJpVAH5Z24Wp6lpVLVPVCIB/wj/dLwDQOWrUTt6wePN4SlX7qWq/jIwMAGwKIKJgqemXVy1E5G+upigiD8Fqr7UiIh2inp4F+yIMAN4BMEJEskWkK4DuAGbWdL6ssRJRkGTUcLxnYSF4nvf8QgD/B/slVlwi8jKAYwHsIyL5AO4CcKyI9Ia1z64EcCUAeNfGTgSwAEApgGtUtay6QvF+rEQURDUN1v1V9eyo5/eISF5VE6jqyDiDn6li/PsA3FfD8sRgUwARBUlNv7zaKSJHuiciMhjAzuQUqTZ4HSsRBU9Na6z/A+B5EWnhPd8E4KLkFKkWeLkVEQVQjYJVVecA6CUizb3nW0TkBgBzk1m46rCNlYiCqFb/IKCqW7xfYAHAjUkoz15hGysRBUld/ppFqh+lfrDGSkRBUpdgTXmcsSmAiIKoyjZWEdmK+AEqABolpUR7gU0BRBQkVQarqjarr4LUBWusRBQk4f77a15uRUQBFOpgFa+Vgk0BRBQkoQ5WhzVWIgoSBisRUYI1iGBlUwARBUmog5XXsRJREIU6WB0GKxEFSbiDlZdbEVEAhTtYPWxjJaIgCXWwso2ViIIo1MHKfxAgoiAKebAaNgUQUZA0iGBljZWIgiTUwSq8KoCIAijUweqwKYCIgqRBBCtrrEQUJKEOVl5uRURBFOpgddgUQERBEvJg5XWsRBQ84Q5W77IABisRBUmog1VYYyWiAAp1sDpsYyWiIGkQwcoaKxEFSaiDlZdbEVEQhTpYHTYFEFGQNIhgZY2ViIIk3MHKm7AQUQCFO1gBCCJsCiCiQElasIrIsyJSKCLfRg1rLSIfishSr9vKGy4i8qiILBORuSLSt8bLgbLGSkSBkswa63MATi437BYAU1W1O4Cp3nMAOAVAd+9xBYAnaroQBisRBU3SglVVpwPYWG7wMADjvf7xAM6MGv68mv8CaCkiHWqynDRRNgUQUaDUdxtrO1Vd7fWvAdDO6+8I4Ieo8fK9YdVijZWIgiZlX16pqsLdnqoWROQKEZklIrN27d7NYCWiwKnvYF3rTvG9bqE3vABA56jxOnnDKlDVp1S1n6r2y87ORhoiDFYiCpT6DtZ3AFzk9V8E4O2o4b/2rg4YCGBzVJNBlQRsYyWiYMlI1oxF5GUAxwLYR0TyAdwF4C8AJorIpQBWATjPG30ygKEAlgHYAeDiGi8H/IEAEQVL0oJVVUdW8tKQOOMqgGv2ZjlpwqYAIgqWBvDLKzYFEFGwNIhgjamxTp0KrK5R8ywRUVI0vGA94wzgscdSVh4iotAHa1p0U0AkAuzYYQ8iohQJfbDG1FhLSmK7REQpEP5glahg3bXLurt3p6w8REShD9a06PuxukBlsBJRCoU7WEVimwJcjZVNAUSUQuEOVpRrY2WNlYgCIPTBGnMTFheorLESUQqFPlgFUX9/zS+viCgAGkCwsimAiIIl9MEacxMWfnlFRAEQ+mCNuQkLa6xEFAANIlj5AwEiCpKGFay8KoCIAiD0wbrnl1dbtgBFRTaQNVYiSqGk/YNAfdnz1yynnw7Mnm0DGaxElEKhr7HuaQpYtQrYts0GsimAiFIo3MEq4v/yavt2fzhrrESUQuEOVkRdbuVqqwCDlYhSKvzBKgqNKFBc7A9kUwARpVDogzVNI9DS0tiBrsa6Zg1w7LHA2rX1Xi4i+ukKd7Cmp0MipYjsLosdXlJilwrMmQN8+ql1iYjqSbiDNSMDEimD7orTplpa6n+htXOndZ99FujXr/7KR0Q/SeEO1sxMu9wq3r+y7t7t/1ur686dC3z9tYUuEVGShDtYMzLsl1fb4gRrSYkfqK7G6rrRVxAQESVY6INVoNDtOyu+Fq/G6oJ169b6KR8R/SSFO1j3NAVUEqzl21hdwDJYiSiJwh2srikgXrBW1RRQX8FaVsZraol+gsIdrOnpEAF0R3HF14LQFDBmjF1HS0Q/KeEOVgCSmQEtK6v4QnSwpqrGumwZsHx5/SyLiAIj9MGalpkOhVR8oaTEb2NNVY11x47Ym8MQ0U9C6INVMjMQifc2glBj3bHDHnv+4oCIfgrCH6yNG1VeY011G+uOHUAk4v8XF9XNl1/a/R+IAi70wZrWtEn8YA1KjTW6S3Vz6qnAgw+muhRE1Qp9sErTJpU3BZS/jjVVwcp21rorLQU2bAA2bkx1SYiqlZL/vBKRlQC2AigDUKqq/USkNYBXAXQBsBLAeaq6qdp5NWlcfVNAUREwYwaDNcy2bLEuf45MIZDKGusvVbW3qrrbTd0CYKqqdgcw1XteLUmT6psC5swBjjrK2juBugfr2LHAG29UPx6bAhJnk3eMZbBSCASpKWAYgPFe/3gAZ9ZkorQ0IDJgILB6NXDaacC119oL0cFaXl2D9Q9/AM4+u/rxWGNNHPfX5gxWCoFUBasCmCIiX4vIFd6wdqq62utfA6BdTWYkAmhmNtC+PfCvfwE33mgvRF/HWl6imgKq+ra/pMS/PSFrrHXHYKUQSVWwHqmqfQGcAuAaETk6+kVVVVj4ViAiV4jILBGZtW7dOgvW6DEzM61bXOy3qZZX22BV9RcS/UeF33xT+TTRYZqoGutbbwXvBjK7dtk9EZLNBWvQ3n9lVIHp03kNcxAVFlaeDQmSkmBV1QKvWwjgTQADAKwVkQ4A4HULK5n2KVXtp6r92rZta00BkagRsrKs677sSCv3FrOza79z/uUvQJ8+1u92cAD4/PPKp4kO1kTUWH/4ATjrLGDChNpNt2kTcMABwKxZdS9DPL/4BXDnnTUb96KLgN//fu+WE7Ya69dfA8ccA0ybltpy3H233bOCfIMGAX/6U1IXUe/BKiJNRKSZ6wdwIoBvAbwD4CJvtIsAvF2z+ZWrFLhg3bzZuq1axU7QsaN/yc7ttwN//3v1C5k71x6lpbHBumhR5dNUV2MtKwOeeKLmR84ff7Tu6tVVj1fe4sV2v4LoYP3mG9vh6qq0FJg3D/jvf6sf9803geefBx56aO+WFbZgLSiwbvnPa9Ei+x+2+vLOO7buyZSVAd99Z/fxSKJU1FjbAZghInMAzATwrqq+D+AvAE4QkaUAjveeV6tCsGZnW3f9euu2aRM7wX77WY11507ghRdqttGtW2cLWbcuNljXrat8mupqrNOnA1dfDbz+evXLB+z0paplbtzo78zR3PgbNvjDJkwA7rmn+iYKVeCzzyo/nS0stNOFJUuqng8APPecdZs3r37ceNxVATt21E/TQ1259b2p3BWDd91lNff6UlgI5OfX3/KCrqjI35eTqN6DVVVXqGov79FTVe/zhm9Q1SGq2l1Vj1fVGl0JnpZWbr9v1MhqqW5nL98UsN9+1l2zxmqB8Vbw2rXA6NF+7ciF9OrVfrBmZdU8WOMF2Ny51q2s1jtmDHDHHbFlAipf5sknA506xbYBR48fHazuZ6Hr1lkbaXGc2y4Cdhp79NGV17BckOfnV9/c4Za5ZUvFMlZn7drYu4TV15eBu3bZFSAvv1z7ttLKgrWgwLaj+mh7VbVg3bIleW3Tq1YBU6YkZ97J4PblhhasiSZSro0VAH7+c/+LpfT02NdcsM6bZzWfwjhNuVOmAOPHW20N8D+ENWv8YO3e3Ybv3AmMHFnx1KK6GqsL1sWL7XTtxBP9Dx2wL6reest/Xl2wfvWVdV97LXZ4VcFaWAhcdpm13cazYoV1K2tLjq4hV3dqFb2e463zqrRvD7zyiv+8vpoDHnzQrlk+/3xg8uTaTVtZsK5ZYweW6DOfZNm82b/ReryzmUQYO9a2n7B8Sec+FwZr1So0BQAWrG7Dbdeu4muAfbkA2Ioun8xuI1y82GYer8bqgjUvz3b68j8YiA7TDz6o2A7pgvX114Hhw4EPPwRuusl/PT/fb1cFqm8KaNvWus88Ezs8XrC6kC4stLbXOXPiz9Mt34V2Za8DwNKl8ccBbB2uXQvsv789j76Ryt7skPURrMXFwP33A6efbmc9la2DylQVrID/GdTFU0/Zl5qViT6A1bU5YOdOO8CUv7/wDz/Yth5dKQgyV854+30ChT5Y09LiNLm58MzOti+nevb0X3M11tmzrVtWZht/cbG/k7tgXbLETqHcqWt0jfXAA226hQvt+YIFsWWIDtbZs4Hf/c5/XlYGfPtt7FFh9Ghrh9y40YJj82brv/de4N13q66xFhf7w/PyYsOqqhrr6tXWkO9qUeW54Jw5s+JrgK0n19Ryzjl29ym3rDFj/CaG7dttx+zVK3b5K1ZYm+ukSfHnrxp/43fBWlJiB6aa/P1NcbHVzmv6pcV339k0w4cDXbtW/HzHjav6RyLxgnXrVr9ZqK536frhB+DKK4HHHqt8nOhg3dsa63/+A5x7LvDww9Yk8t57sa+7+VYV8EHiPpeysqSeNYQ+WA86yLb5mHtzuGA94ADgkEMsxFyTQJcu1nXBCliANmoEPPKIPXdH98WLYzdOV2PNzPQD2oXO/PmxBSt/+u9OqwFrYiguBvr2tedHHmm1AcB2luOO88e94w77RZkLhOgj7ebNwEcf+eXt29d2ZFfmGTP8mqTboEpK/KN2Xp61I6rG3/FcsP74Y/zXCwqAn/3M2iEBv+nilVeA++7zT59deQ47zLouVH7/ewtJd6WAqpXXHRhGjAAOP7zicp980t7D5MkW6CedVH3t4/33rTZf02/I3ee1//62DbkDqPPiizavytp73fqO3nmjw7SuNVZ3llHVtdSV1VhdTWTBgqrPGCIR2/YmTfIvTyofoG4biR4+Ywbw/feVz/e994Crrqr89WSKrmAksTkg9ME6cqRfcdnDBeuBB/rD3NUBrVtbiEafxo4da91nnrHa4Xff2fOPP7ZTfsfVWFu29E+9v/jCugsXxu7c5Xe4NWv82soDDwD77mvLa9Ik9jrZSZPin3a6potIxD+KXHstcMIJ1tQAWMAAtsNs2mT/t+XKt369lSl6Y3KvAf6O8cEHVkN77z0Lzn32seHxylRQYJevPfCATbNqVey4775rXRci0cG6dq0fcnPmWNleftk+s1NPtS/1Jk608Ads+A03WP+TT1qN0R00pk3zrzqI9v77/gH0be/qvcWLrdlmxgy7f0RlweiCtVs34OCDbTr3S4eOx6YAABVFSURBVLrdu+3zULXh8cSrsVYWrPPm1f7LJbdevvnGljV6dMU7f7nPOi3ND9YNG2z7PeIIO5N7911g6lRg8GAbZ8YMvz175Ur/wODOPtx28sknwMCB/uVkd9wB/PnPdmZy4okVr53dsMHOvkpKrJY9blzltfYtW4DzzvM/g8rOXKpSUhL/b5GimyzWrbP32KFD7Zt6qqOqoX0cfvjhGomoHnSQaq9eqiUlaj7/3H4rdfPNusf8+aq3364aiajut5/7LVXsIycn/nBAtWlT1cGDVUeOVO3eXfWTTyqO8913/vL++teKr8+bp5qfb/333KMVdOoUf9n77GPd7GzrLligOmeOqog9b9vWup9+at3HHlN9//3484pXLkD1xRdVi4r8dXDllart26uOGqWakaF6222xZf3yS9VmzWx9qKoee6zqEUfY+j3kEJtH+/aqZWWqb71lz2fNUm3ZUvXaa/1h99xj3XPPVb3oIr88vXvHlu+TT2z9uecjR1oZW7e2z2WffVS3b48tY7t2qsccYxtGmzY2XWamdbt0se706f74336runy59V9/vWqTJvZ+nnvOXyf5+aqXXOKXY8KEip+jqr13QLVzZ3/Yq6/60916qw3bscPWuVu/y5er3n23akGB6muvqRYXx5//Oef487r33vhl+eMfbfjAgfYZ/vOfsWUAVH//e9WLL7b+Xr1UBw1SzcpSXbvW/4yOPtof/6ijbN7/8z/x95G337b+nj1jy/KnP9nw99+38QCbfzwTJtjrf/yjPX/uOft8N2+OP348jzxin3Venuo33/jDL7vML+8bb6j+4x/W/5vf2Os//qj6738rgFlah2xKavAl+3H44YerqurEif42snu3qq5fr9q4sa24ePbf3ybo08dfya1bx24kp51mG6R7fswxqq1aqQ4Zotq/vwW1ey0317p/+5vqBRfYuIMGVdzw3n5b9YUXrH/27IrlOv30+KH35pvWdeHw6aeqY8aopqer/vKX/ng7d6q2aGEbvQusyh4uYNLSrHv//f4GDVhYiajeeaetp+OPjy1rbq4doFassOcuFLOyrOvC9aGHVA8/3Pq//1714INV+/VT/e1vrQw7dliouOUOGODPw4UToLpokR243PNeveyzGDhQ9aOPbNgvfmEHveJi1TVrbFijRqoffhi7/qIf//u/Vv4HH7Tnhx7qfxa5uda/dKk/fvfusdMfc4zNP1ok4q/fpk1tWF6eBb0r0yWXqG7Zovqf/9iwk05SXbXKP3i6svbsaYGyZo3qjTeq/vnPNr/u3VV/9jMbp2NH6950U2w5rr3WDmT5+bbO99/fD5auXa17xBF2UCy/Xv70Jz+wn3zSH96li827V6/425Xbp9LT7bN13IFy2DB/XHdwiUSsfO7gcsEF9vqxx9rz007z9wM3fnWGDrVpmjWz9V1UZMPPPNP2Y/e+Ro7UPQfASMQqEllZDFa3nkeP9veLDz9UjWzZWvkHsO++NvLMmf6H/Je/xG4gf/+7jeuev/yy33/CCaqFhf7zBx/0N9ScHL8fUL3uOtW+fa3/4Ydth2rVympy5f3f/8XWDho3Vu3Qwd7HU0+pvveeDb/jDqs59O+vunCh7bwHH2zzOP10q51UFaqAjQ+oHnigTd+kiU3XoYPqpZf6440bZzW1Fi3sCDZ5soWcez/OnXfGzv/f//aDxD2Ki/3anwtf9wEecIANu/lmf6d45hl/3KIiO2C659nZVsO/4ILYGimgev75to6iDxLZ2ap/+EPF9TB6tNVSos9WCgos0IYN89/f9u1+4AOqN9zgh2dGhoW7s2WLDW/e3Lo//3nsuujdW/XEE2NDun171fvui/1sBg+2rgtQwJbpDrT33usvw22XqlahOOMMew9uu3j6aX+8s86ydX7TTTa/xo1VzzvPfz/t2tlZ0GmnWZAuWOC/z/R0m39V25Yr07BhqosX25lK+XHcweDss1W/+ML627ZV3bXLPwPLyrJ12ayZPb/iCtXLL7dyHH+8bYeRiG0bQ4eqzphh77W0NHa9ALb/qNp+4yo9t99u5XA1aJcBY8YwWKO99ZZtw27bvPtuW/cVfPGFX5t1K97VQFu2tO7zz9vr99xjG97u3f64r7xiH56IbYQbN1rIuA+nuNiO6N262TwiEds4Lr3UjoxnnRWnUFFck8CBB1qtzIlE/CNserpV0VUtWLZts/6iIqv9ABY6I0b4gXL88VbOESP8WsqkSdaW4t7bb3+r+uij/vP33698R3K1VdXYEPziCyvrww/HbuDOuHH2/JZb/GG33GLDXnvNTvuvvtp2srQ0C8VIxNZr+TLcfbdN707P3edX/nHqqf7B0TWtuG7PnrY+X3/dnp94ou280U1Jqv46u/dee96/v7+xtWxp7+GZZ2x6IPaMKCvLamjXXaf6q1/5ZwrRj3btLEyXL1e95hrVTZv8z+bcc+29pqfbvFq3ttCJbj5p29YOAO59ZWXZWZJq7EHJnYK7AzVgn/nQoTbN1Kn+8NNPt3V/330WxPHWrWtacwcCdwYBqPboYeumQwc7GwHs/bumAcA/43NBC6hedZV13Xpq0cKvzZ9xhu2TgG3LbtwTTrD375q7Ona0fbRjR9v/zj7bDsDnnmv7QqNGNt6DD/oH5v32U92+ncFaXnGxHZyOOcZvguzVy/LxX/+ys8mYyuKvf21hGImojh2r+sMPtjGWllaYt779ttW4nB9/9NoevAW/+KL/3AWB42ph0Rt2ZbZutZ3q448rNhns2OHv0P/6V/zpIxHbuNessecrVtj4//mPP878+arjx1v/N9/YstassZD++GO/rG49rFtn7bpvvGE79aBBsct0O1PLlrHDd+5U/ewzqzFF+/FHe81Zvlz1lFNUN2yIHa9LF9vY3fu65prYduIXX7TXvv9e9aWXbH1dfrn/+skn2477xRd+zeuNN+zgeOutfrC++KLN39Wk+vSxMIrmwuDzz+15fr6tx+++i62RNmtmbaBjxvg7/Nat/nyWLLH11KRJxZB67LHYZb71lh003Ofwj3+oHnmkvx262t4JJ1jXhf9bb1ltMdpnn9kZjlNW5i932jQb321TV11lB4h58/zxJ03yxx892oIesIP5mjXW3bHD1mP//n6ZevRQXblS9YEH7PmXX9r8Skr82vmFF/o185NOsnk884zN56CDVN95x3bqESNsXaxaZQc+V54OHazrap/Nm1ub+bRptm2ef759Lh062JnqggUWrKefbmV2TUFehYvBWoWCAmvDLt/c2batVWAuucS201mzbL92zTBJsXmzbYz//Gfd57V6tW3UlX2xUVdbtthK++ST+K/v3l3xi6IlS3RPLTmRhg2r2L67e7d92TBqlH3JUl4kYs0tRx5Z8bWVK/3+jRvttDr6IPrNNxYu0e2Dzvr1FnzxmnE2bbINaOpU2/BU/QPUBx9UHH/VKv+A58KlV6+9+0w3b7aQz821Wt0jj9R82s8/Vz3uOP+MpyorV1pN/t13azbvSMQCceNGe757d8VTyMJCC+/SUut/+WU7iNfUyy/bmcbq1dZOe9xxqlOm2NlOeZs22bbtFBX5zYWRiIWtp67BKjaPcOrXr5/OquHt8IqK7CqkefPsSpHFi+3KkfI/GGnRwi7NLCuz/tatYx9ZWfZo3Rpo3Nguac3KAnJy7CqvrCy7ZNYNy862R0aGXdmTk1Px9gUNxvvv263yGjVK3DyLiuxSm9atazfdzp22ot1NeVJl06aKd1iLtmiR/TqwqAjo3Nk2lL2lahtuXeZRk2VInL9CamBE5Gv1/zaq9tP/VII1HlW79HLmTP8Sz1Wr7NK8jAy7nG7jRnts2ODfGGdvuPvGNm5sl9S6gM7MtO10xw67tDUryzKhUye7FG/XLvtxUvPmdinh1q12GWLLlv6lqe3aWXnT0/2ue2Rk2DyLiuyS1LQ0W152tr2/sjKgaVOgWTPrNm5sl/hmZdm0bty0NCtX8+a2Dtas8fOuVSu7HDc93crUtKn/ftPSbJ4i9luAsjKbd3a2f58HVX/cfff188ENS0uzebv+6IcbXlpq66tZM/8zysy095CRYcMKCqybk+Mf9Nz0IrXLC1X7bDIzY29H4d6LanLzjZKrrsH6k/7oReyHWO7HWNVxO3tJiQXtzp3W7/5p+/vvbQcvK7NhxcW287lH48b+nQfddLt3+4FbWGjTNGliNeuMDBu+YoWFYKNGFlqbNvlB2aiR/ZCrrCz+w11XnZHhX9/+U1ThhuhxiMQP73iPHTv833u4YHaB6jRrZuNFH5hatLADS/R20aKFTZeVZfMo/+vi6HJVdYApKbHtIvrg5LZH92je3Oa3a5dNt3WrbVNNm9r0brvMybFtz5XFLddts+np/oErM9Nfv245paVWBrduorvRlQr3Rx+NGvkHovR0G7ew0PaFoiJbh40b23jbt9uysrOtnO6HZO7PQ1yjX3R/+QdgFZwNG+w9u0pITk5iTrh+0sFaW64WmJlpH3IYRCK2E+Xk+D/ucTtv8+b2frZts9e2bbNH+/b+TuKmLyuzDW7rVtswO3SwaV2NfscO20D32cf63RljJGI7TlmZBU16ui3b/V1YdGiUlNiBJy3Nry1H117jPaJrtllZVj53K0kXFKWl9ujc2d+Z3UGvsvlW98jOtvfqasqAHx6uBrthgwXW7t02XuPGdoDctcsPhawsO1C69eK2r+jaswvs6tZDerp9puvWVTx7yciw8rn7v7sQb97cv4VBdOBt3+7XyAF/2dnZNsyFp1vHkYgfvm657vOLPitxFRMX0K5JzW0j0e8zN9fK0aOHjbtzp21b7dv720txsZXZhb7bpqK3reizEdevaj80a9PGPgdXruLixNxCgMHawKWl+Ufgyu4x3apV1c2AVWnffu+mIwqyujYjN9SvUYiIUobBSkSUYAxWIqIEY7ASESUYg5WIKMEYrERECcZgJSJKMAYrEVGCMViJiBKMwUpElGAMViKiBGOwEhElGIOViCjBGKxERAnGYCUiSjAGKxFRgjFYiYgSjMFKRJRgDFYiogRjsBIRJVjgglVEThaRxSKyTERuSXV5iIhqK1DBKiLpAB4HcAqAQwCMFJFDUlsqIqLaCVSwAhgAYJmqrlDV3QBeATAsxWUiIqqVoAVrRwA/RD3P94YREYVGRqoLUFsicgWAK7ynu0Tk21SWpw72AbA+1YXYC2EtNxDesoe13EB4y35QXSYOWrAWAOgc9byTN2wPVX0KwFMAICKzVLVf/RUvccJa9rCWGwhv2cNabiC8ZReRWXWZPmhNAV8B6C4iXUUkC8AIAO+kuExERLUSqBqrqpaKyLUAPgCQDuBZVZ2f4mIREdVKoIIVAFR1MoDJNRz9qWSWJcnCWvawlhsIb9nDWm4gvGWvU7lFVRNVECIiQvDaWImIQi+0wRqmn76KyEoRmSciee7bRhFpLSIfishSr9sq1eUEABF5VkQKoy9jq6ysYh71PoO5ItI3YOW+W0QKvPWeJyJDo1671Sv3YhE5KTWl3lOWziIyTUQWiMh8EbneGx7o9V5FuQO/3kUkR0Rmisgcr+z3eMO7isiXXhlf9b5Eh4hke8+Xea93qXIBqhq6B+yLreUAugHIAjAHwCGpLlcV5V0JYJ9ywx4EcIvXfwuAB1JdTq8sRwPoC+Db6soKYCiA9wAIgIEAvgxYue8G8Ps44x7ibTPZALp621J6CsveAUBfr78ZgCVeGQO93qsod+DXu7fumnr9mQC+9NblRAAjvOHjAFzl9V8NYJzXPwLAq1XNP6w11obw09dhAMZ7/eMBnJnCsuyhqtMBbCw3uLKyDgPwvJr/AmgpIh3qp6SxKil3ZYYBeEVVd6nqdwCWwbaplFDV1ao62+vfCmAh7BeHgV7vVZS7MoFZ79662+Y9zfQeCuA4AJO84eXXufssJgEYIiJS2fzDGqxh++mrApgiIl97vxwDgHaqutrrXwOgXWqKViOVlTUMn8O13unys1HNLYEtt3eK2QdWgwrNei9XbiAE611E0kUkD0AhgA9hNegiVS2NU749Zfde3wygTWXzDmuwhs2RqtoXdteua0Tk6OgX1c4vQnF5RpjKCuAJAPsD6A1gNYCHUlucqolIUwCvA7hBVbdEvxbk9R6n3KFY76papqq9Yb/wHACgR6LmHdZgrfanr0GiqgVetxDAm7APca07ffO6hakrYbUqK2ugPwdVXevtPBEA/4R/2hm4cotIJiycJqjqG97gwK/3eOUO03oHAFUtAjANwCBYs4q7vj+6fHvK7r3eAsCGyuYZ1mANzU9fRaSJiDRz/QBOBPAtrLwXeaNdBODt1JSwRior6zsAfu19Sz0QwOaoU9eUK9fueBZsvQNW7hHeN71dAXQHMLO+y+d4bXXPAFioqn+LeinQ672ycodhvYtIWxFp6fU3AnACrI14GoBzvNHKr3P3WZwD4GPvLCK+VHwjl6Bv9YbCvoVcDuD2VJeninJ2g30TOgfAfFdWWPvMVABLAXwEoHWqy+qV62XY6VsJrI3p0srKCvtm9XHvM5gHoF/Ayv2CV6653o7RIWr8271yLwZwSorX+ZGw0/y5APK8x9Cgr/cqyh349Q7gMADfeGX8FsCd3vBusLBfBuA1ANne8Bzv+TLv9W5VzZ+/vCIiSrCwNgUQEQUWg5WIKMEYrERECcZgJSJKMAYrEVGCMViJPCJyrIj8O9XloPBjsBIRJRiDlUJHRC7w7qWZJyJPejfT2CYiD3v31pwqIm29cXuLyH+9G4K8GXVP0wNE5CPvfpyzRWR/b/ZNRWSSiCwSkQlV3cGIqDIMVgoVETkYwHAAg9VuoFEGYBSAJgBmqWpPAJ8CuMub5HkAN6vqYbBfA7nhEwA8rqq9ABwB+9UWYHdougF279BuAAYn/U1RgxO4PxMkqsYQAIcD+MqrTDaC3ZwkAuBVb5wXAbwhIi0AtFTVT73h4wG85t27oaOqvgkAqloMAN78Zqpqvvc8D0AXADOS/7aoIWGwUtgIgPGqemvMQJE7yo23t7/V3hXVXwbuI7QX2BRAYTMVwDkisi+w53+h9oNty+6uROcDmKGqmwFsEpGjvOEXAvhU7W73+SJypjePbBFpXK/vgho0Ho0pVFR1gYiMgf0jQxrsblbXANgOYID3WiGsHRawW72N84JzBYCLveEXAnhSRP7ozePcenwb1MDx7lbUIIjINlVtmupyEAFsCiAiSjjWWImIEow1ViKiBGOwEhElGIOViCjBGKxERAnGYCUiSjAGKxFRgv0/lfeOrIQ2URcAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "outputId": "041ce90b-621e-417f-de74-963b9984e29e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,465\n",
            "Trainable params: 2,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "outputId": "8fb7b0d3-266e-4137-9578-cbc8364556ad",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 2036.8237 - val_loss: 238.9794\n",
            "Epoch 2/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 71.2563 - val_loss: 68.6257\n",
            "Epoch 3/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 37.7505 - val_loss: 53.0072\n",
            "Epoch 4/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 36.3269 - val_loss: 40.0826\n",
            "Epoch 5/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 35.4557 - val_loss: 50.7349\n",
            "Epoch 6/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 34.7203 - val_loss: 47.3723\n",
            "Epoch 7/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 34.2006 - val_loss: 52.6564\n",
            "Epoch 8/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 33.9622 - val_loss: 37.4672\n",
            "Epoch 9/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 33.5952 - val_loss: 47.6114\n",
            "Epoch 10/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 33.1122 - val_loss: 37.7540\n",
            "Epoch 11/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 33.0564 - val_loss: 41.3466\n",
            "Epoch 12/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 32.9381 - val_loss: 40.2511\n",
            "Epoch 13/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 32.6315 - val_loss: 42.7363\n",
            "Epoch 14/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 32.5248 - val_loss: 38.1167\n",
            "Epoch 15/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 32.3951 - val_loss: 69.8228\n",
            "Epoch 16/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 32.2158 - val_loss: 41.3308\n",
            "Epoch 17/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 32.1511 - val_loss: 51.1441\n",
            "Epoch 18/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.9991 - val_loss: 40.7372\n",
            "Epoch 19/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.9566 - val_loss: 49.1695\n",
            "Epoch 20/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.8928 - val_loss: 36.3049\n",
            "Epoch 21/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.9076 - val_loss: 38.7025\n",
            "Epoch 22/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.7588 - val_loss: 45.6341\n",
            "Epoch 23/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.5496 - val_loss: 44.5137\n",
            "Epoch 24/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.4811 - val_loss: 41.2404\n",
            "Epoch 25/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.4704 - val_loss: 42.8296\n",
            "Epoch 26/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.4549 - val_loss: 38.2996\n",
            "Epoch 27/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.3584 - val_loss: 38.3157\n",
            "Epoch 28/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.2987 - val_loss: 43.5906\n",
            "Epoch 29/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.2275 - val_loss: 39.7337\n",
            "Epoch 30/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.2127 - val_loss: 37.0406\n",
            "Epoch 31/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.1578 - val_loss: 41.4637\n",
            "Epoch 32/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.0805 - val_loss: 35.5493\n",
            "Epoch 33/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.0732 - val_loss: 58.1645\n",
            "Epoch 34/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.0832 - val_loss: 39.0975\n",
            "Epoch 35/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.9893 - val_loss: 49.1713\n",
            "Epoch 36/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.9566 - val_loss: 43.4382\n",
            "Epoch 37/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.8250 - val_loss: 46.0089\n",
            "Epoch 38/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.7846 - val_loss: 72.1691\n",
            "Epoch 39/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.7413 - val_loss: 38.9745\n",
            "Epoch 40/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.7207 - val_loss: 50.3631\n",
            "Epoch 41/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.7259 - val_loss: 34.4287\n",
            "Epoch 42/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.5948 - val_loss: 37.5774\n",
            "Epoch 43/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.5663 - val_loss: 36.8139\n",
            "Epoch 44/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.5844 - val_loss: 34.3463\n",
            "Epoch 45/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.4576 - val_loss: 38.7905\n",
            "Epoch 46/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.5315 - val_loss: 43.8300\n",
            "Epoch 47/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.4914 - val_loss: 53.7532\n",
            "Epoch 48/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.4373 - val_loss: 49.1647\n",
            "Epoch 49/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.4577 - val_loss: 33.8498\n",
            "Epoch 50/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.4058 - val_loss: 64.1075\n",
            "Epoch 51/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.3955 - val_loss: 48.6810\n",
            "Epoch 52/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.2277 - val_loss: 36.8341\n",
            "Epoch 53/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.2725 - val_loss: 44.2271\n",
            "Epoch 54/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.2724 - val_loss: 39.0655\n",
            "Epoch 55/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.3297 - val_loss: 62.4098\n",
            "Epoch 56/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.1879 - val_loss: 38.8158\n",
            "Epoch 57/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.2193 - val_loss: 40.4460\n",
            "Epoch 58/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.1164 - val_loss: 38.0299\n",
            "Epoch 59/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.1503 - val_loss: 45.6814\n",
            "Epoch 60/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 30.1364 - val_loss: 35.2565\n",
            "Epoch 61/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.1333 - val_loss: 40.9634\n",
            "Epoch 62/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.1116 - val_loss: 43.0504\n",
            "Epoch 63/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.9849 - val_loss: 38.3049\n",
            "Epoch 64/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.0479 - val_loss: 47.8083\n",
            "Epoch 65/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.1294 - val_loss: 38.2555\n",
            "Epoch 66/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.1165 - val_loss: 37.4681\n",
            "Epoch 67/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.9453 - val_loss: 34.5168\n",
            "Epoch 68/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.9360 - val_loss: 37.6396\n",
            "Epoch 69/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.9328 - val_loss: 48.7828\n",
            "Epoch 70/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.9195 - val_loss: 40.0674\n",
            "Epoch 71/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.8638 - val_loss: 44.6918\n",
            "Epoch 72/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.9323 - val_loss: 34.9783\n",
            "Epoch 73/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.8289 - val_loss: 36.2833\n",
            "Epoch 74/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.8473 - val_loss: 34.6731\n",
            "Epoch 75/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.8596 - val_loss: 35.4628\n",
            "Epoch 76/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.8186 - val_loss: 36.7093\n",
            "Epoch 77/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7877 - val_loss: 42.8451\n",
            "Epoch 78/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7690 - val_loss: 37.2998\n",
            "Epoch 79/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7356 - val_loss: 39.5934\n",
            "Epoch 80/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7870 - val_loss: 37.6290\n",
            "Epoch 81/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7295 - val_loss: 34.9538\n",
            "Epoch 82/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7635 - val_loss: 37.3101\n",
            "Epoch 83/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6782 - val_loss: 36.2607\n",
            "Epoch 84/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7368 - val_loss: 47.6767\n",
            "Epoch 85/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6819 - val_loss: 34.6932\n",
            "Epoch 86/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6480 - val_loss: 36.8499\n",
            "Epoch 87/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6341 - val_loss: 57.3967\n",
            "Epoch 88/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6142 - val_loss: 32.9932\n",
            "Epoch 89/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6112 - val_loss: 33.4333\n",
            "Epoch 90/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5730 - val_loss: 34.1057\n",
            "Epoch 91/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6785 - val_loss: 35.3302\n",
            "Epoch 92/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5951 - val_loss: 32.9409\n",
            "Epoch 93/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5255 - val_loss: 33.4747\n",
            "Epoch 94/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5615 - val_loss: 34.3147\n",
            "Epoch 95/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5130 - val_loss: 43.5765\n",
            "Epoch 96/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5820 - val_loss: 35.6451\n",
            "Epoch 97/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5263 - val_loss: 35.2086\n",
            "Epoch 98/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4806 - val_loss: 33.5820\n",
            "Epoch 99/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5044 - val_loss: 33.9275\n",
            "Epoch 100/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4274 - val_loss: 40.8468\n",
            "Epoch 101/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3927 - val_loss: 36.8476\n",
            "Epoch 102/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4793 - val_loss: 35.3487\n",
            "Epoch 103/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4369 - val_loss: 37.3474\n",
            "Epoch 104/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4365 - val_loss: 41.9287\n",
            "Epoch 105/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4424 - val_loss: 35.1285\n",
            "Epoch 106/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4245 - val_loss: 35.7567\n",
            "Epoch 107/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3426 - val_loss: 34.2375\n",
            "Epoch 108/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4671 - val_loss: 34.4818\n",
            "Epoch 109/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.3897 - val_loss: 37.1457\n",
            "Epoch 110/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4137 - val_loss: 35.4857\n",
            "Epoch 111/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4128 - val_loss: 35.7101\n",
            "Epoch 112/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3301 - val_loss: 34.2736\n",
            "Epoch 113/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4246 - val_loss: 45.4979\n",
            "Epoch 114/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4063 - val_loss: 32.8989\n",
            "Epoch 115/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3748 - val_loss: 36.0822\n",
            "Epoch 116/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3157 - val_loss: 46.7424\n",
            "Epoch 117/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3815 - val_loss: 42.7638\n",
            "Epoch 118/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3842 - val_loss: 32.7278\n",
            "Epoch 119/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2862 - val_loss: 36.0772\n",
            "Epoch 120/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3180 - val_loss: 35.8811\n",
            "Epoch 121/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2626 - val_loss: 35.6255\n",
            "Epoch 122/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3386 - val_loss: 41.8612\n",
            "Epoch 123/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2380 - val_loss: 36.7943\n",
            "Epoch 124/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2409 - val_loss: 35.7069\n",
            "Epoch 125/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2293 - val_loss: 48.5702\n",
            "Epoch 126/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2441 - val_loss: 34.5157\n",
            "Epoch 127/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2098 - val_loss: 36.8153\n",
            "Epoch 128/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2067 - val_loss: 33.9651\n",
            "Epoch 129/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2946 - val_loss: 35.6735\n",
            "Epoch 130/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2895 - val_loss: 43.0104\n",
            "Epoch 131/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2103 - val_loss: 38.4029\n",
            "Epoch 132/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1911 - val_loss: 36.0980\n",
            "Epoch 133/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2587 - val_loss: 35.6521\n",
            "Epoch 134/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2105 - val_loss: 38.2057\n",
            "Epoch 135/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2119 - val_loss: 34.0533\n",
            "Epoch 136/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2216 - val_loss: 38.5366\n",
            "Epoch 137/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1487 - val_loss: 39.6404\n",
            "Epoch 138/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1851 - val_loss: 33.2052\n",
            "Epoch 139/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1947 - val_loss: 32.4433\n",
            "Epoch 140/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1868 - val_loss: 33.3065\n",
            "Epoch 141/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2419 - val_loss: 42.7146\n",
            "Epoch 142/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1331 - val_loss: 36.7828\n",
            "Epoch 143/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1681 - val_loss: 33.9232\n",
            "Epoch 144/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1396 - val_loss: 35.6891\n",
            "Epoch 145/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1540 - val_loss: 50.9695\n",
            "Epoch 146/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1209 - val_loss: 32.8408\n",
            "Epoch 147/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1109 - val_loss: 34.9209\n",
            "Epoch 148/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0943 - val_loss: 39.1561\n",
            "Epoch 149/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1931 - val_loss: 33.4882\n",
            "Epoch 150/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1724 - val_loss: 45.0949\n",
            "Epoch 151/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1329 - val_loss: 44.3689\n",
            "Epoch 152/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1361 - val_loss: 36.0138\n",
            "Epoch 153/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0927 - val_loss: 32.6347\n",
            "Epoch 154/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0383 - val_loss: 44.5470\n",
            "Epoch 155/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0312 - val_loss: 32.9512\n",
            "Epoch 156/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0223 - val_loss: 39.0688\n",
            "Epoch 157/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1005 - val_loss: 46.2917\n",
            "Epoch 158/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0846 - val_loss: 36.9981\n",
            "Epoch 159/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1234 - val_loss: 32.8998\n",
            "Epoch 160/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0861 - val_loss: 38.7158\n",
            "Epoch 161/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0765 - val_loss: 43.2225\n",
            "Epoch 162/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1085 - val_loss: 36.3933\n",
            "Epoch 163/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0521 - val_loss: 33.6370\n",
            "Epoch 164/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0541 - val_loss: 35.1682\n",
            "Epoch 165/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0881 - val_loss: 39.6268\n",
            "Epoch 166/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1007 - val_loss: 34.6075\n",
            "Epoch 167/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0346 - val_loss: 32.4584\n",
            "Epoch 168/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0645 - val_loss: 36.2384\n",
            "Epoch 169/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 28.9986 - val_loss: 40.8438\n",
            "Epoch 170/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0270 - val_loss: 33.7601\n",
            "Epoch 171/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0593 - val_loss: 39.7852\n",
            "Epoch 172/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1094 - val_loss: 33.8834\n",
            "Epoch 173/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0625 - val_loss: 37.4000\n",
            "Epoch 174/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9678 - val_loss: 37.3894\n",
            "Epoch 175/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0024 - val_loss: 38.2453\n",
            "Epoch 176/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1284 - val_loss: 39.0468\n",
            "Epoch 177/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9650 - val_loss: 37.2986\n",
            "Epoch 178/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0102 - val_loss: 57.2427\n",
            "Epoch 179/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9867 - val_loss: 35.2487\n",
            "Epoch 180/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8805 - val_loss: 33.5160\n",
            "Epoch 181/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0256 - val_loss: 35.9965\n",
            "Epoch 182/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1159 - val_loss: 34.7748\n",
            "Epoch 183/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0322 - val_loss: 33.3840\n",
            "Epoch 184/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0375 - val_loss: 35.3179\n",
            "Epoch 185/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9572 - val_loss: 35.8313\n",
            "Epoch 186/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9281 - val_loss: 32.8407\n",
            "Epoch 187/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9653 - val_loss: 38.9197\n",
            "Epoch 188/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0040 - val_loss: 39.1790\n",
            "Epoch 189/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0294 - val_loss: 35.6036\n",
            "Epoch 190/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9553 - val_loss: 41.2673\n",
            "Epoch 191/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0209 - val_loss: 43.3987\n",
            "Epoch 192/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0204 - val_loss: 41.6493\n",
            "Epoch 193/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0204 - val_loss: 42.1696\n",
            "Epoch 194/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9745 - val_loss: 31.9056\n",
            "Epoch 195/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8685 - val_loss: 32.4617\n",
            "Epoch 196/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9715 - val_loss: 32.7335\n",
            "Epoch 197/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9336 - val_loss: 33.6970\n",
            "Epoch 198/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9238 - val_loss: 33.7500\n",
            "Epoch 199/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9449 - val_loss: 34.1434\n",
            "Epoch 200/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9150 - val_loss: 36.3494\n",
            "Epoch 201/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9603 - val_loss: 32.1311\n",
            "Epoch 202/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9110 - val_loss: 38.1860\n",
            "Epoch 203/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9576 - val_loss: 35.6437\n",
            "Epoch 204/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9424 - val_loss: 34.4391\n",
            "Epoch 205/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8934 - val_loss: 37.3017\n",
            "Epoch 206/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9542 - val_loss: 36.1220\n",
            "Epoch 207/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8813 - val_loss: 33.5982\n",
            "Epoch 208/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8652 - val_loss: 34.6209\n",
            "Epoch 209/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8792 - val_loss: 40.3955\n",
            "Epoch 210/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8432 - val_loss: 33.0586\n",
            "Epoch 211/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8369 - val_loss: 32.8037\n",
            "Epoch 212/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7937 - val_loss: 33.7357\n",
            "Epoch 213/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8754 - val_loss: 35.4960\n",
            "Epoch 214/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8320 - val_loss: 47.7479\n",
            "Epoch 215/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8178 - val_loss: 35.1276\n",
            "Epoch 216/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8442 - val_loss: 35.8977\n",
            "Epoch 217/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8881 - val_loss: 42.6889\n",
            "Epoch 218/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8695 - val_loss: 35.0180\n",
            "Epoch 219/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8943 - val_loss: 41.7131\n",
            "Epoch 220/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8194 - val_loss: 33.7955\n",
            "Epoch 221/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8228 - val_loss: 33.1977\n",
            "Epoch 222/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8541 - val_loss: 33.1816\n",
            "Epoch 223/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9640 - val_loss: 50.4196\n",
            "Epoch 224/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8125 - val_loss: 32.3041\n",
            "Epoch 225/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 28.8494 - val_loss: 35.1950\n",
            "Epoch 226/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8330 - val_loss: 32.7468\n",
            "Epoch 227/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8133 - val_loss: 36.5782\n",
            "Epoch 228/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8516 - val_loss: 33.3794\n",
            "Epoch 229/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8458 - val_loss: 33.1478\n",
            "Epoch 230/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8715 - val_loss: 37.3333\n",
            "Epoch 231/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7545 - val_loss: 34.5630\n",
            "Epoch 232/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8328 - val_loss: 32.4382\n",
            "Epoch 233/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8093 - val_loss: 36.8756\n",
            "Epoch 234/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8436 - val_loss: 32.9480\n",
            "Epoch 235/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8376 - val_loss: 33.9295\n",
            "Epoch 236/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8586 - val_loss: 32.9044\n",
            "Epoch 237/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7668 - val_loss: 43.0320\n",
            "Epoch 238/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8365 - val_loss: 42.0354\n",
            "Epoch 239/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8540 - val_loss: 36.8696\n",
            "Epoch 240/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8143 - val_loss: 34.5142\n",
            "Epoch 241/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8575 - val_loss: 31.6091\n",
            "Epoch 242/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7387 - val_loss: 38.2772\n",
            "Epoch 243/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7119 - val_loss: 34.4982\n",
            "Epoch 244/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7683 - val_loss: 39.1380\n",
            "Epoch 245/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7579 - val_loss: 33.4454\n",
            "Epoch 246/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6913 - val_loss: 32.8554\n",
            "Epoch 247/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7960 - val_loss: 32.4747\n",
            "Epoch 248/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8750 - val_loss: 39.3526\n",
            "Epoch 249/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7979 - val_loss: 32.7275\n",
            "Epoch 250/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8272 - val_loss: 32.9620\n",
            "Epoch 251/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7373 - val_loss: 37.4396\n",
            "Epoch 252/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7840 - val_loss: 33.2948\n",
            "Epoch 253/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7495 - val_loss: 34.2570\n",
            "Epoch 254/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8405 - val_loss: 32.8183\n",
            "Epoch 255/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7596 - val_loss: 33.3118\n",
            "Epoch 256/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6842 - val_loss: 36.7721\n",
            "Epoch 257/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7123 - val_loss: 33.3144\n",
            "Epoch 258/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7663 - val_loss: 34.1877\n",
            "Epoch 259/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7692 - val_loss: 34.0357\n",
            "Epoch 260/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8271 - val_loss: 31.9560\n",
            "Epoch 261/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8197 - val_loss: 37.7891\n",
            "Epoch 262/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7527 - val_loss: 36.4239\n",
            "Epoch 263/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8135 - val_loss: 35.6795\n",
            "Epoch 264/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7061 - val_loss: 32.7465\n",
            "Epoch 265/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7592 - val_loss: 43.2758\n",
            "Epoch 266/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7854 - val_loss: 35.9349\n",
            "Epoch 267/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6856 - val_loss: 33.5820\n",
            "Epoch 268/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6610 - val_loss: 37.6004\n",
            "Epoch 269/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7234 - val_loss: 32.1073\n",
            "Epoch 270/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7031 - val_loss: 33.9921\n",
            "Epoch 271/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7065 - val_loss: 33.4158\n",
            "Epoch 272/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8327 - val_loss: 34.3371\n",
            "Epoch 273/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7306 - val_loss: 38.2177\n",
            "Epoch 274/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8056 - val_loss: 35.0890\n",
            "Epoch 275/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7220 - val_loss: 32.7301\n",
            "Epoch 276/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6710 - val_loss: 32.8354\n",
            "Epoch 277/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7185 - val_loss: 35.4931\n",
            "Epoch 278/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7397 - val_loss: 34.3964\n",
            "Epoch 279/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7185 - val_loss: 33.1538\n",
            "Epoch 280/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7149 - val_loss: 33.1696\n",
            "Epoch 281/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7760 - val_loss: 33.2307\n",
            "Epoch 282/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6782 - val_loss: 40.8403\n",
            "Epoch 283/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6883 - val_loss: 32.3015\n",
            "Epoch 284/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6721 - val_loss: 32.3901\n",
            "Epoch 285/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6164 - val_loss: 35.6910\n",
            "Epoch 286/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6458 - val_loss: 34.4623\n",
            "Epoch 287/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7218 - val_loss: 34.1142\n",
            "Epoch 288/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7082 - val_loss: 34.1301\n",
            "Epoch 289/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7502 - val_loss: 32.7608\n",
            "Epoch 290/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6758 - val_loss: 38.6504\n",
            "Epoch 291/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7063 - val_loss: 33.6513\n",
            "Epoch 292/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 28.7362 - val_loss: 32.9298\n",
            "Epoch 293/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6971 - val_loss: 35.8343\n",
            "Epoch 294/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7855 - val_loss: 31.6380\n",
            "Epoch 295/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6930 - val_loss: 32.4253\n",
            "Epoch 296/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6505 - val_loss: 34.1701\n",
            "Epoch 297/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6586 - val_loss: 39.4524\n",
            "Epoch 298/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6834 - val_loss: 34.0570\n",
            "Epoch 299/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6253 - val_loss: 32.7005\n",
            "Epoch 300/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5863 - val_loss: 39.9352\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "outputId": "586d357f-1b10-440a-9fad-1e858d1ab2ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  2.6051882816348453 \n",
            "MAE:  4.735608207193525 \n",
            "SD:  5.757446541821676\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "outputId": "c167fbd5-c25c-445a-b428-cbfb3dfc357c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "outputId": "3bd6eb93-6584-4f2a-cfd4-e3ac575d4fce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_15 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,465\n",
            "Trainable params: 2,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "outputId": "476e8178-9f37-4b24-dbb1-f783f7df5bc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 1896.9331 - val_loss: 250.9387\n",
            "Epoch 2/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 59.3966 - val_loss: 49.4073\n",
            "Epoch 3/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 37.4121 - val_loss: 38.3300\n",
            "Epoch 4/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 36.1731 - val_loss: 41.3554\n",
            "Epoch 5/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 35.3108 - val_loss: 62.0885\n",
            "Epoch 6/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 34.3373 - val_loss: 40.5551\n",
            "Epoch 7/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 33.7482 - val_loss: 40.8148\n",
            "Epoch 8/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 33.4314 - val_loss: 38.4066\n",
            "Epoch 9/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 32.9764 - val_loss: 37.2930\n",
            "Epoch 10/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 32.9091 - val_loss: 37.8122\n",
            "Epoch 11/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 32.6472 - val_loss: 36.7697\n",
            "Epoch 12/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 32.4763 - val_loss: 36.7519\n",
            "Epoch 13/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 32.1636 - val_loss: 61.5008\n",
            "Epoch 14/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 32.0407 - val_loss: 38.5485\n",
            "Epoch 15/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.9689 - val_loss: 34.8521\n",
            "Epoch 16/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.7875 - val_loss: 77.7307\n",
            "Epoch 17/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.5769 - val_loss: 36.3392\n",
            "Epoch 18/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.6862 - val_loss: 40.5657\n",
            "Epoch 19/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.4395 - val_loss: 37.3843\n",
            "Epoch 20/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.4261 - val_loss: 47.9249\n",
            "Epoch 21/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.3820 - val_loss: 42.6846\n",
            "Epoch 22/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.0981 - val_loss: 38.1396\n",
            "Epoch 23/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.1480 - val_loss: 38.7165\n",
            "Epoch 24/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.1217 - val_loss: 34.4161\n",
            "Epoch 25/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.0638 - val_loss: 35.8067\n",
            "Epoch 26/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 31.0693 - val_loss: 39.1475\n",
            "Epoch 27/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.9347 - val_loss: 58.8555\n",
            "Epoch 28/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 30.9193 - val_loss: 36.6428\n",
            "Epoch 29/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.8595 - val_loss: 39.1187\n",
            "Epoch 30/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.7856 - val_loss: 35.8635\n",
            "Epoch 31/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.8298 - val_loss: 40.3515\n",
            "Epoch 32/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.7220 - val_loss: 42.2038\n",
            "Epoch 33/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.7460 - val_loss: 62.9964\n",
            "Epoch 34/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.5915 - val_loss: 42.0624\n",
            "Epoch 35/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.6030 - val_loss: 37.4871\n",
            "Epoch 36/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.6354 - val_loss: 40.8821\n",
            "Epoch 37/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.4508 - val_loss: 42.7757\n",
            "Epoch 38/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.4674 - val_loss: 35.3498\n",
            "Epoch 39/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.4120 - val_loss: 36.4896\n",
            "Epoch 40/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.5217 - val_loss: 48.4802\n",
            "Epoch 41/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.4790 - val_loss: 34.6091\n",
            "Epoch 42/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.3855 - val_loss: 36.0877\n",
            "Epoch 43/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.3728 - val_loss: 38.9737\n",
            "Epoch 44/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.2652 - val_loss: 46.2073\n",
            "Epoch 45/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.2000 - val_loss: 42.2580\n",
            "Epoch 46/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.2398 - val_loss: 35.9865\n",
            "Epoch 47/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.1618 - val_loss: 37.3186\n",
            "Epoch 48/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.1476 - val_loss: 35.9284\n",
            "Epoch 49/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.0430 - val_loss: 42.1829\n",
            "Epoch 50/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.1485 - val_loss: 34.7610\n",
            "Epoch 51/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.1125 - val_loss: 41.5619\n",
            "Epoch 52/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.0432 - val_loss: 37.5650\n",
            "Epoch 53/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.1604 - val_loss: 46.6320\n",
            "Epoch 54/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.0528 - val_loss: 40.3192\n",
            "Epoch 55/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.0231 - val_loss: 38.8271\n",
            "Epoch 56/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.9516 - val_loss: 54.1161\n",
            "Epoch 57/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.9539 - val_loss: 41.7806\n",
            "Epoch 58/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.9297 - val_loss: 33.1676\n",
            "Epoch 59/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.9987 - val_loss: 33.7862\n",
            "Epoch 60/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.9751 - val_loss: 43.9893\n",
            "Epoch 61/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 30.0711 - val_loss: 53.4976\n",
            "Epoch 62/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7964 - val_loss: 34.6892\n",
            "Epoch 63/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7514 - val_loss: 35.9894\n",
            "Epoch 64/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7730 - val_loss: 44.8874\n",
            "Epoch 65/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.8463 - val_loss: 33.5024\n",
            "Epoch 66/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7802 - val_loss: 39.5218\n",
            "Epoch 67/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.8099 - val_loss: 34.7766\n",
            "Epoch 68/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7841 - val_loss: 34.6376\n",
            "Epoch 69/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7356 - val_loss: 40.2730\n",
            "Epoch 70/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6398 - val_loss: 34.6964\n",
            "Epoch 71/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7340 - val_loss: 34.3421\n",
            "Epoch 72/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7770 - val_loss: 35.0611\n",
            "Epoch 73/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7363 - val_loss: 36.3268\n",
            "Epoch 74/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.8452 - val_loss: 46.4282\n",
            "Epoch 75/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5976 - val_loss: 39.0503\n",
            "Epoch 76/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5981 - val_loss: 38.8446\n",
            "Epoch 77/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.7084 - val_loss: 44.7187\n",
            "Epoch 78/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6487 - val_loss: 37.2315\n",
            "Epoch 79/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6364 - val_loss: 37.0336\n",
            "Epoch 80/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6701 - val_loss: 42.1681\n",
            "Epoch 81/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6189 - val_loss: 41.7262\n",
            "Epoch 82/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6233 - val_loss: 34.0716\n",
            "Epoch 83/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6143 - val_loss: 38.6892\n",
            "Epoch 84/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5575 - val_loss: 48.2282\n",
            "Epoch 85/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4287 - val_loss: 33.4289\n",
            "Epoch 86/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5226 - val_loss: 36.1465\n",
            "Epoch 87/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5242 - val_loss: 32.0638\n",
            "Epoch 88/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.6356 - val_loss: 31.9462\n",
            "Epoch 89/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5680 - val_loss: 35.7897\n",
            "Epoch 90/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3633 - val_loss: 34.0295\n",
            "Epoch 91/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4179 - val_loss: 34.3705\n",
            "Epoch 92/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4483 - val_loss: 36.4773\n",
            "Epoch 93/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4499 - val_loss: 38.2942\n",
            "Epoch 94/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5807 - val_loss: 31.8021\n",
            "Epoch 95/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 29.3863 - val_loss: 43.2394\n",
            "Epoch 96/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4966 - val_loss: 34.7586\n",
            "Epoch 97/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3861 - val_loss: 36.7855\n",
            "Epoch 98/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4401 - val_loss: 34.6756\n",
            "Epoch 99/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4708 - val_loss: 33.6588\n",
            "Epoch 100/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.5395 - val_loss: 47.0987\n",
            "Epoch 101/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4217 - val_loss: 40.3881\n",
            "Epoch 102/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4026 - val_loss: 32.8892\n",
            "Epoch 103/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2850 - val_loss: 37.9193\n",
            "Epoch 104/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4344 - val_loss: 38.2087\n",
            "Epoch 105/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3752 - val_loss: 42.3698\n",
            "Epoch 106/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3012 - val_loss: 34.5190\n",
            "Epoch 107/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3221 - val_loss: 33.8893\n",
            "Epoch 108/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3219 - val_loss: 38.7512\n",
            "Epoch 109/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3347 - val_loss: 35.7978\n",
            "Epoch 110/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.4016 - val_loss: 38.1997\n",
            "Epoch 111/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3079 - val_loss: 35.2243\n",
            "Epoch 112/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2478 - val_loss: 36.9918\n",
            "Epoch 113/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3559 - val_loss: 45.2905\n",
            "Epoch 114/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3192 - val_loss: 37.1048\n",
            "Epoch 115/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3037 - val_loss: 39.3241\n",
            "Epoch 116/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2709 - val_loss: 41.2213\n",
            "Epoch 117/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2963 - val_loss: 40.1269\n",
            "Epoch 118/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3419 - val_loss: 37.1302\n",
            "Epoch 119/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2672 - val_loss: 62.3002\n",
            "Epoch 120/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.3214 - val_loss: 35.7550\n",
            "Epoch 121/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1639 - val_loss: 36.2033\n",
            "Epoch 122/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2704 - val_loss: 50.8329\n",
            "Epoch 123/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2128 - val_loss: 37.6294\n",
            "Epoch 124/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2760 - val_loss: 33.9683\n",
            "Epoch 125/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2974 - val_loss: 36.4854\n",
            "Epoch 126/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2312 - val_loss: 44.9398\n",
            "Epoch 127/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1778 - val_loss: 35.6688\n",
            "Epoch 128/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2215 - val_loss: 35.0898\n",
            "Epoch 129/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2079 - val_loss: 41.5940\n",
            "Epoch 130/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2728 - val_loss: 36.6467\n",
            "Epoch 131/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1891 - val_loss: 47.3079\n",
            "Epoch 132/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1890 - val_loss: 34.0302\n",
            "Epoch 133/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1072 - val_loss: 33.0441\n",
            "Epoch 134/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1587 - val_loss: 35.3391\n",
            "Epoch 135/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1518 - val_loss: 34.5430\n",
            "Epoch 136/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1943 - val_loss: 36.2461\n",
            "Epoch 137/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1974 - val_loss: 33.8520\n",
            "Epoch 138/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1542 - val_loss: 34.6010\n",
            "Epoch 139/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1342 - val_loss: 46.1917\n",
            "Epoch 140/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1631 - val_loss: 33.7695\n",
            "Epoch 141/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1515 - val_loss: 38.1712\n",
            "Epoch 142/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1724 - val_loss: 32.7012\n",
            "Epoch 143/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0563 - val_loss: 36.5155\n",
            "Epoch 144/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1620 - val_loss: 33.9238\n",
            "Epoch 145/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.2201 - val_loss: 41.2092\n",
            "Epoch 146/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1483 - val_loss: 38.9134\n",
            "Epoch 147/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0484 - val_loss: 41.0939\n",
            "Epoch 148/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1000 - val_loss: 40.6475\n",
            "Epoch 149/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0300 - val_loss: 49.5774\n",
            "Epoch 150/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0707 - val_loss: 34.6718\n",
            "Epoch 151/300\n",
            "1319/1319 [==============================] - 6s 4ms/step - loss: 29.1000 - val_loss: 38.0499\n",
            "Epoch 152/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.1847 - val_loss: 45.9650\n",
            "Epoch 153/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0214 - val_loss: 37.4164\n",
            "Epoch 154/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0251 - val_loss: 32.9403\n",
            "Epoch 155/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0890 - val_loss: 35.6408\n",
            "Epoch 156/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0533 - val_loss: 38.8304\n",
            "Epoch 157/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0981 - val_loss: 32.9171\n",
            "Epoch 158/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9291 - val_loss: 33.6641\n",
            "Epoch 159/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9528 - val_loss: 32.9257\n",
            "Epoch 160/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0141 - val_loss: 37.8906\n",
            "Epoch 161/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0111 - val_loss: 35.6888\n",
            "Epoch 162/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9728 - val_loss: 34.7891\n",
            "Epoch 163/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9693 - val_loss: 44.9832\n",
            "Epoch 164/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9888 - val_loss: 39.3500\n",
            "Epoch 165/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9306 - val_loss: 46.3581\n",
            "Epoch 166/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0733 - val_loss: 34.1568\n",
            "Epoch 167/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9539 - val_loss: 39.1560\n",
            "Epoch 168/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 29.0000 - val_loss: 32.3625\n",
            "Epoch 169/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9758 - val_loss: 34.3908\n",
            "Epoch 170/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9659 - val_loss: 39.9699\n",
            "Epoch 171/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9162 - val_loss: 35.7397\n",
            "Epoch 172/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9110 - val_loss: 41.4213\n",
            "Epoch 173/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9650 - val_loss: 42.0012\n",
            "Epoch 174/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9066 - val_loss: 34.1862\n",
            "Epoch 175/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9115 - val_loss: 33.9878\n",
            "Epoch 176/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8914 - val_loss: 36.7719\n",
            "Epoch 177/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8479 - val_loss: 35.4482\n",
            "Epoch 178/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8953 - val_loss: 33.6063\n",
            "Epoch 179/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9705 - val_loss: 34.7948\n",
            "Epoch 180/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9806 - val_loss: 36.3237\n",
            "Epoch 181/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8588 - val_loss: 45.5972\n",
            "Epoch 182/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9325 - val_loss: 35.8119\n",
            "Epoch 183/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8765 - val_loss: 32.7943\n",
            "Epoch 184/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8177 - val_loss: 35.2812\n",
            "Epoch 185/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8462 - val_loss: 34.1598\n",
            "Epoch 186/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8720 - val_loss: 35.6488\n",
            "Epoch 187/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8645 - val_loss: 32.1859\n",
            "Epoch 188/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8660 - val_loss: 33.3370\n",
            "Epoch 189/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9233 - val_loss: 37.4244\n",
            "Epoch 190/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8790 - val_loss: 32.4312\n",
            "Epoch 191/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8156 - val_loss: 36.8648\n",
            "Epoch 192/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8393 - val_loss: 37.8686\n",
            "Epoch 193/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8086 - val_loss: 39.0175\n",
            "Epoch 194/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8806 - val_loss: 34.0019\n",
            "Epoch 195/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.8998 - val_loss: 33.8317\n",
            "Epoch 196/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8733 - val_loss: 37.8306\n",
            "Epoch 197/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9070 - val_loss: 32.3592\n",
            "Epoch 198/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8301 - val_loss: 32.8261\n",
            "Epoch 199/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.9070 - val_loss: 32.3187\n",
            "Epoch 200/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7436 - val_loss: 32.5382\n",
            "Epoch 201/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7672 - val_loss: 33.2957\n",
            "Epoch 202/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8159 - val_loss: 35.8712\n",
            "Epoch 203/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7923 - val_loss: 39.4336\n",
            "Epoch 204/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8700 - val_loss: 35.4519\n",
            "Epoch 205/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8332 - val_loss: 36.3824\n",
            "Epoch 206/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8064 - val_loss: 36.3185\n",
            "Epoch 207/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8578 - val_loss: 36.0053\n",
            "Epoch 208/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7557 - val_loss: 36.5941\n",
            "Epoch 209/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7255 - val_loss: 42.7549\n",
            "Epoch 210/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7894 - val_loss: 34.4587\n",
            "Epoch 211/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7723 - val_loss: 36.5836\n",
            "Epoch 212/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7691 - val_loss: 34.7914\n",
            "Epoch 213/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8034 - val_loss: 39.7261\n",
            "Epoch 214/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8307 - val_loss: 33.5157\n",
            "Epoch 215/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7759 - val_loss: 35.4482\n",
            "Epoch 216/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7361 - val_loss: 33.6426\n",
            "Epoch 217/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.8578 - val_loss: 32.3612\n",
            "Epoch 218/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6845 - val_loss: 32.8101\n",
            "Epoch 219/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7546 - val_loss: 32.8882\n",
            "Epoch 220/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7312 - val_loss: 39.2015\n",
            "Epoch 221/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7226 - val_loss: 33.4846\n",
            "Epoch 222/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7561 - val_loss: 38.1952\n",
            "Epoch 223/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6566 - val_loss: 36.0733\n",
            "Epoch 224/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6937 - val_loss: 36.3412\n",
            "Epoch 225/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7358 - val_loss: 34.9210\n",
            "Epoch 226/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7721 - val_loss: 35.7725\n",
            "Epoch 227/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7712 - val_loss: 33.5953\n",
            "Epoch 228/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6738 - val_loss: 34.1400\n",
            "Epoch 229/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6625 - val_loss: 33.2195\n",
            "Epoch 230/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7271 - val_loss: 39.6213\n",
            "Epoch 231/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7079 - val_loss: 33.4629\n",
            "Epoch 232/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7412 - val_loss: 33.2551\n",
            "Epoch 233/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6636 - val_loss: 39.0166\n",
            "Epoch 234/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6548 - val_loss: 35.0008\n",
            "Epoch 235/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7117 - val_loss: 36.8623\n",
            "Epoch 236/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7047 - val_loss: 34.5662\n",
            "Epoch 237/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6528 - val_loss: 33.0404\n",
            "Epoch 238/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6853 - val_loss: 34.3582\n",
            "Epoch 239/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6699 - val_loss: 33.2388\n",
            "Epoch 240/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6445 - val_loss: 33.9092\n",
            "Epoch 241/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6673 - val_loss: 32.9140\n",
            "Epoch 242/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7066 - val_loss: 34.3153\n",
            "Epoch 243/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6210 - val_loss: 36.3006\n",
            "Epoch 244/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6679 - val_loss: 40.4690\n",
            "Epoch 245/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5794 - val_loss: 32.6940\n",
            "Epoch 246/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5797 - val_loss: 34.1276\n",
            "Epoch 247/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6811 - val_loss: 39.4311\n",
            "Epoch 248/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7149 - val_loss: 39.3520\n",
            "Epoch 249/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6233 - val_loss: 32.5878\n",
            "Epoch 250/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6831 - val_loss: 34.8099\n",
            "Epoch 251/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7076 - val_loss: 38.7588\n",
            "Epoch 252/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5517 - val_loss: 34.7314\n",
            "Epoch 253/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6213 - val_loss: 33.6816\n",
            "Epoch 254/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6084 - val_loss: 35.6552\n",
            "Epoch 255/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6504 - val_loss: 32.9574\n",
            "Epoch 256/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.7535 - val_loss: 33.2831\n",
            "Epoch 257/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6732 - val_loss: 33.0639\n",
            "Epoch 258/300\n",
            "1319/1319 [==============================] - 6s 5ms/step - loss: 28.6867 - val_loss: 35.4042\n",
            "Epoch 259/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6406 - val_loss: 33.9504\n",
            "Epoch 260/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6602 - val_loss: 33.6278\n",
            "Epoch 261/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6411 - val_loss: 34.7488\n",
            "Epoch 262/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6300 - val_loss: 33.0043\n",
            "Epoch 263/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5612 - val_loss: 33.3963\n",
            "Epoch 264/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6448 - val_loss: 41.9811\n",
            "Epoch 265/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5872 - val_loss: 38.4129\n",
            "Epoch 266/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6187 - val_loss: 34.7002\n",
            "Epoch 267/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5835 - val_loss: 34.2430\n",
            "Epoch 268/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6014 - val_loss: 38.0310\n",
            "Epoch 269/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5580 - val_loss: 34.0786\n",
            "Epoch 270/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5931 - val_loss: 34.6414\n",
            "Epoch 271/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5666 - val_loss: 36.5780\n",
            "Epoch 272/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6432 - val_loss: 36.4625\n",
            "Epoch 273/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6211 - val_loss: 33.8340\n",
            "Epoch 274/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5673 - val_loss: 37.8889\n",
            "Epoch 275/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6360 - val_loss: 32.2779\n",
            "Epoch 276/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6291 - val_loss: 34.2299\n",
            "Epoch 277/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6540 - val_loss: 34.7675\n",
            "Epoch 278/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5849 - val_loss: 33.6203\n",
            "Epoch 279/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5662 - val_loss: 33.7773\n",
            "Epoch 280/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6611 - val_loss: 37.9110\n",
            "Epoch 281/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6368 - val_loss: 36.1034\n",
            "Epoch 282/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5762 - val_loss: 38.3745\n",
            "Epoch 283/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5647 - val_loss: 37.2449\n",
            "Epoch 284/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6219 - val_loss: 35.1429\n",
            "Epoch 285/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4836 - val_loss: 32.5936\n",
            "Epoch 286/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5974 - val_loss: 33.8861\n",
            "Epoch 287/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5350 - val_loss: 34.4549\n",
            "Epoch 288/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5262 - val_loss: 54.2583\n",
            "Epoch 289/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5908 - val_loss: 33.6737\n",
            "Epoch 290/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5630 - val_loss: 32.7223\n",
            "Epoch 291/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5163 - val_loss: 40.4889\n",
            "Epoch 292/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6020 - val_loss: 36.0146\n",
            "Epoch 293/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5791 - val_loss: 40.5217\n",
            "Epoch 294/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5596 - val_loss: 37.5569\n",
            "Epoch 295/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.6069 - val_loss: 38.1505\n",
            "Epoch 296/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5766 - val_loss: 32.5068\n",
            "Epoch 297/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5767 - val_loss: 33.3144\n",
            "Epoch 298/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4325 - val_loss: 38.3674\n",
            "Epoch 299/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.5668 - val_loss: 32.7830\n",
            "Epoch 300/300\n",
            "1319/1319 [==============================] - 5s 4ms/step - loss: 28.4539 - val_loss: 33.3919\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "outputId": "7307c364-b5c9-4e08-c236-77eb7f62190e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.09966815343499183 \n",
            "MAE:  4.257364048669277 \n",
            "SD:  5.777715530224153\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "outputId": "66f5a9ef-2311-46cc-bf72-9a7054aa93a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "outputId": "9b12f6c1-6536-4485-d74e-1584a9da83ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  0.3536714730723782 \n",
            "Ensemble_std:  5.791982546687328\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BP_hv3_7(1)(1).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}