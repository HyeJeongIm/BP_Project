{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/BP_hv1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60Iv1poCkLbi",
        "outputId": "41a3948d-8fdc-4946-ca42-c8498bde1440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version :  3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)]\n",
            "TensorFlow version :  2.3.0\n",
            "Keras version :  2.4.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad,Adadelta\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLX3G7UkkLbm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러오기\n",
        "train_data = io.loadmat('C:/Users/82109/Desktop/imhz/data/h_v1/train_shuffled_raw_v1.mat')\n",
        "test_data = io.loadmat('C:/Users/82109/Desktop/imhz/data/h_v1/test_not_shuffled_raw_v1.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "e0c597df-9ede-4dcc-f92b-f70a689b8b91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(166415, 36)\n",
            "(29269, 36)\n",
            "(166415, 1)\n",
            "(29269, 1)\n",
            "(166415, 1)\n",
            "(29269, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3JM6lVktcwY"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "1ea64b21-0334-4c75-a9ce-11c4e0910bdc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>260.031256</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.108750</td>\n",
              "      <td>0.08375</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>0.055000</td>\n",
              "      <td>0.467500</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.091250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.445000</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.091250</td>\n",
              "      <td>0.076250</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.133750</td>\n",
              "      <td>0.614192</td>\n",
              "      <td>0.348660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>247.917536</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.108750</td>\n",
              "      <td>0.08375</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>0.053750</td>\n",
              "      <td>0.447500</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.465625</td>\n",
              "      <td>0.168125</td>\n",
              "      <td>0.086875</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.581875</td>\n",
              "      <td>0.258750</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>0.593663</td>\n",
              "      <td>0.335900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>253.789909</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.106875</td>\n",
              "      <td>0.08250</td>\n",
              "      <td>0.058750</td>\n",
              "      <td>0.053750</td>\n",
              "      <td>0.450625</td>\n",
              "      <td>0.183125</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.098750</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.570000</td>\n",
              "      <td>0.276250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.142500</td>\n",
              "      <td>0.702310</td>\n",
              "      <td>0.380963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>252.278748</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.108750</td>\n",
              "      <td>0.08500</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.055000</td>\n",
              "      <td>0.456250</td>\n",
              "      <td>0.176250</td>\n",
              "      <td>0.088750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.432500</td>\n",
              "      <td>0.169375</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.076250</td>\n",
              "      <td>0.546875</td>\n",
              "      <td>0.261250</td>\n",
              "      <td>0.153750</td>\n",
              "      <td>0.133750</td>\n",
              "      <td>0.574484</td>\n",
              "      <td>0.331242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>256.550298</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08625</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.055625</td>\n",
              "      <td>0.460625</td>\n",
              "      <td>0.186875</td>\n",
              "      <td>0.092500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.421875</td>\n",
              "      <td>0.168125</td>\n",
              "      <td>0.091875</td>\n",
              "      <td>0.077500</td>\n",
              "      <td>0.537500</td>\n",
              "      <td>0.258750</td>\n",
              "      <td>0.156875</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.722804</td>\n",
              "      <td>0.394318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>393.388904</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.083750</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.607500</td>\n",
              "      <td>0.251250</td>\n",
              "      <td>0.146250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.636250</td>\n",
              "      <td>0.456875</td>\n",
              "      <td>0.197500</td>\n",
              "      <td>0.171875</td>\n",
              "      <td>0.808125</td>\n",
              "      <td>0.575625</td>\n",
              "      <td>0.283750</td>\n",
              "      <td>0.249375</td>\n",
              "      <td>0.643935</td>\n",
              "      <td>0.339343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>398.002860</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.083750</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.623750</td>\n",
              "      <td>0.272500</td>\n",
              "      <td>0.153750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.576250</td>\n",
              "      <td>0.283750</td>\n",
              "      <td>0.153750</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.745000</td>\n",
              "      <td>0.408750</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.575827</td>\n",
              "      <td>0.315949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>398.973396</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.083750</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.603750</td>\n",
              "      <td>0.256250</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.364375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.743125</td>\n",
              "      <td>0.491875</td>\n",
              "      <td>0.248750</td>\n",
              "      <td>0.217500</td>\n",
              "      <td>0.618326</td>\n",
              "      <td>0.326485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>392.341511</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.083750</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.592500</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.147500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.570000</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.743750</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.251250</td>\n",
              "      <td>0.220625</td>\n",
              "      <td>0.600506</td>\n",
              "      <td>0.318283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>394.770431</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.173125</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.083125</td>\n",
              "      <td>0.074375</td>\n",
              "      <td>0.598750</td>\n",
              "      <td>0.253125</td>\n",
              "      <td>0.146875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.601250</td>\n",
              "      <td>0.365000</td>\n",
              "      <td>0.165000</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.785000</td>\n",
              "      <td>0.495000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.610681</td>\n",
              "      <td>0.334934</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 36 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1        2         3        4         5         6   \\\n",
              "0    260.031256  0.166250  0.58625  0.108750  0.08375  0.060000  0.055000   \n",
              "1    247.917536  0.166250  0.57500  0.108750  0.08375  0.060000  0.053750   \n",
              "2    253.789909  0.163750  0.57500  0.106875  0.08250  0.058750  0.053750   \n",
              "3    252.278748  0.168750  0.58875  0.108750  0.08500  0.061250  0.055000   \n",
              "4    256.550298  0.170625  0.59125  0.111250  0.08625  0.061250  0.055625   \n",
              "..          ...       ...      ...       ...      ...       ...       ...   \n",
              "98   393.388904  0.273750  0.84875  0.168750  0.12500  0.083750  0.075000   \n",
              "99   398.002860  0.325000  0.78750  0.181250  0.12375  0.083750  0.075000   \n",
              "100  398.973396  0.287500  0.80250  0.171250  0.12375  0.083750  0.075000   \n",
              "101  392.341511  0.335000  0.77625  0.175000  0.12500  0.083750  0.075000   \n",
              "102  394.770431  0.340000  0.81000  0.173125  0.12375  0.083125  0.074375   \n",
              "\n",
              "           7         8         9   ...        26        27        28  \\\n",
              "0    0.467500  0.187500  0.091250  ...  0.445000  0.172500  0.091250   \n",
              "1    0.447500  0.172500  0.086250  ...  0.465625  0.168125  0.086875   \n",
              "2    0.450625  0.183125  0.090000  ...  0.460000  0.183750  0.098750   \n",
              "3    0.456250  0.176250  0.088750  ...  0.432500  0.169375  0.090000   \n",
              "4    0.460625  0.186875  0.092500  ...  0.421875  0.168125  0.091875   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "98   0.607500  0.251250  0.146250  ...  0.636250  0.456875  0.197500   \n",
              "99   0.623750  0.272500  0.153750  ...  0.576250  0.283750  0.153750   \n",
              "100  0.603750  0.256250  0.152500  ...  0.575000  0.364375  0.162500   \n",
              "101  0.592500  0.255000  0.147500  ...  0.570000  0.351250  0.165625   \n",
              "102  0.598750  0.253125  0.146875  ...  0.601250  0.365000  0.165000   \n",
              "\n",
              "           29        30        31        32        33        34        35  \n",
              "0    0.076250  0.562500  0.260000  0.155000  0.133750  0.614192  0.348660  \n",
              "1    0.073125  0.581875  0.258750  0.151250  0.131875  0.593663  0.335900  \n",
              "2    0.082500  0.570000  0.276250  0.166250  0.142500  0.702310  0.380963  \n",
              "3    0.076250  0.546875  0.261250  0.153750  0.133750  0.574484  0.331242  \n",
              "4    0.077500  0.537500  0.258750  0.156875  0.136250  0.722804  0.394318  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "98   0.171875  0.808125  0.575625  0.283750  0.249375  0.643935  0.339343  \n",
              "99   0.132500  0.745000  0.408750  0.238750  0.208750  0.575827  0.315949  \n",
              "100  0.140000  0.743125  0.491875  0.248750  0.217500  0.618326  0.326485  \n",
              "101  0.143750  0.743750  0.477500  0.251250  0.220625  0.600506  0.318283  \n",
              "102  0.143750  0.785000  0.495000  0.255000  0.223750  0.610681  0.334934  \n",
              "\n",
              "[103 rows x 36 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "db19b290-a32f-428a-ceaa-8d328847af48"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>297.062256</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>0.616250</td>\n",
              "      <td>0.106250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.058750</td>\n",
              "      <td>0.053750</td>\n",
              "      <td>0.491250</td>\n",
              "      <td>0.228750</td>\n",
              "      <td>0.101250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.491250</td>\n",
              "      <td>0.228750</td>\n",
              "      <td>0.101250</td>\n",
              "      <td>0.07875</td>\n",
              "      <td>0.5975</td>\n",
              "      <td>0.31125</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.789459</td>\n",
              "      <td>0.447898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>280.140984</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.623750</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.087500</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.056250</td>\n",
              "      <td>0.452500</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.102500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.491250</td>\n",
              "      <td>0.228750</td>\n",
              "      <td>0.101250</td>\n",
              "      <td>0.07875</td>\n",
              "      <td>0.5975</td>\n",
              "      <td>0.31125</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.789459</td>\n",
              "      <td>0.447898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>286.474035</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.639375</td>\n",
              "      <td>0.107500</td>\n",
              "      <td>0.083125</td>\n",
              "      <td>0.059375</td>\n",
              "      <td>0.054375</td>\n",
              "      <td>0.471875</td>\n",
              "      <td>0.208125</td>\n",
              "      <td>0.097500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.491250</td>\n",
              "      <td>0.228750</td>\n",
              "      <td>0.101250</td>\n",
              "      <td>0.07875</td>\n",
              "      <td>0.5975</td>\n",
              "      <td>0.31125</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.789459</td>\n",
              "      <td>0.447898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>296.949890</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.648750</td>\n",
              "      <td>0.107500</td>\n",
              "      <td>0.083750</td>\n",
              "      <td>0.058750</td>\n",
              "      <td>0.053750</td>\n",
              "      <td>0.513750</td>\n",
              "      <td>0.221250</td>\n",
              "      <td>0.107500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.491250</td>\n",
              "      <td>0.228750</td>\n",
              "      <td>0.101250</td>\n",
              "      <td>0.07875</td>\n",
              "      <td>0.5975</td>\n",
              "      <td>0.31125</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.789459</td>\n",
              "      <td>0.447898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>319.620796</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.635000</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.056250</td>\n",
              "      <td>0.522500</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.118750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.491250</td>\n",
              "      <td>0.228750</td>\n",
              "      <td>0.101250</td>\n",
              "      <td>0.07875</td>\n",
              "      <td>0.5975</td>\n",
              "      <td>0.31125</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.789459</td>\n",
              "      <td>0.447898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>279.673567</td>\n",
              "      <td>0.178750</td>\n",
              "      <td>0.640625</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.089375</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.057500</td>\n",
              "      <td>0.513750</td>\n",
              "      <td>0.176250</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.549375</td>\n",
              "      <td>0.213125</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.07125</td>\n",
              "      <td>0.6600</td>\n",
              "      <td>0.29875</td>\n",
              "      <td>0.146875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>0.649722</td>\n",
              "      <td>0.318253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>306.671042</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.636250</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.085625</td>\n",
              "      <td>0.060625</td>\n",
              "      <td>0.055625</td>\n",
              "      <td>0.532500</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.089375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.549375</td>\n",
              "      <td>0.213125</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.07125</td>\n",
              "      <td>0.6600</td>\n",
              "      <td>0.29875</td>\n",
              "      <td>0.146875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>0.649722</td>\n",
              "      <td>0.318253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>297.227546</td>\n",
              "      <td>0.166875</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.109375</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.060625</td>\n",
              "      <td>0.055000</td>\n",
              "      <td>0.535000</td>\n",
              "      <td>0.189375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.549375</td>\n",
              "      <td>0.213125</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.07125</td>\n",
              "      <td>0.6600</td>\n",
              "      <td>0.29875</td>\n",
              "      <td>0.146875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>0.649722</td>\n",
              "      <td>0.318253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>297.057388</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.113125</td>\n",
              "      <td>0.087500</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.056875</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>0.190625</td>\n",
              "      <td>0.087500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.549375</td>\n",
              "      <td>0.213125</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.07125</td>\n",
              "      <td>0.6600</td>\n",
              "      <td>0.29875</td>\n",
              "      <td>0.146875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>0.649722</td>\n",
              "      <td>0.318253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>322.783724</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.671250</td>\n",
              "      <td>0.114375</td>\n",
              "      <td>0.088750</td>\n",
              "      <td>0.064375</td>\n",
              "      <td>0.058125</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>0.095000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.549375</td>\n",
              "      <td>0.213125</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.07125</td>\n",
              "      <td>0.6600</td>\n",
              "      <td>0.29875</td>\n",
              "      <td>0.146875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>0.649722</td>\n",
              "      <td>0.318253</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 36 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2         3         4         5         6   \\\n",
              "0    297.062256  0.162500  0.616250  0.106250  0.082500  0.058750  0.053750   \n",
              "1    280.140984  0.175000  0.623750  0.112500  0.087500  0.062500  0.056250   \n",
              "2    286.474035  0.163125  0.639375  0.107500  0.083125  0.059375  0.054375   \n",
              "3    296.949890  0.160000  0.648750  0.107500  0.083750  0.058750  0.053750   \n",
              "4    319.620796  0.170000  0.635000  0.111250  0.086250  0.062500  0.056250   \n",
              "..          ...       ...       ...       ...       ...       ...       ...   \n",
              "98   279.673567  0.178750  0.640625  0.116250  0.089375  0.063750  0.057500   \n",
              "99   306.671042  0.170000  0.636250  0.110000  0.085625  0.060625  0.055625   \n",
              "100  297.227546  0.166875  0.650000  0.109375  0.084375  0.060625  0.055000   \n",
              "101  297.057388  0.173750  0.646250  0.113125  0.087500  0.062500  0.056875   \n",
              "102  322.783724  0.173750  0.671250  0.114375  0.088750  0.064375  0.058125   \n",
              "\n",
              "           7         8         9   ...        26        27        28       29  \\\n",
              "0    0.491250  0.228750  0.101250  ...  0.491250  0.228750  0.101250  0.07875   \n",
              "1    0.452500  0.207500  0.102500  ...  0.491250  0.228750  0.101250  0.07875   \n",
              "2    0.471875  0.208125  0.097500  ...  0.491250  0.228750  0.101250  0.07875   \n",
              "3    0.513750  0.221250  0.107500  ...  0.491250  0.228750  0.101250  0.07875   \n",
              "4    0.522500  0.275000  0.118750  ...  0.491250  0.228750  0.101250  0.07875   \n",
              "..        ...       ...       ...  ...       ...       ...       ...      ...   \n",
              "98   0.513750  0.176250  0.084375  ...  0.549375  0.213125  0.084375  0.07125   \n",
              "99   0.532500  0.211875  0.089375  ...  0.549375  0.213125  0.084375  0.07125   \n",
              "100  0.535000  0.189375  0.085000  ...  0.549375  0.213125  0.084375  0.07125   \n",
              "101  0.525000  0.190625  0.087500  ...  0.549375  0.213125  0.084375  0.07125   \n",
              "102  0.550000  0.215000  0.095000  ...  0.549375  0.213125  0.084375  0.07125   \n",
              "\n",
              "         30       31        32        33        34        35  \n",
              "0    0.5975  0.31125  0.161250  0.132500  0.789459  0.447898  \n",
              "1    0.5975  0.31125  0.161250  0.132500  0.789459  0.447898  \n",
              "2    0.5975  0.31125  0.161250  0.132500  0.789459  0.447898  \n",
              "3    0.5975  0.31125  0.161250  0.132500  0.789459  0.447898  \n",
              "4    0.5975  0.31125  0.161250  0.132500  0.789459  0.447898  \n",
              "..      ...      ...       ...       ...       ...       ...  \n",
              "98   0.6600  0.29875  0.146875  0.126875  0.649722  0.318253  \n",
              "99   0.6600  0.29875  0.146875  0.126875  0.649722  0.318253  \n",
              "100  0.6600  0.29875  0.146875  0.126875  0.649722  0.318253  \n",
              "101  0.6600  0.29875  0.146875  0.126875  0.649722  0.318253  \n",
              "102  0.6600  0.29875  0.146875  0.126875  0.649722  0.318253  \n",
              "\n",
              "[103 rows x 36 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boXGGctokLbn"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jJNDT_-kLbn"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ko5M8WWkLbo"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzZ8ZTMMkLbo",
        "outputId": "9e59c4a5-b516-46c9-c0f8-f49c45697362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 64)                2368      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 7,105\n",
            "Trainable params: 6,849\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        "    \n",
        "model_SBP = build_model()\n",
        "model_SBP.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuWId9Q5kLbp",
        "outputId": "393f8e1a-119c-4bc8-e5b2-f9b378c47aa2",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "   1/5201 [..............................] - ETA: 5s - loss: 12397.9600WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 348.6057 - val_loss: 243.4475\n",
            "Epoch 2/100\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 185.8531 - val_loss: 703.6746\n",
            "Epoch 3/100\n",
            "5201/5201 [==============================] - 5s 874us/step - loss: 180.7066 - val_loss: 362.8182\n",
            "Epoch 4/100\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 176.5699 - val_loss: 201.6138\n",
            "Epoch 5/100\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 173.1027 - val_loss: 208.7765\n",
            "Epoch 6/100\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 169.2388 - val_loss: 421.9398\n",
            "Epoch 7/100\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 165.8264 - val_loss: 221.5729\n",
            "Epoch 8/100\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 164.4005 - val_loss: 347.9635\n",
            "Epoch 9/100\n",
            "5201/5201 [==============================] - 4s 858us/step - loss: 162.4133 - val_loss: 393.4490\n",
            "Epoch 10/100\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 161.2440 - val_loss: 248.6390\n",
            "Epoch 11/100\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 160.1334 - val_loss: 403.7665\n",
            "Epoch 12/100\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 159.1825 - val_loss: 363.4940\n",
            "Epoch 13/100\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 157.9982 - val_loss: 260.2570\n",
            "Epoch 14/100\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 156.3203 - val_loss: 215.9045\n",
            "Epoch 15/100\n",
            "5201/5201 [==============================] - 4s 856us/step - loss: 155.7800 - val_loss: 195.7853\n",
            "Epoch 16/100\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 155.1666 - val_loss: 246.3445\n",
            "Epoch 17/100\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 154.0998 - val_loss: 193.7831\n",
            "Epoch 18/100\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 154.1480 - val_loss: 208.4235\n",
            "Epoch 19/100\n",
            "5201/5201 [==============================] - 5s 876us/step - loss: 153.3262 - val_loss: 388.9563\n",
            "Epoch 20/100\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 152.4843 - val_loss: 518.5263\n",
            "Epoch 21/100\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 152.4539 - val_loss: 203.9236\n",
            "Epoch 22/100\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 151.3475 - val_loss: 224.5795\n",
            "Epoch 23/100\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 151.4866 - val_loss: 190.1957\n",
            "Epoch 24/100\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 150.7799 - val_loss: 298.1131\n",
            "Epoch 25/100\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 150.5186 - val_loss: 174.2538\n",
            "Epoch 26/100\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 150.2802 - val_loss: 490.4765\n",
            "Epoch 27/100\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 149.2660 - val_loss: 319.2891\n",
            "Epoch 28/100\n",
            "5201/5201 [==============================] - 5s 872us/step - loss: 149.4477 - val_loss: 209.7498\n",
            "Epoch 29/100\n",
            "5201/5201 [==============================] - 5s 928us/step - loss: 148.8474 - val_loss: 168.8225\n",
            "Epoch 30/100\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 148.7487 - val_loss: 392.1623\n",
            "Epoch 31/100\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 148.7962 - val_loss: 209.1267\n",
            "Epoch 32/100\n",
            "5201/5201 [==============================] - 5s 904us/step - loss: 148.6415 - val_loss: 237.6896\n",
            "Epoch 33/100\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 148.2699 - val_loss: 285.6657\n",
            "Epoch 34/100\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 147.4569 - val_loss: 544.9529\n",
            "Epoch 35/100\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 148.0829 - val_loss: 216.1938\n",
            "Epoch 36/100\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 147.4918 - val_loss: 198.6189\n",
            "Epoch 37/100\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 147.2656 - val_loss: 257.5677\n",
            "Epoch 38/100\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 146.6640 - val_loss: 250.4369\n",
            "Epoch 39/100\n",
            "5201/5201 [==============================] - 5s 866us/step - loss: 146.7076 - val_loss: 248.2764\n",
            "Epoch 40/100\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 146.6541 - val_loss: 200.9449\n",
            "Epoch 41/100\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 146.7635 - val_loss: 241.8825\n",
            "Epoch 42/100\n",
            "5201/5201 [==============================] - 5s 872us/step - loss: 145.6767 - val_loss: 297.1762\n",
            "Epoch 43/100\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 145.6271 - val_loss: 355.0300\n",
            "Epoch 44/100\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 145.4904 - val_loss: 192.4244\n",
            "Epoch 45/100\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 145.4468 - val_loss: 184.2144\n",
            "Epoch 46/100\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 145.5016 - val_loss: 177.5784\n",
            "Epoch 47/100\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 145.2218 - val_loss: 240.8746\n",
            "Epoch 48/100\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 144.5470 - val_loss: 294.5303\n",
            "Epoch 49/100\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 145.3934 - val_loss: 186.8303\n",
            "Epoch 50/100\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 144.9774 - val_loss: 223.0981\n",
            "Epoch 51/100\n",
            "5201/5201 [==============================] - 5s 932us/step - loss: 144.7365 - val_loss: 191.9855\n",
            "Epoch 52/100\n",
            "5201/5201 [==============================] - 5s 936us/step - loss: 144.9512 - val_loss: 177.9060\n",
            "Epoch 53/100\n",
            "5201/5201 [==============================] - 4s 865us/step - loss: 144.2145 - val_loss: 230.1614\n",
            "Epoch 54/100\n",
            "5201/5201 [==============================] - 5s 883us/step - loss: 143.8006 - val_loss: 229.9788\n",
            "Epoch 55/100\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 144.5268 - val_loss: 297.1817\n",
            "Epoch 56/100\n",
            "5201/5201 [==============================] - 4s 850us/step - loss: 144.0963 - val_loss: 177.9380\n",
            "Epoch 57/100\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 143.9938 - val_loss: 188.9191\n",
            "Epoch 58/100\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 143.6234 - val_loss: 288.2609\n",
            "Epoch 59/100\n",
            "5201/5201 [==============================] - 5s 904us/step - loss: 143.8217 - val_loss: 216.4598\n",
            "Epoch 60/100\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 143.7584 - val_loss: 197.7647\n",
            "Epoch 61/100\n",
            "5201/5201 [==============================] - 5s 876us/step - loss: 143.3664 - val_loss: 167.2118\n",
            "Epoch 62/100\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 143.0483 - val_loss: 243.5300\n",
            "Epoch 63/100\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 143.2698 - val_loss: 171.0539\n",
            "Epoch 64/100\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 143.1202 - val_loss: 236.4327\n",
            "Epoch 65/100\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 142.9587 - val_loss: 221.6999\n",
            "Epoch 66/100\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 142.4447 - val_loss: 176.4538\n",
            "Epoch 67/100\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 142.8098 - val_loss: 205.8181\n",
            "Epoch 68/100\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 142.9756 - val_loss: 345.8663\n",
            "Epoch 69/100\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 142.3668 - val_loss: 252.9581\n",
            "Epoch 70/100\n",
            "5201/5201 [==============================] - 4s 864us/step - loss: 142.8972 - val_loss: 240.8978\n",
            "Epoch 71/100\n",
            "5201/5201 [==============================] - 4s 863us/step - loss: 142.3574 - val_loss: 249.9342\n",
            "Epoch 72/100\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 141.5295 - val_loss: 204.0450\n",
            "Epoch 73/100\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 142.2067 - val_loss: 188.1841\n",
            "Epoch 74/100\n",
            "5201/5201 [==============================] - 5s 878us/step - loss: 141.8239 - val_loss: 307.2278\n",
            "Epoch 75/100\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 142.1237 - val_loss: 250.4796\n",
            "Epoch 76/100\n",
            "5201/5201 [==============================] - 5s 867us/step - loss: 141.4785 - val_loss: 197.3332\n",
            "Epoch 77/100\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 141.7460 - val_loss: 187.7461\n",
            "Epoch 78/100\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 141.7620 - val_loss: 189.9116\n",
            "Epoch 79/100\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 141.9586 - val_loss: 173.8114\n",
            "Epoch 80/100\n",
            "5201/5201 [==============================] - 5s 877us/step - loss: 141.7914 - val_loss: 250.1879\n",
            "Epoch 81/100\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 141.2958 - val_loss: 206.3728\n",
            "Epoch 82/100\n",
            "5201/5201 [==============================] - 5s 876us/step - loss: 141.0149 - val_loss: 263.0575\n",
            "Epoch 83/100\n",
            "5201/5201 [==============================] - 5s 877us/step - loss: 141.7653 - val_loss: 179.2076\n",
            "Epoch 84/100\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 141.0964 - val_loss: 165.4598\n",
            "Epoch 85/100\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 141.0924 - val_loss: 259.7442ETA: 0s - lo - ETA: 0s - loss: 141.16\n",
            "Epoch 86/100\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 140.9597 - val_loss: 215.1616\n",
            "Epoch 87/100\n",
            "5201/5201 [==============================] - 5s 878us/step - loss: 140.9384 - val_loss: 205.7657\n",
            "Epoch 88/100\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 140.7054 - val_loss: 210.0962\n",
            "Epoch 89/100\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 140.7865 - val_loss: 266.9285\n",
            "Epoch 90/100\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 141.4068 - val_loss: 235.8523\n",
            "Epoch 91/100\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 140.6494 - val_loss: 248.9234\n",
            "Epoch 92/100\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 140.8528 - val_loss: 242.9278\n",
            "Epoch 93/100\n",
            "5201/5201 [==============================] - 5s 928us/step - loss: 140.9183 - val_loss: 179.3125\n",
            "Epoch 94/100\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 140.4185 - val_loss: 261.3751\n",
            "Epoch 95/100\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 140.5120 - val_loss: 179.7099\n",
            "Epoch 96/100\n",
            "5201/5201 [==============================] - 5s 931us/step - loss: 140.5151 - val_loss: 182.5691\n",
            "Epoch 97/100\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 140.4063 - val_loss: 175.2993\n",
            "Epoch 98/100\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 140.4143 - val_loss: 211.4711\n",
            "Epoch 99/100\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 140.3090 - val_loss: 298.6218\n",
            "Epoch 100/100\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 140.1101 - val_loss: 185.2784\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adadelta,SGD\n",
        "\n",
        "#parameter\n",
        "batch_size = 32\n",
        "epoch = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "model_SBP.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\n",
        "# model_SBP.summary()\n",
        "history = model_SBP.fit(X_train, sbp_train, batch_size = batch_size, epochs = epoch, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIH6v8uFkLbq",
        "outputId": "b666c7a5-9a7a-4781-f6a1-e02e67727529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -0.8880920576794858 \n",
            "MAE:  10.110658528005178 \n",
            "SD:  13.582695622538372\n"
          ]
        }
      ],
      "source": [
        "pred = model_SBP.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)\n",
        "\n",
        "\n",
        "# ME:  -0.8880920576794858 \n",
        "# MAE:  10.110658528005178 \n",
        "# SD:  13.582695622538372"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "0bmp3KgIkLbq",
        "outputId": "755441f3-2314-46cd-86fa-4b133a4ad0b1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epoch, 0, 1000])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBdIpD3akLbq"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poM15tmukLbr",
        "outputId": "427d31b3-4ae4-4d1f-a06f-072c479d5560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 64)                2368      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 15,937\n",
            "Trainable params: 15,425\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))  \n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        "    \n",
        "model_SBP2 = build_model()\n",
        "model_SBP2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YhNQ_IZkLbr",
        "outputId": "93b1e186-1fda-4992-c6bc-9a51f23ac2e5",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 1270.2181 - val_loss: 213.6772\n",
            "Epoch 2/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 185.0764 - val_loss: 1231.5331\n",
            "Epoch 3/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 179.6411 - val_loss: 248.9944\n",
            "Epoch 4/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 173.8427 - val_loss: 323.7600\n",
            "Epoch 5/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 170.6302 - val_loss: 288.7877\n",
            "Epoch 6/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 167.6842 - val_loss: 305.2148\n",
            "Epoch 7/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 165.2605 - val_loss: 223.0946\n",
            "Epoch 8/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 163.2151 - val_loss: 469.9742\n",
            "Epoch 9/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 161.1751 - val_loss: 206.8120\n",
            "Epoch 10/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 159.8616 - val_loss: 337.1782\n",
            "Epoch 11/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 158.2624 - val_loss: 343.2212\n",
            "Epoch 12/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 155.8238 - val_loss: 220.3544: 0s - loss: 15\n",
            "Epoch 13/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 154.1808 - val_loss: 231.1729\n",
            "Epoch 14/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 152.5842 - val_loss: 209.4517\n",
            "Epoch 15/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 151.3787 - val_loss: 232.6439\n",
            "Epoch 16/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 150.5541 - val_loss: 262.8394\n",
            "Epoch 17/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 148.9953 - val_loss: 462.6464\n",
            "Epoch 18/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 148.4483 - val_loss: 306.2560\n",
            "Epoch 19/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 146.8950 - val_loss: 179.5278\n",
            "Epoch 20/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 146.2468 - val_loss: 289.7280\n",
            "Epoch 21/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 145.3918 - val_loss: 252.1470\n",
            "Epoch 22/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 144.3580 - val_loss: 209.0435\n",
            "Epoch 23/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 143.8341 - val_loss: 198.3964\n",
            "Epoch 24/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 143.1982 - val_loss: 275.3477\n",
            "Epoch 25/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 142.3894 - val_loss: 275.8787\n",
            "Epoch 26/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 141.8049 - val_loss: 829.2098\n",
            "Epoch 27/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 141.4827 - val_loss: 185.5207\n",
            "Epoch 28/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 140.2032 - val_loss: 422.1341\n",
            "Epoch 29/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 140.0831 - val_loss: 286.6971\n",
            "Epoch 30/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 139.1306 - val_loss: 228.8567\n",
            "Epoch 31/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 139.1702 - val_loss: 231.8938\n",
            "Epoch 32/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 138.3902 - val_loss: 213.1874\n",
            "Epoch 33/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 137.5437 - val_loss: 223.8403\n",
            "Epoch 34/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 137.2955 - val_loss: 246.1329\n",
            "Epoch 35/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 137.1001 - val_loss: 205.0616\n",
            "Epoch 36/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 136.2679 - val_loss: 185.3068\n",
            "Epoch 37/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 136.2913 - val_loss: 274.3683\n",
            "Epoch 38/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 135.6470 - val_loss: 270.4701\n",
            "Epoch 39/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 135.0969 - val_loss: 188.1822\n",
            "Epoch 40/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 134.9158 - val_loss: 290.8390\n",
            "Epoch 41/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 134.7387 - val_loss: 231.9525\n",
            "Epoch 42/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 134.4634 - val_loss: 393.8169\n",
            "Epoch 43/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 134.3725 - val_loss: 328.1916\n",
            "Epoch 44/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 133.4766 - val_loss: 243.7352\n",
            "Epoch 45/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 133.4126 - val_loss: 247.0249\n",
            "Epoch 46/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 133.1410 - val_loss: 175.2045\n",
            "Epoch 47/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 132.6260 - val_loss: 198.4410\n",
            "Epoch 48/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 131.9058 - val_loss: 198.3677\n",
            "Epoch 49/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 132.5862 - val_loss: 223.0802\n",
            "Epoch 50/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 131.7285 - val_loss: 182.4784\n",
            "Epoch 51/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 131.1715 - val_loss: 181.4176\n",
            "Epoch 52/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 131.8523 - val_loss: 172.0472\n",
            "Epoch 53/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 130.5070 - val_loss: 264.5582\n",
            "Epoch 54/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 130.3009 - val_loss: 216.1635\n",
            "Epoch 55/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 130.3382 - val_loss: 258.2752\n",
            "Epoch 56/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 129.9321 - val_loss: 174.4232\n",
            "Epoch 57/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 129.1663 - val_loss: 195.3491\n",
            "Epoch 58/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 129.1475 - val_loss: 201.0410\n",
            "Epoch 59/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 129.2964 - val_loss: 183.3729\n",
            "Epoch 60/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 129.1696 - val_loss: 211.2191\n",
            "Epoch 61/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 128.3125 - val_loss: 205.1388\n",
            "Epoch 62/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 128.9972 - val_loss: 198.9070\n",
            "Epoch 63/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 128.4790 - val_loss: 209.9361\n",
            "Epoch 64/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 127.9937 - val_loss: 251.0749\n",
            "Epoch 65/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 128.2284 - val_loss: 172.0301\n",
            "Epoch 66/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 128.1446 - val_loss: 227.4554\n",
            "Epoch 67/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 127.7746 - val_loss: 233.2981\n",
            "Epoch 68/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 127.6450 - val_loss: 215.1151\n",
            "Epoch 69/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 127.3774 - val_loss: 178.1063\n",
            "Epoch 70/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 126.8516 - val_loss: 203.0216\n",
            "Epoch 71/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 126.9338 - val_loss: 294.2797\n",
            "Epoch 72/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 126.9060 - val_loss: 236.1472\n",
            "Epoch 73/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 126.4037 - val_loss: 295.8444\n",
            "Epoch 74/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 125.9690 - val_loss: 219.4943\n",
            "Epoch 75/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 125.9218 - val_loss: 169.0128\n",
            "Epoch 76/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 126.1213 - val_loss: 276.7491\n",
            "Epoch 77/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 125.4646 - val_loss: 195.1102\n",
            "Epoch 78/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 125.5122 - val_loss: 198.6092\n",
            "Epoch 79/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 125.2795 - val_loss: 270.0142\n",
            "Epoch 80/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 125.0362 - val_loss: 192.2025\n",
            "Epoch 81/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 125.5218 - val_loss: 215.8773\n",
            "Epoch 82/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 124.5369 - val_loss: 270.9634\n",
            "Epoch 83/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 124.9250 - val_loss: 259.3936\n",
            "Epoch 84/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 124.3689 - val_loss: 325.3015\n",
            "Epoch 85/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 124.4942 - val_loss: 181.2114\n",
            "Epoch 86/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 124.2421 - val_loss: 216.8436\n",
            "Epoch 87/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 124.4056 - val_loss: 839.4669\n",
            "Epoch 88/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 123.7989 - val_loss: 187.3251\n",
            "Epoch 89/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 124.4356 - val_loss: 175.2856\n",
            "Epoch 90/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 124.2098 - val_loss: 318.2144\n",
            "Epoch 91/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 123.6075 - val_loss: 289.5968\n",
            "Epoch 92/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 123.9541 - val_loss: 191.8445\n",
            "Epoch 93/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 123.1546 - val_loss: 175.5948\n",
            "Epoch 94/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 123.2136 - val_loss: 190.7193\n",
            "Epoch 95/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 123.3965 - val_loss: 189.9929\n",
            "Epoch 96/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 123.1324 - val_loss: 206.8439\n",
            "Epoch 97/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 122.6632 - val_loss: 235.6300\n",
            "Epoch 98/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 122.6657 - val_loss: 243.6058\n",
            "Epoch 99/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 123.4562 - val_loss: 186.4187\n",
            "Epoch 100/100\n",
            "5201/5201 [==============================] - 6s 1ms/step - loss: 122.2462 - val_loss: 175.0617\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adadelta,SGD\n",
        "\n",
        "#parameter\n",
        "batch_size = 32\n",
        "epoch = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "model_SBP2.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\n",
        "# model_SBP.summary()\n",
        "history = model_SBP2.fit(X_train, sbp_train, batch_size = batch_size, epochs = epoch, validation_data=(X_test, sbp_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZZkNT7-kLbs",
        "outputId": "608e7e5c-9c4b-42df-c4bc-5dbebf3e81e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  2.3008194141952036 \n",
            "MAE:  9.823266679938868 \n",
            "SD:  13.02950061522532\n"
          ]
        }
      ],
      "source": [
        "pred = model_SBP2.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)\n",
        "\n",
        "\n",
        "# ME:  2.3008194141952036 \n",
        "# MAE:  9.823266679938868 \n",
        "# SD:  13.02950061522532"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "v7Q-cgOpkLbs",
        "outputId": "775ae70d-f898-4987-b9ac-a0d3da352c6e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epoch, 0, 1000])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odhTCfgWkLbs"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgxGOO46kLbs",
        "outputId": "29b20086-25ac-4642-94fa-70ba04af39b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_14 (Dense)             (None, 64)                2368      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 15,937\n",
            "Trainable params: 15,425\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))  \n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        "    \n",
        "model_SBP2 = build_model()\n",
        "model_SBP2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeGNoanDkLbt",
        "outputId": "b63052f6-346d-4836-e1f7-2c4ad1783e57",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "5201/5201 [==============================] - 5s 937us/step - loss: 135.0451 - val_loss: 150.9042\n",
            "Epoch 2/200\n",
            "5201/5201 [==============================] - 5s 936us/step - loss: 134.4276 - val_loss: 153.7107\n",
            "Epoch 3/200\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 134.3963 - val_loss: 147.5344\n",
            "Epoch 4/200\n",
            "5201/5201 [==============================] - 5s 943us/step - loss: 134.6244 - val_loss: 148.1585\n",
            "Epoch 5/200\n",
            "5201/5201 [==============================] - 5s 951us/step - loss: 134.2630 - val_loss: 146.9270\n",
            "Epoch 6/200\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 134.5055 - val_loss: 146.0578\n",
            "Epoch 7/200\n",
            "5201/5201 [==============================] - 5s 948us/step - loss: 133.3854 - val_loss: 147.3911\n",
            "Epoch 8/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 133.8955 - val_loss: 150.5328\n",
            "Epoch 9/200\n",
            "5201/5201 [==============================] - 5s 948us/step - loss: 133.9467 - val_loss: 147.2974\n",
            "Epoch 10/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 134.2415 - val_loss: 149.3504\n",
            "Epoch 11/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 134.1795 - val_loss: 146.2094\n",
            "Epoch 12/200\n",
            "5201/5201 [==============================] - 5s 944us/step - loss: 133.9877 - val_loss: 148.8320\n",
            "Epoch 13/200\n",
            "5201/5201 [==============================] - 5s 939us/step - loss: 133.8613 - val_loss: 145.4550\n",
            "Epoch 14/200\n",
            "5201/5201 [==============================] - 5s 942us/step - loss: 133.9191 - val_loss: 145.6509\n",
            "Epoch 15/200\n",
            "5201/5201 [==============================] - 5s 945us/step - loss: 133.8689 - val_loss: 147.8751\n",
            "Epoch 16/200\n",
            "5201/5201 [==============================] - 5s 943us/step - loss: 133.1795 - val_loss: 148.1817\n",
            "Epoch 17/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 133.8199 - val_loss: 147.9627\n",
            "Epoch 18/200\n",
            "5201/5201 [==============================] - 5s 933us/step - loss: 133.3902 - val_loss: 149.0430\n",
            "Epoch 19/200\n",
            "5201/5201 [==============================] - 5s 904us/step - loss: 133.8862 - val_loss: 148.1212\n",
            "Epoch 20/200\n",
            "5201/5201 [==============================] - 5s 978us/step - loss: 134.2616 - val_loss: 147.3865\n",
            "Epoch 21/200\n",
            "5201/5201 [==============================] - 5s 965us/step - loss: 133.4903 - val_loss: 146.3244\n",
            "Epoch 22/200\n",
            "5201/5201 [==============================] - 5s 934us/step - loss: 134.0148 - val_loss: 147.0205\n",
            "Epoch 23/200\n",
            "5201/5201 [==============================] - 5s 943us/step - loss: 133.6758 - val_loss: 149.0402\n",
            "Epoch 24/200\n",
            "5201/5201 [==============================] - 5s 964us/step - loss: 134.0315 - val_loss: 145.3670\n",
            "Epoch 25/200\n",
            "5201/5201 [==============================] - 5s 931us/step - loss: 134.1639 - val_loss: 146.2816\n",
            "Epoch 26/200\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 133.3532 - val_loss: 146.5477\n",
            "Epoch 27/200\n",
            "5201/5201 [==============================] - 5s 945us/step - loss: 133.9185 - val_loss: 146.8268\n",
            "Epoch 28/200\n",
            "5201/5201 [==============================] - 5s 955us/step - loss: 133.5450 - val_loss: 147.5258\n",
            "Epoch 29/200\n",
            "5201/5201 [==============================] - 5s 937us/step - loss: 133.8570 - val_loss: 145.6022\n",
            "Epoch 30/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 133.6719 - val_loss: 145.4339\n",
            "Epoch 31/200\n",
            "5201/5201 [==============================] - 5s 938us/step - loss: 133.6605 - val_loss: 144.4055\n",
            "Epoch 32/200\n",
            "5201/5201 [==============================] - 5s 963us/step - loss: 133.9421 - val_loss: 145.0642\n",
            "Epoch 33/200\n",
            "5201/5201 [==============================] - 5s 938us/step - loss: 134.0904 - val_loss: 155.4018\n",
            "Epoch 34/200\n",
            "5201/5201 [==============================] - 5s 943us/step - loss: 133.4907 - val_loss: 146.3662\n",
            "Epoch 35/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 133.7931 - val_loss: 145.3157\n",
            "Epoch 36/200\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 133.8813 - val_loss: 152.6146\n",
            "Epoch 37/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 133.7010 - val_loss: 147.2742\n",
            "Epoch 38/200\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 133.4594 - val_loss: 145.2548\n",
            "Epoch 39/200\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 133.4608 - val_loss: 157.0794\n",
            "Epoch 40/200\n",
            "5201/5201 [==============================] - 5s 961us/step - loss: 133.5920 - val_loss: 149.0531\n",
            "Epoch 41/200\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 133.5110 - val_loss: 146.1881\n",
            "Epoch 42/200\n",
            "5201/5201 [==============================] - 5s 932us/step - loss: 133.9849 - val_loss: 153.4138\n",
            "Epoch 43/200\n",
            "5201/5201 [==============================] - 5s 943us/step - loss: 133.4843 - val_loss: 146.8897\n",
            "Epoch 44/200\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 133.7557 - val_loss: 146.3646\n",
            "Epoch 45/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 133.2731 - val_loss: 144.7788\n",
            "Epoch 46/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 133.8560 - val_loss: 147.6805\n",
            "Epoch 47/200\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 133.4418 - val_loss: 150.5130\n",
            "Epoch 48/200\n",
            "5201/5201 [==============================] - 5s 938us/step - loss: 133.3196 - val_loss: 144.6366\n",
            "Epoch 49/200\n",
            "5201/5201 [==============================] - 5s 946us/step - loss: 133.8837 - val_loss: 145.9276\n",
            "Epoch 50/200\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 133.2587 - val_loss: 155.5238\n",
            "Epoch 51/200\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 133.4883 - val_loss: 148.8199\n",
            "Epoch 52/200\n",
            "5201/5201 [==============================] - 5s 932us/step - loss: 133.2839 - val_loss: 146.5946\n",
            "Epoch 53/200\n",
            "5201/5201 [==============================] - 5s 942us/step - loss: 133.5387 - val_loss: 146.2600\n",
            "Epoch 54/200\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 133.0359 - val_loss: 146.1811\n",
            "Epoch 55/200\n",
            "5201/5201 [==============================] - 5s 937us/step - loss: 133.2208 - val_loss: 145.4303\n",
            "Epoch 56/200\n",
            "5201/5201 [==============================] - 5s 949us/step - loss: 133.8423 - val_loss: 146.2253\n",
            "Epoch 57/200\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 133.4444 - val_loss: 156.2341\n",
            "Epoch 58/200\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 133.4369 - val_loss: 147.6065\n",
            "Epoch 59/200\n",
            "5201/5201 [==============================] - 5s 937us/step - loss: 133.2738 - val_loss: 146.1556\n",
            "Epoch 60/200\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 132.9818 - val_loss: 148.6595\n",
            "Epoch 61/200\n",
            "5201/5201 [==============================] - 5s 941us/step - loss: 133.5205 - val_loss: 147.4037\n",
            "Epoch 62/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 133.4713 - val_loss: 145.6788\n",
            "Epoch 63/200\n",
            "5201/5201 [==============================] - 5s 946us/step - loss: 132.9987 - val_loss: 150.3692\n",
            "Epoch 64/200\n",
            "5201/5201 [==============================] - 5s 951us/step - loss: 133.2940 - val_loss: 146.6738\n",
            "Epoch 65/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 133.6370 - val_loss: 149.6844A: 0s - l\n",
            "Epoch 66/200\n",
            "5201/5201 [==============================] - 5s 937us/step - loss: 133.4176 - val_loss: 147.7670\n",
            "Epoch 67/200\n",
            "5201/5201 [==============================] - 5s 941us/step - loss: 133.9422 - val_loss: 145.5633\n",
            "Epoch 68/200\n",
            "5201/5201 [==============================] - 5s 947us/step - loss: 133.1826 - val_loss: 149.3546\n",
            "Epoch 69/200\n",
            "5201/5201 [==============================] - 5s 938us/step - loss: 133.1828 - val_loss: 153.6909\n",
            "Epoch 70/200\n",
            "5201/5201 [==============================] - 5s 951us/step - loss: 133.2702 - val_loss: 148.6447\n",
            "Epoch 71/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 133.4703 - val_loss: 146.1208\n",
            "Epoch 72/200\n",
            "5201/5201 [==============================] - 5s 952us/step - loss: 132.8694 - val_loss: 146.7052\n",
            "Epoch 73/200\n",
            "5201/5201 [==============================] - 5s 961us/step - loss: 133.1022 - val_loss: 146.1638\n",
            "Epoch 74/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 133.2349 - val_loss: 151.6339\n",
            "Epoch 75/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 133.2518 - val_loss: 146.5025\n",
            "Epoch 76/200\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 133.3195 - val_loss: 145.6732\n",
            "Epoch 77/200\n",
            "5201/5201 [==============================] - 5s 949us/step - loss: 133.0367 - val_loss: 148.2776\n",
            "Epoch 78/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 133.4560 - val_loss: 148.8019\n",
            "Epoch 79/200\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 132.9586 - val_loss: 148.2723\n",
            "Epoch 80/200\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 133.4623 - val_loss: 148.6945\n",
            "Epoch 81/200\n",
            "5201/5201 [==============================] - 5s 960us/step - loss: 133.3390 - val_loss: 146.4639\n",
            "Epoch 82/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 133.0649 - val_loss: 151.7255\n",
            "Epoch 83/200\n",
            "5201/5201 [==============================] - 5s 937us/step - loss: 132.9815 - val_loss: 146.3223\n",
            "Epoch 84/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 133.2568 - val_loss: 146.4238\n",
            "Epoch 85/200\n",
            "5201/5201 [==============================] - 5s 928us/step - loss: 133.2830 - val_loss: 146.9026\n",
            "Epoch 86/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 133.4816 - val_loss: 149.8835\n",
            "Epoch 87/200\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 133.5144 - val_loss: 146.4537\n",
            "Epoch 88/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 132.4910 - val_loss: 147.5267\n",
            "Epoch 89/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 133.5457 - val_loss: 148.0865\n",
            "Epoch 90/200\n",
            "5201/5201 [==============================] - 5s 933us/step - loss: 133.3787 - val_loss: 146.7173\n",
            "Epoch 91/200\n",
            "5201/5201 [==============================] - 5s 928us/step - loss: 133.0103 - val_loss: 150.5945\n",
            "Epoch 92/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 133.3436 - val_loss: 145.3664\n",
            "Epoch 93/200\n",
            "5201/5201 [==============================] - 5s 937us/step - loss: 132.7567 - val_loss: 149.3848\n",
            "Epoch 94/200\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 132.9890 - val_loss: 147.6328\n",
            "Epoch 95/200\n",
            "5201/5201 [==============================] - 5s 939us/step - loss: 132.6849 - val_loss: 158.6290\n",
            "Epoch 96/200\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 133.3241 - val_loss: 150.0857\n",
            "Epoch 97/200\n",
            "5201/5201 [==============================] - 5s 950us/step - loss: 133.2108 - val_loss: 146.5314\n",
            "Epoch 98/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 133.0600 - val_loss: 146.5309\n",
            "Epoch 99/200\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 132.8165 - val_loss: 153.5840\n",
            "Epoch 100/200\n",
            "5201/5201 [==============================] - 5s 936us/step - loss: 132.7576 - val_loss: 149.2836\n",
            "Epoch 101/200\n",
            "5201/5201 [==============================] - 5s 939us/step - loss: 133.0580 - val_loss: 146.4903\n",
            "Epoch 102/200\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 132.7132 - val_loss: 152.1892\n",
            "Epoch 103/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 132.9911 - val_loss: 147.6595\n",
            "Epoch 104/200\n",
            "5201/5201 [==============================] - 5s 942us/step - loss: 132.9250 - val_loss: 145.7827\n",
            "Epoch 105/200\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 133.0416 - val_loss: 157.2570\n",
            "Epoch 106/200\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 133.3412 - val_loss: 148.1049\n",
            "Epoch 107/200\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 132.7903 - val_loss: 153.9863\n",
            "Epoch 108/200\n",
            "5201/5201 [==============================] - 5s 936us/step - loss: 132.8120 - val_loss: 150.1640\n",
            "Epoch 109/200\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 132.9224 - val_loss: 148.6273\n",
            "Epoch 110/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 132.7887 - val_loss: 147.5677\n",
            "Epoch 111/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 133.0408 - val_loss: 147.2733\n",
            "Epoch 112/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 132.9354 - val_loss: 147.5733\n",
            "Epoch 113/200\n",
            "5201/5201 [==============================] - 5s 950us/step - loss: 133.3590 - val_loss: 148.5930\n",
            "Epoch 114/200\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 133.4025 - val_loss: 154.2455\n",
            "Epoch 115/200\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 133.0000 - val_loss: 150.1459\n",
            "Epoch 116/200\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 133.1345 - val_loss: 146.4290\n",
            "Epoch 117/200\n",
            "5201/5201 [==============================] - 5s 938us/step - loss: 133.2373 - val_loss: 146.9008\n",
            "Epoch 118/200\n",
            "5201/5201 [==============================] - 5s 958us/step - loss: 133.1346 - val_loss: 153.7809\n",
            "Epoch 119/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 133.1447 - val_loss: 146.7451\n",
            "Epoch 120/200\n",
            "5201/5201 [==============================] - 5s 937us/step - loss: 132.7414 - val_loss: 150.1600\n",
            "Epoch 121/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 132.6639 - val_loss: 149.9256\n",
            "Epoch 122/200\n",
            "5201/5201 [==============================] - 5s 956us/step - loss: 133.3783 - val_loss: 147.8061\n",
            "Epoch 123/200\n",
            "5201/5201 [==============================] - 5s 928us/step - loss: 132.6946 - val_loss: 146.7157\n",
            "Epoch 124/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 132.9746 - val_loss: 157.1819\n",
            "Epoch 125/200\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 132.9952 - val_loss: 148.3856\n",
            "Epoch 126/200\n",
            "5201/5201 [==============================] - 5s 932us/step - loss: 132.8989 - val_loss: 153.1695\n",
            "Epoch 127/200\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 132.7773 - val_loss: 149.3873\n",
            "Epoch 128/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 132.8006 - val_loss: 147.1109\n",
            "Epoch 129/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 132.8023 - val_loss: 145.8954\n",
            "Epoch 130/200\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 133.0475 - val_loss: 146.5739\n",
            "Epoch 131/200\n",
            "5201/5201 [==============================] - 5s 941us/step - loss: 132.9815 - val_loss: 145.4014\n",
            "Epoch 132/200\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 132.5858 - val_loss: 145.4707\n",
            "Epoch 133/200\n",
            "5201/5201 [==============================] - 5s 935us/step - loss: 133.0383 - val_loss: 146.2099\n",
            "Epoch 134/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 133.1630 - val_loss: 149.7924\n",
            "Epoch 135/200\n",
            "5201/5201 [==============================] - 5s 936us/step - loss: 132.9687 - val_loss: 146.1302\n",
            "Epoch 136/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 132.7369 - val_loss: 147.0257\n",
            "Epoch 137/200\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 133.1561 - val_loss: 171.9128\n",
            "Epoch 138/200\n",
            "5201/5201 [==============================] - 5s 949us/step - loss: 133.1326 - val_loss: 147.8433\n",
            "Epoch 139/200\n",
            "5201/5201 [==============================] - 5s 948us/step - loss: 133.3364 - val_loss: 149.5668\n",
            "Epoch 140/200\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 132.9948 - val_loss: 151.2872\n",
            "Epoch 141/200\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 132.6880 - val_loss: 145.7738\n",
            "Epoch 142/200\n",
            "5201/5201 [==============================] - 5s 936us/step - loss: 132.6838 - val_loss: 145.8957\n",
            "Epoch 143/200\n",
            "5201/5201 [==============================] - 5s 937us/step - loss: 132.7978 - val_loss: 150.1443\n",
            "Epoch 144/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 133.0343 - val_loss: 149.4291\n",
            "Epoch 145/200\n",
            "5201/5201 [==============================] - 5s 956us/step - loss: 132.9617 - val_loss: 144.8006\n",
            "Epoch 146/200\n",
            "5201/5201 [==============================] - 5s 963us/step - loss: 132.7560 - val_loss: 146.5758\n",
            "Epoch 147/200\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 132.8447 - val_loss: 145.1361\n",
            "Epoch 148/200\n",
            "5201/5201 [==============================] - 5s 937us/step - loss: 132.9613 - val_loss: 146.7516\n",
            "Epoch 149/200\n",
            "5201/5201 [==============================] - 5s 935us/step - loss: 132.9100 - val_loss: 145.9043\n",
            "Epoch 150/200\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 132.7686 - val_loss: 154.1049\n",
            "Epoch 151/200\n",
            "5201/5201 [==============================] - 5s 951us/step - loss: 132.7168 - val_loss: 148.0558\n",
            "Epoch 152/200\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 132.4615 - val_loss: 146.1469\n",
            "Epoch 153/200\n",
            "5201/5201 [==============================] - 5s 934us/step - loss: 132.2337 - val_loss: 150.9156\n",
            "Epoch 154/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 133.1028 - val_loss: 152.2949\n",
            "Epoch 155/200\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 132.6909 - val_loss: 144.7406\n",
            "Epoch 156/200\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 132.6719 - val_loss: 149.9211\n",
            "Epoch 157/200\n",
            "5201/5201 [==============================] - 5s 942us/step - loss: 132.5804 - val_loss: 149.2931\n",
            "Epoch 158/200\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 132.9636 - val_loss: 151.8193\n",
            "Epoch 159/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 132.5269 - val_loss: 149.8089\n",
            "Epoch 160/200\n",
            "5201/5201 [==============================] - 5s 943us/step - loss: 132.7262 - val_loss: 151.5427\n",
            "Epoch 161/200\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 133.1919 - val_loss: 147.0022\n",
            "Epoch 162/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 132.9152 - val_loss: 145.7036\n",
            "Epoch 163/200\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 132.6040 - val_loss: 146.3535\n",
            "Epoch 164/200\n",
            "5201/5201 [==============================] - 5s 932us/step - loss: 132.8494 - val_loss: 151.5913\n",
            "Epoch 165/200\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 132.5625 - val_loss: 148.5905\n",
            "Epoch 166/200\n",
            "5201/5201 [==============================] - 5s 942us/step - loss: 133.0260 - val_loss: 145.9291\n",
            "Epoch 167/200\n",
            "5201/5201 [==============================] - 5s 939us/step - loss: 133.2208 - val_loss: 146.6920\n",
            "Epoch 168/200\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 132.8189 - val_loss: 148.9238\n",
            "Epoch 169/200\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 132.3183 - val_loss: 146.4288\n",
            "Epoch 170/200\n",
            "5201/5201 [==============================] - 5s 943us/step - loss: 132.6752 - val_loss: 147.7801\n",
            "Epoch 171/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 133.1548 - val_loss: 161.3060\n",
            "Epoch 172/200\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 132.3593 - val_loss: 150.9229\n",
            "Epoch 173/200\n",
            "5201/5201 [==============================] - 5s 935us/step - loss: 132.9435 - val_loss: 146.8470\n",
            "Epoch 174/200\n",
            "5201/5201 [==============================] - 5s 962us/step - loss: 132.3584 - val_loss: 154.0683\n",
            "Epoch 175/200\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 132.3205 - val_loss: 148.2970\n",
            "Epoch 176/200\n",
            "5201/5201 [==============================] - 5s 939us/step - loss: 132.6156 - val_loss: 149.3090\n",
            "Epoch 177/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 132.3324 - val_loss: 147.7576\n",
            "Epoch 178/200\n",
            "5201/5201 [==============================] - 5s 931us/step - loss: 132.4836 - val_loss: 146.0345\n",
            "Epoch 179/200\n",
            "5201/5201 [==============================] - 5s 941us/step - loss: 132.6524 - val_loss: 148.2856\n",
            "Epoch 180/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 132.9516 - val_loss: 148.4431\n",
            "Epoch 181/200\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 132.3774 - val_loss: 147.9623\n",
            "Epoch 182/200\n",
            "5201/5201 [==============================] - 5s 935us/step - loss: 132.3787 - val_loss: 147.9571\n",
            "Epoch 183/200\n",
            "5201/5201 [==============================] - 5s 950us/step - loss: 132.4534 - val_loss: 153.6448\n",
            "Epoch 184/200\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 132.8416 - val_loss: 146.0685\n",
            "Epoch 185/200\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 133.3125 - val_loss: 152.5976\n",
            "Epoch 186/200\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 132.9012 - val_loss: 150.6446\n",
            "Epoch 187/200\n",
            "5201/5201 [==============================] - 5s 955us/step - loss: 132.6219 - val_loss: 146.9437\n",
            "Epoch 188/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 132.7650 - val_loss: 149.6897 ETA: 0s - loss: \n",
            "Epoch 189/200\n",
            "5201/5201 [==============================] - 5s 938us/step - loss: 132.5960 - val_loss: 150.9977\n",
            "Epoch 190/200\n",
            "5201/5201 [==============================] - 5s 941us/step - loss: 132.9953 - val_loss: 147.7147\n",
            "Epoch 191/200\n",
            "5201/5201 [==============================] - 5s 953us/step - loss: 132.7054 - val_loss: 145.8668\n",
            "Epoch 192/200\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 132.2532 - val_loss: 157.6758\n",
            "Epoch 193/200\n",
            "5201/5201 [==============================] - 5s 934us/step - loss: 132.7003 - val_loss: 146.6046\n",
            "Epoch 194/200\n",
            "5201/5201 [==============================] - 5s 945us/step - loss: 132.7108 - val_loss: 148.6020\n",
            "Epoch 195/200\n",
            "5201/5201 [==============================] - 5s 950us/step - loss: 132.6060 - val_loss: 144.9749\n",
            "Epoch 196/200\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 132.6423 - val_loss: 146.7531\n",
            "Epoch 197/200\n",
            "5201/5201 [==============================] - 5s 933us/step - loss: 132.5916 - val_loss: 148.2045\n",
            "Epoch 198/200\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 132.9043 - val_loss: 149.7481\n",
            "Epoch 199/200\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 132.4094 - val_loss: 147.5327\n",
            "Epoch 200/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 132.5876 - val_loss: 146.0299\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adadelta,SGD\n",
        "\n",
        "#parameter\n",
        "batch_size = 32\n",
        "epoch = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "model_SBP.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\n",
        "# model_SBP.summary()\n",
        "history = model_SBP.fit(X_train, sbp_train, batch_size = batch_size, epochs = epoch, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "887IleVIkLbt",
        "outputId": "20d14e4a-36e0-4c7a-e3e5-e34404e144dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -0.20655137256061798 \n",
            "MAE:  8.736114286842694 \n",
            "SD:  12.082522321943067\n"
          ]
        }
      ],
      "source": [
        "pred = model_SBP.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)\n",
        "\n",
        "\n",
        "# ME:  -0.20655137256061798 \n",
        "# MAE:  8.736114286842694 \n",
        "# SD:  12.082522321943067"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "gcPyr4KNkLbt",
        "outputId": "d3dc21cb-9744-4203-edef-df50d67a888a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epoch, 0, 1000])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONe3Hu3iBgDG"
      },
      "source": [
        "batch size는 32 일때가 최적이다 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVaoJg7hBjan"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtFjBAtRkLbt"
      },
      "source": [
        "## 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tugLToHpkLbu",
        "outputId": "169ef5f0-f6a0-4ac4-fb05-18a0c57f86f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_19 (Dense)             (None, 64)                2368      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 15,937\n",
            "Trainable params: 15,425\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))  \n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        "    \n",
        "model_SBP2 = build_model()\n",
        "model_SBP2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5-P13K0kLbu",
        "outputId": "aee9455e-0086-4e84-e76b-fe3eb03a21f0",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 132.1691 - val_loss: 150.1409\n",
            "Epoch 2/300\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 132.5952 - val_loss: 146.4950\n",
            "Epoch 3/300\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 132.3549 - val_loss: 146.7797\n",
            "Epoch 4/300\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 132.5496 - val_loss: 153.6616\n",
            "Epoch 5/300\n",
            "5201/5201 [==============================] - 5s 874us/step - loss: 132.5021 - val_loss: 146.3128\n",
            "Epoch 6/300\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 132.8623 - val_loss: 145.9408\n",
            "Epoch 7/300\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 132.4830 - val_loss: 146.7182\n",
            "Epoch 8/300\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 132.5221 - val_loss: 159.5817\n",
            "Epoch 9/300\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 132.4628 - val_loss: 147.0005\n",
            "Epoch 10/300\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 132.3134 - val_loss: 148.2136\n",
            "Epoch 11/300\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 132.8531 - val_loss: 147.3961\n",
            "Epoch 12/300\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 132.6534 - val_loss: 148.4253\n",
            "Epoch 13/300\n",
            "5201/5201 [==============================] - 5s 936us/step - loss: 132.3814 - val_loss: 148.6153\n",
            "Epoch 14/300\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 132.2975 - val_loss: 152.6825\n",
            "Epoch 15/300\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 132.5700 - val_loss: 151.8488\n",
            "Epoch 16/300\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 132.4599 - val_loss: 150.8668\n",
            "Epoch 17/300\n",
            "5201/5201 [==============================] - 5s 904us/step - loss: 132.6441 - val_loss: 149.0455\n",
            "Epoch 18/300\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 132.7400 - val_loss: 152.5083\n",
            "Epoch 19/300\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 132.2887 - val_loss: 151.0394\n",
            "Epoch 20/300\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 132.1675 - val_loss: 155.6391\n",
            "Epoch 21/300\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 132.8081 - val_loss: 148.4509\n",
            "Epoch 22/300\n",
            "5201/5201 [==============================] - 5s 935us/step - loss: 132.1214 - val_loss: 147.4935\n",
            "Epoch 23/300\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 132.0035 - val_loss: 159.5339\n",
            "Epoch 24/300\n",
            "5201/5201 [==============================] - 5s 940us/step - loss: 132.3968 - val_loss: 148.0967\n",
            "Epoch 25/300\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 132.3853 - val_loss: 148.6619\n",
            "Epoch 26/300\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 132.3225 - val_loss: 147.4030\n",
            "Epoch 27/300\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 132.7936 - val_loss: 157.4870\n",
            "Epoch 28/300\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 131.8139 - val_loss: 145.6033\n",
            "Epoch 29/300\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 132.1913 - val_loss: 148.1335\n",
            "Epoch 30/300\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 132.9969 - val_loss: 147.1923\n",
            "Epoch 31/300\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 132.2757 - val_loss: 149.4958\n",
            "Epoch 32/300\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 132.5781 - val_loss: 155.3810\n",
            "Epoch 33/300\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 132.3573 - val_loss: 147.7886\n",
            "Epoch 34/300\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 132.6703 - val_loss: 146.5427\n",
            "Epoch 35/300\n",
            "5201/5201 [==============================] - 5s 939us/step - loss: 132.1259 - val_loss: 146.9463\n",
            "Epoch 36/300\n",
            "5201/5201 [==============================] - 5s 950us/step - loss: 132.1267 - val_loss: 145.4476\n",
            "Epoch 37/300\n",
            "5201/5201 [==============================] - 5s 951us/step - loss: 132.6938 - val_loss: 144.2473\n",
            "Epoch 38/300\n",
            "5201/5201 [==============================] - 5s 951us/step - loss: 132.2944 - val_loss: 145.6326\n",
            "Epoch 39/300\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 132.7174 - val_loss: 145.3368\n",
            "Epoch 40/300\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 132.4512 - val_loss: 149.6051\n",
            "Epoch 41/300\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 132.0226 - val_loss: 149.1805\n",
            "Epoch 42/300\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 132.2995 - val_loss: 150.2319\n",
            "Epoch 43/300\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 132.7031 - val_loss: 154.1959\n",
            "Epoch 44/300\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 132.0886 - val_loss: 147.4885\n",
            "Epoch 45/300\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 131.9216 - val_loss: 146.7100\n",
            "Epoch 46/300\n",
            "5201/5201 [==============================] - 5s 940us/step - loss: 132.4178 - val_loss: 146.1810\n",
            "Epoch 47/300\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 132.2104 - val_loss: 149.6158\n",
            "Epoch 48/300\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 132.6810 - val_loss: 149.7971\n",
            "Epoch 49/300\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 132.1615 - val_loss: 148.2157\n",
            "Epoch 50/300\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 131.8549 - val_loss: 154.3557\n",
            "Epoch 51/300\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 132.4833 - val_loss: 148.1182\n",
            "Epoch 52/300\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 132.1533 - val_loss: 147.2934\n",
            "Epoch 53/300\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 132.2484 - val_loss: 150.4235\n",
            "Epoch 54/300\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 132.1581 - val_loss: 145.5571\n",
            "Epoch 55/300\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 132.2696 - val_loss: 146.2921\n",
            "Epoch 56/300\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 132.0958 - val_loss: 164.0277\n",
            "Epoch 57/300\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 132.1164 - val_loss: 152.7462\n",
            "Epoch 58/300\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 132.1010 - val_loss: 155.6010\n",
            "Epoch 59/300\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 132.1806 - val_loss: 145.9224\n",
            "Epoch 60/300\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 132.4729 - val_loss: 145.1009\n",
            "Epoch 61/300\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 131.7115 - val_loss: 147.0188\n",
            "Epoch 62/300\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 132.3204 - val_loss: 146.5152\n",
            "Epoch 63/300\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 132.4519 - val_loss: 150.4270\n",
            "Epoch 64/300\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 132.1280 - val_loss: 145.3292\n",
            "Epoch 65/300\n",
            "5201/5201 [==============================] - 5s 949us/step - loss: 132.3777 - val_loss: 149.9821\n",
            "Epoch 66/300\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 132.3007 - val_loss: 148.9587\n",
            "Epoch 67/300\n",
            "5201/5201 [==============================] - 5s 946us/step - loss: 132.2573 - val_loss: 147.9057\n",
            "Epoch 68/300\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 131.8793 - val_loss: 148.6269\n",
            "Epoch 69/300\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 132.4103 - val_loss: 145.9104\n",
            "Epoch 70/300\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 132.6608 - val_loss: 147.5342\n",
            "Epoch 71/300\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 132.3827 - val_loss: 150.7542\n",
            "Epoch 72/300\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 132.5036 - val_loss: 147.1535\n",
            "Epoch 73/300\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 132.6288 - val_loss: 148.6805\n",
            "Epoch 74/300\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 132.4391 - val_loss: 146.7935\n",
            "Epoch 75/300\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 132.3217 - val_loss: 146.2761\n",
            "Epoch 76/300\n",
            "5201/5201 [==============================] - 5s 934us/step - loss: 132.0066 - val_loss: 147.0197\n",
            "Epoch 77/300\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 132.4841 - val_loss: 145.6185\n",
            "Epoch 78/300\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 132.3535 - val_loss: 149.7371\n",
            "Epoch 79/300\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 132.1374 - val_loss: 150.2093\n",
            "Epoch 80/300\n",
            "5201/5201 [==============================] - 5s 935us/step - loss: 132.1770 - val_loss: 154.6738\n",
            "Epoch 81/300\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 132.1000 - val_loss: 147.2953\n",
            "Epoch 82/300\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 131.9931 - val_loss: 146.8273\n",
            "Epoch 83/300\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 131.7640 - val_loss: 147.3227\n",
            "Epoch 84/300\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 132.0173 - val_loss: 148.6270\n",
            "Epoch 85/300\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 131.9422 - val_loss: 145.5192\n",
            "Epoch 86/300\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 132.3003 - val_loss: 145.2493\n",
            "Epoch 87/300\n",
            "5201/5201 [==============================] - 5s 875us/step - loss: 132.0826 - val_loss: 147.9913\n",
            "Epoch 88/300\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 132.3570 - val_loss: 148.3159\n",
            "Epoch 89/300\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 131.9711 - val_loss: 148.6664\n",
            "Epoch 90/300\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 132.0417 - val_loss: 149.7197\n",
            "Epoch 91/300\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 132.2393 - val_loss: 148.9986\n",
            "Epoch 92/300\n",
            "5201/5201 [==============================] - 5s 928us/step - loss: 131.9178 - val_loss: 145.6341\n",
            "Epoch 93/300\n",
            "5201/5201 [==============================] - 5s 875us/step - loss: 131.7610 - val_loss: 160.7556\n",
            "Epoch 94/300\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 132.0415 - val_loss: 151.6456\n",
            "Epoch 95/300\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 132.4156 - val_loss: 149.2882\n",
            "Epoch 96/300\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 131.5878 - val_loss: 146.8892\n",
            "Epoch 97/300\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 131.9281 - val_loss: 147.8828\n",
            "Epoch 98/300\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 131.7906 - val_loss: 148.3986\n",
            "Epoch 99/300\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 131.9645 - val_loss: 146.9577\n",
            "Epoch 100/300\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 131.7908 - val_loss: 147.6874\n",
            "Epoch 101/300\n",
            "5201/5201 [==============================] - 5s 904us/step - loss: 131.9501 - val_loss: 146.6309\n",
            "Epoch 102/300\n",
            "5201/5201 [==============================] - 5s 939us/step - loss: 132.1977 - val_loss: 152.9916\n",
            "Epoch 103/300\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 132.1108 - val_loss: 147.4398\n",
            "Epoch 104/300\n",
            "5201/5201 [==============================] - 5s 934us/step - loss: 132.3419 - val_loss: 154.0401\n",
            "Epoch 105/300\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 132.1291 - val_loss: 149.5640\n",
            "Epoch 106/300\n",
            "5201/5201 [==============================] - 5s 944us/step - loss: 132.1718 - val_loss: 147.2594\n",
            "Epoch 107/300\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 131.9964 - val_loss: 150.1486\n",
            "Epoch 108/300\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 131.8432 - val_loss: 145.4224\n",
            "Epoch 109/300\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 132.0970 - val_loss: 146.6232\n",
            "Epoch 110/300\n",
            "5201/5201 [==============================] - 5s 949us/step - loss: 131.8481 - val_loss: 146.6837\n",
            "Epoch 111/300\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 132.6302 - val_loss: 153.1042\n",
            "Epoch 112/300\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 131.6752 - val_loss: 147.3780\n",
            "Epoch 113/300\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 132.0328 - val_loss: 145.8343\n",
            "Epoch 114/300\n",
            "5201/5201 [==============================] - 5s 952us/step - loss: 131.9017 - val_loss: 154.3944\n",
            "Epoch 115/300\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 132.0312 - val_loss: 145.4672\n",
            "Epoch 116/300\n",
            "5201/5201 [==============================] - 5s 932us/step - loss: 132.0112 - val_loss: 160.6444\n",
            "Epoch 117/300\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 131.7335 - val_loss: 148.3856\n",
            "Epoch 118/300\n",
            "5201/5201 [==============================] - 5s 933us/step - loss: 131.6506 - val_loss: 146.5797\n",
            "Epoch 119/300\n",
            "5201/5201 [==============================] - 5s 904us/step - loss: 132.2794 - val_loss: 149.4254\n",
            "Epoch 120/300\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 132.3110 - val_loss: 148.0060\n",
            "Epoch 121/300\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 131.7835 - val_loss: 149.7858\n",
            "Epoch 122/300\n",
            "5201/5201 [==============================] - 5s 928us/step - loss: 131.8967 - val_loss: 149.0610\n",
            "Epoch 123/300\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 131.6963 - val_loss: 147.6948\n",
            "Epoch 124/300\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 131.7241 - val_loss: 146.3182\n",
            "Epoch 125/300\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 131.7022 - val_loss: 151.8895\n",
            "Epoch 126/300\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 131.9368 - val_loss: 147.5021\n",
            "Epoch 127/300\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 132.0722 - val_loss: 155.7447\n",
            "Epoch 128/300\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 131.8378 - val_loss: 149.1396\n",
            "Epoch 129/300\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 131.8308 - val_loss: 145.6223\n",
            "Epoch 130/300\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 131.5631 - val_loss: 146.5687\n",
            "Epoch 131/300\n",
            "5201/5201 [==============================] - 5s 940us/step - loss: 132.2146 - val_loss: 168.8534\n",
            "Epoch 132/300\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 131.9533 - val_loss: 148.3882\n",
            "Epoch 133/300\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 131.7129 - val_loss: 145.8475\n",
            "Epoch 134/300\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 131.6681 - val_loss: 145.5536\n",
            "Epoch 135/300\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 131.3823 - val_loss: 145.8470\n",
            "Epoch 136/300\n",
            "5201/5201 [==============================] - 5s 932us/step - loss: 131.6051 - val_loss: 168.1233\n",
            "Epoch 137/300\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 131.8293 - val_loss: 147.2693\n",
            "Epoch 138/300\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 131.4087 - val_loss: 149.1385\n",
            "Epoch 139/300\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 132.0664 - val_loss: 146.4970\n",
            "Epoch 140/300\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 131.3948 - val_loss: 147.6211\n",
            "Epoch 141/300\n",
            "5201/5201 [==============================] - 5s 941us/step - loss: 131.5591 - val_loss: 148.5680\n",
            "Epoch 142/300\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 131.9607 - val_loss: 148.4460\n",
            "Epoch 143/300\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 131.8069 - val_loss: 145.6396\n",
            "Epoch 144/300\n",
            "5201/5201 [==============================] - 5s 950us/step - loss: 131.6961 - val_loss: 145.9418\n",
            "Epoch 145/300\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 132.1028 - val_loss: 145.6080\n",
            "Epoch 146/300\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 131.8874 - val_loss: 146.5986\n",
            "Epoch 147/300\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 131.4790 - val_loss: 148.2065\n",
            "Epoch 148/300\n",
            "5201/5201 [==============================] - 5s 954us/step - loss: 131.8560 - val_loss: 151.6059\n",
            "Epoch 149/300\n",
            "5201/5201 [==============================] - 5s 904us/step - loss: 131.5360 - val_loss: 148.6765\n",
            "Epoch 150/300\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 131.9668 - val_loss: 148.9159\n",
            "Epoch 151/300\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 131.9550 - val_loss: 150.8481\n",
            "Epoch 152/300\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 131.5163 - val_loss: 152.0727\n",
            "Epoch 153/300\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 132.0070 - val_loss: 146.1651\n",
            "Epoch 154/300\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 131.8887 - val_loss: 149.6779\n",
            "Epoch 155/300\n",
            "5201/5201 [==============================] - 4s 863us/step - loss: 131.8391 - val_loss: 147.2853\n",
            "Epoch 156/300\n",
            "5201/5201 [==============================] - 5s 866us/step - loss: 132.0310 - val_loss: 146.0494\n",
            "Epoch 157/300\n",
            "5201/5201 [==============================] - 4s 861us/step - loss: 131.8323 - val_loss: 148.8623\n",
            "Epoch 158/300\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 131.6242 - val_loss: 146.4173\n",
            "Epoch 159/300\n",
            "5201/5201 [==============================] - 5s 875us/step - loss: 131.9915 - val_loss: 147.1735\n",
            "Epoch 160/300\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 131.7308 - val_loss: 150.7989\n",
            "Epoch 161/300\n",
            "5201/5201 [==============================] - 5s 876us/step - loss: 131.8696 - val_loss: 151.5099\n",
            "Epoch 162/300\n",
            "5201/5201 [==============================] - 4s 862us/step - loss: 132.0233 - val_loss: 146.7097\n",
            "Epoch 163/300\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 131.9605 - val_loss: 157.7865\n",
            "Epoch 164/300\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 131.7756 - val_loss: 154.6282\n",
            "Epoch 165/300\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 131.8848 - val_loss: 146.1996\n",
            "Epoch 166/300\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 131.9552 - val_loss: 154.5567\n",
            "Epoch 167/300\n",
            "5201/5201 [==============================] - 5s 867us/step - loss: 131.8988 - val_loss: 146.1843\n",
            "Epoch 168/300\n",
            "5201/5201 [==============================] - 5s 876us/step - loss: 131.4395 - val_loss: 155.0165\n",
            "Epoch 169/300\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 131.7253 - val_loss: 147.7896\n",
            "Epoch 170/300\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 131.9200 - val_loss: 146.5300\n",
            "Epoch 171/300\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 131.6757 - val_loss: 146.6461\n",
            "Epoch 172/300\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 131.5573 - val_loss: 146.6356\n",
            "Epoch 173/300\n",
            "5201/5201 [==============================] - 5s 874us/step - loss: 132.0046 - val_loss: 152.1142\n",
            "Epoch 174/300\n",
            "5201/5201 [==============================] - 4s 844us/step - loss: 131.4787 - val_loss: 144.9873\n",
            "Epoch 175/300\n",
            "5201/5201 [==============================] - 5s 871us/step - loss: 131.6908 - val_loss: 147.2139\n",
            "Epoch 176/300\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 131.7282 - val_loss: 146.1051\n",
            "Epoch 177/300\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 131.8969 - val_loss: 145.2051\n",
            "Epoch 178/300\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 132.3354 - val_loss: 159.9907\n",
            "Epoch 179/300\n",
            "5201/5201 [==============================] - 5s 868us/step - loss: 132.2285 - val_loss: 147.670932.35\n",
            "Epoch 180/300\n",
            "5201/5201 [==============================] - 4s 864us/step - loss: 131.8895 - val_loss: 146.9487\n",
            "Epoch 181/300\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 131.8363 - val_loss: 149.0405\n",
            "Epoch 182/300\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 131.6240 - val_loss: 145.5985\n",
            "Epoch 183/300\n",
            "5201/5201 [==============================] - 5s 877us/step - loss: 131.7486 - val_loss: 145.6973\n",
            "Epoch 184/300\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 132.0079 - val_loss: 147.2941\n",
            "Epoch 185/300\n",
            "5201/5201 [==============================] - 5s 874us/step - loss: 131.1902 - val_loss: 146.1417\n",
            "Epoch 186/300\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 131.6961 - val_loss: 147.3451\n",
            "Epoch 187/300\n",
            "5201/5201 [==============================] - 5s 870us/step - loss: 132.2570 - val_loss: 150.9444\n",
            "Epoch 188/300\n",
            "5201/5201 [==============================] - 4s 856us/step - loss: 131.5387 - val_loss: 149.8603\n",
            "Epoch 189/300\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 132.1299 - val_loss: 146.3450\n",
            "Epoch 190/300\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 131.4036 - val_loss: 149.8469 ETA: 1 - ETA: 0s - loss: 13\n",
            "Epoch 191/300\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 131.0351 - val_loss: 163.2553\n",
            "Epoch 192/300\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 131.7654 - val_loss: 147.4375\n",
            "Epoch 193/300\n",
            "5201/5201 [==============================] - 4s 858us/step - loss: 131.7124 - val_loss: 147.2053\n",
            "Epoch 194/300\n",
            "5201/5201 [==============================] - 5s 872us/step - loss: 132.0036 - val_loss: 153.8824\n",
            "Epoch 195/300\n",
            "5201/5201 [==============================] - 5s 869us/step - loss: 131.6734 - val_loss: 147.8654\n",
            "Epoch 196/300\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 131.4914 - val_loss: 149.4154\n",
            "Epoch 197/300\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 131.2332 - val_loss: 156.4419\n",
            "Epoch 198/300\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 131.2135 - val_loss: 150.1506\n",
            "Epoch 199/300\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 131.3111 - val_loss: 156.0180\n",
            "Epoch 200/300\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 131.9193 - val_loss: 150.0663\n",
            "Epoch 201/300\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 131.8085 - val_loss: 146.4487\n",
            "Epoch 202/300\n",
            "5201/5201 [==============================] - 4s 853us/step - loss: 131.3983 - val_loss: 147.1811\n",
            "Epoch 203/300\n",
            "5201/5201 [==============================] - 5s 869us/step - loss: 131.4320 - val_loss: 147.0824\n",
            "Epoch 204/300\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 131.4671 - val_loss: 146.0635\n",
            "Epoch 205/300\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 131.6323 - val_loss: 147.6874\n",
            "Epoch 206/300\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 131.2334 - val_loss: 157.4600\n",
            "Epoch 207/300\n",
            "5201/5201 [==============================] - 5s 875us/step - loss: 131.8568 - val_loss: 147.3570\n",
            "Epoch 208/300\n",
            "5201/5201 [==============================] - 5s 878us/step - loss: 131.5666 - val_loss: 149.3059\n",
            "Epoch 209/300\n",
            "5201/5201 [==============================] - 4s 863us/step - loss: 131.6316 - val_loss: 147.1628\n",
            "Epoch 210/300\n",
            "5201/5201 [==============================] - 5s 874us/step - loss: 131.8680 - val_loss: 148.6065\n",
            "Epoch 211/300\n",
            "5201/5201 [==============================] - 5s 868us/step - loss: 131.8967 - val_loss: 145.6857\n",
            "Epoch 212/300\n",
            "5201/5201 [==============================] - 5s 876us/step - loss: 132.0078 - val_loss: 149.6588\n",
            "Epoch 213/300\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 131.7981 - val_loss: 145.7640\n",
            "Epoch 214/300\n",
            "5201/5201 [==============================] - 5s 868us/step - loss: 131.3817 - val_loss: 152.1121\n",
            "Epoch 215/300\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 131.3723 - val_loss: 146.8459\n",
            "Epoch 216/300\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 131.9559 - val_loss: 147.4335\n",
            "Epoch 217/300\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 131.6609 - val_loss: 169.4218\n",
            "Epoch 218/300\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 131.7171 - val_loss: 146.7257\n",
            "Epoch 219/300\n",
            "5201/5201 [==============================] - 4s 859us/step - loss: 131.7117 - val_loss: 148.2520\n",
            "Epoch 220/300\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 131.8237 - val_loss: 146.6190\n",
            "Epoch 221/300\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 131.6138 - val_loss: 147.9698\n",
            "Epoch 222/300\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 131.5853 - val_loss: 145.5638\n",
            "Epoch 223/300\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 131.5505 - val_loss: 158.8896\n",
            "Epoch 224/300\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 132.1415 - val_loss: 144.5078\n",
            "Epoch 225/300\n",
            "5201/5201 [==============================] - 5s 873us/step - loss: 131.3005 - val_loss: 150.0635\n",
            "Epoch 226/300\n",
            "5201/5201 [==============================] - 4s 861us/step - loss: 131.4157 - val_loss: 153.2060\n",
            "Epoch 227/300\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 131.3602 - val_loss: 148.3936\n",
            "Epoch 228/300\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 131.7352 - val_loss: 148.3547\n",
            "Epoch 229/300\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 131.8374 - val_loss: 146.6718\n",
            "Epoch 230/300\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 131.5612 - val_loss: 153.9892\n",
            "Epoch 231/300\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 131.6584 - val_loss: 162.3869\n",
            "Epoch 232/300\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 131.4366 - val_loss: 146.4536\n",
            "Epoch 233/300\n",
            "5201/5201 [==============================] - 5s 876us/step - loss: 131.5048 - val_loss: 157.7244\n",
            "Epoch 234/300\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 131.5264 - val_loss: 146.5287\n",
            "Epoch 235/300\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 131.6456 - val_loss: 147.5804\n",
            "Epoch 236/300\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 131.3913 - val_loss: 146.1503\n",
            "Epoch 237/300\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 130.7506 - val_loss: 144.6355\n",
            "Epoch 238/300\n",
            "5201/5201 [==============================] - 5s 878us/step - loss: 131.4816 - val_loss: 146.1450\n",
            "Epoch 239/300\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 131.4556 - val_loss: 148.6452\n",
            "Epoch 240/300\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 131.1487 - val_loss: 145.9661\n",
            "Epoch 241/300\n",
            "5201/5201 [==============================] - 5s 877us/step - loss: 131.4411 - val_loss: 145.4702\n",
            "Epoch 242/300\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 131.3668 - val_loss: 161.3201\n",
            "Epoch 243/300\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 131.0238 - val_loss: 146.5729\n",
            "Epoch 244/300\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 131.3689 - val_loss: 149.7147\n",
            "Epoch 245/300\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 131.6899 - val_loss: 146.8874\n",
            "Epoch 246/300\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 131.4201 - val_loss: 152.9607\n",
            "Epoch 247/300\n",
            "5201/5201 [==============================] - 4s 856us/step - loss: 131.5362 - val_loss: 148.8974\n",
            "Epoch 248/300\n",
            "5201/5201 [==============================] - 5s 876us/step - loss: 131.6110 - val_loss: 150.0103\n",
            "Epoch 249/300\n",
            "5201/5201 [==============================] - 5s 876us/step - loss: 130.8628 - val_loss: 149.3674\n",
            "Epoch 250/300\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 131.9017 - val_loss: 146.3574\n",
            "Epoch 251/300\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 131.4198 - val_loss: 147.1277\n",
            "Epoch 252/300\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 131.2885 - val_loss: 147.2937\n",
            "Epoch 253/300\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 131.4043 - val_loss: 157.0096\n",
            "Epoch 254/300\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 131.6009 - val_loss: 146.6122\n",
            "Epoch 255/300\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 131.4899 - val_loss: 151.6364\n",
            "Epoch 256/300\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 131.6062 - val_loss: 146.9080\n",
            "Epoch 257/300\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 131.1495 - val_loss: 160.2771\n",
            "Epoch 258/300\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 131.2917 - val_loss: 148.3585\n",
            "Epoch 259/300\n",
            "5201/5201 [==============================] - 5s 938us/step - loss: 131.3839 - val_loss: 148.2852\n",
            "Epoch 260/300\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 131.4660 - val_loss: 144.7118\n",
            "Epoch 261/300\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 131.1691 - val_loss: 145.6291\n",
            "Epoch 262/300\n",
            "5201/5201 [==============================] - 5s 935us/step - loss: 131.6938 - val_loss: 150.3121\n",
            "Epoch 263/300\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 131.2969 - val_loss: 151.0768\n",
            "Epoch 264/300\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 131.7163 - val_loss: 145.9188\n",
            "Epoch 265/300\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 131.1857 - val_loss: 148.7360\n",
            "Epoch 266/300\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 131.4403 - val_loss: 149.4682\n",
            "Epoch 267/300\n",
            "5201/5201 [==============================] - 5s 933us/step - loss: 131.5123 - val_loss: 146.2792\n",
            "Epoch 268/300\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 131.5919 - val_loss: 150.3685\n",
            "Epoch 269/300\n",
            "5201/5201 [==============================] - 5s 937us/step - loss: 131.3027 - val_loss: 162.8285\n",
            "Epoch 270/300\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 131.9445 - val_loss: 145.4557\n",
            "Epoch 271/300\n",
            "5201/5201 [==============================] - 5s 932us/step - loss: 131.3358 - val_loss: 145.5277\n",
            "Epoch 272/300\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 131.7934 - val_loss: 165.0764\n",
            "Epoch 273/300\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 131.4108 - val_loss: 149.5468\n",
            "Epoch 274/300\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 131.2089 - val_loss: 148.2431\n",
            "Epoch 275/300\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 131.3798 - val_loss: 145.8954\n",
            "Epoch 276/300\n",
            "5201/5201 [==============================] - 5s 935us/step - loss: 131.0990 - val_loss: 149.2647\n",
            "Epoch 277/300\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 131.2326 - val_loss: 144.7350\n",
            "Epoch 278/300\n",
            "5201/5201 [==============================] - 5s 935us/step - loss: 130.9962 - val_loss: 152.9173\n",
            "Epoch 279/300\n",
            "5201/5201 [==============================] - 5s 938us/step - loss: 131.5812 - val_loss: 146.3562\n",
            "Epoch 280/300\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 131.0654 - val_loss: 145.4420\n",
            "Epoch 281/300\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 131.4965 - val_loss: 144.5547\n",
            "Epoch 282/300\n",
            "5201/5201 [==============================] - 5s 944us/step - loss: 131.3166 - val_loss: 148.1512\n",
            "Epoch 283/300\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 131.4774 - val_loss: 147.3298\n",
            "Epoch 284/300\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 131.4329 - val_loss: 148.8400\n",
            "Epoch 285/300\n",
            "5201/5201 [==============================] - 5s 944us/step - loss: 131.0202 - val_loss: 146.4323\n",
            "Epoch 286/300\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 131.7097 - val_loss: 148.9001\n",
            "Epoch 287/300\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 131.6447 - val_loss: 149.6207\n",
            "Epoch 288/300\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 130.8636 - val_loss: 176.3835\n",
            "Epoch 289/300\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 130.9800 - val_loss: 150.1246\n",
            "Epoch 290/300\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 131.4175 - val_loss: 145.2717\n",
            "Epoch 291/300\n",
            "5201/5201 [==============================] - 5s 931us/step - loss: 131.1729 - val_loss: 148.3656\n",
            "Epoch 292/300\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 131.4424 - val_loss: 146.5773\n",
            "Epoch 293/300\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 131.2291 - val_loss: 147.3145\n",
            "Epoch 294/300\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 131.1181 - val_loss: 148.8349\n",
            "Epoch 295/300\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 131.2611 - val_loss: 147.0983\n",
            "Epoch 296/300\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 131.0677 - val_loss: 154.0027\n",
            "Epoch 297/300\n",
            "5201/5201 [==============================] - 5s 936us/step - loss: 131.3030 - val_loss: 149.8291\n",
            "Epoch 298/300\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 131.2811 - val_loss: 149.4957\n",
            "Epoch 299/300\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 131.3232 - val_loss: 147.6949\n",
            "Epoch 300/300\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 131.3645 - val_loss: 147.9767\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adadelta,SGD\n",
        "\n",
        "#parameter\n",
        "batch_size = 32\n",
        "epoch = 300\n",
        "learning_rate = 0.001\n",
        "\n",
        "model_SBP.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\n",
        "# model_SBP.summary()\n",
        "history = model_SBP.fit(X_train, sbp_train, batch_size = batch_size, epochs = epoch, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyVJ8H5NkLbu",
        "outputId": "e2ad1652-a914-4eff-98a0-bf48cf79ae7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  1.2690671866028076 \n",
            "MAE:  8.791128839298564 \n",
            "SD:  12.098189981772805\n"
          ]
        }
      ],
      "source": [
        "pred = model_SBP.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)\n",
        "\n",
        "\n",
        "# ME:  7.9854436717721455 \n",
        "# MAE:  12.17038882271132 \n",
        "# SD:  13.5069653065601"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "cEc8PHXpkLbu",
        "outputId": "4e42f55e-10c7-49e8-b56d-cff9e12d6164"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVwAAAFBCAYAAAA7XhdpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlxElEQVR4nO3de7yUZbn/8c+1FgsWLEAEEREQUDFESUAwjaIDbY8Vmtb2UIpbN+5Kd0q7xPqVtrODusvDTisKCreZmockj6ihaKmABJ6AQBRhiXJQkIPAOly/P65nnOG8gLXuYZbf9+s1r3nOcz3388x3nrln1ixzd0REpOmVFbsAEZEPCgWuiEgiClwRkUQUuCIiiShwRUQSUeCKiCTSZIFrZuPNbKmZvVgwraOZPWJm87L7vbPpZmY3mNl8M3vezAYVrHNOtvw8MzunqeoVEWlqTXmF+3vg+M2mjQEec/c+wGPZOMAJQJ/sNgr4JURAA5cDHwGOAi7PhbSISKlpssB19ynA25tNHgFMyIYnACcXTL/ZwzNABzPrChwHPOLub7v7O8AjbBniIiIlIXUfbhd3X5INvwl0yYa7AYsKllucTdvWdBGRktOiWA/s7m5mjfZ3xWY2iuiOoKqq6si+ffs21qZFRAB47rnnlrt7511dP3XgvmVmXd19SdZlsDSbXg30KFiuezatGvjkZtMf39qG3X0sMBZg8ODBPn369MatXEQ+8Mxs4e6sn7pLYSKQ+6bBOcC9BdPPzr6tcDSwKut6eBg41sz2zj4sOzabJiJScprsCtfM/khcne5jZouJbxv8FLjDzM4DFgJfyhZ/ADgRmA+sA84FcPe3zeyHwLRsuf92980/iBMRKQnWHH+eUV0KItIUzOw5dx+8q+sX7UMzEWk6NTU1LF68mPXr1xe7lJJUWVlJ9+7dqaioaNTtKnBFmqHFixfTrl07evXqhZkVu5yS4u6sWLGCxYsX07t370bdtn5LQaQZWr9+PZ06dVLY7gIzo1OnTk3y7kCBK9JMKWx3XVO1nQJXRCQRBa6IlLS2bdsWu4QGU+CKiCSiwBWRJvHaa6/Rt29fRo4cySGHHMJZZ53Fo48+ytChQ+nTpw9Tp07liSeeYMCAAQwYMICBAweyevVqAK655hqGDBnChz/8YS6//PIGPZ67861vfYvDDz+c/v37c/vttwOwZMkShg0bxoABAzj88MN58sknqaurY+TIke8ve+211zZZOxTS18JEmruLL4aZMxt3mwMGwHXX7XCx+fPn86c//Ynx48czZMgQbr31Vp566ikmTpzIj3/8Y+rq6rjxxhsZOnQoa9asobKykkmTJjFv3jymTp2Ku/P5z3+eKVOmMGzYsO0+1t13383MmTOZNWsWy5cvZ8iQIQwbNoxbb72V4447ju9+97vU1dWxbt06Zs6cSXV1NS++GP8fYeXKlbvfJg2gK1wRaTK9e/emf//+lJWVcdhhhzF8+HDMjP79+/Paa68xdOhQRo8ezQ033MDKlStp0aIFkyZNYtKkSQwcOJBBgwYxZ84c5s2bt8PHeuqppzjjjDMoLy+nS5cufOITn2DatGkMGTKE3/3ud1xxxRW88MILtGvXjgMPPJAFCxZw0UUX8dBDD9G+ffsEraErXJHmrwFXok2lVatW7w+XlZW9P15WVkZtbS1jxozhpJNO4oEHHmDo0KE8/PDDuDuXXXYZF1xwQaPUMGzYMKZMmcL999/PyJEjGT16NGeffTazZs3i4Ycf5le/+hV33HEH48ePb5TH2x5d4YpI0bzyyiv079+fSy+9lCFDhjBnzhyOO+44xo8fz5o1awCorq5m6dKlO9gSfPzjH+f222+nrq6OZcuWMWXKFI466igWLlxIly5d+Pd//3fOP/98ZsyYwfLly6mvr+fUU0/lyiuvZMaMGU29q4CucEWkiK677jomT578fpfDCSecQKtWrZg9ezbHHHMMEF/7uuWWW9h33323u61TTjmFp59+miOOOAIz4+qrr2a//fZjwoQJXHPNNVRUVNC2bVtuvvlmqqurOffcc6mvrwfgJz/5SZPvK+jXwkSapdmzZ3PooYcWu4yStrU23N1fC1OXgohIIupSEJE93ooVKxg+fPgW0x977DE6depUhIp2jQJXRPZ4nTp1YmZjf5e4CNSlINJMNcfPZ1JpqrZT4Io0Q5WVlaxYsUKhuwtyP0BeWVnZ6NtWl4JIM9S9e3cWL17MsmXLil1KScr9i53GpsAVaYYqKioa/d/DyO5Tl4KISCIKXBGRRBS4IiKJKHBFRBJR4IqIJKLAFRFJRIErIpKIAldEJBEFrohIIgpcEZFEFLgiIokocEVEElHgiogkosAVEUlEgSsikogCV0QkEQWuiEgiClwRkUQUuCIiiShwRUQSUeCKiCSiwBURSUSBKyKSiAJXRCQRBa6ISCJFCVwzu8TMXjKzF83sj2ZWaWa9zexZM5tvZrebWcts2VbZ+Pxsfq9i1CwisruSB66ZdQP+Exjs7ocD5cDpwFXAte5+MPAOcF62ynnAO9n0a7PlRERKTrG6FFoArc2sBdAGWAJ8Grgzmz8BODkbHpGNk80fbmaWrlQRkcaRPHDdvRr4H+B1ImhXAc8BK929NltsMdAtG+4GLMrWrc2W77T5ds1slJlNN7Ppy5Yta9qdEBHZBcXoUtibuGrtDewPVAHH7+523X2suw9298GdO3fe3c2JiDS6YnQpfAZ41d2XuXsNcDcwFOiQdTEAdAeqs+FqoAdANn8vYEXakkVEdl8xAvd14Ggza5P1xQ4HXgYmA6dly5wD3JsNT8zGyeb/1d09Yb0iIo2iGH24zxIffs0AXshqGAtcCow2s/lEH+24bJVxQKds+mhgTOqaRUQagzXHi8XBgwf79OnTi12GiDQzZvacuw/e1fX1l2YiIokocEVEElHgiogkosAVEUlEgSsikogCV0QkEQWuiEgiClwRkUQUuCIiiShwRUQSUeCKiCSiwBURSUSBKyKSiAJXRCQRBa6ISCIKXBGRRBS4IiKJKHBFRBJR4IqIJKLAFRFJRIErIpKIAldEJBEFrohIIgpcEZFEFLgiIokocEVEElHgiogkosAVEUlEgSsikogCV0QkEQWuiEgiClwRkUQUuCIiiShwRUQSUeCKiCSiwBURSUSBKyKSiAJXRCQRBa6ISCIKXBGRRBS4IiKJKHBFRBJR4IqIJKLAFRFJRIErIpJIUQLXzDqY2Z1mNsfMZpvZMWbW0cweMbN52f3e2bJmZjeY2Xwze97MBhWjZhGR3VWsK9zrgYfcvS9wBDAbGAM85u59gMeycYATgD7ZbRTwy/TliojsvuSBa2Z7AcOAcQDuvtHdVwIjgAnZYhOAk7PhEcDNHp4BOphZ16RFi4g0gmJc4fYGlgG/M7N/mNlvzawK6OLuS7Jl3gS6ZMPdgEUF6y/OpomIlJRiBG4LYBDwS3cfCKwl330AgLs74DuzUTMbZWbTzWz6smXLGq1YEZHGUozAXQwsdvdns/E7iQB+K9dVkN0vzeZXAz0K1u+eTduEu49198HuPrhz585NVryIyK5KHrju/iawyMw+lE0aDrwMTATOyaadA9ybDU8Ezs6+rXA0sKqg60FEpGS0KNLjXgT8wcxaAguAc4nwv8PMzgMWAl/Kln0AOBGYD6zLlhURKTlFCVx3nwkM3sqs4VtZ1oGvN3VNIiJNTX9pJiKSiAJXRCQRBa6ISCIKXBGRRBS4IiKJKHBFRBJR4IqIJKLAFRFJRIErIpKIAldEJBEFrohIIgpcEZFEFLgiIok0KHDNrMrMyrLhQ8zs82ZW0bSliYg0Lw29wp0CVJpZN2AS8BXg901VlIhIc9TQwDV3Xwd8AbjJ3b8IHNZ0ZYmIND8NDlwzOwY4C7g/m1beNCWJiDRPDQ3ci4HLgHvc/SUzOxCY3GRViYg0Qw36Fzvu/gTwBED24dlyd//PpixMRKS5aei3FG41s/ZmVgW8CLxsZt9q2tJERJqXhnYp9HP3d4GTgQeB3sQ3FUREpIEaGrgV2fduTwYmunsN4E1WlYhIM9TQwP018BpQBUwxs57Au01VlIhIc9TQD81uAG4omLTQzD7VNCWJiDRPDf3QbC8z+7mZTc9uPyOudkVEpIEa2qUwHlgNfCm7vQv8rqmKEhFpjhrUpQAc5O6nFoz/wMxmNkE9IiLNVkOvcN8zs4/lRsxsKPBe05QkItI8NfQK9z+Am81sr2z8HeCcpilJRKR5aui3FGYBR5hZ+2z8XTO7GHi+CWsTEWlWduo/Prj7u9lfnAGMboJ6RESard35FzvWaFWIiHwA7E7g6k97RUR2wnb7cM1sNVsPVgNaN0lFIiLN1HYD193bpSpERKS5079JFxFJRIErIpKIAldEJBEFrohIIgpcEZFEFLgiIokocEVEElHgiogkosAVEUlEgSsikkjRAtfMys3sH2Z2Xzbe28yeNbP5Zna7mbXMprfKxudn83sVq2YRkd1RzCvcbwCzC8avAq5194OJ/yhxXjb9POCdbPq12XIiIiWnKIFrZt2Bk4DfZuMGfBq4M1tkAnByNjwiGyebPzxbXkSkpBTrCvc64NtAfTbeCVjp7rXZ+GKgWzbcDVgEkM1flS0vIlJSkgeumX0WWOruzzXydkeZ2XQzm75s2bLG3LSISKMoxhXuUODzZvYacBvRlXA90MHMcr/P2x2ozoargR4A2fy9gBWbb9Tdx7r7YHcf3Llz56bdAxGRXZA8cN39Mnfv7u69gNOBv7r7WcBk4LRssXOAe7PhieT/Jftp2fL69z4iUnL2pO/hXgqMNrP5RB/tuGz6OKBTNn00MKZI9YmI7Jbt/oudpubujwOPZ8MLgKO2ssx64ItJCxMRaQJ70hWuiEizpsAVEUlEgSsikogCV0QkEQWuiEgiClwRkUQUuCIiiShwRUQSUeCKiCSiwBURSUSBKyKSiAJXRCQRBa6ISCIKXBGRRBS4IiKJKHBFRBJR4IqIJKLAFRFJRIErIpKIAldEJBEFrohIIgpcEZFEFLgiIokocEVEElHgiogkosAVEUlEgSsikogCV0QkEQWuiEgiClwRkUQUuCIiiShwRUQSUeCKiCSiwBURSUSBKyKSiAJXRCQRBa6ISCIKXBGRRBS4IiKJKHBFRBJR4IqIJKLAFRFJRIErIpKIAldEJBEFrohIIgpcEZFEkgeumfUws8lm9rKZvWRm38imdzSzR8xsXna/dzbdzOwGM5tvZs+b2aDUNYuINIZiXOHWAt90937A0cDXzawfMAZ4zN37AI9l4wAnAH2y2yjgl+lLFhHZfckD192XuPuMbHg1MBvoBowAJmSLTQBOzoZHADd7eAboYGZd01YtIrL7itqHa2a9gIHAs0AXd1+SzXoT6JINdwMWFay2OJu2+bZGmdl0M5u+bNmypitaRGQXFS1wzawtcBdwsbu/WzjP3R3wndmeu49198HuPrhz586NWKmISOMoSuCaWQURtn9w97uzyW/lugqy+6XZ9GqgR8Hq3bNpIiIlpRjfUjBgHDDb3X9eMGsicE42fA5wb8H0s7NvKxwNrCroehARKRktivCYQ4GvAC+Y2cxs2neAnwJ3mNl5wELgS9m8B4ATgfnAOuDcpNWKiDSS5IHr7k8Bto3Zw7eyvANfb9KiREQS0F+aicgH01/+Al/4AvhOfT6/WxS4IvLBNHEi3HMPvP12sodU4ErpeeABOPvsTaeNHw9PPlmcekrNb34DH/tYsasovldfjfsFC5I9pAJ3T7Z+PfziF7BxY7Er2bPcfjv83//B0uybg+7wjW/ANdcUt67G8sYbsGjRjpfbVQ8/DH/7G6xY0fB13KGurulq2llvvBE1zZoFp54az5WdlQvc3H0CH6zAnTsX3nln59ebMwdWr278enbkz3+Giy6Cu+/e4aJNbtw4+OtfG3eb8+bBz3++831oc+fG/csvx/2SJbBmTX58TzNjBvzznw1f/owzIkSaypw5cZ9rx5x167Ydqv/1X1BVtePQXbQITjll59+mr14dV9719TtedtYs6N4dJk2Cu+6K58ezz+7c49XVweuvx7ACtwnU1MDRR8M3v9mw5VetgpUrYe1aOPJI+M53YvrSpXDmmfmrK4ANG3YtyHdkxoy4v+eepr3i2ZGamgj+iy5q3A8Yrr8+jseOwih3JeMet1xQvPRS3OcCZMGCCI2m8pOfRL21tflpNTU7Xu8LX4DzzmvYY6xZA3//exz7wn2pr4+r+pUrI1y2dhz+93/ht7/d/vZra+OFDjZt99pa6NMHrrpqy3U2bowXxg0b4JJL8s+Frbn77rhQeOih7ddRXw9XXgmvvBLj48bBqFEwefL21wO4777Y/yefhBdeiGnPPLPj9QpVV+ePY8LAxd2b3e3II4+Mn7q56y73n/wkhp96Kp6u++3nXl/v71u1yv03v3GfMcO9rs79lVfc77nHvWdP9wED3O+9N9br0SPW++lPY/z66/Pb+OIX3bt2dV+3zrfroYfcjzvOfdmyTafX17t/5Svu//M/m04fPjwXMXF75ZXtb7+hli93HzTI/Re/aNjy06bla5g6devLvP66+x/+4D5+vPvXvuZeW7vp/Joa95tucn/rrdjGwIHuBx4Y27zuum0/dnW1+913x3I33xzr52r56ldjmZtuyk+bMWPH+/Pee+5r18ZwXZ37c8/lx7dm3bpos8LH/chH3L//ffe2bWO/C/ez8Di9/nqsU17uvnLljmt78MH84zzxhPvo0e733ef+xz/mz0OIc7bQmjXubdq477NP1LC5+nr3OXPc583Lb3/MmPz8p5+OaUOHur/zjvsPf+j+5psx77bbNj0PW7SIZbbmjDM2PTZz57rfcceWyz3+eCw3alQ8Bz/3uRj/5jd33EbDhsWyxx/vftBBMTxixKbLTJrk/swz295G7vHN3P/lX7acv3Zt7MukSZtMBqb7bmRT0cOxKW5HtmkTJ3rnzrGLL73kfsUV+RPmttvcFy2KxvzMZ/LTu3XLr5O77bVXfviKK9y7dInhz3wmnlh33pmf/+tfb3ng1qxx/9733C+80L2qKpb71rdi3v33u3/5y/mTtKLC/eGH3U84IcK5Y0f3/v3z27/yylivri6eiN//fjxpLrkk9uWhh+Lx/vjHWO8vf4kTq64uX8+NN7ofemhsr2PHWH7lyli3piaWnTHDfeLE2N+LLooXFHBv2dK9Xz/3f/zD/dVXIxDee8999er8k6BFi7gfPdr94ovdx42Lxx07Nv/E+NKXNm3j44+PULn++tiniy92f+QR9ylT3MvK3Fu1iuV69479zD3OJz4R2/7GN/Lb+tSnIlSWL499+OtfN32BXbzYvW9f9/33jxe4Pn1ivVNPdb/0Uvfp0yMA7rrLfeNG9+9+N8Iyt/2uXTetHWIbuReYs8+O5efMifFcUObOu1tuiXOv0NVXu++7b4TUf/xHvg0//vG432efqLnwMffd133Jkvw2br01P+/MM90nT3Z/9FH3hQtjHy65JOadfXbcl5XF9qurY38vvzy//oc+lG/LJ5+MF+eDD3Zv3z6/zC235Nu1vj7aedWq/IsoRHv26xfDp5wS5/l998U6o0Zt2Y7g3rp1hLS7+1VXuXfqFOfM449HjffdF+1jFs8ns3wbvfCC+9/+FgHfrp179+5xfrq7v/FGtMXEiZENI0fGeoMGxcVVdbX7a6/Fc+DBB90vuCDmt2sXz92nn3afOVOBu7XbkWb5k6Oiwv3DH44nWM+eWz/IP/95XD197GNxBTFuXIRh7sn4kY9s+qTr2HHT9fv3jyu2ykr3Qw5xP/bYuOq5+up8oFdWxtXtSSfFSdKvX2yzc+c4+fv2zQdy7kBDXL3V18eTo1evCL9u3TZ9/MLa9t8/tl9WtmkgDByYD+/+/ePJkDtRc0/wY491//Sn8+vlTurc+KRJ8Q5h86ud3HCbNvG4I0ZsWtunPhX706bNlm1/xBGbjptFW+VCIbf9447z94MmF9zl5e7nnx/jhx2W30Zl5ab7f+yx8QQ666yooaoqnowQx+G00zatNxequRfXwv35+9833e9OneL+4IPjyVu4L717549r+/b5cOjVK4Kvc+c4LhDvpgoDMxdc++0X67Vo4f7b38Z+/OUvsc0OHaJdDz44ltv8vNj8+BTejjkm38YVFVvOz7Vr7jZ2bLQj5M+JAw5w/9GP3D/72Rjfe++tPz/69o12z5073bvHPm3+4jV0aP4cuOiiLY9r7lZV5f71r+fHv/rVuBjY2r584QsR1Lm2z20f3IcMiTaFeFEvL9+09sGDIxMKztvdDVxz93T9F4kMPuQQn37ooTBsGLRvD9deG/ff+Q4sXgzl5fDee9CvH/TtCwcckF/ZHSz7Q7jly+MrSB/9aPTlrl0Lzz8Phx8O55wDI0fCXnvB+edHn+7PfhYf4Eyblu+Qb9ECrr46+r4g+nqvuw6eew46doSbbooPGNq0gWXL4oOA3r3hggtg//3jQ4FevaKOf/3XqO/YY2HIkOgr6907Pmx46qnYztVXw4knRn333AP77AP33ht9jW+9BT17xjZbtIDf/z76wbp0ice/8srY9x//GIYOhYMOiuXOPBMGDoz5y5dH/fvtF/X97W+x7jvvxD6++ioMHx59ee7wve/FNvv3j/a/7Ta4/34499zYztSpUceKFXDWWfHBTPfusf5tt0W/8erVcNJJsdxPfwqf+ET0/154YbQBwLe/DUccER+G3Hdf1HbccdEuP/tZ1GAGn/1sLNurV+xL167RVzpyJHz+89EnOG1atPG0afH1qUsugTvuiON/3nnwta/BRz4SX5y/8MLoO/7zn+Oxe/aMNhs7NtphypSo48IL4xgecEB8m2LVKvj0p+O8Of54GDMm+mifeSb2bepUeOQR+Nznoh333x96FPyG0xNPxDdY9t8/Hr+uDn7wg2irmpqYVlMT59fnPhcfeH7ve/ENhY9+NLb14IP558LkydE2P/pR1P3oo9E3Pm1anKtXXQWPPRZ19ewJTz8N8+fHem3bwle/Gu2xaFG01f33x3YWLIj2q6+PGi+7DN58M87bL385+p3POitqufRSmD0bRo+Ofuxjjok6LrggHuPb345l/u3fov5Bg6CyMups2TLOr4UL4xzs1CmO0bhx0Saf/GTsX5cu8Xy47LJY3x1OPx3+8Y+Y/+67cTwWLIgP//r0iXP7vvugTRvstNOec/fBu5pNzTNwBw/26dOnF6+Auro4Ybp3jxOjRSP9BXVtbdwqK2O8vh7KGvFzz9raqL1Vq8bb5gdRfX2EWFVVtGl5ef5FfE83d26EYcuWDVv+tdfiwqF9+xgvvGDZVbW18aHWAQdsf1v19fkX0kLvvRfncdu28aLzz3/CIYdARcXWt5O7fm3Ac8nMFLibKwxc97iAadly2+1dqL4+1ikvj2NmtuVxqK2NDK2vj4uedu14/7Eg1smdd3V1sX7unNj8fCxcp7E0xjkvIlva3cAtxq+FNblZs+IirVWr+JZNLoBat86/IOZC0CyCs74+gnT9+pjWsWOs27JlvGvbsCHemdTWxruOD30o3hmtXBmBW1MTj9OyZbyLf+ONCOXcN3vatInxNWviAsI91t24MbbZo0e8yzSLF4a6uqipvDz/QrF6dWy/Q4foucjt44YNsY3cPqxeHftXURHrt2gRt4qKTe9z86qqYr9Xr477iorYTseOsZ2ysnjctWvjHXDPnlFP7oXJLC4q1q7Nf5WzdevY59atY155ebzTO/jg/EVf7kWwZct41/bWW7FvnTrlj01NTexfWVmss6t297oi944YNj13OneO+t59N/a3vDyO8b77RlusXRvja9fGfrZvH8fMLNq6d+/o5Xr77Thv1q2Lc2vjxuhlqajY8lZbGz1WFRXRvq1bx5ue1q2jDdesifHa2mj7DRviYq9Dh3jcwnOnvDx/jM3iwrJHj6iztjbaP3efO79z505ZWTw3VqyIfcmdZ4XnWOF4eXn02OTap7w8trtxY7Rvly6bPgfr62O4vj6/n+vWxbRWrWI/9tkn2r5Tp9hHiBrXrInb3nvH/m/YkD92udoL931r08rKor716/Nv/nZXs7zC7dx5sJ977nRqauIgtG0bJ14ufHO3wqvZXGPnvtu9bFk+iJYujRO4XbtYrqoqupq6do2T86238k+idesiOPfdN7aTu/pdty5OrDZtopsJItByT6KFC2PZ8vJ8IJWVxTZqa6PO9u3j5Fq1KkJv/fo4IQqfBOXlsR33eLzc+rlbTU3cctNraiIMWreO9Vq1imktWkQI5Nppw4bY7332ia9OtmyZfxdQXx/71aZNLFNWFu393nux35WVsc1cd2ObNrH/GzfGraYmpvXoEd2qq1blj1EukN3z7zh21e6um3txyT3x6uvj2FdWxrF5773Yn6qqCKGqqjj3qqpi/2prY99yAdOiRbRlz57xUUBunblz47H23z9/vHK33FdHe/aMOtavj8fN3e+9d9xyAdi6df7CY9Wq/HHIvZDlLjZyx3HffeNc3Lhxyxfo3AtgTU3+xX3hwjiPO3TI17e1W269Xr2i+3/p0nyA547v0qUx3KpV/ljnLpLWrcufJ61axb526RLnS4cO8YK9dm1sZ8OGOJdzxyH34p87lwv3t3B4a9Ny9eSeX3Pn6gp3Cz17xmdHIiKNaXe76j44f2kmIlJkClwRkUQUuCIiiShwRUQSUeCKiCSiwBURSUSBKyKSiAJXRCQRBa6ISCIKXBGRRBS4IiKJKHBFRBJR4IqIJKLAFRFJRIErIpKIAldEJBEFrohIIgpcEZFEFLgiIokocEVEElHgiogkosAVEUlEgSsikogCV0QkEQWuiEgiClwRkUQUuCIiiShwRUQSUeCKiCSiwBURSaRkAtfMjjezuWY238zGFLseEZGdVRKBa2blwI3ACUA/4Awz61fcqkREdk5JBC5wFDDf3Re4+0bgNmBEkWsSEdkppRK43YBFBeOLs2kiIiWjRbELaCxmNgoYlY1uMLMXi1nPbtgHWF7sInZBqdYNpVt7qdYNpVv7h3Zn5VIJ3GqgR8F492za+9x9LDAWwMymu/vgdOU1nlKtvVTrhtKtvVTrhtKt3cym7876pdKlMA3oY2a9zawlcDowscg1iYjslJK4wnX3WjO7EHgYKAfGu/tLRS5LRGSnlETgArj7A8ADDVx8bFPW0sRKtfZSrRtKt/ZSrRtKt/bdqtvcvbEKERGR7SiVPlwRkZLX7AK3lP4E2MxeM7MXzGxm7tNPM+toZo+Y2bzsfu9i1wlgZuPNbGnh1+22VauFG7Jj8LyZDdrD6r7CzKqzdp9pZicWzLssq3uumR1XnKrfr6WHmU02s5fN7CUz+0Y2fY9u9+3Uvce3u5lVmtlUM5uV1f6DbHpvM3s2q/H27MN7zKxVNj4/m99ruw/g7s3mRnyg9gpwINASmAX0K3Zd26n3NWCfzaZdDYzJhscAVxW7zqyWYcAg4MUd1QqcCDwIGHA08OweVvcVwH9tZdl+2TnTCuidnUvlRay9KzAoG24H/DOrcY9u9+3Uvce3e9Z2bbPhCuDZrC3vAE7Ppv8K+Go2/DXgV9nw6cDt29t+c7vCbQ5/AjwCmJANTwBOLl4pee4+BXh7s8nbqnUEcLOHZ4AOZtY1SaGb2Ubd2zICuM3dN7j7q8B84pwqCndf4u4zsuHVwGziLyz36HbfTt3bsse0e9Z2a7LRiuzmwKeBO7Ppm7d57ljcCQw3M9vW9ptb4JbanwA7MMnMnsv+Ug6gi7svyYbfBLoUp7QG2VatpXAcLszedo8v6LbZY+vO3qoOJK64SqbdN6sbSqDdzazczGYCS4FHiCvule5emy1SWN/7tWfzVwGdtrXt5ha4peZj7j6I+BW0r5vZsMKZHu9TSuJrJKVUK/BL4CBgALAE+FlRq9kBM2sL3AVc7O7vFs7bk9t9K3WXRLu7e527DyD+ovUooG9jbbu5Be4O/wR4T+Lu1dn9UuAe4uC+lXsbmN0vLV6FO7StWvfo4+Dub2VPqnrgN+Tfvu5xdZtZBRFaf3D3u7PJe3y7b63uUmp3AHdfCUwGjiG6Z3J/t1BY3/u1Z/P3AlZsa5vNLXBL5k+AzazKzNrlhoFjgReJes/JFjsHuLc4FTbItmqdCJydfWp+NLCq4C1w0W3Wr3kK0e4QdZ+effLcG+gDTE1dX07WFzgOmO3uPy+YtUe3+7bqLoV2N7POZtYhG24N/AvRBz0ZOC1bbPM2zx2L04C/Zu86tq4YnwQ28aeMJxKfir4CfLfY9WynzgOJT2ZnAS/laiX6fx4D5gGPAh2LXWtW1x+Jt4E1RB/Weduqlfik98bsGLwADN7D6v6/rK7nsydM14Llv5vVPRc4ocht/jGiu+B5YGZ2O3FPb/ft1L3HtzvwYeAfWY0vAt/Pph9IvAjMB/4EtMqmV2bj87P5B25v+/pLMxGRRJpbl4KIyB5LgSsikogCV0QkEQWuiEgiClwRkUQUuCI7YGafNLP7il2HlD4FrohIIgpcaTbM7MvZb5nONLNfZz9CssbMrs1+2/QxM+ucLTvAzJ7JfkjlnoLflD3YzB7Nfg91hpkdlG2+rZndaWZzzOwP2/tFKJFtUeBKs2BmhwL/Cgz1+OGROuAsoAqY7u6HAU8Al2er3Axc6u4fJv76KTf9D8CN7n4E8FHir9QgfvHqYuK3Ww8EhjbxLkkzVDL/RFJkB4YDRwLTsovP1sSPutQDt2fL3ALcbWZ7AR3c/Yls+gTgT9lvW3Rz93sA3H09QLa9qe6+OBufCfQCnmryvZJmRYErzYUBE9z9sk0mmn1vs+V29W/ZNxQM16HnjuwCdSlIc/EYcJqZ7Qvv/9+vnsQ5nvuVpzOBp9x9FfCOmX08m/4V4AmP/06w2MxOzrbRyszapNwJad70Ki3Ngru/bGb/j/gPGmXEr4N9HVgLHJXNW0r080L8pN6vskBdAJybTf8K8Gsz++9sG19MuBvSzOnXwqRZM7M17t622HWIgLoURESS0RWuiEgiusIVEUlEgSsikogCV0QkEQWuiEgiClwRkUQUuCIiifx/3UejR6NPP00AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epoch, 0, 1000])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRNa8JmUkLbu"
      },
      "source": [
        "## 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3qs_PmIkLbv",
        "outputId": "b049a677-675a-4f0a-90b0-6a3e469e25e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_24 (Dense)             (None, 64)                2368      \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 15,937\n",
            "Trainable params: 15,425\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))  \n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        "    \n",
        "model_SBP2 = build_model()\n",
        "model_SBP2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti50wmgYkLbv",
        "outputId": "0833d2ff-7672-4a35-8dbb-3590a44eb966",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "10401/10401 [==============================] - 9s 889us/step - loss: 144.5503 - val_loss: 153.3499\n",
            "Epoch 2/200\n",
            "10401/10401 [==============================] - 9s 866us/step - loss: 144.7708 - val_loss: 148.2712\n",
            "Epoch 3/200\n",
            "10401/10401 [==============================] - 9s 871us/step - loss: 143.7626 - val_loss: 147.5875\n",
            "Epoch 4/200\n",
            "10401/10401 [==============================] - 9s 845us/step - loss: 144.4267 - val_loss: 157.9368\n",
            "Epoch 5/200\n",
            "10401/10401 [==============================] - 9s 847us/step - loss: 144.5799 - val_loss: 152.2765\n",
            "Epoch 6/200\n",
            "10401/10401 [==============================] - 9s 873us/step - loss: 144.3503 - val_loss: 145.8047\n",
            "Epoch 7/200\n",
            "10401/10401 [==============================] - 9s 847us/step - loss: 143.9450 - val_loss: 154.1359\n",
            "Epoch 8/200\n",
            "10401/10401 [==============================] - 9s 899us/step - loss: 143.7529 - val_loss: 150.8286\n",
            "Epoch 9/200\n",
            "10401/10401 [==============================] - 9s 869us/step - loss: 143.9458 - val_loss: 148.8720\n",
            "Epoch 10/200\n",
            "10401/10401 [==============================] - 9s 865us/step - loss: 143.9230 - val_loss: 162.0853\n",
            "Epoch 11/200\n",
            "10401/10401 [==============================] - 9s 852us/step - loss: 143.9685 - val_loss: 145.9364\n",
            "Epoch 12/200\n",
            "10401/10401 [==============================] - 9s 860us/step - loss: 144.1649 - val_loss: 148.8042\n",
            "Epoch 13/200\n",
            "10401/10401 [==============================] - 9s 878us/step - loss: 144.1102 - val_loss: 161.0737\n",
            "Epoch 14/200\n",
            "10401/10401 [==============================] - 9s 860us/step - loss: 144.2964 - val_loss: 147.1841\n",
            "Epoch 15/200\n",
            "10401/10401 [==============================] - 9s 855us/step - loss: 143.8368 - val_loss: 146.0103\n",
            "Epoch 16/200\n",
            "10401/10401 [==============================] - 9s 851us/step - loss: 144.4064 - val_loss: 147.7160\n",
            "Epoch 17/200\n",
            "10401/10401 [==============================] - 9s 855us/step - loss: 143.6064 - val_loss: 152.7752\n",
            "Epoch 18/200\n",
            "10401/10401 [==============================] - 9s 888us/step - loss: 144.3582 - val_loss: 155.5031\n",
            "Epoch 19/200\n",
            "10401/10401 [==============================] - 9s 851us/step - loss: 144.0455 - val_loss: 150.5926\n",
            "Epoch 20/200\n",
            "10401/10401 [==============================] - 9s 864us/step - loss: 143.9603 - val_loss: 146.3266\n",
            "Epoch 21/200\n",
            "10401/10401 [==============================] - 9s 868us/step - loss: 144.3556 - val_loss: 147.4246\n",
            "Epoch 22/200\n",
            "10401/10401 [==============================] - 9s 869us/step - loss: 144.1743 - val_loss: 147.5302\n",
            "Epoch 23/200\n",
            "10401/10401 [==============================] - 9s 863us/step - loss: 143.5125 - val_loss: 151.7930\n",
            "Epoch 24/200\n",
            "10401/10401 [==============================] - 9s 861us/step - loss: 143.6694 - val_loss: 154.5929\n",
            "Epoch 25/200\n",
            "10401/10401 [==============================] - 9s 849us/step - loss: 143.6478 - val_loss: 146.5533\n",
            "Epoch 26/200\n",
            "10401/10401 [==============================] - 9s 859us/step - loss: 143.7970 - val_loss: 157.2424\n",
            "Epoch 27/200\n",
            "10401/10401 [==============================] - 9s 872us/step - loss: 143.4604 - val_loss: 148.2066\n",
            "Epoch 28/200\n",
            "10401/10401 [==============================] - 9s 861us/step - loss: 143.8361 - val_loss: 150.4171\n",
            "Epoch 29/200\n",
            "10401/10401 [==============================] - 9s 859us/step - loss: 143.5815 - val_loss: 148.7449\n",
            "Epoch 30/200\n",
            "10401/10401 [==============================] - 9s 868us/step - loss: 143.2960 - val_loss: 146.5280\n",
            "Epoch 31/200\n",
            "10401/10401 [==============================] - 9s 852us/step - loss: 143.9278 - val_loss: 147.1669\n",
            "Epoch 32/200\n",
            "10401/10401 [==============================] - 9s 859us/step - loss: 143.9512 - val_loss: 148.2871\n",
            "Epoch 33/200\n",
            "10401/10401 [==============================] - 9s 862us/step - loss: 144.1564 - val_loss: 152.5962\n",
            "Epoch 34/200\n",
            "10401/10401 [==============================] - 9s 855us/step - loss: 143.7797 - val_loss: 146.6718\n",
            "Epoch 35/200\n",
            "10401/10401 [==============================] - 9s 871us/step - loss: 143.6759 - val_loss: 151.4709\n",
            "Epoch 36/200\n",
            "10401/10401 [==============================] - 9s 860us/step - loss: 143.5780 - val_loss: 145.8815\n",
            "Epoch 37/200\n",
            "10401/10401 [==============================] - 9s 863us/step - loss: 144.1060 - val_loss: 148.2577\n",
            "Epoch 38/200\n",
            "10401/10401 [==============================] - 9s 855us/step - loss: 143.9918 - val_loss: 153.1079\n",
            "Epoch 39/200\n",
            "10401/10401 [==============================] - 9s 870us/step - loss: 143.9196 - val_loss: 147.5652\n",
            "Epoch 40/200\n",
            "10401/10401 [==============================] - 9s 878us/step - loss: 143.8893 - val_loss: 145.7522\n",
            "Epoch 41/200\n",
            "10401/10401 [==============================] - 9s 860us/step - loss: 143.9786 - val_loss: 162.2031\n",
            "Epoch 42/200\n",
            "10401/10401 [==============================] - 9s 880us/step - loss: 143.2745 - val_loss: 147.8110\n",
            "Epoch 43/200\n",
            "10401/10401 [==============================] - 9s 865us/step - loss: 143.3940 - val_loss: 156.4103\n",
            "Epoch 44/200\n",
            "10401/10401 [==============================] - 9s 868us/step - loss: 143.2460 - val_loss: 144.9780\n",
            "Epoch 45/200\n",
            "10401/10401 [==============================] - 9s 848us/step - loss: 143.2072 - val_loss: 148.3050\n",
            "Epoch 46/200\n",
            "10401/10401 [==============================] - 9s 862us/step - loss: 143.3839 - val_loss: 145.4700\n",
            "Epoch 47/200\n",
            "10401/10401 [==============================] - 9s 863us/step - loss: 143.0845 - val_loss: 145.8991\n",
            "Epoch 48/200\n",
            "10401/10401 [==============================] - 9s 855us/step - loss: 143.2260 - val_loss: 152.3075\n",
            "Epoch 49/200\n",
            "10401/10401 [==============================] - 9s 883us/step - loss: 143.2339 - val_loss: 144.6104\n",
            "Epoch 50/200\n",
            "10401/10401 [==============================] - 9s 863us/step - loss: 143.4033 - val_loss: 186.8655\n",
            "Epoch 51/200\n",
            "10401/10401 [==============================] - 9s 854us/step - loss: 143.6630 - val_loss: 151.2262\n",
            "Epoch 52/200\n",
            "10401/10401 [==============================] - 9s 862us/step - loss: 143.6153 - val_loss: 145.1757\n",
            "Epoch 53/200\n",
            "10401/10401 [==============================] - 9s 874us/step - loss: 143.0784 - val_loss: 146.2151\n",
            "Epoch 54/200\n",
            "10401/10401 [==============================] - 9s 864us/step - loss: 144.1649 - val_loss: 150.3890\n",
            "Epoch 55/200\n",
            "10401/10401 [==============================] - 9s 851us/step - loss: 143.5721 - val_loss: 146.2808\n",
            "Epoch 56/200\n",
            "10401/10401 [==============================] - 9s 859us/step - loss: 143.6765 - val_loss: 148.3185\n",
            "Epoch 57/200\n",
            "10401/10401 [==============================] - 9s 860us/step - loss: 143.2978 - val_loss: 147.1701\n",
            "Epoch 58/200\n",
            "10401/10401 [==============================] - 9s 864us/step - loss: 142.9434 - val_loss: 147.6924\n",
            "Epoch 59/200\n",
            "10401/10401 [==============================] - 9s 858us/step - loss: 143.1674 - val_loss: 147.3672\n",
            "Epoch 60/200\n",
            "10401/10401 [==============================] - 9s 847us/step - loss: 143.8574 - val_loss: 152.3534\n",
            "Epoch 61/200\n",
            "10401/10401 [==============================] - 9s 882us/step - loss: 144.2042 - val_loss: 145.0783\n",
            "Epoch 62/200\n",
            "10401/10401 [==============================] - 9s 866us/step - loss: 143.6184 - val_loss: 145.1230\n",
            "Epoch 63/200\n",
            "10401/10401 [==============================] - 9s 875us/step - loss: 143.0561 - val_loss: 145.1217\n",
            "Epoch 64/200\n",
            "10401/10401 [==============================] - 9s 884us/step - loss: 143.1517 - val_loss: 144.9430\n",
            "Epoch 65/200\n",
            "10401/10401 [==============================] - 9s 849us/step - loss: 143.6982 - val_loss: 149.8777\n",
            "Epoch 66/200\n",
            "10401/10401 [==============================] - 9s 839us/step - loss: 143.4753 - val_loss: 152.2574\n",
            "Epoch 67/200\n",
            "10401/10401 [==============================] - 9s 860us/step - loss: 143.2132 - val_loss: 146.9821\n",
            "Epoch 68/200\n",
            "10401/10401 [==============================] - 9s 871us/step - loss: 143.2495 - val_loss: 146.6239\n",
            "Epoch 69/200\n",
            "10401/10401 [==============================] - 9s 863us/step - loss: 143.7517 - val_loss: 147.8242\n",
            "Epoch 70/200\n",
            "10401/10401 [==============================] - 9s 877us/step - loss: 143.4331 - val_loss: 164.4846\n",
            "Epoch 71/200\n",
            "10401/10401 [==============================] - 9s 856us/step - loss: 144.2892 - val_loss: 145.3242\n",
            "Epoch 72/200\n",
            "10401/10401 [==============================] - 9s 870us/step - loss: 143.2771 - val_loss: 148.4519\n",
            "Epoch 73/200\n",
            "10401/10401 [==============================] - 9s 874us/step - loss: 142.8552 - val_loss: 146.6979\n",
            "Epoch 74/200\n",
            "10401/10401 [==============================] - 9s 874us/step - loss: 143.3508 - val_loss: 151.9148\n",
            "Epoch 75/200\n",
            "10401/10401 [==============================] - 9s 844us/step - loss: 143.7237 - val_loss: 149.6254\n",
            "Epoch 76/200\n",
            "10401/10401 [==============================] - 9s 852us/step - loss: 143.3213 - val_loss: 157.4113\n",
            "Epoch 77/200\n",
            "10401/10401 [==============================] - 9s 872us/step - loss: 142.9259 - val_loss: 146.6263\n",
            "Epoch 78/200\n",
            "10401/10401 [==============================] - 9s 862us/step - loss: 143.3691 - val_loss: 154.1416\n",
            "Epoch 79/200\n",
            "10401/10401 [==============================] - 9s 862us/step - loss: 143.2910 - val_loss: 144.8645\n",
            "Epoch 80/200\n",
            "10401/10401 [==============================] - 9s 863us/step - loss: 143.2672 - val_loss: 145.5058\n",
            "Epoch 81/200\n",
            "10401/10401 [==============================] - 9s 867us/step - loss: 143.2454 - val_loss: 156.7048\n",
            "Epoch 82/200\n",
            "10401/10401 [==============================] - 9s 842us/step - loss: 143.7572 - val_loss: 152.8198\n",
            "Epoch 83/200\n",
            "10401/10401 [==============================] - 9s 874us/step - loss: 143.8232 - val_loss: 145.7664\n",
            "Epoch 84/200\n",
            "10401/10401 [==============================] - 9s 853us/step - loss: 143.4932 - val_loss: 153.2279\n",
            "Epoch 85/200\n",
            "10401/10401 [==============================] - 9s 853us/step - loss: 143.5537 - val_loss: 146.8434\n",
            "Epoch 86/200\n",
            "10401/10401 [==============================] - 9s 868us/step - loss: 143.3253 - val_loss: 145.0349\n",
            "Epoch 87/200\n",
            "10401/10401 [==============================] - 9s 854us/step - loss: 142.9202 - val_loss: 148.3627\n",
            "Epoch 88/200\n",
            "10401/10401 [==============================] - 9s 855us/step - loss: 143.0220 - val_loss: 145.8338\n",
            "Epoch 89/200\n",
            "10401/10401 [==============================] - 9s 843us/step - loss: 143.2609 - val_loss: 145.2077\n",
            "Epoch 90/200\n",
            "10401/10401 [==============================] - 9s 853us/step - loss: 143.5999 - val_loss: 148.9557\n",
            "Epoch 91/200\n",
            "10401/10401 [==============================] - 9s 879us/step - loss: 142.5640 - val_loss: 144.4427\n",
            "Epoch 92/200\n",
            "10401/10401 [==============================] - 9s 866us/step - loss: 144.0371 - val_loss: 156.5201\n",
            "Epoch 93/200\n",
            "10401/10401 [==============================] - 9s 862us/step - loss: 143.1880 - val_loss: 146.7053\n",
            "Epoch 94/200\n",
            "10401/10401 [==============================] - 9s 860us/step - loss: 143.5337 - val_loss: 146.6608\n",
            "Epoch 95/200\n",
            "10401/10401 [==============================] - 9s 849us/step - loss: 143.2038 - val_loss: 145.7365\n",
            "Epoch 96/200\n",
            "10401/10401 [==============================] - 9s 871us/step - loss: 143.5709 - val_loss: 147.3270\n",
            "Epoch 97/200\n",
            "10401/10401 [==============================] - 9s 852us/step - loss: 142.6825 - val_loss: 145.3845\n",
            "Epoch 98/200\n",
            "10401/10401 [==============================] - 9s 870us/step - loss: 143.4553 - val_loss: 146.6292\n",
            "Epoch 99/200\n",
            "10401/10401 [==============================] - 9s 850us/step - loss: 143.2466 - val_loss: 144.8974\n",
            "Epoch 100/200\n",
            "10401/10401 [==============================] - 9s 861us/step - loss: 142.9745 - val_loss: 160.1351\n",
            "Epoch 101/200\n",
            "10401/10401 [==============================] - 9s 856us/step - loss: 143.2290 - val_loss: 147.2183\n",
            "Epoch 102/200\n",
            "10401/10401 [==============================] - 9s 853us/step - loss: 143.6426 - val_loss: 145.3004\n",
            "Epoch 103/200\n",
            "10401/10401 [==============================] - 9s 856us/step - loss: 143.1709 - val_loss: 149.0396\n",
            "Epoch 104/200\n",
            "10401/10401 [==============================] - 9s 883us/step - loss: 143.1927 - val_loss: 148.8035\n",
            "Epoch 105/200\n",
            "10401/10401 [==============================] - 9s 863us/step - loss: 143.5049 - val_loss: 147.4218\n",
            "Epoch 106/200\n",
            "10401/10401 [==============================] - 9s 864us/step - loss: 143.3292 - val_loss: 143.2093\n",
            "Epoch 107/200\n",
            "10401/10401 [==============================] - 9s 863us/step - loss: 143.0795 - val_loss: 146.8545\n",
            "Epoch 108/200\n",
            "10401/10401 [==============================] - 9s 845us/step - loss: 143.4757 - val_loss: 145.2151\n",
            "Epoch 109/200\n",
            "10401/10401 [==============================] - 9s 857us/step - loss: 142.9136 - val_loss: 144.3624\n",
            "Epoch 110/200\n",
            "10401/10401 [==============================] - 9s 863us/step - loss: 142.9864 - val_loss: 145.0142\n",
            "Epoch 111/200\n",
            "10401/10401 [==============================] - 9s 860us/step - loss: 143.0624 - val_loss: 145.6908\n",
            "Epoch 112/200\n",
            "10401/10401 [==============================] - 9s 849us/step - loss: 143.5312 - val_loss: 153.3325\n",
            "Epoch 113/200\n",
            "10401/10401 [==============================] - 9s 859us/step - loss: 142.9452 - val_loss: 155.6813\n",
            "Epoch 114/200\n",
            "10401/10401 [==============================] - 9s 869us/step - loss: 142.9249 - val_loss: 144.5567\n",
            "Epoch 115/200\n",
            "10401/10401 [==============================] - 9s 874us/step - loss: 143.3649 - val_loss: 144.9632\n",
            "Epoch 116/200\n",
            "10401/10401 [==============================] - 9s 864us/step - loss: 143.1512 - val_loss: 148.2613\n",
            "Epoch 117/200\n",
            "10401/10401 [==============================] - 9s 855us/step - loss: 142.8186 - val_loss: 145.3728\n",
            "Epoch 118/200\n",
            "10401/10401 [==============================] - 9s 864us/step - loss: 143.1593 - val_loss: 147.1811\n",
            "Epoch 119/200\n",
            "10401/10401 [==============================] - 9s 890us/step - loss: 143.2326 - val_loss: 145.1868\n",
            "Epoch 120/200\n",
            "10401/10401 [==============================] - 9s 873us/step - loss: 143.0902 - val_loss: 147.8010\n",
            "Epoch 121/200\n",
            "10401/10401 [==============================] - 9s 873us/step - loss: 143.4116 - val_loss: 145.2732\n",
            "Epoch 122/200\n",
            "10401/10401 [==============================] - 9s 864us/step - loss: 142.9483 - val_loss: 145.2190\n",
            "Epoch 123/200\n",
            "10401/10401 [==============================] - 9s 861us/step - loss: 143.2752 - val_loss: 147.2490\n",
            "Epoch 124/200\n",
            "10401/10401 [==============================] - 9s 840us/step - loss: 143.4240 - val_loss: 145.0616\n",
            "Epoch 125/200\n",
            "10401/10401 [==============================] - 9s 858us/step - loss: 143.0704 - val_loss: 148.7077\n",
            "Epoch 126/200\n",
            "10401/10401 [==============================] - 9s 866us/step - loss: 142.6060 - val_loss: 158.4179\n",
            "Epoch 127/200\n",
            "10401/10401 [==============================] - 9s 876us/step - loss: 143.6613 - val_loss: 164.4357\n",
            "Epoch 128/200\n",
            "10401/10401 [==============================] - 9s 851us/step - loss: 143.1617 - val_loss: 154.7227\n",
            "Epoch 129/200\n",
            "10401/10401 [==============================] - 9s 877us/step - loss: 143.7463 - val_loss: 145.9794\n",
            "Epoch 130/200\n",
            "10401/10401 [==============================] - 9s 877us/step - loss: 143.3768 - val_loss: 146.0744\n",
            "Epoch 131/200\n",
            "10401/10401 [==============================] - 9s 857us/step - loss: 143.5806 - val_loss: 146.0695\n",
            "Epoch 132/200\n",
            "10401/10401 [==============================] - 9s 880us/step - loss: 142.9065 - val_loss: 151.9789\n",
            "Epoch 133/200\n",
            "10401/10401 [==============================] - 9s 881us/step - loss: 143.0904 - val_loss: 149.0425\n",
            "Epoch 134/200\n",
            "10401/10401 [==============================] - 9s 872us/step - loss: 143.3071 - val_loss: 149.1960\n",
            "Epoch 135/200\n",
            "10401/10401 [==============================] - 9s 861us/step - loss: 142.7279 - val_loss: 149.4660\n",
            "Epoch 136/200\n",
            "10401/10401 [==============================] - 9s 851us/step - loss: 143.4111 - val_loss: 144.6550\n",
            "Epoch 137/200\n",
            "10401/10401 [==============================] - 9s 869us/step - loss: 143.3437 - val_loss: 146.1943\n",
            "Epoch 138/200\n",
            "10401/10401 [==============================] - 9s 858us/step - loss: 142.9178 - val_loss: 146.6369\n",
            "Epoch 139/200\n",
            "10401/10401 [==============================] - 9s 883us/step - loss: 143.0914 - val_loss: 164.9741\n",
            "Epoch 140/200\n",
            "10401/10401 [==============================] - 9s 869us/step - loss: 143.1947 - val_loss: 147.0897\n",
            "Epoch 141/200\n",
            "10401/10401 [==============================] - 9s 865us/step - loss: 143.1296 - val_loss: 152.6779\n",
            "Epoch 142/200\n",
            "10401/10401 [==============================] - 9s 885us/step - loss: 143.2365 - val_loss: 145.7191\n",
            "Epoch 143/200\n",
            "10401/10401 [==============================] - 9s 881us/step - loss: 143.4144 - val_loss: 148.7042\n",
            "Epoch 144/200\n",
            "10401/10401 [==============================] - 9s 881us/step - loss: 143.5029 - val_loss: 145.2092\n",
            "Epoch 145/200\n",
            "10401/10401 [==============================] - 9s 847us/step - loss: 142.9132 - val_loss: 144.2101\n",
            "Epoch 146/200\n",
            "10401/10401 [==============================] - 9s 879us/step - loss: 142.7460 - val_loss: 156.5339\n",
            "Epoch 147/200\n",
            "10401/10401 [==============================] - 9s 849us/step - loss: 143.0122 - val_loss: 147.4217\n",
            "Epoch 148/200\n",
            "10401/10401 [==============================] - 9s 863us/step - loss: 143.0718 - val_loss: 157.9959\n",
            "Epoch 149/200\n",
            "10401/10401 [==============================] - 9s 867us/step - loss: 142.9631 - val_loss: 161.8791\n",
            "Epoch 150/200\n",
            "10401/10401 [==============================] - 9s 860us/step - loss: 143.4191 - val_loss: 152.6363\n",
            "Epoch 151/200\n",
            "10401/10401 [==============================] - 9s 869us/step - loss: 142.8016 - val_loss: 148.1566\n",
            "Epoch 152/200\n",
            "10401/10401 [==============================] - 9s 883us/step - loss: 143.1614 - val_loss: 159.0383\n",
            "Epoch 153/200\n",
            "10401/10401 [==============================] - 9s 863us/step - loss: 143.0326 - val_loss: 144.5846\n",
            "Epoch 154/200\n",
            "10401/10401 [==============================] - 9s 862us/step - loss: 143.1387 - val_loss: 148.1239\n",
            "Epoch 155/200\n",
            "10401/10401 [==============================] - 9s 876us/step - loss: 142.4731 - val_loss: 145.8395\n",
            "Epoch 156/200\n",
            "10401/10401 [==============================] - 9s 865us/step - loss: 143.2241 - val_loss: 144.9375\n",
            "Epoch 157/200\n",
            "10401/10401 [==============================] - 9s 870us/step - loss: 143.0661 - val_loss: 152.0434\n",
            "Epoch 158/200\n",
            "10401/10401 [==============================] - 9s 872us/step - loss: 143.5911 - val_loss: 145.3009\n",
            "Epoch 159/200\n",
            "10401/10401 [==============================] - 9s 879us/step - loss: 142.3272 - val_loss: 158.5107\n",
            "Epoch 160/200\n",
            "10401/10401 [==============================] - 9s 888us/step - loss: 142.5479 - val_loss: 144.6282\n",
            "Epoch 161/200\n",
            "10401/10401 [==============================] - 9s 880us/step - loss: 143.2236 - val_loss: 146.5323\n",
            "Epoch 162/200\n",
            "10401/10401 [==============================] - 9s 851us/step - loss: 142.8814 - val_loss: 144.3136\n",
            "Epoch 163/200\n",
            "10401/10401 [==============================] - 9s 855us/step - loss: 143.0405 - val_loss: 150.7714\n",
            "Epoch 164/200\n",
            "10401/10401 [==============================] - 9s 880us/step - loss: 142.9201 - val_loss: 148.8793\n",
            "Epoch 165/200\n",
            "10401/10401 [==============================] - 9s 853us/step - loss: 143.3991 - val_loss: 144.6784\n",
            "Epoch 166/200\n",
            "10401/10401 [==============================] - 9s 871us/step - loss: 142.8533 - val_loss: 150.7029\n",
            "Epoch 167/200\n",
            "10401/10401 [==============================] - 9s 867us/step - loss: 142.8837 - val_loss: 160.8132\n",
            "Epoch 168/200\n",
            "10401/10401 [==============================] - 9s 875us/step - loss: 143.0289 - val_loss: 145.9679\n",
            "Epoch 169/200\n",
            "10401/10401 [==============================] - 9s 853us/step - loss: 142.8221 - val_loss: 143.6283\n",
            "Epoch 170/200\n",
            "10401/10401 [==============================] - 9s 860us/step - loss: 143.6399 - val_loss: 143.6233\n",
            "Epoch 171/200\n",
            "10401/10401 [==============================] - 9s 844us/step - loss: 142.9632 - val_loss: 149.2543\n",
            "Epoch 172/200\n",
            "10401/10401 [==============================] - 9s 865us/step - loss: 142.6669 - val_loss: 149.2360\n",
            "Epoch 173/200\n",
            "10401/10401 [==============================] - 9s 848us/step - loss: 142.8988 - val_loss: 147.6397\n",
            "Epoch 174/200\n",
            "10401/10401 [==============================] - 9s 870us/step - loss: 142.7769 - val_loss: 146.9773\n",
            "Epoch 175/200\n",
            "10401/10401 [==============================] - 9s 856us/step - loss: 143.0211 - val_loss: 144.7829\n",
            "Epoch 176/200\n",
            "10401/10401 [==============================] - 9s 864us/step - loss: 143.1740 - val_loss: 148.4397\n",
            "Epoch 177/200\n",
            "10401/10401 [==============================] - 9s 866us/step - loss: 142.7287 - val_loss: 146.6077\n",
            "Epoch 178/200\n",
            "10401/10401 [==============================] - 9s 859us/step - loss: 142.9229 - val_loss: 150.5833\n",
            "Epoch 179/200\n",
            "10401/10401 [==============================] - 9s 873us/step - loss: 142.8346 - val_loss: 151.1373\n",
            "Epoch 180/200\n",
            "10401/10401 [==============================] - 9s 846us/step - loss: 142.8616 - val_loss: 145.9903\n",
            "Epoch 181/200\n",
            "10401/10401 [==============================] - 9s 861us/step - loss: 143.1029 - val_loss: 144.1429\n",
            "Epoch 182/200\n",
            "10401/10401 [==============================] - 9s 860us/step - loss: 143.0925 - val_loss: 145.4592\n",
            "Epoch 183/200\n",
            "10401/10401 [==============================] - 9s 855us/step - loss: 143.0547 - val_loss: 147.0014\n",
            "Epoch 184/200\n",
            "10401/10401 [==============================] - 9s 858us/step - loss: 143.1790 - val_loss: 148.9875\n",
            "Epoch 185/200\n",
            "10401/10401 [==============================] - 9s 848us/step - loss: 143.0479 - val_loss: 157.9067\n",
            "Epoch 186/200\n",
            "10401/10401 [==============================] - 9s 873us/step - loss: 143.3202 - val_loss: 149.5573\n",
            "Epoch 187/200\n",
            "10401/10401 [==============================] - 9s 858us/step - loss: 142.7930 - val_loss: 147.0254\n",
            "Epoch 188/200\n",
            "10401/10401 [==============================] - 9s 872us/step - loss: 143.1154 - val_loss: 150.2774\n",
            "Epoch 189/200\n",
            "10401/10401 [==============================] - 9s 867us/step - loss: 142.1943 - val_loss: 151.2030\n",
            "Epoch 190/200\n",
            "10401/10401 [==============================] - 9s 853us/step - loss: 142.3462 - val_loss: 151.7574\n",
            "Epoch 191/200\n",
            "10401/10401 [==============================] - 9s 866us/step - loss: 142.7291 - val_loss: 149.3855\n",
            "Epoch 192/200\n",
            "10401/10401 [==============================] - 9s 853us/step - loss: 142.7386 - val_loss: 144.9326\n",
            "Epoch 193/200\n",
            "10401/10401 [==============================] - 9s 872us/step - loss: 143.1032 - val_loss: 144.8524\n",
            "Epoch 194/200\n",
            "10401/10401 [==============================] - 9s 881us/step - loss: 142.2366 - val_loss: 164.9925\n",
            "Epoch 195/200\n",
            "10401/10401 [==============================] - 9s 864us/step - loss: 142.7873 - val_loss: 144.6651\n",
            "Epoch 196/200\n",
            "10401/10401 [==============================] - 9s 866us/step - loss: 143.1748 - val_loss: 145.9220\n",
            "Epoch 197/200\n",
            "10401/10401 [==============================] - 9s 868us/step - loss: 143.2344 - val_loss: 143.9644\n",
            "Epoch 198/200\n",
            "10401/10401 [==============================] - 9s 868us/step - loss: 143.0757 - val_loss: 161.9176\n",
            "Epoch 199/200\n",
            "10401/10401 [==============================] - 9s 867us/step - loss: 142.7997 - val_loss: 149.0194\n",
            "Epoch 200/200\n",
            "10401/10401 [==============================] - 9s 856us/step - loss: 142.6898 - val_loss: 160.4493\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adadelta,SGD\n",
        "\n",
        "#parameter\n",
        "batch_size = 16\n",
        "epoch = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "model_SBP.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\n",
        "# model_SBP.summary()\n",
        "history = model_SBP.fit(X_train, sbp_train, batch_size = batch_size, epochs = epoch, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G-DfGYVkLbv",
        "outputId": "e6b1d4bf-5089-4811-805b-e748e98a9f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -3.487038146637009 \n",
            "MAE:  9.748998527800413 \n",
            "SD:  12.177435829595252\n"
          ]
        }
      ],
      "source": [
        "pred = model_SBP.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)\n",
        "\n",
        "\n",
        "\n",
        "# ME:  -3.487038146637009 \n",
        "# MAE:  9.748998527800413 \n",
        "# SD:  12.177435829595252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "HmmO7xsckLbv",
        "outputId": "f78ed7e7-babe-4981-974d-a2f4a588e7ac"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epoch, 0, 1000])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw6YLVgikLbv"
      },
      "outputs": [],
      "source": [
        "#sbp_err_idx_high = np.where((abs(err) >= 10))\n",
        "#np.savetxt(\"C:/Users/mina/Desktop/CSP/BP/BP/CART_pkl/New_CART_ppg_ir/Err_hist/set4_ppg_ir_without_ref_10_idx_high_sbp.txt\", sbp_err_idx_high, fmt = '%d', delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhYTeFl0kLbw",
        "outputId": "2a6a6af1-8add-4c98-e5ec-903c065e462e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  -2.5040195015384867 \n",
            "Ensemble_std:  13.847933178647143\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/5\n",
        "Ensemble_std = total_std/5\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZTXyjC7yxrJ"
      },
      "source": [
        "## 6 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDKRMVUsyxrJ",
        "outputId": "6ebfbe3b-7532-4f48-a9f4-3707c46aafd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_47 (Dense)             (None, 64)                2368      \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 42,433\n",
            "Trainable params: 41,153\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))  \n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        "    \n",
        "model_SBP2 = build_model()\n",
        "model_SBP2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta47HdHByxrK",
        "outputId": "848fc887-03ae-48a4-ea93-a25e7acfc23f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "   1/5201 [..............................] - ETA: 0s - loss: 102.8157WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
            "5177/5201 [============================>.] - ETA: 0s - loss: 129.7192WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
            "5201/5201 [==============================] - 5s 960us/step - loss: 129.7236 - val_loss: 150.4473\n",
            "Epoch 2/200\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 129.3401 - val_loss: 162.8097\n",
            "Epoch 3/200\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 129.3698 - val_loss: 156.2618\n",
            "Epoch 4/200\n",
            "5201/5201 [==============================] - 5s 932us/step - loss: 129.5646 - val_loss: 152.4632\n",
            "Epoch 5/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 129.4677 - val_loss: 144.4976\n",
            "Epoch 6/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 129.4471 - val_loss: 148.3072\n",
            "Epoch 7/200\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 129.4716 - val_loss: 145.0137\n",
            "Epoch 8/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 129.2377 - val_loss: 152.2659\n",
            "Epoch 9/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 129.1050 - val_loss: 146.3924\n",
            "Epoch 10/200\n",
            "5201/5201 [==============================] - 5s 883us/step - loss: 129.3450 - val_loss: 144.1058\n",
            "Epoch 11/200\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 129.3263 - val_loss: 142.8579\n",
            "Epoch 12/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 129.4622 - val_loss: 146.9344\n",
            "Epoch 13/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 129.5103 - val_loss: 146.6099\n",
            "Epoch 14/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 129.4954 - val_loss: 144.7114\n",
            "Epoch 15/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 129.4170 - val_loss: 144.7294\n",
            "Epoch 16/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 129.7377 - val_loss: 147.3354\n",
            "Epoch 17/200\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 129.4543 - val_loss: 144.4418\n",
            "Epoch 18/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 129.3630 - val_loss: 143.7430\n",
            "Epoch 19/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 129.6573 - val_loss: 147.8673\n",
            "Epoch 20/200\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 129.4411 - val_loss: 149.4856\n",
            "Epoch 21/200\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 129.5808 - val_loss: 146.2318\n",
            "Epoch 22/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 129.6970 - val_loss: 144.4267\n",
            "Epoch 23/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 129.2422 - val_loss: 144.0024\n",
            "Epoch 24/200\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 129.2640 - val_loss: 144.6179\n",
            "Epoch 25/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 129.6736 - val_loss: 145.2845\n",
            "Epoch 26/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 129.2731 - val_loss: 151.3700\n",
            "Epoch 27/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 129.0143 - val_loss: 155.8515\n",
            "Epoch 28/200\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 129.5113 - val_loss: 200.5194\n",
            "Epoch 29/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 129.0615 - val_loss: 155.5199\n",
            "Epoch 30/200\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 129.6578 - val_loss: 143.5012\n",
            "Epoch 31/200\n",
            "5201/5201 [==============================] - 5s 938us/step - loss: 129.2529 - val_loss: 147.5596\n",
            "Epoch 32/200\n",
            "5201/5201 [==============================] - 5s 944us/step - loss: 128.6908 - val_loss: 146.5733\n",
            "Epoch 33/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 129.4518 - val_loss: 143.3127\n",
            "Epoch 34/200\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 129.6383 - val_loss: 143.6279\n",
            "Epoch 35/200\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 129.3926 - val_loss: 142.9173\n",
            "Epoch 36/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 129.4451 - val_loss: 145.6193\n",
            "Epoch 37/200\n",
            "5201/5201 [==============================] - 5s 945us/step - loss: 129.0993 - val_loss: 143.1492\n",
            "Epoch 38/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 129.3476 - val_loss: 142.9666\n",
            "Epoch 39/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 128.9644 - val_loss: 147.1116\n",
            "Epoch 40/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 129.0675 - val_loss: 149.0179\n",
            "Epoch 41/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 129.3636 - val_loss: 145.0181\n",
            "Epoch 42/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 129.4629 - val_loss: 148.1998\n",
            "Epoch 43/200\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 129.2452 - val_loss: 146.6297\n",
            "Epoch 44/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 129.4030 - val_loss: 147.1839\n",
            "Epoch 45/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 129.6343 - val_loss: 143.5795\n",
            "Epoch 46/200\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 129.2111 - val_loss: 149.2991\n",
            "Epoch 47/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 129.6761 - val_loss: 155.8436 ETA: 0s - loss: 1 - ETA: 0s - loss: \n",
            "Epoch 48/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 129.2938 - val_loss: 145.0559\n",
            "Epoch 49/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 129.1638 - val_loss: 148.6969\n",
            "Epoch 50/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 128.7799 - val_loss: 151.4859\n",
            "Epoch 51/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 129.7870 - val_loss: 152.3654\n",
            "Epoch 52/200\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 129.5084 - val_loss: 146.4273\n",
            "Epoch 53/200\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 129.3199 - val_loss: 146.5241\n",
            "Epoch 54/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 129.0797 - val_loss: 143.4102\n",
            "Epoch 55/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 129.3048 - val_loss: 146.9097\n",
            "Epoch 56/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 129.6128 - val_loss: 146.1888\n",
            "Epoch 57/200\n",
            "5201/5201 [==============================] - 5s 951us/step - loss: 129.1454 - val_loss: 145.0025\n",
            "Epoch 58/200\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 129.1989 - val_loss: 147.2842\n",
            "Epoch 59/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 129.2831 - val_loss: 143.0325\n",
            "Epoch 60/200\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 128.8450 - val_loss: 145.1746\n",
            "Epoch 61/200\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 129.1645 - val_loss: 143.3525\n",
            "Epoch 62/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 128.7294 - val_loss: 159.6295\n",
            "Epoch 63/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 128.9072 - val_loss: 143.3918\n",
            "Epoch 64/200\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 129.3373 - val_loss: 149.1964\n",
            "Epoch 65/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 129.0191 - val_loss: 166.6659\n",
            "Epoch 66/200\n",
            "5201/5201 [==============================] - 5s 931us/step - loss: 129.7232 - val_loss: 142.1125\n",
            "Epoch 67/200\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 128.9977 - val_loss: 162.7711\n",
            "Epoch 68/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 129.4274 - val_loss: 144.3918\n",
            "Epoch 69/200\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 129.6179 - val_loss: 148.4090\n",
            "Epoch 70/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 129.1539 - val_loss: 144.9629\n",
            "Epoch 71/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 129.4930 - val_loss: 144.6148\n",
            "Epoch 72/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 129.0559 - val_loss: 146.3289\n",
            "Epoch 73/200\n",
            "5201/5201 [==============================] - 5s 935us/step - loss: 129.0783 - val_loss: 144.8554\n",
            "Epoch 74/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 129.5328 - val_loss: 151.3257\n",
            "Epoch 75/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 129.4266 - val_loss: 144.0050\n",
            "Epoch 76/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 129.2235 - val_loss: 147.7772129.1\n",
            "Epoch 77/200\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 129.1712 - val_loss: 147.1695\n",
            "Epoch 78/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 129.2281 - val_loss: 143.6222\n",
            "Epoch 79/200\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 129.3651 - val_loss: 143.8578\n",
            "Epoch 80/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 129.5908 - val_loss: 144.9933\n",
            "Epoch 81/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 129.5516 - val_loss: 143.3039\n",
            "Epoch 82/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 129.0273 - val_loss: 142.1174\n",
            "Epoch 83/200\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 129.2381 - val_loss: 146.0890\n",
            "Epoch 84/200\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 129.5856 - val_loss: 150.0502\n",
            "Epoch 85/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 129.0659 - val_loss: 144.4812\n",
            "Epoch 86/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 128.8945 - val_loss: 152.3302\n",
            "Epoch 87/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 129.6139 - val_loss: 143.8873\n",
            "Epoch 88/200\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 129.3484 - val_loss: 149.0266\n",
            "Epoch 89/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 129.7667 - val_loss: 143.4079\n",
            "Epoch 90/200\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 129.1320 - val_loss: 144.3290\n",
            "Epoch 91/200\n",
            "5201/5201 [==============================] - 5s 940us/step - loss: 129.3471 - val_loss: 145.9913\n",
            "Epoch 92/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 129.0611 - val_loss: 145.3682\n",
            "Epoch 93/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 129.1941 - val_loss: 148.4985\n",
            "Epoch 94/200\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 129.1144 - val_loss: 157.5038\n",
            "Epoch 95/200\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 129.5468 - val_loss: 144.2707\n",
            "Epoch 96/200\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 129.2901 - val_loss: 144.3239\n",
            "Epoch 97/200\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 128.8510 - val_loss: 143.9724\n",
            "Epoch 98/200\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 129.1420 - val_loss: 147.3366\n",
            "Epoch 99/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.9367 - val_loss: 145.6547\n",
            "Epoch 100/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 129.1656 - val_loss: 148.4600\n",
            "Epoch 101/200\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 128.9611 - val_loss: 144.8338\n",
            "Epoch 102/200\n",
            "5201/5201 [==============================] - 5s 877us/step - loss: 128.8449 - val_loss: 145.9911\n",
            "Epoch 103/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 129.3897 - val_loss: 144.5893\n",
            "Epoch 104/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 128.8174 - val_loss: 145.1325\n",
            "Epoch 105/200\n",
            "5201/5201 [==============================] - 5s 936us/step - loss: 129.0392 - val_loss: 142.4954\n",
            "Epoch 106/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 129.1784 - val_loss: 142.6095\n",
            "Epoch 107/200\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 129.2364 - val_loss: 144.3986\n",
            "Epoch 108/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 129.1616 - val_loss: 149.2180\n",
            "Epoch 109/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 129.1440 - val_loss: 146.1907\n",
            "Epoch 110/200\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 129.2795 - val_loss: 142.5973\n",
            "Epoch 111/200\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 129.0397 - val_loss: 149.9501\n",
            "Epoch 112/200\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 128.9037 - val_loss: 159.4868\n",
            "Epoch 113/200\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 129.3836 - val_loss: 145.0633\n",
            "Epoch 114/200\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 129.0574 - val_loss: 145.7104\n",
            "Epoch 115/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 129.2923 - val_loss: 146.7167\n",
            "Epoch 116/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 129.4136 - val_loss: 149.8656\n",
            "Epoch 117/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 129.1908 - val_loss: 142.7162\n",
            "Epoch 118/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 129.0661 - val_loss: 144.8830\n",
            "Epoch 119/200\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 129.0883 - val_loss: 144.8996\n",
            "Epoch 120/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 129.5710 - val_loss: 147.8334\n",
            "Epoch 121/200\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 128.7760 - val_loss: 144.3033\n",
            "Epoch 122/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 128.9104 - val_loss: 146.2286\n",
            "Epoch 123/200\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 128.9288 - val_loss: 147.8772\n",
            "Epoch 124/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 129.0372 - val_loss: 156.8164\n",
            "Epoch 125/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 128.8820 - val_loss: 146.8463\n",
            "Epoch 126/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 129.5459 - val_loss: 164.0278\n",
            "Epoch 127/200\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 129.3470 - val_loss: 150.1392\n",
            "Epoch 128/200\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 129.1053 - val_loss: 152.9228\n",
            "Epoch 129/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 129.2551 - val_loss: 144.6099\n",
            "Epoch 130/200\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 129.5397 - val_loss: 143.5766\n",
            "Epoch 131/200\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 128.7658 - val_loss: 146.4246\n",
            "Epoch 132/200\n",
            "5201/5201 [==============================] - 5s 873us/step - loss: 128.2579 - val_loss: 145.6631\n",
            "Epoch 133/200\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 128.9327 - val_loss: 153.3658\n",
            "Epoch 134/200\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 129.6815 - val_loss: 142.6201\n",
            "Epoch 135/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 129.3124 - val_loss: 154.6277\n",
            "Epoch 136/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.8899 - val_loss: 150.4178\n",
            "Epoch 137/200\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 128.8097 - val_loss: 157.9498\n",
            "Epoch 138/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 129.0546 - val_loss: 157.6414\n",
            "Epoch 139/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 128.6860 - val_loss: 145.0961\n",
            "Epoch 140/200\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 129.2247 - val_loss: 142.3438\n",
            "Epoch 141/200\n",
            "5201/5201 [==============================] - 5s 940us/step - loss: 128.4650 - val_loss: 148.4300\n",
            "Epoch 142/200\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 128.9995 - val_loss: 147.0819\n",
            "Epoch 143/200\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 129.0471 - val_loss: 144.3701\n",
            "Epoch 144/200\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 129.3519 - val_loss: 147.2876\n",
            "Epoch 145/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 129.2902 - val_loss: 144.6001\n",
            "Epoch 146/200\n",
            "5201/5201 [==============================] - 5s 874us/step - loss: 129.3542 - val_loss: 158.6005\n",
            "Epoch 147/200\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 128.8549 - val_loss: 153.6797\n",
            "Epoch 148/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 129.3613 - val_loss: 148.6546\n",
            "Epoch 149/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 129.2348 - val_loss: 155.7242\n",
            "Epoch 150/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 128.8432 - val_loss: 151.0118\n",
            "Epoch 151/200\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 128.7979 - val_loss: 147.5145\n",
            "Epoch 152/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 128.8524 - val_loss: 156.6778\n",
            "Epoch 153/200\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 128.9885 - val_loss: 144.8188\n",
            "Epoch 154/200\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 129.5169 - val_loss: 142.4426\n",
            "Epoch 155/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.9127 - val_loss: 148.3587\n",
            "Epoch 156/200\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 128.4517 - val_loss: 143.8583\n",
            "Epoch 157/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 129.0394 - val_loss: 152.2650\n",
            "Epoch 158/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 129.0504 - val_loss: 144.3489\n",
            "Epoch 159/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 128.8230 - val_loss: 145.0565\n",
            "Epoch 160/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 128.6831 - val_loss: 144.1648\n",
            "Epoch 161/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.7183 - val_loss: 147.5842\n",
            "Epoch 162/200\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 128.8956 - val_loss: 150.2022\n",
            "Epoch 163/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 128.9244 - val_loss: 142.6858\n",
            "Epoch 164/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 129.2640 - val_loss: 147.6592\n",
            "Epoch 165/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 129.1857 - val_loss: 146.7415\n",
            "Epoch 166/200\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 129.3454 - val_loss: 143.2086\n",
            "Epoch 167/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 128.8547 - val_loss: 144.2032\n",
            "Epoch 168/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 129.2308 - val_loss: 147.2089\n",
            "Epoch 169/200\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 128.7254 - val_loss: 144.5547\n",
            "Epoch 170/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 128.9861 - val_loss: 147.3087\n",
            "Epoch 171/200\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 128.7018 - val_loss: 144.5173\n",
            "Epoch 172/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 128.5634 - val_loss: 152.8536\n",
            "Epoch 173/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 129.0560 - val_loss: 144.9376\n",
            "Epoch 174/200\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 129.0061 - val_loss: 147.9951\n",
            "Epoch 175/200\n",
            "5201/5201 [==============================] - 5s 928us/step - loss: 129.1127 - val_loss: 144.5835\n",
            "Epoch 176/200\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 128.8894 - val_loss: 143.8866\n",
            "Epoch 177/200\n",
            "5201/5201 [==============================] - 5s 876us/step - loss: 129.1136 - val_loss: 142.7782\n",
            "Epoch 178/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 129.0677 - val_loss: 144.4081\n",
            "Epoch 179/200\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 128.8943 - val_loss: 152.8643\n",
            "Epoch 180/200\n",
            "5201/5201 [==============================] - 5s 878us/step - loss: 129.1057 - val_loss: 155.7505\n",
            "Epoch 181/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 128.9589 - val_loss: 145.4666\n",
            "Epoch 182/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 129.1602 - val_loss: 144.3538\n",
            "Epoch 183/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 129.1033 - val_loss: 143.5082\n",
            "Epoch 184/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 129.0143 - val_loss: 155.4663\n",
            "Epoch 185/200\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 128.9786 - val_loss: 146.1855\n",
            "Epoch 186/200\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 128.7995 - val_loss: 146.3514\n",
            "Epoch 187/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 128.8259 - val_loss: 144.1278\n",
            "Epoch 188/200\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 128.7522 - val_loss: 153.0600\n",
            "Epoch 189/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 129.0961 - val_loss: 145.8778\n",
            "Epoch 190/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.9587 - val_loss: 154.2487\n",
            "Epoch 191/200\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 129.0865 - val_loss: 149.1003\n",
            "Epoch 192/200\n",
            "5201/5201 [==============================] - 5s 871us/step - loss: 128.8038 - val_loss: 143.1746\n",
            "Epoch 193/200\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 128.8728 - val_loss: 150.0326\n",
            "Epoch 194/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 129.0566 - val_loss: 152.6331\n",
            "Epoch 195/200\n",
            "5201/5201 [==============================] - 5s 877us/step - loss: 128.7944 - val_loss: 143.1250\n",
            "Epoch 196/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 128.7200 - val_loss: 150.2408\n",
            "Epoch 197/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 128.8639 - val_loss: 145.4218\n",
            "Epoch 198/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 129.2315 - val_loss: 155.4608\n",
            "Epoch 199/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 129.0571 - val_loss: 150.09000s - loss: 128.91\n",
            "Epoch 200/200\n",
            "5201/5201 [==============================] - 5s 904us/step - loss: 128.6682 - val_loss: 143.5766\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adadelta,SGD\n",
        "\n",
        "#parameter\n",
        "batch_size = 32\n",
        "epoch = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "model_SBP.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\n",
        "# model_SBP.summary()\n",
        "history = model_SBP.fit(X_train, sbp_train, batch_size = batch_size, epochs = epoch, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpUfE4nzyxrK",
        "outputId": "c1a501e5-ca77-45c2-d302-9bf9f355a428"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  0.6731970073437484 \n",
            "MAE:  8.647903201434863 \n",
            "SD:  11.963422452912603\n"
          ]
        }
      ],
      "source": [
        "pred = model_SBP.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)\n",
        "\n",
        "\n",
        "# ME:  6.800765103145049 \n",
        "# MAE:  11.795270132307712 \n",
        "# SD:  14.233791200044193"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "9VY05_hxyxrK",
        "outputId": "a75a4ce0-8971-4387-9e4b-57c3c9eb5208"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epoch, 0, 1000])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owrNAszpyxrK"
      },
      "outputs": [],
      "source": [
        "#sbp_err_idx_high = np.where((abs(err) >= 10))\n",
        "#np.savetxt(\"C:/Users/mina/Desktop/CSP/BP/BP/CART_pkl/New_CART_ppg_ir/Err_hist/set4_ppg_ir_without_ref_10_idx_high_sbp.txt\", sbp_err_idx_high, fmt = '%d', delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r69JSu4ayxrK",
        "outputId": "db3aa44e-c67c-4097-e79c-b6a7a2d20c36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  -1.143866480909477 \n",
            "Ensemble_std:  16.69469141865598\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/5\n",
        "Ensemble_std = total_std/5\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkwY_N1oyxrK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqzhWYmlyxrL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWsTP-ewJlvi"
      },
      "source": [
        "## 7 (dense(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpata_bLJlvi"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZiOlZv-Jlvj",
        "outputId": "0e88c815-0c2f-4785-b96a-717aaf579ac2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_58 (Dense)             (None, 64)                2368      \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_54 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_55 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_57 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 64,513\n",
            "Trainable params: 62,593\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))  \n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        "    \n",
        "model_SBP2 = build_model()\n",
        "model_SBP2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-8ln5KAJlvj",
        "outputId": "f081d24c-7987-4d8d-f650-51df79ee09bb",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 128.6904 - val_loss: 143.5476\n",
            "Epoch 2/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 129.2542 - val_loss: 145.9788\n",
            "Epoch 3/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 128.9133 - val_loss: 159.4920\n",
            "Epoch 4/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 128.8289 - val_loss: 152.9500\n",
            "Epoch 5/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 128.6866 - val_loss: 145.2266\n",
            "Epoch 6/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 128.9008 - val_loss: 146.0276\n",
            "Epoch 7/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 129.1848 - val_loss: 148.2579\n",
            "Epoch 8/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 129.2074 - val_loss: 144.7117\n",
            "Epoch 9/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 128.9286 - val_loss: 150.2360\n",
            "Epoch 10/200\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 129.0114 - val_loss: 146.3426\n",
            "Epoch 11/200\n",
            "5201/5201 [==============================] - 4s 865us/step - loss: 128.8004 - val_loss: 156.6053\n",
            "Epoch 12/200\n",
            "5201/5201 [==============================] - 5s 868us/step - loss: 129.0257 - val_loss: 148.6206\n",
            "Epoch 13/200\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 129.0245 - val_loss: 154.6935\n",
            "Epoch 14/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 128.5962 - val_loss: 149.1429\n",
            "Epoch 15/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 128.8821 - val_loss: 148.1818\n",
            "Epoch 16/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 129.4380 - val_loss: 150.6872\n",
            "Epoch 17/200\n",
            "5201/5201 [==============================] - 5s 874us/step - loss: 129.0026 - val_loss: 148.6095\n",
            "Epoch 18/200\n",
            "5201/5201 [==============================] - 5s 877us/step - loss: 128.8201 - val_loss: 144.6176\n",
            "Epoch 19/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 128.9408 - val_loss: 153.6782\n",
            "Epoch 20/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 129.2261 - val_loss: 145.8225\n",
            "Epoch 21/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 128.7516 - val_loss: 155.4765\n",
            "Epoch 22/200\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 128.7170 - val_loss: 152.2160\n",
            "Epoch 23/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 128.7620 - val_loss: 166.6994\n",
            "Epoch 24/200\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 129.0813 - val_loss: 151.7321\n",
            "Epoch 25/200\n",
            "5201/5201 [==============================] - 5s 870us/step - loss: 128.9493 - val_loss: 147.3274\n",
            "Epoch 26/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 129.1913 - val_loss: 145.5241\n",
            "Epoch 27/200\n",
            "5201/5201 [==============================] - 5s 883us/step - loss: 128.6028 - val_loss: 145.8307\n",
            "Epoch 28/200\n",
            "5201/5201 [==============================] - 4s 864us/step - loss: 128.8582 - val_loss: 143.5223\n",
            "Epoch 29/200\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 128.9677 - val_loss: 153.3843\n",
            "Epoch 30/200\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 128.6566 - val_loss: 143.7121\n",
            "Epoch 31/200\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 128.8970 - val_loss: 149.2170\n",
            "Epoch 32/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 128.6978 - val_loss: 146.6035\n",
            "Epoch 33/200\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 129.2226 - val_loss: 156.7664\n",
            "Epoch 34/200\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 129.2690 - val_loss: 149.7961\n",
            "Epoch 35/200\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 129.4066 - val_loss: 149.9372\n",
            "Epoch 36/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 128.6326 - val_loss: 146.0855\n",
            "Epoch 37/200\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 129.2677 - val_loss: 144.1806\n",
            "Epoch 38/200\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 128.7878 - val_loss: 152.6059\n",
            "Epoch 39/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 128.6608 - val_loss: 142.7607\n",
            "Epoch 40/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 128.6733 - val_loss: 142.7960\n",
            "Epoch 41/200\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 129.1907 - val_loss: 144.7891\n",
            "Epoch 42/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 128.9294 - val_loss: 145.9297\n",
            "Epoch 43/200\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 128.6531 - val_loss: 143.7770\n",
            "Epoch 44/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.9331 - val_loss: 146.6003\n",
            "Epoch 45/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 128.8367 - val_loss: 146.0819\n",
            "Epoch 46/200\n",
            "5201/5201 [==============================] - 4s 864us/step - loss: 128.8347 - val_loss: 143.4456\n",
            "Epoch 47/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 128.9100 - val_loss: 146.3241\n",
            "Epoch 48/200\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 128.8787 - val_loss: 153.1171\n",
            "Epoch 49/200\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 128.6134 - val_loss: 152.4732\n",
            "Epoch 50/200\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 128.3940 - val_loss: 143.9908\n",
            "Epoch 51/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 129.0878 - val_loss: 142.0137\n",
            "Epoch 52/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 129.2176 - val_loss: 148.9029\n",
            "Epoch 53/200\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 128.6081 - val_loss: 166.7224\n",
            "Epoch 54/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 128.4971 - val_loss: 145.6471\n",
            "Epoch 55/200\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 128.6591 - val_loss: 143.1499\n",
            "Epoch 56/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 128.7259 - val_loss: 142.8302\n",
            "Epoch 57/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 128.8009 - val_loss: 147.3608\n",
            "Epoch 58/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 128.7533 - val_loss: 146.9280\n",
            "Epoch 59/200\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 128.8925 - val_loss: 145.6756\n",
            "Epoch 60/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 128.8791 - val_loss: 145.1641\n",
            "Epoch 61/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 129.0404 - val_loss: 145.2752\n",
            "Epoch 62/200\n",
            "5201/5201 [==============================] - 5s 904us/step - loss: 128.6601 - val_loss: 145.2496\n",
            "Epoch 63/200\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 128.6963 - val_loss: 143.5082\n",
            "Epoch 64/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 128.8497 - val_loss: 149.5471\n",
            "Epoch 65/200\n",
            "5201/5201 [==============================] - 4s 860us/step - loss: 128.8890 - val_loss: 144.7399\n",
            "Epoch 66/200\n",
            "5201/5201 [==============================] - 5s 878us/step - loss: 128.6911 - val_loss: 144.7869\n",
            "Epoch 67/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 128.5953 - val_loss: 160.9913\n",
            "Epoch 68/200\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 128.7847 - val_loss: 144.4713\n",
            "Epoch 69/200\n",
            "5201/5201 [==============================] - 5s 868us/step - loss: 128.7711 - val_loss: 145.5468\n",
            "Epoch 70/200\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 128.8750 - val_loss: 141.7483\n",
            "Epoch 71/200\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 128.4610 - val_loss: 149.9708\n",
            "Epoch 72/200\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 128.2320 - val_loss: 143.9189\n",
            "Epoch 73/200\n",
            "5201/5201 [==============================] - 5s 928us/step - loss: 128.9676 - val_loss: 144.1992\n",
            "Epoch 74/200\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 128.9060 - val_loss: 147.0488\n",
            "Epoch 75/200\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 128.7984 - val_loss: 152.4786\n",
            "Epoch 76/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 128.6706 - val_loss: 142.4436\n",
            "Epoch 77/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 128.9338 - val_loss: 142.5462\n",
            "Epoch 78/200\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 128.6008 - val_loss: 145.5375\n",
            "Epoch 79/200\n",
            "5201/5201 [==============================] - 5s 904us/step - loss: 128.9120 - val_loss: 146.5977\n",
            "Epoch 80/200\n",
            "5201/5201 [==============================] - 5s 877us/step - loss: 128.4882 - val_loss: 147.0269\n",
            "Epoch 81/200\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 128.8847 - val_loss: 144.8452\n",
            "Epoch 82/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.5838 - val_loss: 144.3337\n",
            "Epoch 83/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 129.0419 - val_loss: 149.5336\n",
            "Epoch 84/200\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 129.0920 - val_loss: 147.0362\n",
            "Epoch 85/200\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 128.5019 - val_loss: 169.9651\n",
            "Epoch 86/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 129.0744 - val_loss: 146.4512\n",
            "Epoch 87/200\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 128.6753 - val_loss: 160.9271\n",
            "Epoch 88/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 128.7296 - val_loss: 146.3207\n",
            "Epoch 89/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 128.4153 - val_loss: 144.3139\n",
            "Epoch 90/200\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 128.7027 - val_loss: 141.6637 ETA: 0s - loss: \n",
            "Epoch 91/200\n",
            "5201/5201 [==============================] - 5s 878us/step - loss: 128.5692 - val_loss: 142.3359\n",
            "Epoch 92/200\n",
            "5201/5201 [==============================] - 5s 877us/step - loss: 128.5876 - val_loss: 144.8732\n",
            "Epoch 93/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.4185 - val_loss: 156.5549\n",
            "Epoch 94/200\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 129.0383 - val_loss: 142.7593\n",
            "Epoch 95/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 128.6442 - val_loss: 149.7491\n",
            "Epoch 96/200\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 128.6491 - val_loss: 143.9307\n",
            "Epoch 97/200\n",
            "5201/5201 [==============================] - 5s 874us/step - loss: 128.9189 - val_loss: 146.1029\n",
            "Epoch 98/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 128.7367 - val_loss: 143.6526\n",
            "Epoch 99/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 128.9350 - val_loss: 145.3394\n",
            "Epoch 100/200\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 128.5180 - val_loss: 143.4787\n",
            "Epoch 101/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 129.2942 - val_loss: 143.0016\n",
            "Epoch 102/200\n",
            "5201/5201 [==============================] - 5s 868us/step - loss: 128.3834 - val_loss: 144.7122\n",
            "Epoch 103/200\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 128.4564 - val_loss: 148.5395\n",
            "Epoch 104/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 128.2780 - val_loss: 143.3656\n",
            "Epoch 105/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.4160 - val_loss: 147.2136\n",
            "Epoch 106/200\n",
            "5201/5201 [==============================] - 5s 875us/step - loss: 128.7333 - val_loss: 145.4600\n",
            "Epoch 107/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.8861 - val_loss: 145.6491\n",
            "Epoch 108/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 128.3803 - val_loss: 143.7533\n",
            "Epoch 109/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 128.3483 - val_loss: 143.7781\n",
            "Epoch 110/200\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 128.4409 - val_loss: 143.2878\n",
            "Epoch 111/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 128.6344 - val_loss: 146.1375\n",
            "Epoch 112/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 128.7352 - val_loss: 142.6705\n",
            "Epoch 113/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 128.9218 - val_loss: 142.8371\n",
            "Epoch 114/200\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 128.4298 - val_loss: 146.0184\n",
            "Epoch 115/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 129.0199 - val_loss: 142.7830\n",
            "Epoch 116/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 128.3879 - val_loss: 142.7543\n",
            "Epoch 117/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 128.8819 - val_loss: 150.3169\n",
            "Epoch 118/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.6327 - val_loss: 144.8888\n",
            "Epoch 119/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 128.7589 - val_loss: 144.9369\n",
            "Epoch 120/200\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 128.5453 - val_loss: 147.9085\n",
            "Epoch 121/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.9935 - val_loss: 146.0271\n",
            "Epoch 122/200\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 128.8339 - val_loss: 146.0630\n",
            "Epoch 123/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 128.5668 - val_loss: 146.6545\n",
            "Epoch 124/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 128.3959 - val_loss: 170.3155\n",
            "Epoch 125/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 128.3717 - val_loss: 144.0213\n",
            "Epoch 126/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 128.2111 - val_loss: 147.9938\n",
            "Epoch 127/200\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 128.2283 - val_loss: 144.0510\n",
            "Epoch 128/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 128.6026 - val_loss: 144.6282\n",
            "Epoch 129/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 128.7615 - val_loss: 146.7558\n",
            "Epoch 130/200\n",
            "5201/5201 [==============================] - 5s 874us/step - loss: 128.4417 - val_loss: 147.8481\n",
            "Epoch 131/200\n",
            "5201/5201 [==============================] - 5s 883us/step - loss: 128.2058 - val_loss: 146.5519\n",
            "Epoch 132/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 128.8441 - val_loss: 146.1353\n",
            "Epoch 133/200\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 128.2813 - val_loss: 146.7443\n",
            "Epoch 134/200\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 128.6495 - val_loss: 146.7199\n",
            "Epoch 135/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 128.2372 - val_loss: 145.2273\n",
            "Epoch 136/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 128.6243 - val_loss: 144.9588\n",
            "Epoch 137/200\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 128.8021 - val_loss: 145.7115\n",
            "Epoch 138/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 128.6127 - val_loss: 142.7523\n",
            "Epoch 139/200\n",
            "5201/5201 [==============================] - 5s 877us/step - loss: 128.4485 - val_loss: 146.4344\n",
            "Epoch 140/200\n",
            "5201/5201 [==============================] - 4s 863us/step - loss: 128.5767 - val_loss: 144.4106\n",
            "Epoch 141/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 128.7507 - val_loss: 155.1752\n",
            "Epoch 142/200\n",
            "5201/5201 [==============================] - 5s 878us/step - loss: 128.8384 - val_loss: 149.4758\n",
            "Epoch 143/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 128.5845 - val_loss: 146.1840\n",
            "Epoch 144/200\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 128.7072 - val_loss: 144.5068\n",
            "Epoch 145/200\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 128.5312 - val_loss: 151.8463\n",
            "Epoch 146/200\n",
            "5201/5201 [==============================] - 5s 875us/step - loss: 128.2793 - val_loss: 146.0589\n",
            "Epoch 147/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 128.5227 - val_loss: 151.0362\n",
            "Epoch 148/200\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 128.1931 - val_loss: 143.5719\n",
            "Epoch 149/200\n",
            "5201/5201 [==============================] - 5s 875us/step - loss: 128.4820 - val_loss: 145.2092\n",
            "Epoch 150/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 128.3661 - val_loss: 142.6940\n",
            "Epoch 151/200\n",
            "5201/5201 [==============================] - 5s 871us/step - loss: 128.6051 - val_loss: 145.0833\n",
            "Epoch 152/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 128.3104 - val_loss: 144.6145\n",
            "Epoch 153/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 128.7308 - val_loss: 147.6553\n",
            "Epoch 154/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.2744 - val_loss: 146.6283\n",
            "Epoch 155/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.4488 - val_loss: 145.3427\n",
            "Epoch 156/200\n",
            "5201/5201 [==============================] - 5s 869us/step - loss: 128.2461 - val_loss: 144.2945\n",
            "Epoch 157/200\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 128.3137 - val_loss: 144.6370\n",
            "Epoch 158/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 128.8519 - val_loss: 144.2217\n",
            "Epoch 159/200\n",
            "5201/5201 [==============================] - 5s 875us/step - loss: 128.6077 - val_loss: 146.1763\n",
            "Epoch 160/200\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 129.2428 - val_loss: 158.4805\n",
            "Epoch 161/200\n",
            "5201/5201 [==============================] - 5s 871us/step - loss: 129.1119 - val_loss: 145.0812\n",
            "Epoch 162/200\n",
            "5201/5201 [==============================] - 5s 876us/step - loss: 128.7637 - val_loss: 145.0002\n",
            "Epoch 163/200\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 128.3124 - val_loss: 151.6246\n",
            "Epoch 164/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 128.8814 - val_loss: 144.6026\n",
            "Epoch 165/200\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 128.7498 - val_loss: 143.2388\n",
            "Epoch 166/200\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 128.6810 - val_loss: 156.2323\n",
            "Epoch 167/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.5979 - val_loss: 151.4407\n",
            "Epoch 168/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.2891 - val_loss: 145.4053\n",
            "Epoch 169/200\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 128.1257 - val_loss: 145.0840\n",
            "Epoch 170/200\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 128.7822 - val_loss: 153.0347\n",
            "Epoch 171/200\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 127.9532 - val_loss: 143.6315\n",
            "Epoch 172/200\n",
            "5201/5201 [==============================] - 5s 904us/step - loss: 128.7070 - val_loss: 146.3961\n",
            "Epoch 173/200\n",
            "5201/5201 [==============================] - 5s 867us/step - loss: 128.6177 - val_loss: 141.7045\n",
            "Epoch 174/200\n",
            "5201/5201 [==============================] - 5s 876us/step - loss: 128.5347 - val_loss: 144.7057\n",
            "Epoch 175/200\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 128.4680 - val_loss: 146.5579\n",
            "Epoch 176/200\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 128.8838 - val_loss: 144.0771\n",
            "Epoch 177/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 128.3200 - val_loss: 143.4421\n",
            "Epoch 178/200\n",
            "5201/5201 [==============================] - 5s 874us/step - loss: 128.9112 - val_loss: 148.1922\n",
            "Epoch 179/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 128.4398 - val_loss: 146.0275\n",
            "Epoch 180/200\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 128.7551 - val_loss: 147.0646\n",
            "Epoch 181/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 128.6299 - val_loss: 147.0622\n",
            "Epoch 182/200\n",
            "5201/5201 [==============================] - 5s 874us/step - loss: 128.5595 - val_loss: 147.9700\n",
            "Epoch 183/200\n",
            "5201/5201 [==============================] - 5s 936us/step - loss: 128.5782 - val_loss: 145.3205\n",
            "Epoch 184/200\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 128.1666 - val_loss: 145.7826\n",
            "Epoch 185/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 128.0979 - val_loss: 142.7505\n",
            "Epoch 186/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 128.8454 - val_loss: 143.4015\n",
            "Epoch 187/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 128.3658 - val_loss: 166.7700\n",
            "Epoch 188/200\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 128.7574 - val_loss: 144.0910\n",
            "Epoch 189/200\n",
            "5201/5201 [==============================] - 4s 864us/step - loss: 128.0718 - val_loss: 144.9472\n",
            "Epoch 190/200\n",
            "5201/5201 [==============================] - 5s 873us/step - loss: 128.6130 - val_loss: 148.4534\n",
            "Epoch 191/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 128.3683 - val_loss: 151.5995\n",
            "Epoch 192/200\n",
            "5201/5201 [==============================] - 5s 877us/step - loss: 128.5424 - val_loss: 147.8969\n",
            "Epoch 193/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 128.4703 - val_loss: 150.9022\n",
            "Epoch 194/200\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 128.7821 - val_loss: 144.7625\n",
            "Epoch 195/200\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 128.9046 - val_loss: 147.1265\n",
            "Epoch 196/200\n",
            "5201/5201 [==============================] - 5s 904us/step - loss: 128.6350 - val_loss: 155.6021\n",
            "Epoch 197/200\n",
            "5201/5201 [==============================] - 5s 880us/step - loss: 128.2075 - val_loss: 159.2663\n",
            "Epoch 198/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 128.4387 - val_loss: 150.9393\n",
            "Epoch 199/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 128.1518 - val_loss: 143.8169\n",
            "Epoch 200/200\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 128.5036 - val_loss: 151.4659\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adadelta,SGD\n",
        "\n",
        "#parameter\n",
        "batch_size = 32\n",
        "epoch = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "model_SBP.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\n",
        "# model_SBP.summary()\n",
        "history = model_SBP.fit(X_train, sbp_train, batch_size = batch_size, epochs = epoch, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSGmvjY8Jlvj",
        "outputId": "d99fb170-5d7f-4fbe-c7dc-28d6a2e5ee57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -2.4202556167442344 \n",
            "MAE:  9.157946333778934 \n",
            "SD:  12.066823720813296\n"
          ]
        }
      ],
      "source": [
        "pred = model_SBP.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)\n",
        "\n",
        "\n",
        "# ME:  -2.4202556167442344 \n",
        "# MAE:  9.157946333778934 \n",
        "# SD:  12.066823720813296"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "f5quOu65Jlvj",
        "outputId": "4370fbaa-b699-48b9-ce5b-1e26f294ea5d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epoch, 0, 1000])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgFe04oKJe8V"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNh68ZpLJzoI"
      },
      "source": [
        "## 8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVKDodY-JzoI"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbMY9qPUJzoI",
        "outputId": "91e29848-db3c-480c-9894-03381173d5df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_74 (Dense)             (None, 64)                2368      \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_78 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_65 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_81 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_66 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_67 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_84 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 41,153\n",
            "Trainable params: 40,513\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))  \n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(Dense(64))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    \n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        "    \n",
        "model_SBP2 = build_model()\n",
        "model_SBP2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etAA6zJ-JzoI",
        "outputId": "dd68b7e8-2719-4580-e7e9-56bb7abab528",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 128.3925 - val_loss: 148.6349\n",
            "Epoch 2/200\n",
            "5201/5201 [==============================] - 5s 873us/step - loss: 128.7518 - val_loss: 143.7084\n",
            "Epoch 3/200\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 128.6031 - val_loss: 146.1542\n",
            "Epoch 4/200\n",
            "5201/5201 [==============================] - 5s 877us/step - loss: 128.4769 - val_loss: 153.6405\n",
            "Epoch 5/200\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 128.4650 - val_loss: 150.2936\n",
            "Epoch 6/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 128.8834 - val_loss: 151.0045\n",
            "Epoch 7/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 128.6935 - val_loss: 142.5834\n",
            "Epoch 8/200\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 128.6681 - val_loss: 143.2954\n",
            "Epoch 9/200\n",
            "5201/5201 [==============================] - 5s 881us/step - loss: 128.2910 - val_loss: 155.3150\n",
            "Epoch 10/200\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 128.0554 - val_loss: 144.0235\n",
            "Epoch 11/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 128.5396 - val_loss: 148.9142\n",
            "Epoch 12/200\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 128.2775 - val_loss: 142.9573\n",
            "Epoch 13/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 128.5037 - val_loss: 146.4588\n",
            "Epoch 14/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 128.1132 - val_loss: 144.7566\n",
            "Epoch 15/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 128.4739 - val_loss: 154.9563\n",
            "Epoch 16/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 128.6406 - val_loss: 146.7422\n",
            "Epoch 17/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 128.4023 - val_loss: 146.6137\n",
            "Epoch 18/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 128.2197 - val_loss: 144.5935\n",
            "Epoch 19/200\n",
            "5201/5201 [==============================] - 5s 883us/step - loss: 128.3205 - val_loss: 154.6725\n",
            "Epoch 20/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 128.6373 - val_loss: 145.5900\n",
            "Epoch 21/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 128.3261 - val_loss: 145.2672\n",
            "Epoch 22/200\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 128.8952 - val_loss: 148.1390\n",
            "Epoch 23/200\n",
            "5201/5201 [==============================] - 5s 931us/step - loss: 128.3438 - val_loss: 143.9343\n",
            "Epoch 24/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 128.4357 - val_loss: 149.4552\n",
            "Epoch 25/200\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 128.5000 - val_loss: 148.2721\n",
            "Epoch 26/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 128.8020 - val_loss: 143.2323\n",
            "Epoch 27/200\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 128.2234 - val_loss: 167.5052\n",
            "Epoch 28/200\n",
            "5201/5201 [==============================] - 5s 944us/step - loss: 128.4623 - val_loss: 152.3801\n",
            "Epoch 29/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 127.8849 - val_loss: 143.4985\n",
            "Epoch 30/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 128.3672 - val_loss: 153.4823\n",
            "Epoch 31/200\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 128.3284 - val_loss: 148.7674\n",
            "Epoch 32/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 127.9424 - val_loss: 150.1820\n",
            "Epoch 33/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 128.1576 - val_loss: 146.1255\n",
            "Epoch 34/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 128.3327 - val_loss: 142.5996\n",
            "Epoch 35/200\n",
            "5201/5201 [==============================] - 5s 935us/step - loss: 128.6747 - val_loss: 168.9465\n",
            "Epoch 36/200\n",
            "5201/5201 [==============================] - 5s 928us/step - loss: 128.5040 - val_loss: 146.9219\n",
            "Epoch 37/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 128.2932 - val_loss: 146.9920\n",
            "Epoch 38/200\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 129.0110 - val_loss: 143.2520\n",
            "Epoch 39/200\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 128.2218 - val_loss: 146.7428\n",
            "Epoch 40/200\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 128.6820 - val_loss: 147.4538\n",
            "Epoch 41/200\n",
            "5201/5201 [==============================] - 5s 936us/step - loss: 128.3210 - val_loss: 142.3523\n",
            "Epoch 42/200\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 128.2139 - val_loss: 142.2024\n",
            "Epoch 43/200\n",
            "5201/5201 [==============================] - 5s 932us/step - loss: 128.7486 - val_loss: 144.7514\n",
            "Epoch 44/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 128.2129 - val_loss: 142.9375\n",
            "Epoch 45/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 128.5839 - val_loss: 144.5115\n",
            "Epoch 46/200\n",
            "5201/5201 [==============================] - 5s 933us/step - loss: 128.3492 - val_loss: 146.3513\n",
            "Epoch 47/200\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 128.6320 - val_loss: 143.6658\n",
            "Epoch 48/200\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 128.5897 - val_loss: 143.6129\n",
            "Epoch 49/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 128.2188 - val_loss: 143.2133\n",
            "Epoch 50/200\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 127.9867 - val_loss: 143.0722\n",
            "Epoch 51/200\n",
            "5201/5201 [==============================] - 5s 932us/step - loss: 128.7166 - val_loss: 144.0265\n",
            "Epoch 52/200\n",
            "5201/5201 [==============================] - 5s 938us/step - loss: 128.1246 - val_loss: 173.4864\n",
            "Epoch 53/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 128.0718 - val_loss: 146.0709\n",
            "Epoch 54/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 128.4420 - val_loss: 147.9981\n",
            "Epoch 55/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 127.8521 - val_loss: 146.9297\n",
            "Epoch 56/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 128.2558 - val_loss: 145.9163\n",
            "Epoch 57/200\n",
            "5201/5201 [==============================] - 5s 957us/step - loss: 128.1844 - val_loss: 150.0808\n",
            "Epoch 58/200\n",
            "5201/5201 [==============================] - 5s 898us/step - loss: 128.7003 - val_loss: 145.8979\n",
            "Epoch 59/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 128.4408 - val_loss: 145.2806\n",
            "Epoch 60/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 128.0502 - val_loss: 142.5918\n",
            "Epoch 61/200\n",
            "5201/5201 [==============================] - 5s 932us/step - loss: 128.5405 - val_loss: 145.7683\n",
            "Epoch 62/200\n",
            "5201/5201 [==============================] - 5s 938us/step - loss: 128.1514 - val_loss: 142.6440\n",
            "Epoch 63/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 128.6348 - val_loss: 148.5251\n",
            "Epoch 64/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 128.3297 - val_loss: 144.2213\n",
            "Epoch 65/200\n",
            "5201/5201 [==============================] - 5s 919us/step - loss: 128.1491 - val_loss: 148.7556\n",
            "Epoch 66/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 128.4244 - val_loss: 145.8477\n",
            "Epoch 67/200\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 128.1784 - val_loss: 144.3231\n",
            "Epoch 68/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 128.6446 - val_loss: 155.1713\n",
            "Epoch 69/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 128.3046 - val_loss: 192.3976\n",
            "Epoch 70/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 128.2501 - val_loss: 143.0729\n",
            "Epoch 71/200\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 128.0554 - val_loss: 158.7188\n",
            "Epoch 72/200\n",
            "5201/5201 [==============================] - 5s 929us/step - loss: 128.1450 - val_loss: 149.0365\n",
            "Epoch 73/200\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 128.4354 - val_loss: 147.1520\n",
            "Epoch 74/200\n",
            "5201/5201 [==============================] - 5s 941us/step - loss: 128.0255 - val_loss: 155.8872\n",
            "Epoch 75/200\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 128.4409 - val_loss: 146.2801\n",
            "Epoch 76/200\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 128.2788 - val_loss: 144.7929\n",
            "Epoch 77/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 128.2320 - val_loss: 147.1013\n",
            "Epoch 78/200\n",
            "5201/5201 [==============================] - 5s 931us/step - loss: 128.3915 - val_loss: 144.5974\n",
            "Epoch 79/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 128.1124 - val_loss: 144.4899\n",
            "Epoch 80/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 128.4622 - val_loss: 142.5692\n",
            "Epoch 81/200\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 128.3482 - val_loss: 144.5228\n",
            "Epoch 82/200\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 128.2972 - val_loss: 144.7778\n",
            "Epoch 83/200\n",
            "5201/5201 [==============================] - 5s 937us/step - loss: 127.7970 - val_loss: 149.8324\n",
            "Epoch 84/200\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 128.4920 - val_loss: 144.5782\n",
            "Epoch 85/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 127.9307 - val_loss: 145.8768\n",
            "Epoch 86/200\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 128.3017 - val_loss: 144.3357\n",
            "Epoch 87/200\n",
            "5201/5201 [==============================] - 5s 934us/step - loss: 128.2420 - val_loss: 147.1824\n",
            "Epoch 88/200\n",
            "5201/5201 [==============================] - 5s 951us/step - loss: 128.5065 - val_loss: 145.7477\n",
            "Epoch 89/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 128.3967 - val_loss: 145.8053\n",
            "Epoch 90/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 128.0264 - val_loss: 146.7512\n",
            "Epoch 91/200\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 128.1958 - val_loss: 152.7282\n",
            "Epoch 92/200\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 128.0817 - val_loss: 145.0544\n",
            "Epoch 93/200\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 128.5953 - val_loss: 144.5004\n",
            "Epoch 94/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 128.1391 - val_loss: 153.2923\n",
            "Epoch 95/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 128.5795 - val_loss: 143.2182\n",
            "Epoch 96/200\n",
            "5201/5201 [==============================] - 5s 940us/step - loss: 128.6491 - val_loss: 143.1494\n",
            "Epoch 97/200\n",
            "5201/5201 [==============================] - 5s 927us/step - loss: 128.2670 - val_loss: 144.9363\n",
            "Epoch 98/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 128.5979 - val_loss: 149.9720\n",
            "Epoch 99/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.2727 - val_loss: 149.2392\n",
            "Epoch 100/200\n",
            "5201/5201 [==============================] - 5s 953us/step - loss: 128.3103 - val_loss: 147.0636\n",
            "Epoch 101/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 128.5132 - val_loss: 146.4232\n",
            "Epoch 102/200\n",
            "5201/5201 [==============================] - 5s 906us/step - loss: 128.5456 - val_loss: 150.0414\n",
            "Epoch 103/200\n",
            "5201/5201 [==============================] - 5s 923us/step - loss: 128.2330 - val_loss: 146.1091\n",
            "Epoch 104/200\n",
            "5201/5201 [==============================] - 5s 945us/step - loss: 128.3010 - val_loss: 146.0177\n",
            "Epoch 105/200\n",
            "5201/5201 [==============================] - 5s 934us/step - loss: 128.3751 - val_loss: 152.3195\n",
            "Epoch 106/200\n",
            "5201/5201 [==============================] - 5s 900us/step - loss: 128.2794 - val_loss: 148.4697\n",
            "Epoch 107/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 128.1659 - val_loss: 146.2619\n",
            "Epoch 108/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 127.6545 - val_loss: 148.3030\n",
            "Epoch 109/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 128.3185 - val_loss: 144.6868\n",
            "Epoch 110/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 128.1910 - val_loss: 145.1665\n",
            "Epoch 111/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 127.6842 - val_loss: 146.4433\n",
            "Epoch 112/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 128.2948 - val_loss: 147.4157\n",
            "Epoch 113/200\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 128.3358 - val_loss: 146.0192\n",
            "Epoch 114/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 128.1540 - val_loss: 149.4411\n",
            "Epoch 115/200\n",
            "5201/5201 [==============================] - 5s 924us/step - loss: 127.8198 - val_loss: 145.9722\n",
            "Epoch 116/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.3411 - val_loss: 146.1226\n",
            "Epoch 117/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 127.9292 - val_loss: 154.4178\n",
            "Epoch 118/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 128.2898 - val_loss: 150.7535\n",
            "Epoch 119/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 128.0661 - val_loss: 147.2140\n",
            "Epoch 120/200\n",
            "5201/5201 [==============================] - 5s 928us/step - loss: 128.1632 - val_loss: 147.7145\n",
            "Epoch 121/200\n",
            "5201/5201 [==============================] - 5s 879us/step - loss: 128.3896 - val_loss: 149.2578\n",
            "Epoch 122/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.4886 - val_loss: 144.5356\n",
            "Epoch 123/200\n",
            "5201/5201 [==============================] - 5s 908us/step - loss: 127.8316 - val_loss: 148.3751\n",
            "Epoch 124/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 127.8761 - val_loss: 148.1293\n",
            "Epoch 125/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 128.1690 - val_loss: 143.6970\n",
            "Epoch 126/200\n",
            "5201/5201 [==============================] - 5s 885us/step - loss: 127.9949 - val_loss: 144.6819\n",
            "Epoch 127/200\n",
            "5201/5201 [==============================] - 5s 882us/step - loss: 128.1946 - val_loss: 143.2638\n",
            "Epoch 128/200\n",
            "5201/5201 [==============================] - 5s 910us/step - loss: 128.4531 - val_loss: 144.0982\n",
            "Epoch 129/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 128.3089 - val_loss: 144.9027\n",
            "Epoch 130/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 128.7977 - val_loss: 145.5084\n",
            "Epoch 131/200\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 128.3490 - val_loss: 147.5963\n",
            "Epoch 132/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 128.2177 - val_loss: 144.5699\n",
            "Epoch 133/200\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 127.9754 - val_loss: 145.4979\n",
            "Epoch 134/200\n",
            "5201/5201 [==============================] - 5s 892us/step - loss: 128.2234 - val_loss: 144.1685\n",
            "Epoch 135/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.2419 - val_loss: 147.6474\n",
            "Epoch 136/200\n",
            "5201/5201 [==============================] - 5s 887us/step - loss: 127.8509 - val_loss: 170.0754\n",
            "Epoch 137/200\n",
            "5201/5201 [==============================] - 5s 895us/step - loss: 128.0493 - val_loss: 144.5239\n",
            "Epoch 138/200\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 128.2816 - val_loss: 149.2138\n",
            "Epoch 139/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 127.9558 - val_loss: 149.3463\n",
            "Epoch 140/200\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 128.5038 - val_loss: 144.8462\n",
            "Epoch 141/200\n",
            "5201/5201 [==============================] - 5s 869us/step - loss: 128.1334 - val_loss: 158.5099\n",
            "Epoch 142/200\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 128.4469 - val_loss: 142.6166\n",
            "Epoch 143/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.2514 - val_loss: 156.1162\n",
            "Epoch 144/200\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 128.2016 - val_loss: 155.2337\n",
            "Epoch 145/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 128.4144 - val_loss: 146.9829\n",
            "Epoch 146/200\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 128.4700 - val_loss: 144.6754\n",
            "Epoch 147/200\n",
            "5201/5201 [==============================] - 5s 884us/step - loss: 127.9459 - val_loss: 147.8826\n",
            "Epoch 148/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.1643 - val_loss: 147.7740\n",
            "Epoch 149/200\n",
            "5201/5201 [==============================] - 5s 888us/step - loss: 128.4175 - val_loss: 145.2465\n",
            "Epoch 150/200\n",
            "5201/5201 [==============================] - 5s 925us/step - loss: 128.1759 - val_loss: 144.5425\n",
            "Epoch 151/200\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 128.2028 - val_loss: 160.2099\n",
            "Epoch 152/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.3530 - val_loss: 146.1515\n",
            "Epoch 153/200\n",
            "5201/5201 [==============================] - 5s 918us/step - loss: 127.7190 - val_loss: 149.1261\n",
            "Epoch 154/200\n",
            "5201/5201 [==============================] - 5s 916us/step - loss: 128.1022 - val_loss: 153.0482\n",
            "Epoch 155/200\n",
            "5201/5201 [==============================] - 5s 915us/step - loss: 128.3326 - val_loss: 146.6373\n",
            "Epoch 156/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.4408 - val_loss: 144.9440\n",
            "Epoch 157/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 127.8518 - val_loss: 144.4383\n",
            "Epoch 158/200\n",
            "5201/5201 [==============================] - 5s 909us/step - loss: 128.2195 - val_loss: 154.6702\n",
            "Epoch 159/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 127.6514 - val_loss: 144.2477\n",
            "Epoch 160/200\n",
            "5201/5201 [==============================] - 5s 920us/step - loss: 128.3840 - val_loss: 144.7282\n",
            "Epoch 161/200\n",
            "5201/5201 [==============================] - 5s 891us/step - loss: 128.3415 - val_loss: 146.6142\n",
            "Epoch 162/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 128.1871 - val_loss: 145.0513\n",
            "Epoch 163/200\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 128.0872 - val_loss: 146.3893\n",
            "Epoch 164/200\n",
            "5201/5201 [==============================] - 5s 944us/step - loss: 127.8104 - val_loss: 146.7384\n",
            "Epoch 165/200\n",
            "5201/5201 [==============================] - 5s 889us/step - loss: 128.5555 - val_loss: 150.4739\n",
            "Epoch 166/200\n",
            "5201/5201 [==============================] - 5s 946us/step - loss: 128.2616 - val_loss: 142.1203\n",
            "Epoch 167/200\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 128.4195 - val_loss: 144.5990\n",
            "Epoch 168/200\n",
            "5201/5201 [==============================] - 5s 913us/step - loss: 128.0868 - val_loss: 151.5149\n",
            "Epoch 169/200\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 128.1148 - val_loss: 145.0632\n",
            "Epoch 170/200\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 127.8099 - val_loss: 151.0362\n",
            "Epoch 171/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 128.4606 - val_loss: 143.4634\n",
            "Epoch 172/200\n",
            "5201/5201 [==============================] - 5s 926us/step - loss: 128.2919 - val_loss: 147.1764\n",
            "Epoch 173/200\n",
            "5201/5201 [==============================] - 5s 930us/step - loss: 127.8319 - val_loss: 142.9559\n",
            "Epoch 174/200\n",
            "5201/5201 [==============================] - 5s 886us/step - loss: 128.2301 - val_loss: 156.1316\n",
            "Epoch 175/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 128.0848 - val_loss: 165.5013\n",
            "Epoch 176/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.1484 - val_loss: 152.8530\n",
            "Epoch 177/200\n",
            "5201/5201 [==============================] - 5s 901us/step - loss: 128.2412 - val_loss: 158.5466\n",
            "Epoch 178/200\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 127.8448 - val_loss: 148.0503s: 1\n",
            "Epoch 179/200\n",
            "5201/5201 [==============================] - 5s 899us/step - loss: 128.4736 - val_loss: 148.0888\n",
            "Epoch 180/200\n",
            "5201/5201 [==============================] - 5s 921us/step - loss: 128.0489 - val_loss: 142.7842\n",
            "Epoch 181/200\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 128.3947 - val_loss: 143.2662\n",
            "Epoch 182/200\n",
            "5201/5201 [==============================] - 5s 902us/step - loss: 128.1883 - val_loss: 143.6779\n",
            "Epoch 183/200\n",
            "5201/5201 [==============================] - 5s 938us/step - loss: 128.2806 - val_loss: 147.3999\n",
            "Epoch 184/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 128.5562 - val_loss: 144.8244\n",
            "Epoch 185/200\n",
            "5201/5201 [==============================] - 5s 917us/step - loss: 128.4769 - val_loss: 146.5643\n",
            "Epoch 186/200\n",
            "5201/5201 [==============================] - 5s 896us/step - loss: 128.3529 - val_loss: 143.9294\n",
            "Epoch 187/200\n",
            "5201/5201 [==============================] - 5s 890us/step - loss: 127.9218 - val_loss: 147.9096\n",
            "Epoch 188/200\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 128.4557 - val_loss: 145.7437\n",
            "Epoch 189/200\n",
            "5201/5201 [==============================] - 5s 894us/step - loss: 128.1624 - val_loss: 143.9327\n",
            "Epoch 190/200\n",
            "5201/5201 [==============================] - 5s 905us/step - loss: 128.4902 - val_loss: 148.6744\n",
            "Epoch 191/200\n",
            "5201/5201 [==============================] - 5s 897us/step - loss: 127.9821 - val_loss: 145.4115\n",
            "Epoch 192/200\n",
            "5201/5201 [==============================] - 5s 903us/step - loss: 128.3390 - val_loss: 146.7148\n",
            "Epoch 193/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 127.9635 - val_loss: 143.4863\n",
            "Epoch 194/200\n",
            "5201/5201 [==============================] - 5s 922us/step - loss: 128.3398 - val_loss: 144.6503\n",
            "Epoch 195/200\n",
            "5201/5201 [==============================] - 5s 912us/step - loss: 128.1582 - val_loss: 146.3065\n",
            "Epoch 196/200\n",
            "5201/5201 [==============================] - 5s 931us/step - loss: 128.5450 - val_loss: 143.1759\n",
            "Epoch 197/200\n",
            "5201/5201 [==============================] - 5s 907us/step - loss: 127.7890 - val_loss: 142.5054\n",
            "Epoch 198/200\n",
            "5201/5201 [==============================] - 5s 914us/step - loss: 128.4123 - val_loss: 146.7612\n",
            "Epoch 199/200\n",
            "5201/5201 [==============================] - 5s 893us/step - loss: 128.1935 - val_loss: 144.6457\n",
            "Epoch 200/200\n",
            "5201/5201 [==============================] - 5s 911us/step - loss: 127.7221 - val_loss: 144.4447\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam,Adagrad,Adadelta,SGD\n",
        "\n",
        "#parameter\n",
        "batch_size = 32\n",
        "epoch = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "model_SBP.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\n",
        "# model_SBP.summary()\n",
        "history = model_SBP.fit(X_train, sbp_train, batch_size = batch_size, epochs = epoch, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-7_77nwJzoJ",
        "outputId": "f5545d66-64c6-4343-a61a-0a0a0d97fa42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  0.7050894222259282 \n",
            "MAE:  8.730976042856787 \n",
            "SD:  11.997817086572004\n"
          ]
        }
      ],
      "source": [
        "pred = model_SBP.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)\n",
        "\n",
        "# ME:  -0.2947758469780435 \n",
        "# MAE:  9.818598407487531 \n",
        "# SD:  13.19426213934431"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "sPeIj5MkJzoJ",
        "outputId": "8534122a-7ca5-4d73-cda1-8a0ca7d414f5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['val_loss','loss'])\n",
        "\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epoch, 0, 200])\n",
        "fig = plt.gcf()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3jJNDT_-kLbn",
        "RBdIpD3akLbq",
        "odhTCfgWkLbs",
        "OtFjBAtRkLbt",
        "xRNa8JmUkLbu",
        "NZTXyjC7yxrJ",
        "nWsTP-ewJlvi",
        "mNh68ZpLJzoI",
        "O6_Z3A1foZRR",
        "a7vBd4Pk5-WK",
        "FkjLWV8Mxmhd",
        "LR0fuDdHytTs",
        "_y3SP13BzHzd"
      ],
      "name": "BP_hv1-1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}