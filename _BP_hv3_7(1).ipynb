{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/_BP_hv3_7(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiiiBla2-j1S"
      },
      "source": [
        "# batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCoux5AOZnK",
        "outputId": "05ec15c5-4d35-4fbf-df2c-7869c9eda218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version :  3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]\n",
            "TensorFlow version :  2.3.0\n",
            "Keras version :  2.4.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러오기\n",
        "# train_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/train_shuffled_raw_v1.mat')\n",
        "# test_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/test_not_shuffled_raw_v1.mat')\n",
        "\n",
        "train_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/new/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('C:/Users/LEE/Desktop/imhzz/new/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "b570a293-9e53-473a-f3ed-a9b19951a83d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "8f731ca9-0203-46e7-87ab-30015e694029"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "8abc45d0-bd5c-49cd-8d12-1cde358bdca2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "outputs": [],
      "source": [
        "#parameter\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN",
        "outputId": "ff09bdb8-21e4-4bf8-8e0f-93e2ff52a627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 16)                2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 2,465\n",
            "Trainable params: 2,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI8SHBwBOZnO"
      },
      "outputs": [],
      "source": [
        "# model = model1()\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGT6-7NcOZnO",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 12392.1729 - val_loss: 12240.1309\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 11865.0195 - val_loss: 11415.2324\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 11180.9395 - val_loss: 10818.8818\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 10265.9033 - val_loss: 9439.0283\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 9160.0430 - val_loss: 8714.8936\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 7917.0479 - val_loss: 6999.2500\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 6590.9155 - val_loss: 6374.0957\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 6s 38ms/step - loss: 5248.2075 - val_loss: 5067.8960\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 3950.6633 - val_loss: 3229.0586\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 5s 30ms/step - loss: 2758.3962 - val_loss: 2062.6372\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 1803.4177 - val_loss: 1380.6702\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 1121.8236 - val_loss: 1093.8035\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 664.3174 - val_loss: 724.2858\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 387.6452 - val_loss: 468.0480\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 234.1010 - val_loss: 164.4078\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 158.3719 - val_loss: 194.1208\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 125.4982 - val_loss: 125.4871\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 111.6297 - val_loss: 172.0663\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 105.2845 - val_loss: 123.5031\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 103.3188 - val_loss: 160.6098\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 10s 60ms/step - loss: 101.6911 - val_loss: 121.0873\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 100.1067 - val_loss: 126.1935\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 99.2753 - val_loss: 109.5342\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 99.1755 - val_loss: 115.6585\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 97.8738 - val_loss: 116.5972\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 96.2062 - val_loss: 115.2627\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 95.3345 - val_loss: 110.7423\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 95.5919 - val_loss: 105.4673\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 94.5684 - val_loss: 109.1988\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 94.3556 - val_loss: 118.9563\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 94.4841 - val_loss: 102.1532\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 10s 60ms/step - loss: 93.4200 - val_loss: 114.7918\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 92.8276 - val_loss: 114.8146\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 92.0621 - val_loss: 103.0673\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 92.2230 - val_loss: 122.9845\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 91.8638 - val_loss: 103.2309\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 91.3064 - val_loss: 110.3596\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 91.2526 - val_loss: 101.3307\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 91.0132 - val_loss: 139.2529\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 90.8930 - val_loss: 128.1979\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 90.5927 - val_loss: 122.0127\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 90.1423 - val_loss: 107.1096\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 10s 60ms/step - loss: 89.5815 - val_loss: 124.3662\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 88.8260 - val_loss: 114.4629\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 89.2965 - val_loss: 133.8869\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 89.1108 - val_loss: 109.9095\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 6s 35ms/step - loss: 88.3386 - val_loss: 110.0857\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 6s 36ms/step - loss: 87.9662 - val_loss: 109.0739\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.7703 - val_loss: 99.2943\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 87.1699 - val_loss: 110.2628\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.0598 - val_loss: 108.4743\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 87.1711 - val_loss: 104.9947\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 86.6986 - val_loss: 129.2460\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.4896 - val_loss: 108.0515\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 86.4016 - val_loss: 99.8594\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 86.2824 - val_loss: 100.4885\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.1866 - val_loss: 95.2509\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 85.4277 - val_loss: 132.0615\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 85.5057 - val_loss: 109.2430\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.5768 - val_loss: 98.4853\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 85.2552 - val_loss: 123.1895\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.7132 - val_loss: 110.7320\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 85.0371 - val_loss: 110.8489\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 84.2045 - val_loss: 103.7253\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.8846 - val_loss: 101.3928\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 84.1422 - val_loss: 101.8316\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.0940 - val_loss: 106.1756\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 84.1441 - val_loss: 100.7533\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 84.3638 - val_loss: 124.2652\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.7153 - val_loss: 107.7076\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 82.9724 - val_loss: 103.0550\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 83.2802 - val_loss: 112.0916\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.4398 - val_loss: 122.9158\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 83.3759 - val_loss: 119.3689\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.1869 - val_loss: 109.3177\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 82.7271 - val_loss: 101.6913\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.5629 - val_loss: 100.4292\n",
            "Epoch 78/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 82.3646 - val_loss: 154.2250\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 82.6016 - val_loss: 102.7957\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.2197 - val_loss: 106.9617\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 82.1598 - val_loss: 107.0664\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 81.9575 - val_loss: 141.5059\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.7141 - val_loss: 98.6394\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 81.7318 - val_loss: 97.2616\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.2706 - val_loss: 105.1702\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 81.8579 - val_loss: 107.9262\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 81.4969 - val_loss: 100.6906\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.1165 - val_loss: 94.0483\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 81.3126 - val_loss: 98.1232\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.8672 - val_loss: 109.1807\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.9884 - val_loss: 103.6413\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 81.0371 - val_loss: 131.6717\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.6511 - val_loss: 98.0273\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.7393 - val_loss: 204.4445\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.2361 - val_loss: 102.1795\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.7919 - val_loss: 111.4684\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.3924 - val_loss: 113.9388\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.1209 - val_loss: 100.6648\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.2560 - val_loss: 115.1874\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.6425 - val_loss: 101.9573\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.5209 - val_loss: 99.5962\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.7815 - val_loss: 99.9737\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.5229 - val_loss: 103.3141\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.0264 - val_loss: 110.6810\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.1977 - val_loss: 111.6360\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.1896 - val_loss: 92.1511\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.7517 - val_loss: 108.9166\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.8674 - val_loss: 111.5501\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.3234 - val_loss: 91.7711\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.6915 - val_loss: 100.9340\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.4894 - val_loss: 167.4611\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.6003 - val_loss: 92.3688\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.2569 - val_loss: 187.8835\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.1448 - val_loss: 100.5355\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.6619 - val_loss: 111.4374\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.1213 - val_loss: 103.1762\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.8544 - val_loss: 111.7627\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.1609 - val_loss: 110.9128\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.9179 - val_loss: 97.4428\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.9757 - val_loss: 93.6223\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.9700 - val_loss: 90.8604\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.7763 - val_loss: 107.7091\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.6592 - val_loss: 94.4816\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.4988 - val_loss: 112.6174\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 77.4715 - val_loss: 92.7721\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.7316 - val_loss: 95.6687\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.2908 - val_loss: 112.7432\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.2408 - val_loss: 130.7846\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.3232 - val_loss: 100.7470\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.1908 - val_loss: 95.6368\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.2556 - val_loss: 97.3701\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.2852 - val_loss: 89.7662\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.8382 - val_loss: 110.6779\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.2362 - val_loss: 113.7206\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.0250 - val_loss: 90.5223\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.6819 - val_loss: 102.3064\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.6513 - val_loss: 99.4634\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.8665 - val_loss: 96.8755\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.6989 - val_loss: 144.4083\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.6563 - val_loss: 122.6558\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.7963 - val_loss: 92.5289\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.4292 - val_loss: 140.1975\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.5553 - val_loss: 238.5064\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.7528 - val_loss: 114.1510\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.4115 - val_loss: 94.9181\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.3698 - val_loss: 102.8524\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.3938 - val_loss: 153.6389\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.2967 - val_loss: 91.2258\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.0620 - val_loss: 95.0221\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.3638 - val_loss: 105.4758\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.2893 - val_loss: 106.4544\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.9124 - val_loss: 99.9093\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.1846 - val_loss: 127.6287\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.0010 - val_loss: 121.0142\n",
            "Epoch 155/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 75.9337 - val_loss: 98.0772\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 75.6855 - val_loss: 100.4197\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.9266 - val_loss: 130.9657\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.8726 - val_loss: 92.0954\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.6620 - val_loss: 106.8830\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.7959 - val_loss: 111.2219\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.8203 - val_loss: 133.8430\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.7294 - val_loss: 115.1374\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.6481 - val_loss: 122.3703\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.3650 - val_loss: 94.0710\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.5281 - val_loss: 117.1476\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.5982 - val_loss: 104.8594\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.5158 - val_loss: 89.2235\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.5388 - val_loss: 100.9085\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.3147 - val_loss: 95.3870\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.3327 - val_loss: 97.4345\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.3115 - val_loss: 89.8975\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.2823 - val_loss: 138.8635\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.2202 - val_loss: 101.2825\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.0943 - val_loss: 101.8398\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.1941 - val_loss: 94.0552\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.0477 - val_loss: 93.5798\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.8874 - val_loss: 96.5245\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.9670 - val_loss: 124.2456\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.9615 - val_loss: 93.5876\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.1467 - val_loss: 95.7161\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.7745 - val_loss: 125.0566\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.8098 - val_loss: 102.1264\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.8594 - val_loss: 90.3824\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.7549 - val_loss: 99.7760\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.6909 - val_loss: 102.5552\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.8152 - val_loss: 98.6538\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.8478 - val_loss: 132.3427\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.6714 - val_loss: 94.3705\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.5161 - val_loss: 130.7164\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.6321 - val_loss: 102.1205\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.7677 - val_loss: 90.5751\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.5036 - val_loss: 103.6259\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.5798 - val_loss: 89.3334\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.4060 - val_loss: 100.1617\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.4760 - val_loss: 97.1288\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.5845 - val_loss: 106.7570\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.5051 - val_loss: 103.8740\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.3887 - val_loss: 100.7760\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.2903 - val_loss: 106.2741\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.3808 - val_loss: 92.5799\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.2705 - val_loss: 92.1898\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1560 - val_loss: 91.1492\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1189 - val_loss: 145.5168\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.2828 - val_loss: 95.5490\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.3135 - val_loss: 89.8749\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.1044 - val_loss: 88.2413\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.0969 - val_loss: 93.0142\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1579 - val_loss: 91.7369\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.0755 - val_loss: 101.7142\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1172 - val_loss: 91.7096\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1228 - val_loss: 117.5131\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.0153 - val_loss: 91.9627\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1087 - val_loss: 93.7352\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.9128 - val_loss: 108.8750\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9281 - val_loss: 107.5181\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9960 - val_loss: 102.6059\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.2274 - val_loss: 127.8586\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.0484 - val_loss: 125.7009\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8811 - val_loss: 94.5809\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9380 - val_loss: 92.8341\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8991 - val_loss: 131.6827\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.1050 - val_loss: 99.5129\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8612 - val_loss: 94.0491\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.9034 - val_loss: 103.3633\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8029 - val_loss: 93.1552\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.7672 - val_loss: 104.1849\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8314 - val_loss: 137.0107\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.7680 - val_loss: 125.9689\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.6215 - val_loss: 101.3403\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8729 - val_loss: 118.2033\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9243 - val_loss: 98.9243\n",
            "Epoch 232/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7760 - val_loss: 107.3647\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.5939 - val_loss: 96.6009\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6774 - val_loss: 88.6931\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7605 - val_loss: 132.0009\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6271 - val_loss: 92.4334\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5896 - val_loss: 89.8370\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.7653 - val_loss: 130.7034\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8322 - val_loss: 104.9475\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3999 - val_loss: 130.0593\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.6207 - val_loss: 139.7275\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 73.56 - 2s 10ms/step - loss: 73.5692 - val_loss: 111.6705\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2975 - val_loss: 90.5493\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6144 - val_loss: 91.0082\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5077 - val_loss: 105.3036\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.4500 - val_loss: 99.4693\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4286 - val_loss: 102.2555\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.5357 - val_loss: 92.5615\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4709 - val_loss: 91.8080\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3933 - val_loss: 98.0252\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.5323 - val_loss: 93.7186\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4945 - val_loss: 149.8427\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4003 - val_loss: 91.6100\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3976 - val_loss: 110.8638\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4925 - val_loss: 93.9969\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4592 - val_loss: 108.7274\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3359 - val_loss: 102.9152\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3564 - val_loss: 98.0349\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2737 - val_loss: 132.7069\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4818 - val_loss: 97.2659\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4316 - val_loss: 89.8049\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2937 - val_loss: 95.3803\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2859 - val_loss: 93.1496\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.1937 - val_loss: 115.6080\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1563 - val_loss: 89.0633\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3370 - val_loss: 113.1845\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2048 - val_loss: 90.1882\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3199 - val_loss: 96.6516\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2211 - val_loss: 119.2613\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3613 - val_loss: 91.6202\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0782 - val_loss: 94.1742\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.9931 - val_loss: 100.7621\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2543 - val_loss: 110.5081\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3120 - val_loss: 102.6269\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0894 - val_loss: 106.4724\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1288 - val_loss: 112.6587\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0928 - val_loss: 88.9961\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2351 - val_loss: 122.6506\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0258 - val_loss: 116.3982\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3178 - val_loss: 98.1605\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1196 - val_loss: 88.5043\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0591 - val_loss: 98.1122\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9977 - val_loss: 97.1623\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0314 - val_loss: 93.3015\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9866 - val_loss: 91.4984\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.9772 - val_loss: 103.3612\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9824 - val_loss: 88.6698\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7702 - val_loss: 97.0214\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9305 - val_loss: 89.9382\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0144 - val_loss: 96.7955\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9801 - val_loss: 202.4650\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.8546 - val_loss: 99.3953\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7953 - val_loss: 94.3503\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.0115 - val_loss: 95.6968\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.8694 - val_loss: 96.0153\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9097 - val_loss: 87.5221\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.8482 - val_loss: 97.1236\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7990 - val_loss: 92.4687\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9895 - val_loss: 91.2246\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9225 - val_loss: 88.9647\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9577 - val_loss: 96.0675\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6800 - val_loss: 101.3519\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.8522 - val_loss: 98.3151\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.5966 - val_loss: 101.4482\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.8134 - val_loss: 100.4980\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6194 - val_loss: 89.2415\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.8027 - val_loss: 90.1659\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7165 - val_loss: 98.4094\n",
            "Epoch 309/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6835 - val_loss: 115.9949\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6009 - val_loss: 115.2555\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7222 - val_loss: 103.8269\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.5783 - val_loss: 109.2211\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6450 - val_loss: 94.9351\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6499 - val_loss: 92.8545\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7780 - val_loss: 113.1827\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6909 - val_loss: 85.8156\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6319 - val_loss: 103.3676\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6617 - val_loss: 171.1399\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6187 - val_loss: 91.8217\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6617 - val_loss: 98.2726\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.5471 - val_loss: 95.8802\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6044 - val_loss: 95.5111\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6656 - val_loss: 93.1141\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7101 - val_loss: 105.3308\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7067 - val_loss: 93.0166\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6106 - val_loss: 89.6790\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.5266 - val_loss: 109.7985\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4034 - val_loss: 104.0873\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.5256 - val_loss: 90.0838\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.5457 - val_loss: 100.6424\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4732 - val_loss: 98.6935\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.2994 - val_loss: 112.8448\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 72.4031 ETA: 0s - l - 0s 2ms/step - loss: 72.4528 - val_loss: 114.9239\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.5804 - val_loss: 101.6387\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.4764 - val_loss: 95.7853\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.3795 - val_loss: 106.8852\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.3538 - val_loss: 99.3447\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.3789 - val_loss: 90.7109\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4514 - val_loss: 100.1079\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.3909 - val_loss: 113.7192\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.3233 - val_loss: 93.6052\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.4675 - val_loss: 120.2069\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.3834 - val_loss: 106.0536\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4443 - val_loss: 107.4249\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.2540 - val_loss: 112.7513\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.3244 - val_loss: 95.6415\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.3112 - val_loss: 103.5550\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.2892 - val_loss: 134.1820\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.2735 - val_loss: 101.4196\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.3272 - val_loss: 91.1260\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.2264 - val_loss: 91.6582\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.1820 - val_loss: 97.2833\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.2380 - val_loss: 97.7072\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.0995 - val_loss: 109.3700\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.2236 - val_loss: 105.4157\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4306 - val_loss: 99.9809\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.1764 - val_loss: 91.0310\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.1449 - val_loss: 92.9203\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.2393 - val_loss: 89.9227\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.3199 - val_loss: 115.6779\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.1844 - val_loss: 90.9818\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.2081 - val_loss: 98.1208\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.1799 - val_loss: 103.7825\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.0861 - val_loss: 127.3330\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.1485 - val_loss: 104.0091\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9702 - val_loss: 90.1221\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.0621 - val_loss: 95.8894\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.2175 - val_loss: 91.0615\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.0938 - val_loss: 98.7819\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.1695 - val_loss: 92.4778\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.2547 - val_loss: 102.2807\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.2582 - val_loss: 97.3086\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.0540 - val_loss: 96.1098\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.1687 - val_loss: 120.7355\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9664 - val_loss: 107.4834\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9208 - val_loss: 98.8883\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 71.9706 - val_loss: 88.4705\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.0625 - val_loss: 100.5705\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.0448 - val_loss: 87.7229\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 71.8822 - val_loss: 97.0802\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.1094 - val_loss: 92.2528\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.2657 - val_loss: 91.0128\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9533 - val_loss: 90.5681\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.0423 - val_loss: 110.6920\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9736 - val_loss: 96.4725\n",
            "Epoch 386/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 72.0831 - val_loss: 94.9086\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9825 - val_loss: 97.9754\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.8382 - val_loss: 90.7930\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.9482 - val_loss: 96.6837\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9903 - val_loss: 91.8785\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.0007 - val_loss: 92.5785\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9108 - val_loss: 104.4090\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.1116 - val_loss: 124.2414\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.0035 - val_loss: 111.0081\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.0869 - val_loss: 87.8513\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.8091 - val_loss: 91.9461\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.7788 - val_loss: 87.7169\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9437 - val_loss: 103.1393\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9965 - val_loss: 91.6616\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8989 - val_loss: 92.7888\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.8646 - val_loss: 126.3343\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.9350 - val_loss: 102.7286\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.0499 - val_loss: 92.5265\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9110 - val_loss: 143.3451\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8046 - val_loss: 108.8513\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9036 - val_loss: 129.1304\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8778 - val_loss: 112.6019\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.8752 - val_loss: 97.6493\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.7988 - val_loss: 95.7167\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8652 - val_loss: 93.7179\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.6751 - val_loss: 98.4409\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.7883 - val_loss: 116.1637\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.8667 - val_loss: 202.2313\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.8688 - val_loss: 176.6208\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.0136 - val_loss: 97.3673\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.9524 - val_loss: 106.1170\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6983 - val_loss: 89.5695\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.7911 - val_loss: 98.5554\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.7086 - val_loss: 95.1712\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.7125 - val_loss: 115.1607\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.6518 - val_loss: 90.8403\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.7125 - val_loss: 99.0786\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 71.7222 - val_loss: 138.8097\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.6717 - val_loss: 89.6042\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8019 - val_loss: 87.8376\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 71.7059 - val_loss: 109.9537\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.7813 - val_loss: 102.9095\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.7645 - val_loss: 92.1102\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.7484 - val_loss: 100.3606\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8719 - val_loss: 105.8284\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 71.7242 - val_loss: 86.2663\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6289 - val_loss: 92.2212\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.7579 - val_loss: 137.8347\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.5211 - val_loss: 98.2277\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.7030 - val_loss: 92.1081\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.8078 - val_loss: 90.7010\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.5605 - val_loss: 89.1858\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 71.7713 - val_loss: 102.1681\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.7719 - val_loss: 105.6431\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6880 - val_loss: 95.0138\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.5853 - val_loss: 90.1436\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6221 - val_loss: 148.1837\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.5469 - val_loss: 91.5567\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.6747 - val_loss: 91.4390\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.5674 - val_loss: 92.9055\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.5032 - val_loss: 93.7105\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6101 - val_loss: 90.4190\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.4867 - val_loss: 96.3567\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.7007 - val_loss: 104.8817\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6806 - val_loss: 140.8070\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.4977 - val_loss: 99.4632\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6996 - val_loss: 92.7242\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.4540 - val_loss: 95.1492\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.5647 - val_loss: 104.3177\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6894 - val_loss: 89.1721\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.6096 - val_loss: 90.7479\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.5303 - val_loss: 94.1421\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.4896 - val_loss: 104.0664\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.6522 - val_loss: 93.4843\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.5315 - val_loss: 96.2195\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.6151 - val_loss: 97.8241\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.4740 - val_loss: 118.6032\n",
            "Epoch 463/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 71.6062 - val_loss: 93.2980\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.4652 - val_loss: 99.0870\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6324 - val_loss: 92.8780\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.5276 - val_loss: 89.1454\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.3723 - val_loss: 93.1937\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 71.4234 - val_loss: 104.6755\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.5109 - val_loss: 95.3548\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.4576 - val_loss: 170.5690\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.1898 - val_loss: 93.5226\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.3040 - val_loss: 88.7586\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 71.4272 - val_loss: 102.6493\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6129 - val_loss: 101.5492\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.5758 - val_loss: 90.8221\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.5180 - val_loss: 89.2068\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.5683 - val_loss: 132.3134\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.3465 - val_loss: 163.6877\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.5639 - val_loss: 183.2317\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.5352 - val_loss: 115.2463\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.4048 - val_loss: 94.3310\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.6147 - val_loss: 95.5948\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.4495 - val_loss: 110.2197\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.2788 - val_loss: 96.7914\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.3952 - val_loss: 99.1474\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.4156 - val_loss: 94.9440\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.3940 - val_loss: 105.0177\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.4538 - val_loss: 90.9276\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.3250 - val_loss: 97.2922\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.5185 - val_loss: 115.2360\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 71.2196 - val_loss: 114.7920\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.4039 - val_loss: 92.2426\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.3100 - val_loss: 90.8668\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.5314 - val_loss: 110.9382\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.3768 - val_loss: 102.5667\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.3639 - val_loss: 92.5153\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.3216 - val_loss: 97.1806\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.3246 - val_loss: 93.2025\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 71.4293 - val_loss: 103.7433\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.3973 - val_loss: 95.9559\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6Dc0xVwOZnO",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -1.9911961977427606 \n",
            "MAE:  7.373488518977635 \n",
            "SD:  9.591194913038715\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZLKCzHOZnO",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su2Sj5jZCSwH",
        "outputId": "4a85f233-7730-4d19-a30a-66ad769ace4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 16)                2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 2,465\n",
            "Trainable params: 2,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPRh6v-mCSwH",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 12276.5010 - val_loss: 12194.0078\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 11691.5811 - val_loss: 11373.1299\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 10972.1836 - val_loss: 10243.8682\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 10007.1787 - val_loss: 8934.2158\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 8870.5986 - val_loss: 7758.8008\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 7575.8687 - val_loss: 7732.3022\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 6007.7456 - val_loss: 6388.5752\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 4371.7725 - val_loss: 4465.2373\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 3041.9482 - val_loss: 4358.1675\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 2017.7373 - val_loss: 1949.6033\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 1267.8287 - val_loss: 1454.5522\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 764.1911 - val_loss: 835.9315\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 451.0307 - val_loss: 519.9040\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 276.4271 - val_loss: 931.2005\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 183.6656 - val_loss: 636.2595\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 141.7783 - val_loss: 237.4913\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 126.0104 - val_loss: 379.8010\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 117.3351 - val_loss: 176.0065\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 114.0656 - val_loss: 180.1338\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 112.1273 - val_loss: 145.7776\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 110.3371 - val_loss: 187.9581\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 109.5263 - val_loss: 128.7667\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 108.3193 - val_loss: 285.1836\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 107.8764 - val_loss: 201.8959\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 107.4558 - val_loss: 158.4447\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 106.7974 - val_loss: 141.3120\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 106.3451 - val_loss: 140.7840\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 106.8678 - val_loss: 126.0504\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 105.7527 - val_loss: 114.8608\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 105.3668 - val_loss: 117.4051\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 105.0365 - val_loss: 118.9339\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.1208 - val_loss: 185.4046\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 104.9154 - val_loss: 117.6488\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 104.7494 - val_loss: 122.7519\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 104.0465 - val_loss: 138.1489\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 104.3478 - val_loss: 197.0085\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 103.1404 - val_loss: 121.3713\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 103.2751 - val_loss: 142.5725\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 102.7667 - val_loss: 140.0028\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.6028 - val_loss: 210.2799\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 102.0483 - val_loss: 117.0715\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 102.0190 - val_loss: 119.9850\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 101.7307 - val_loss: 174.9036\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 102.3503 - val_loss: 136.3574\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.9678 - val_loss: 180.0744\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 100.0120 - val_loss: 184.6121\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 100.0853 - val_loss: 209.8168\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 100.1318 - val_loss: 239.1066\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 98.7103 - val_loss: 131.4991\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 98.6757 - val_loss: 115.1032\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 97.7968 - val_loss: 164.5991\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.8269 - val_loss: 116.0680\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 97.6709 - val_loss: 130.5816\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 96.7541 - val_loss: 116.0511\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 96.4899 - val_loss: 142.7515\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 95.7431 - val_loss: 109.7164\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 94.9939 - val_loss: 119.9195\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 94.4850 - val_loss: 114.0274\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 94.4370 - val_loss: 111.8607\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.7052 - val_loss: 121.4988\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 93.4534 - val_loss: 110.8285\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.5495 - val_loss: 146.1809\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 92.8356 - val_loss: 115.4339\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 92.8722 - val_loss: 131.8954\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.6249 - val_loss: 119.9449\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 92.0606 - val_loss: 143.1572\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.5014 - val_loss: 126.2194\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 91.2098 - val_loss: 161.3512\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 90.8013 - val_loss: 132.0663\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 90.5367 - val_loss: 113.2039\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 90.6023 - val_loss: 149.4573\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.0506 - val_loss: 111.8545\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 89.9764 - val_loss: 123.3236\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 89.7333 - val_loss: 108.1889\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 89.0232 - val_loss: 124.6369\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 89.2276 - val_loss: 116.2514\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.6587 - val_loss: 134.1912\n",
            "Epoch 78/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - ETA: 0s - loss: 89.04 - 0s 2ms/step - loss: 88.8097 - val_loss: 104.8531\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 88.3692 - val_loss: 118.5081\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.1891 - val_loss: 117.1721\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 87.7609 - val_loss: 126.6014\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 87.8341 - val_loss: 100.9010\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.1506 - val_loss: 108.1230\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 87.4499 - val_loss: 115.1110\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.2578 - val_loss: 123.2181\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 86.8045 - val_loss: 110.1174\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 86.6052 - val_loss: 107.9953\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.8525 - val_loss: 107.7831\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 86.2835 - val_loss: 102.1712\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.1847 - val_loss: 104.2822\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 85.5944 - val_loss: 128.2511\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 85.7853 - val_loss: 116.5675\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.6545 - val_loss: 100.7096\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 85.8046 - val_loss: 104.4301\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.3039 - val_loss: 112.0060\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 85.2260 - val_loss: 114.8496\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 84.9445 - val_loss: 117.6732\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.6806 - val_loss: 145.7157\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 84.7804 - val_loss: 103.5000\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.5559 - val_loss: 112.8993\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 84.1294 - val_loss: 104.0518\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 84.4660 - val_loss: 130.4414\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.1927 - val_loss: 108.1311\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 83.7886 - val_loss: 106.5066\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.9504 - val_loss: 109.4360\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 83.5831 - val_loss: 117.9759\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 83.7358 - val_loss: 114.9501\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.3797 - val_loss: 118.2967\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 83.4447 - val_loss: 111.3034\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.9527 - val_loss: 105.8086\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 82.9010 - val_loss: 106.4570\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 83.1064 - val_loss: 112.7175\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.6234 - val_loss: 126.0006\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 82.4363 - val_loss: 105.8661\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.7125 - val_loss: 102.0696\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 82.5215 - val_loss: 113.9336\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 82.2277 - val_loss: 114.4253\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.0569 - val_loss: 128.7065\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 81.9097 - val_loss: 114.0403\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.7252 - val_loss: 101.6153\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 81.7768 - val_loss: 97.8097\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 81.4256 - val_loss: 98.0650\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.3523 - val_loss: 144.4141\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 81.1429 - val_loss: 124.3364\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.0393 - val_loss: 116.3280\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.9369 - val_loss: 102.5427\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.9980 - val_loss: 103.3355\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.0123 - val_loss: 98.8574\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.6377 - val_loss: 108.0209\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.3505 - val_loss: 96.8333\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.4860 - val_loss: 102.9151\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.0850 - val_loss: 138.0559\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.2372 - val_loss: 94.0861\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.0795 - val_loss: 108.6810\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.1979 - val_loss: 123.5633\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.0751 - val_loss: 103.2122\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.9109 - val_loss: 125.6898\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.9011 - val_loss: 109.5085\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.7357 - val_loss: 98.2034\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.7432 - val_loss: 95.7124\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.3544 - val_loss: 126.2385\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.4275 - val_loss: 99.9834\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.4055 - val_loss: 121.5453\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.2343 - val_loss: 94.2769\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.2139 - val_loss: 104.4035\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.2120 - val_loss: 107.1474\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.1487 - val_loss: 100.2959\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.9893 - val_loss: 101.2918\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.9710 - val_loss: 127.4091\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.7238 - val_loss: 93.0725\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.7094 - val_loss: 116.1003\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.7219 - val_loss: 117.0857\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.8483 - val_loss: 121.5668\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.6871 - val_loss: 103.3608\n",
            "Epoch 155/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 78.5502 - val_loss: 98.4823\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.4384 - val_loss: 92.8200\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.4399 - val_loss: 96.8524\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.4816 - val_loss: 115.4764\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.2403 - val_loss: 93.1434\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.3804 - val_loss: 94.9505\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.1315 - val_loss: 105.7039\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.3879 - val_loss: 95.2566\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 78.0591 - val_loss: 100.5667\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.0990 - val_loss: 94.9570\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.9479 - val_loss: 104.8160\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.0789 - val_loss: 99.4884\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.0490 - val_loss: 122.2562\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.9158 - val_loss: 93.9452\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.7972 - val_loss: 102.2183\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.8504 - val_loss: 97.1878\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.8584 - val_loss: 103.7287\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.6465 - val_loss: 129.2191\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.6776 - val_loss: 120.8096\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.9424 - val_loss: 101.6242\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.5344 - val_loss: 106.0146\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.5582 - val_loss: 122.7268\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.4839 - val_loss: 99.8711TA: 0s - loss: 7\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.4457 - val_loss: 107.4303\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.5997 - val_loss: 92.7082\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.4723 - val_loss: 145.9566\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.4285 - val_loss: 97.1246\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.3914 - val_loss: 106.7574\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.4705 - val_loss: 97.8731\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.3522 - val_loss: 90.9082\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.2699 - val_loss: 137.8073\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.3161 - val_loss: 99.2039\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.2412 - val_loss: 104.4858\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.3519 - val_loss: 91.9659\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.4134 - val_loss: 94.3412\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.9829 - val_loss: 128.1075\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.2833 - val_loss: 115.4535\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.2181 - val_loss: 96.2609\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.0503 - val_loss: 101.1155\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.1356 - val_loss: 89.0187\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.8852 - val_loss: 104.8976\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.0570 - val_loss: 97.7733\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.9861 - val_loss: 99.0580\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.1378 - val_loss: 99.8551\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.1052 - val_loss: 177.0511\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.9224 - val_loss: 95.9575\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.8427 - val_loss: 94.9374\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.8661 - val_loss: 194.6209\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.9519 - val_loss: 99.2333\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.8279 - val_loss: 117.4526\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.6723 - val_loss: 112.6547\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 76.6762 - val_loss: 109.4882\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.6273 - val_loss: 95.0801\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.7987 - val_loss: 100.3478\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.6645 - val_loss: 95.0825\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.5345 - val_loss: 128.9535\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.6843 - val_loss: 91.6455\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.5268 - val_loss: 97.4889\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.7125 - val_loss: 108.9492\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.5370 - val_loss: 90.6448\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.4965 - val_loss: 123.2876\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 76.3446 - val_loss: 95.8466\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.3810 - val_loss: 132.9025\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.5116 - val_loss: 108.3315\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.6108 - val_loss: 90.2618\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.4377 - val_loss: 97.0824\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.2424 - val_loss: 96.2055\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.1813 - val_loss: 94.6889\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.3312 - val_loss: 95.6842\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.2500 - val_loss: 98.6023\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.1803 - val_loss: 90.9594\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.3389 - val_loss: 104.9794\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.1145 - val_loss: 120.6648\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.2891 - val_loss: 139.5704\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.0753 - val_loss: 146.6513\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.9729 - val_loss: 120.3130\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.2205 - val_loss: 103.1295\n",
            "Epoch 232/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 76.1132 - val_loss: 166.1846\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.9742 - val_loss: 94.7477\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.2743 - val_loss: 94.8386\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.7933 - val_loss: 89.9870\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.8448 - val_loss: 95.0063\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.7913 - val_loss: 100.8052\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.8457 - val_loss: 100.4700\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.8798 - val_loss: 89.6167\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.1078 - val_loss: 111.5428\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.7182 - val_loss: 94.1058\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.7328 - val_loss: 96.0141\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.6409 - val_loss: 94.0964\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.5985 - val_loss: 104.4319\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.5887 - val_loss: 104.0236\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.5702 - val_loss: 96.3085\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 75.4949 - val_loss: 108.0312\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.5527 - val_loss: 98.8828\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.4632 - val_loss: 116.5488\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.3962 - val_loss: 137.9434\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.5030 - val_loss: 100.9664\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.4498 - val_loss: 93.4608\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.5158 - val_loss: 113.7111\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.4136 - val_loss: 106.6674\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.2331 - val_loss: 99.9610\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.5999 - val_loss: 95.1872\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.2365 - val_loss: 104.9186\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.2823 - val_loss: 141.7077\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.3163 - val_loss: 91.8865\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.3844 - val_loss: 100.7399\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.2503 - val_loss: 94.6658\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.1723 - val_loss: 94.9931\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.2272 - val_loss: 94.0950\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.2628 - val_loss: 93.2001\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.0545 - val_loss: 96.2909\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.1591 - val_loss: 110.9872\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.1527 - val_loss: 94.8100\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.0434 - val_loss: 101.5067\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.1919 - val_loss: 113.7817\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.1636 - val_loss: 96.6325\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.2081 - val_loss: 102.3639\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.0623 - val_loss: 94.3331\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.9707 - val_loss: 104.4470\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.9576 - val_loss: 96.6110\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 75.0689 - val_loss: 91.3124\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.0638 - val_loss: 93.3154\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.8405 - val_loss: 95.4738\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.9399 - val_loss: 100.2875\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.0422 - val_loss: 98.7701\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.9517 - val_loss: 96.2114\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.9605 - val_loss: 114.6073\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.8506 - val_loss: 102.1011\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.9726 - val_loss: 93.0173\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.8672 - val_loss: 91.2772\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.7323 - val_loss: 97.0477\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.9384 - val_loss: 90.0701\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.9021 - val_loss: 99.8663\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.8313 - val_loss: 96.9288\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.8083 - val_loss: 96.0326\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.7884 - val_loss: 98.8031\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.6324 - val_loss: 99.4215\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.7846 - val_loss: 114.3945\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.7216 - val_loss: 89.5390\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.6883 - val_loss: 87.5265\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.6422 - val_loss: 113.5304\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.7734 - val_loss: 103.1139\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.5960 - val_loss: 102.3324\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.6844 - val_loss: 93.0369\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.7029 - val_loss: 90.2717\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.5233 - val_loss: 97.8330\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.4930 - val_loss: 95.6288\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.5985 - val_loss: 124.7398\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.4727 - val_loss: 92.8765\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.5071 - val_loss: 97.2754\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.5135 - val_loss: 99.0704\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.4119 - val_loss: 91.1717\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.6929 - val_loss: 94.4742\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.4100 - val_loss: 99.0681\n",
            "Epoch 309/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 74.5527 - val_loss: 116.6392\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.6372 - val_loss: 106.8114\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.5728 - val_loss: 100.3516\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.3070 - val_loss: 103.7666\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.4966 - val_loss: 91.2783\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.4221 - val_loss: 109.9205\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.3162 - val_loss: 99.8039\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.4308 - val_loss: 114.1936\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.2863 - val_loss: 88.8145\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.3846 - val_loss: 93.5609\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.2792 - val_loss: 94.0780\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.3655 - val_loss: 99.8205\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.3981 - val_loss: 92.5922\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.4053 - val_loss: 107.0187\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.3584 - val_loss: 93.0706\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.2263 - val_loss: 102.7431\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.2050 - val_loss: 89.8986\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.2287 - val_loss: 92.9132\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1480 - val_loss: 95.9851\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.3391 - val_loss: 99.2298\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1512 - val_loss: 100.9461\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.2974 - val_loss: 146.7919\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.2296 - val_loss: 98.0741\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.3175 - val_loss: 103.0151\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.1305 - val_loss: 88.6580\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.2896 - val_loss: 100.6044\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1614 - val_loss: 98.5144\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.2691 - val_loss: 92.1344\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1443 - val_loss: 103.3322\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.0843 - val_loss: 92.3715\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.0849 - val_loss: 102.7010\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1244 - val_loss: 98.1051\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.2435 - val_loss: 93.2492\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1292 - val_loss: 94.3924\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.1346 - val_loss: 93.4363\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.2205 - val_loss: 94.5923\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9282 - val_loss: 116.8916\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.1092 - val_loss: 91.3984\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9651 - val_loss: 89.2698\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.9298 - val_loss: 99.1081\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9114 - val_loss: 91.8391\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9116 - val_loss: 89.4785\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.9917 - val_loss: 94.9581\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.0505 - val_loss: 97.7425\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.0612 - val_loss: 93.8466\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1283 - val_loss: 100.1866\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9280 - val_loss: 88.3611\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8787 - val_loss: 101.5518\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8900 - val_loss: 93.1244\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8702 - val_loss: 87.1499\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8251 - val_loss: 91.4074\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8350 - val_loss: 106.0832\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8883 - val_loss: 99.9178\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.0428 - val_loss: 92.9698\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.0588 - val_loss: 103.3116\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9020 - val_loss: 108.5580\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8677 - val_loss: 102.1617\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.7563 - val_loss: 98.0217\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9787 - val_loss: 92.9907\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5794 - val_loss: 91.7226\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8127 - val_loss: 90.3940\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7490 - val_loss: 94.0073\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.7757 - val_loss: 97.4927\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.7905 - val_loss: 104.8640\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7508 - val_loss: 94.6986\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8656 - val_loss: 88.3822\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.0681 - val_loss: 104.6020\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8011 - val_loss: 115.2487\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.7112 - val_loss: 88.7819\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8544 - val_loss: 104.5353\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.7851 - val_loss: 97.8867\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.6151 - val_loss: 110.3008\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.7069 - val_loss: 102.6945\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6735 - val_loss: 146.9038\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7865 - val_loss: 91.5472\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6298 - val_loss: 91.4081\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7348 - val_loss: 99.1537\n",
            "Epoch 386/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8506 - val_loss: 105.7306\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.5075 - val_loss: 106.7713\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5263 - val_loss: 95.0501\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6620 - val_loss: 97.2032\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6912 - val_loss: 95.5032\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.6761 - val_loss: 94.0185\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.5996 - val_loss: 92.7302\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.6018 - val_loss: 92.4304\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.5776 - val_loss: 87.8951\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5730 - val_loss: 92.9846\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.5242 - val_loss: 111.6100\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4635 - val_loss: 107.6555\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5422 - val_loss: 92.0059\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.5158 - val_loss: 92.4476\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5429 - val_loss: 88.8449\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6367 - val_loss: 95.0627\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6526 - val_loss: 97.1936\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4814 - val_loss: 91.0110\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6648 - val_loss: 95.3979\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5401 - val_loss: 92.3713\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4638 - val_loss: 91.1032\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3504 - val_loss: 136.8543\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4324 - val_loss: 102.4012\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4457 - val_loss: 105.0628\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4270 - val_loss: 94.7229\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.4623 - val_loss: 91.3467\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3793 - val_loss: 106.0613\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5004 - val_loss: 97.5285\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3142 - val_loss: 93.4259\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3852 - val_loss: 108.6534\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4719 - val_loss: 107.4385\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3571 - val_loss: 114.7754\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4943 - val_loss: 92.7601\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3585 - val_loss: 129.1832\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5050 - val_loss: 91.1291\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2771 - val_loss: 108.3264\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4713 - val_loss: 90.8436\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5275 - val_loss: 110.3953\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4825 - val_loss: 95.1622\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4335 - val_loss: 113.5852\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4937 - val_loss: 97.9015\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4612 - val_loss: 130.3931\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4610 - val_loss: 103.0836\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.5045 - val_loss: 93.3551\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4538 - val_loss: 95.0840\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.4452 - val_loss: 90.0420\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3127 - val_loss: 103.1275\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3865 - val_loss: 88.9706\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4414 - val_loss: 104.0417\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2548 - val_loss: 89.9523\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2197 - val_loss: 93.4517\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2087 - val_loss: 90.4279\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4470 - val_loss: 92.0639\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2575 - val_loss: 90.7976\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3314 - val_loss: 95.9642\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3748 - val_loss: 98.7388\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3511 - val_loss: 100.0934\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3783 - val_loss: 113.5293\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2452 - val_loss: 97.4679\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3801 - val_loss: 96.4804\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3648 - val_loss: 159.3625\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3142 - val_loss: 90.5351\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1417 - val_loss: 91.0218\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2841 - val_loss: 94.9978\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2007 - val_loss: 99.7949\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2009 - val_loss: 97.0678\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1543 - val_loss: 97.3072\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2419 - val_loss: 131.6874\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0844 - val_loss: 100.3130\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3101 - val_loss: 92.4713\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2891 - val_loss: 91.3512\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2258 - val_loss: 92.0857\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1012 - val_loss: 94.7570\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1600 - val_loss: 99.0116\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0212 - val_loss: 89.1069\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0488 - val_loss: 103.7496\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2272 - val_loss: 113.5347\n",
            "Epoch 463/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2940 - val_loss: 89.5446\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1037 - val_loss: 90.3441\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1560 - val_loss: 90.4654\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1640 - val_loss: 93.0299\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.0181 - val_loss: 93.2325\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0498 - val_loss: 108.2597\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.1274 - val_loss: 88.1346\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1194 - val_loss: 93.4646\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1224 - val_loss: 116.6155\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1528 - val_loss: 91.1516\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1720 - val_loss: 94.2316\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2554 - val_loss: 108.5810\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0989 - val_loss: 96.2904\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.0166 - val_loss: 89.6718\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1562 - val_loss: 109.8046\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1315 - val_loss: 92.3737\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0665 - val_loss: 91.9368\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9728 - val_loss: 96.0492\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0476 - val_loss: 143.4968\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0868 - val_loss: 92.7129\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1029 - val_loss: 89.8792\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0832 - val_loss: 99.7532\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0334 - val_loss: 95.7631\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.0942 - val_loss: 89.1938\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9746 - val_loss: 97.8020\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0028 - val_loss: 89.0824\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0516 - val_loss: 99.5251\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0438 - val_loss: 96.4407\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0015 - val_loss: 105.0633\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9967 - val_loss: 111.6599\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.8598 - val_loss: 103.1834\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0673 - val_loss: 97.2977\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0455 - val_loss: 116.2662\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.9540 - val_loss: 97.4088\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0872 - val_loss: 102.9977\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.8723 - val_loss: 138.0863\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0365 - val_loss: 95.9671\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.8244 - val_loss: 91.5549\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -0.19448080246294389 \n",
            "MAE:  7.113953236461245 \n",
            "SD:  9.566452707519256\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "outputId": "4a85f233-7730-4d19-a30a-66ad769ace4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 16)                2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 2,465\n",
            "Trainable params: 2,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXAOqd2CTsa",
        "outputId": "237f72c2-95d3-49c8-8823-acc73fef0c66",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 12205.4600 - val_loss: 12129.6035\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 11586.5645 - val_loss: 11554.4453\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 10761.9053 - val_loss: 9947.3613\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 9665.9268 - val_loss: 8346.6777\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 8323.5420 - val_loss: 6629.5674\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 6836.8560 - val_loss: 5766.0981\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 5339.2642 - val_loss: 4621.0469\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 3957.6868 - val_loss: 3402.8655\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2785.8452 - val_loss: 2535.8979\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 1856.2128 - val_loss: 1962.7787\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1144.9578 - val_loss: 2222.1077\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 586.9357 - val_loss: 1513.5551\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 312.9554 - val_loss: 482.6765\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 187.3756 - val_loss: 399.7771\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 134.0534 - val_loss: 133.1368\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 112.7352 - val_loss: 198.0492\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 103.9084 - val_loss: 120.9442\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 99.7659 - val_loss: 142.7800\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 97.9588 - val_loss: 127.5592\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 96.0952 - val_loss: 130.1491\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 95.5466 - val_loss: 143.0655\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 94.6468 - val_loss: 133.1137\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 93.7399 - val_loss: 145.8349\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 93.0967 - val_loss: 126.4091\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 92.4024 - val_loss: 110.5326\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 92.3475 - val_loss: 122.6854\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 91.9915 - val_loss: 116.5121\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 91.5890 - val_loss: 131.1504\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 91.3386 - val_loss: 149.9115\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 91.3060 - val_loss: 109.9917\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.2786 - val_loss: 149.0238\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 90.3201 - val_loss: 116.1824\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 90.0462 - val_loss: 116.5417\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 89.8074 - val_loss: 124.8441\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 89.5099 - val_loss: 110.1143\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.9967 - val_loss: 111.2991\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 88.7466 - val_loss: 152.5439\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 88.9376 - val_loss: 114.6997\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 88.2470 - val_loss: 117.2814\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 88.0225 - val_loss: 101.1145\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.6748 - val_loss: 104.0617\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 87.5265 - val_loss: 107.8203\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 87.0934 - val_loss: 110.5751\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 86.9433 - val_loss: 110.5602\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 86.4766 - val_loss: 120.2757\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 86.3789 - val_loss: 112.4608\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 86.2264 - val_loss: 107.9715\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.9528 - val_loss: 109.8409\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 85.9374 - val_loss: 107.1781\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 85.7169 - val_loss: 112.8782\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 85.1571 - val_loss: 103.3379\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 84.9885 - val_loss: 110.0967\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.8611 - val_loss: 144.3616\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 84.7759 - val_loss: 110.0579\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.6734 - val_loss: 113.9739\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 84.3315 - val_loss: 126.8302\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 84.7364 - val_loss: 124.7427\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 84.2321 - val_loss: 128.0924\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 84.0813 - val_loss: 121.0534\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.8979 - val_loss: 122.0198\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 83.6781 - val_loss: 115.2056\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 83.6731 - val_loss: 102.9720\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.5609 - val_loss: 149.6626\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 83.3948 - val_loss: 117.0886\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 83.2251 - val_loss: 111.6269\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 82.7908 - val_loss: 99.0280\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 83.0670 - val_loss: 127.7271\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.6583 - val_loss: 99.9957\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 82.6029 - val_loss: 121.4769\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 82.4097 - val_loss: 103.3682\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 82.1082 - val_loss: 111.7531\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 82.0468 - val_loss: 116.8281\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.9376 - val_loss: 109.1464\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 81.7478 - val_loss: 99.5302\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.2640 - val_loss: 109.3141\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 81.4009 - val_loss: 97.9702\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 81.1073 - val_loss: 105.7047\n",
            "Epoch 78/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 81.0210 - val_loss: 153.7087\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.8710 - val_loss: 94.3355\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 81.0846 - val_loss: 119.4199\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.9012 - val_loss: 151.4646\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.3343 - val_loss: 116.8923\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.3819 - val_loss: 156.2021\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.3628 - val_loss: 118.2321\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 80.0503 - val_loss: 123.5429\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 80.0584 - val_loss: 96.2402\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.9436 - val_loss: 116.5950\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.7963 - val_loss: 98.1127\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.8884 - val_loss: 118.5302\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.6934 - val_loss: 247.0207\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.5317 - val_loss: 100.7018\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.3923 - val_loss: 110.4327\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.2787 - val_loss: 101.6583\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.1694 - val_loss: 103.8791\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.3668 - val_loss: 165.2804\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.0592 - val_loss: 105.5581\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 79.0740 - val_loss: 139.3797\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.8649 - val_loss: 97.1061\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.9877 - val_loss: 104.3488\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.9148 - val_loss: 133.7226\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 78.9331 - val_loss: 119.9170\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.7083 - val_loss: 92.1870\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.6189 - val_loss: 95.7477\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.5321 - val_loss: 130.8814\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.4723 - val_loss: 102.1254\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.5660 - val_loss: 100.8061\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.3533 - val_loss: 91.7721\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.1597 - val_loss: 160.7710\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.2499 - val_loss: 92.0813\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.3519 - val_loss: 118.3029\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.2922 - val_loss: 91.6737\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 78.0380 - val_loss: 98.2236\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.0641 - val_loss: 96.3816\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.8954 - val_loss: 100.4051\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 78.0156 - val_loss: 95.4843\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.7679 - val_loss: 96.0760\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.7758 - val_loss: 103.4539\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 77.8683 - val_loss: 102.7095\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.6722 - val_loss: 140.4034\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.7635 - val_loss: 94.6766\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.6370 - val_loss: 121.7539\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.5186 - val_loss: 96.7220\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.4982 - val_loss: 106.3510\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.6264 - val_loss: 93.9189\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.5306 - val_loss: 94.0275\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.4275 - val_loss: 96.5053\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.5513 - val_loss: 100.5710\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 77.1562 - val_loss: 104.2858\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.1785 - val_loss: 113.1128\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.3201 - val_loss: 99.1244\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.1696 - val_loss: 97.3583\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.2989 - val_loss: 105.1762\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.9064 - val_loss: 106.9155\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.2678 - val_loss: 93.2608\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 77.0478 - val_loss: 98.6366\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 77.1019 - val_loss: 142.6442\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.9969 - val_loss: 93.8439\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.8624 - val_loss: 104.2762\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.6582 - val_loss: 132.0835\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.8894 - val_loss: 97.2452\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.6799 - val_loss: 94.6173\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.7837 - val_loss: 109.3331\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.6746 - val_loss: 103.7559\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.8356 - val_loss: 97.4537\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 76.4912 - val_loss: 91.0696\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.5934 - val_loss: 126.5084\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.6650 - val_loss: 109.7961\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.4507 - val_loss: 115.9415\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.5596 - val_loss: 97.9485\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 76.6640 - val_loss: 109.1002\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.4556 - val_loss: 94.5173\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.3230 - val_loss: 104.6149\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.4666 - val_loss: 91.1623\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.2706 - val_loss: 93.1659\n",
            "Epoch 155/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 76.0825 - val_loss: 95.1508\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.3451 - val_loss: 103.5505\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.3860 - val_loss: 98.9801\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.0628 - val_loss: 100.3429\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.1556 - val_loss: 90.8168\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.2190 - val_loss: 96.1091\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.0148 - val_loss: 108.0164\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.1395 - val_loss: 117.9187\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 76.0819 - val_loss: 106.6457\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.0699 - val_loss: 97.1478\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.9160 - val_loss: 117.3398\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.8064 - val_loss: 104.2745\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.9394 - val_loss: 132.0722\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.7702 - val_loss: 156.9992\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 76.0483 - val_loss: 95.7683\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.8812 - val_loss: 114.7507\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.8763 - val_loss: 112.1052\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.6642 - val_loss: 111.9790\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.8379 - val_loss: 99.0839\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.7332 - val_loss: 108.7748\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.8818 - val_loss: 98.9542\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.5752 - val_loss: 119.5078\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.5489 - val_loss: 106.9087\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.6838 - val_loss: 98.9698\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.6156 - val_loss: 96.9305\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.7458 - val_loss: 116.3957\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.7547 - val_loss: 111.9514\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.3230 - val_loss: 89.3349\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.3783 - val_loss: 100.8032\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.3680 - val_loss: 111.4765\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.4181 - val_loss: 104.3747\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.4963 - val_loss: 117.8285\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.4260 - val_loss: 123.5390\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.4869 - val_loss: 98.1072\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.4216 - val_loss: 98.3993\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.3894 - val_loss: 94.1035\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.3174 - val_loss: 97.1790\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.3702 - val_loss: 107.3842\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.2612 - val_loss: 101.9825\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.2144 - val_loss: 120.5134\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 75.1999 - val_loss: 190.6372\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.0965 - val_loss: 117.7976\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.2843 - val_loss: 95.5442\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.3490 - val_loss: 113.1163\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.3798 - val_loss: 132.5594\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 75.2116 - val_loss: 95.1486\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.2332 - val_loss: 93.9940\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.2210 - val_loss: 95.4445\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.1698 - val_loss: 90.3214\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.9266 - val_loss: 98.4359\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.0144 - val_loss: 168.4279\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.0547 - val_loss: 98.5425\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.1011 - val_loss: 98.3557\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.0298 - val_loss: 96.7773\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.0804 - val_loss: 130.6011\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 75.0319 - val_loss: 96.6966\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 75.0354 - val_loss: 96.1303\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.0458 - val_loss: 100.5893\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.0745 - val_loss: 107.0218\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.9689 - val_loss: 169.3393\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 75.0158 - val_loss: 106.0761\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.7777 - val_loss: 97.9191\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 75.0017 - val_loss: 100.8431\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.9311 - val_loss: 103.0858\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.6701 - val_loss: 99.5641\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.9589 - val_loss: 100.0951\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.7935 - val_loss: 96.7953\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.7182 - val_loss: 108.5167\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.8263 - val_loss: 92.9834\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.8338 - val_loss: 126.8357\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.6560 - val_loss: 110.8471\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.7018 - val_loss: 120.8565\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.8812 - val_loss: 108.3564\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.8058 - val_loss: 92.8178\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.9450 - val_loss: 95.4653\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.5836 - val_loss: 168.9846\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.8075 - val_loss: 110.4575\n",
            "Epoch 232/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 74.7430 - val_loss: 98.3814\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.6830 - val_loss: 94.5687\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.4804 - val_loss: 92.3514\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.6934 - val_loss: 101.5260\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.7955 - val_loss: 91.2617\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.6085 - val_loss: 96.9825\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.5698 - val_loss: 108.6959\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.5616 - val_loss: 152.2455\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.5912 - val_loss: 109.4724\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.5159 - val_loss: 101.8021\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.5455 - val_loss: 92.4712\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.5592 - val_loss: 90.7610\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.3585 - val_loss: 119.8009\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.2398 - val_loss: 104.5699\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.4136 - val_loss: 100.3696\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.3596 - val_loss: 92.6402\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.3694 - val_loss: 96.9164\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.4305 - val_loss: 103.7996\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.4827 - val_loss: 101.2211\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.2308 - val_loss: 121.9577\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.5452 - val_loss: 91.2334\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.2670 - val_loss: 94.8581\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.2947 - val_loss: 125.8758\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.3123 - val_loss: 94.4087\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.1955 - val_loss: 130.8292\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.2927 - val_loss: 94.8459\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.2048 - val_loss: 99.2312\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.1386 - val_loss: 107.3384\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.1908 - val_loss: 95.1499\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.0860 - val_loss: 99.4235\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.0374 - val_loss: 90.6318\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.0988 - val_loss: 104.7388\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.9427 - val_loss: 97.1828\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 74.2088 - val_loss: 97.6011\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 74.0744 - val_loss: 98.3047\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.0271 - val_loss: 94.6286\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.9965 - val_loss: 96.9312\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.0421 - val_loss: 107.7230\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.9572 - val_loss: 101.8264\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9292 - val_loss: 100.3337\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8546 - val_loss: 108.5103\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8690 - val_loss: 95.6352\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9422 - val_loss: 102.2131\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8749 - val_loss: 97.3556\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 74.0653 - val_loss: 101.2252\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.9020 - val_loss: 115.5213\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8908 - val_loss: 104.5307\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8818 - val_loss: 89.7081\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7116 - val_loss: 96.3899\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8859 - val_loss: 107.1922\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8408 - val_loss: 100.1627\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.9271 - val_loss: 127.7358\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8023 - val_loss: 173.4691\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.8297 - val_loss: 90.1688\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8694 - val_loss: 92.9000\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7551 - val_loss: 92.9063\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8252 - val_loss: 97.3569\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8591 - val_loss: 110.4884\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.6876 - val_loss: 94.1995\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8139 - val_loss: 105.1491\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7718 - val_loss: 92.6731\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.7647 - val_loss: 90.8871\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8060 - val_loss: 98.5211\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7518 - val_loss: 94.3791\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.8531 - val_loss: 102.8137\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7613 - val_loss: 99.5302\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.9260 - val_loss: 98.0432\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.5576 - val_loss: 129.2528\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.6002 - val_loss: 90.9929\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.6121 - val_loss: 104.5064\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.6266 - val_loss: 99.2087\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.5411 - val_loss: 93.4185\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5347 - val_loss: 104.2773\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.4892 - val_loss: 94.1130\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6093 - val_loss: 96.5843\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5591 - val_loss: 91.5958\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6257 - val_loss: 95.6021\n",
            "Epoch 309/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 73.7156 - val_loss: 118.7659\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6252 - val_loss: 106.4137\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.5574 - val_loss: 106.1909\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5288 - val_loss: 98.0890\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.5124 - val_loss: 101.4138\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5781 - val_loss: 95.2181\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6126 - val_loss: 94.3731\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4298 - val_loss: 107.3155\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5461 - val_loss: 106.6246\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.5835 - val_loss: 106.3087\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4764 - val_loss: 99.6785\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4804 - val_loss: 102.0187\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.3874 - val_loss: 99.8520\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4828 - val_loss: 134.1697\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4004 - val_loss: 93.0275\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4650 - val_loss: 92.8242\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4384 - val_loss: 99.1647\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3652 - val_loss: 103.8948\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3897 - val_loss: 90.9942\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4520 - val_loss: 106.9643\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3100 - val_loss: 114.1643\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4121 - val_loss: 95.2245\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.6296 - val_loss: 94.3522\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3388 - val_loss: 100.5734\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2697 - val_loss: 95.7171\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.3455 - val_loss: 97.6254\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.4955 - val_loss: 95.2746\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3096 - val_loss: 98.1644\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.5341 - val_loss: 119.5484\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2430 - val_loss: 104.1991\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1769 - val_loss: 104.3771\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2134 - val_loss: 102.6765\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3087 - val_loss: 118.5331\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.4446 - val_loss: 113.1481\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3912 - val_loss: 92.0348\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2952 - val_loss: 96.8337\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.1942 - val_loss: 126.9069\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2771 - val_loss: 94.9009\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.2642 - val_loss: 95.0959\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2569 - val_loss: 107.0461\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2868 - val_loss: 110.2614\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2072 - val_loss: 138.9836\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2462 - val_loss: 100.1708\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0877 - val_loss: 111.0287\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.3134 - val_loss: 108.9828\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1278 - val_loss: 95.8852\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9573 - val_loss: 101.4691\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1158 - val_loss: 101.7305\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2885 - val_loss: 99.9782\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.0835 - val_loss: 93.1189\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2853 - val_loss: 123.8252\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0308 - val_loss: 103.4084\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1212 - val_loss: 88.5383\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.2345 - val_loss: 98.1111\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0614 - val_loss: 94.5047\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0492 - val_loss: 95.9600\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.1850 - val_loss: 98.6018\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1253 - val_loss: 103.2343\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0853 - val_loss: 101.2870\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0593 - val_loss: 109.5590\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.2005 - val_loss: 102.7121\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1701 - val_loss: 107.4956\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1086 - val_loss: 95.1607\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1695 - val_loss: 96.5822\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0435 - val_loss: 92.2825\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9788 - val_loss: 91.2467\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0799 - val_loss: 132.4336\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9928 - val_loss: 100.9001\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.0842 - val_loss: 128.0081\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.1427 - val_loss: 105.0849\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1441 - val_loss: 102.2976\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.0829 - val_loss: 110.3310\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0414 - val_loss: 95.1314\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 73.1106 - val_loss: 92.9737\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.8054 - val_loss: 95.5267\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.8860 - val_loss: 91.9781\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0871 - val_loss: 93.2112\n",
            "Epoch 386/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0158 - val_loss: 101.5242\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9453 - val_loss: 104.3501\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0730 - val_loss: 91.5483\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0468 - val_loss: 118.8217\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1427 - val_loss: 104.3522\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9699 - val_loss: 92.1743\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9874 - val_loss: 103.9312\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9717 - val_loss: 134.9202\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0607 - val_loss: 95.2357\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0825 - val_loss: 87.9078\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9216 - val_loss: 93.1679\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.2113 - val_loss: 89.8792\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7514 - val_loss: 110.0510\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9591 - val_loss: 90.5527\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.7764 - val_loss: 90.6217\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9462 - val_loss: 88.3068\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.0869 - val_loss: 102.7384\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7915 - val_loss: 91.0674\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7870 - val_loss: 103.8511\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9190 - val_loss: 101.1607\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7304 - val_loss: 94.5833\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9082 - val_loss: 99.9064\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9003 - val_loss: 97.2274\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.8645 - val_loss: 94.1154\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.8032 - val_loss: 100.7721\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9625 - val_loss: 104.0352\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.8316 - val_loss: 91.3382\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 73.0340 - val_loss: 121.0011\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.8376 - val_loss: 102.2406\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9799 - val_loss: 101.4680\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7041 - val_loss: 94.7189\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7429 - val_loss: 123.0029\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.9278 - val_loss: 91.7098\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 73.1200 - val_loss: 103.2479\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7485 - val_loss: 96.3589\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.8286 - val_loss: 87.8718\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7049 - val_loss: 89.8474\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7883 - val_loss: 93.2129\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7879 - val_loss: 136.8346\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.8322 - val_loss: 104.8666\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.8683 - val_loss: 96.1898\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.8904 - val_loss: 121.4274\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.8048 - val_loss: 106.6255\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.8385 - val_loss: 98.4447\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7395 - val_loss: 117.5295\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7106 - val_loss: 108.7686\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6183 - val_loss: 104.7653\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7123 - val_loss: 109.2330\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.5984 - val_loss: 105.4938\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.5971 - val_loss: 90.5488\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.9325 - val_loss: 94.8090\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7536 - val_loss: 99.2568\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6499 - val_loss: 90.5604\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7829 - val_loss: 99.5540\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7689 - val_loss: 93.4785\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7251 - val_loss: 130.6309\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7093 - val_loss: 97.0208\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6914 - val_loss: 130.4684\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.5892 - val_loss: 87.3813\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7415 - val_loss: 95.0745\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.8136 - val_loss: 93.4273\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7184 - val_loss: 100.9538\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.6762 - val_loss: 99.0710\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.8356 - val_loss: 95.8459\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6855 - val_loss: 96.4578\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7150 - val_loss: 96.2477\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.7269 - val_loss: 92.4225\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6682 - val_loss: 92.4316\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.5934 - val_loss: 107.2394\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6751 - val_loss: 97.3528\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6572 - val_loss: 88.9875\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.5695 - val_loss: 91.6814\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6543 - val_loss: 95.1194\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7859 - val_loss: 95.2630\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4673 - val_loss: 93.8255\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6673 - val_loss: 105.8919\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.8309 - val_loss: 92.0086\n",
            "Epoch 463/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 72.5881 - val_loss: 109.7253\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7282 - val_loss: 93.6029\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6268 - val_loss: 104.4891\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.5651 - val_loss: 88.9016\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.5412 - val_loss: 91.7879\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6372 - val_loss: 101.0338\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6567 - val_loss: 119.2617\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6199 - val_loss: 89.4736\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6908 - val_loss: 97.1059\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6350 - val_loss: 99.2521\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.7346 - val_loss: 109.7056\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.6697 - val_loss: 89.0840\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.7950 - val_loss: 105.6948\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.4094 - val_loss: 94.4618\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4388 - val_loss: 92.4465\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.8115 - val_loss: 93.7122\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4975 - val_loss: 95.2167\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.4713 - val_loss: 101.6759\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.4702 - val_loss: 96.0057\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4850 - val_loss: 90.6438\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.4022 - val_loss: 105.7722\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.5894 - val_loss: 100.3005\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.6024 - val_loss: 101.0187\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.6066 - val_loss: 91.5146\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4972 - val_loss: 89.9311\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.4753 - val_loss: 91.6914\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.5432 - val_loss: 89.7897\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.4905 - val_loss: 87.4136\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.5107 - val_loss: 94.6962\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4394 - val_loss: 94.0097\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.5220 - val_loss: 98.8873\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4971 - val_loss: 89.9811\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.5197 - val_loss: 120.7876\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.4749 - val_loss: 109.0949\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4746 - val_loss: 94.5724\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 72.5756 - val_loss: 113.9049\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 72.4694 - val_loss: 111.1360\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 72.4614 - val_loss: 92.0224\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696v_fuFCTsa",
        "outputId": "417d748f-b74e-45b9-d253-cb21b3fb2dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  2.2460788441501975 \n",
            "MAE:  7.1339241561651985 \n",
            "SD:  9.32617689051801\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "outputId": "1fef52e3-6824-462e-9a1a-04c254f50318"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdZF2osWCUQS",
        "outputId": "1fe5268f-c925-4e2d-930a-b3c27b37b074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  0.020133947981497702 \n",
            "Ensemble_std:  9.49460817035866\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 16)                2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 2,465\n",
            "Trainable params: 2,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 3571.7700 - val_loss: 3500.0105\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 3198.5488 - val_loss: 3102.7871\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2753.0173 - val_loss: 2467.3660\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 2238.5845 - val_loss: 1743.7589\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1701.5342 - val_loss: 1233.4082\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 1192.7629 - val_loss: 740.5782\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 757.6277 - val_loss: 597.8160\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 430.4448 - val_loss: 210.2993\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 221.8742 - val_loss: 134.9277\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 110.1534 - val_loss: 99.7030\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 61.2988 - val_loss: 64.2930\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 43.0081 - val_loss: 43.5012\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.9689 - val_loss: 59.0435\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 35.2346 - val_loss: 45.4033\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.6355 - val_loss: 55.6479\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 34.2569 - val_loss: 44.6184\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 33.8416 - val_loss: 39.5149\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.6554 - val_loss: 39.1512\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 33.3861 - val_loss: 71.6229\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.2438 - val_loss: 56.2930\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 33.0534 - val_loss: 43.9425\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.6837 - val_loss: 40.8264\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.8075 - val_loss: 41.4360\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.4095 - val_loss: 38.0479\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.2869 - val_loss: 47.5581\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.3370 - val_loss: 39.7044\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.0496 - val_loss: 37.6651\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0423 - val_loss: 45.4449\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.9690 - val_loss: 37.8579\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7564 - val_loss: 43.1297\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.7411 - val_loss: 36.2863\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.6796 - val_loss: 38.6111\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5391 - val_loss: 47.0337\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.3658 - val_loss: 38.3983\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3338 - val_loss: 41.2075\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 31.2326 - val_loss: 39.0378\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.0882 - val_loss: 41.5675\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9040 - val_loss: 48.9942\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.8844 - val_loss: 39.8620\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8000 - val_loss: 39.5944\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.7047 - val_loss: 49.4635\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.6071 - val_loss: 40.2524\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.5547 - val_loss: 37.4748\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.4782 - val_loss: 65.6020\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.3792 - val_loss: 36.9474\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.2521 - val_loss: 46.4456\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.2938 - val_loss: 39.0763\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.2947 - val_loss: 40.8792\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.1452 - val_loss: 36.0205\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.0528 - val_loss: 45.9891\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.9477 - val_loss: 48.7450\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.0795 - val_loss: 41.5332\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.0629 - val_loss: 38.1536\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.8402 - val_loss: 39.2228\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.8674 - val_loss: 38.1870\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.7737 - val_loss: 36.2903\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.8617 - val_loss: 37.1941\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.6541 - val_loss: 37.1161\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.7830 - val_loss: 37.3302\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6499 - val_loss: 38.0833\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.7234 - val_loss: 42.3066\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6480 - val_loss: 38.9636\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 29.6732 - val_loss: 39.4557\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.6606 - val_loss: 42.3265\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.5262 - val_loss: 38.5842\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.4668 - val_loss: 35.8859\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.4794 - val_loss: 40.1182\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.4078 - val_loss: 43.3055\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.4309 - val_loss: 38.4259\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.3794 - val_loss: 40.7767\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.3635 - val_loss: 38.6512\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.4332 - val_loss: 38.6104\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.3123 - val_loss: 41.7203\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.3957 - val_loss: 41.2345\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.4410 - val_loss: 44.1682\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 29.2704 - val_loss: 48.5553\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.2725 - val_loss: 38.9378\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 29.2031 - val_loss: 36.5513\n",
            "Epoch 79/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 29.2797 - val_loss: 41.0449\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.3126 - val_loss: 36.4876\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.1773 - val_loss: 40.6301\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.3376 - val_loss: 47.8190\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 29.2109 - val_loss: 45.9221\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.2345 - val_loss: 37.1258\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.1830 - val_loss: 36.6141\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.0888 - val_loss: 38.3264\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.1059 - val_loss: 47.4555\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 29.0895 - val_loss: 37.1923\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.1954 - val_loss: 39.9437\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.0685 - val_loss: 44.9558\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.0968 - val_loss: 40.2110\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.1501 - val_loss: 45.8042\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9754 - val_loss: 47.2063\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.0075 - val_loss: 38.3974\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9786 - val_loss: 37.9659\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.0864 - val_loss: 38.4506\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9953 - val_loss: 36.8855\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9655 - val_loss: 51.4263\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9275 - val_loss: 36.8497\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9328 - val_loss: 36.1804\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9283 - val_loss: 44.8207\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9487 - val_loss: 41.9333\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.8974 - val_loss: 37.2905\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9609 - val_loss: 38.6380\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.8842 - val_loss: 36.4003\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.8222 - val_loss: 60.8392\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.8905 - val_loss: 37.9261\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.8508 - val_loss: 41.9448\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7982 - val_loss: 36.2730\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.8870 - val_loss: 42.3405\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7958 - val_loss: 48.4104\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.8284 - val_loss: 38.1701\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7990 - val_loss: 35.7923\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.8684 - val_loss: 35.4529\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7943 - val_loss: 34.8319\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7457 - val_loss: 42.9589\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.8340 - val_loss: 36.2871\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.8907 - val_loss: 35.9115\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.8486 - val_loss: 45.1041\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7948 - val_loss: 55.9692\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7298 - val_loss: 43.1442\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7784 - val_loss: 41.3225\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7546 - val_loss: 37.0392\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7639 - val_loss: 36.2943\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6599 - val_loss: 35.1651\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7205 - val_loss: 48.2431\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6764 - val_loss: 46.3930\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7119 - val_loss: 36.3417\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6822 - val_loss: 41.8206\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5987 - val_loss: 34.6844\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7128 - val_loss: 43.0351\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6085 - val_loss: 37.8122\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5760 - val_loss: 35.9759\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6287 - val_loss: 52.1732\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6382 - val_loss: 51.8308\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6222 - val_loss: 44.4222\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7008 - val_loss: 36.4341\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6067 - val_loss: 49.1392\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6213 - val_loss: 38.8191\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.5611 - val_loss: 37.2404\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5660 - val_loss: 37.2301\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5000 - val_loss: 34.6029\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5705 - val_loss: 36.3614\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5439 - val_loss: 49.7014\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.5245 - val_loss: 40.2810\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5842 - val_loss: 43.1518\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5306 - val_loss: 47.2193\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.6107 - val_loss: 41.0774\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4994 - val_loss: 39.5072\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5035 - val_loss: 35.8965\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5245 - val_loss: 39.1267\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5145 - val_loss: 37.9447\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.4973 - val_loss: 40.1159\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5035 - val_loss: 45.6038\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4949 - val_loss: 47.1299\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5221 - val_loss: 38.6293\n",
            "Epoch 157/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4031 - val_loss: 40.5977\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4175 - val_loss: 35.7540\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4597 - val_loss: 38.7329\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.4416 - val_loss: 39.4672\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4445 - val_loss: 36.0810\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4360 - val_loss: 36.5648\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5181 - val_loss: 35.6912\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4543 - val_loss: 39.0385\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4556 - val_loss: 38.2967\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3968 - val_loss: 39.4907\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.3938 - val_loss: 35.7386\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4597 - val_loss: 51.8393\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4545 - val_loss: 36.0454\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.3610 - val_loss: 41.1471\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5640 - val_loss: 37.7291\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3642 - val_loss: 36.7599\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4231 - val_loss: 36.5962\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3577 - val_loss: 41.6924\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4194 - val_loss: 40.5215\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3987 - val_loss: 38.9914\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4129 - val_loss: 34.4160\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2892 - val_loss: 36.9016\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3220 - val_loss: 49.8962\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3780 - val_loss: 39.5705\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4086 - val_loss: 42.9003\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3100 - val_loss: 35.0256\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2496 - val_loss: 35.7374\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2937 - val_loss: 41.1917\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3175 - val_loss: 34.4357\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3067 - val_loss: 39.2289\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.3757 - val_loss: 45.2910\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2708 - val_loss: 40.1188\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3295 - val_loss: 36.6851\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.3918 - val_loss: 38.0063\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2919 - val_loss: 58.3221\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2501 - val_loss: 34.8899\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2850 - val_loss: 37.4047\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2391 - val_loss: 37.5012\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2963 - val_loss: 40.4015\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2802 - val_loss: 45.5913\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2287 - val_loss: 39.0218\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1796 - val_loss: 36.4095\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2185 - val_loss: 35.6818\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2941 - val_loss: 35.5557\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2187 - val_loss: 54.8543\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.2737 - val_loss: 44.5404\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1951 - val_loss: 37.6054\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2487 - val_loss: 44.8659\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1596 - val_loss: 35.8029\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1820 - val_loss: 44.6387\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2507 - val_loss: 44.2484\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2800 - val_loss: 36.7537\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1464 - val_loss: 42.2324\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2074 - val_loss: 41.9626\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2136 - val_loss: 49.4740\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2123 - val_loss: 37.0501\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2406 - val_loss: 38.1597\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1658 - val_loss: 36.1319\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2137 - val_loss: 35.6870\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1954 - val_loss: 35.1228\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1979 - val_loss: 46.9089\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1490 - val_loss: 36.4434\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.1651 - val_loss: 37.9238\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1627 - val_loss: 36.8223\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1674 - val_loss: 52.6667\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1835 - val_loss: 41.0708\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1201 - val_loss: 39.9881\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1096 - val_loss: 37.6823\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0728 - val_loss: 43.1509\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0860 - val_loss: 36.0966\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.0650 - val_loss: 38.5763\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0899 - val_loss: 41.5180\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0778 - val_loss: 35.0088\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1170 - val_loss: 38.3666\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0994 - val_loss: 37.9770\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0925 - val_loss: 35.9109\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1117 - val_loss: 40.8096\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0364 - val_loss: 43.1586\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 235/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0402 - val_loss: 37.6317\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1100 - val_loss: 40.3726\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0096 - val_loss: 35.1326\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0778 - val_loss: 37.6446\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0102 - val_loss: 40.3956\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0797 - val_loss: 43.9942\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0706 - val_loss: 35.6882\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1381 - val_loss: 44.1855\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0556 - val_loss: 35.5408\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0364 - val_loss: 38.9552\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0701 - val_loss: 38.0070\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0631 - val_loss: 41.6259\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9717 - val_loss: 34.3706\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0553 - val_loss: 36.6879\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9847 - val_loss: 43.3498\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0203 - val_loss: 36.1281\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0628 - val_loss: 38.1902\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0518 - val_loss: 37.0529\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9682 - val_loss: 35.1353\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0198 - val_loss: 40.2975\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0330 - val_loss: 34.0965\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9908 - val_loss: 39.9573\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9345 - val_loss: 41.4628\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0291 - val_loss: 37.2243\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9748 - val_loss: 53.5773\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9349 - val_loss: 36.2738\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9547 - val_loss: 36.1225\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9999 - val_loss: 34.9859\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0012 - val_loss: 38.3199\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.9535 - val_loss: 37.0040\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9054 - val_loss: 37.1492\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.0022 - val_loss: 34.2125\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9248 - val_loss: 40.2149\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9325 - val_loss: 49.4616\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9873 - val_loss: 41.0337\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9370 - val_loss: 40.1645\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0303 - val_loss: 34.2982\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9538 - val_loss: 63.7514\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9535 - val_loss: 47.4569\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9565 - val_loss: 42.7288\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9734 - val_loss: 40.3409\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8610 - val_loss: 37.8116\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9386 - val_loss: 43.4292\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9393 - val_loss: 40.4879\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9194 - val_loss: 38.0727\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9480 - val_loss: 37.2787\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9690 - val_loss: 38.6274\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9406 - val_loss: 34.7518\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9517 - val_loss: 36.2147\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8809 - val_loss: 36.5679\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8988 - val_loss: 46.1860\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9427 - val_loss: 43.4708\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8586 - val_loss: 34.2234\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9222 - val_loss: 35.5276\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8992 - val_loss: 39.3881\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8165 - val_loss: 42.3094\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8813 - val_loss: 39.4236\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8962 - val_loss: 39.4923\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8328 - val_loss: 35.5652\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9105 - val_loss: 39.5260\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9406 - val_loss: 38.7252\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8954 - val_loss: 35.5950\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8845 - val_loss: 35.5121\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8972 - val_loss: 37.4408\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8544 - val_loss: 37.1881\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9190 - val_loss: 36.7884\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8786 - val_loss: 39.6191\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8883 - val_loss: 40.9733\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8829 - val_loss: 34.4434\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8381 - val_loss: 38.7477\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9064 - val_loss: 37.4964\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8409 - val_loss: 38.9896\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8554 - val_loss: 46.7794\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8442 - val_loss: 36.7464\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8349 - val_loss: 39.6461\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7947 - val_loss: 55.2971\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8254 - val_loss: 36.5739\n",
            "Epoch 312/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7607 - val_loss: 41.0212\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8485 - val_loss: 46.9502\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.8989 - val_loss: 38.1540\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8275 - val_loss: 40.2108\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8021 - val_loss: 35.7999\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8802 - val_loss: 40.2093\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8306 - val_loss: 33.9763\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8108 - val_loss: 40.1932\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7963 - val_loss: 41.4746\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8144 - val_loss: 40.6750\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8256 - val_loss: 33.3528\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8362 - val_loss: 35.3499\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8018 - val_loss: 38.8801\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8053 - val_loss: 36.6720\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8264 - val_loss: 38.9630\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7988 - val_loss: 37.5249\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8531 - val_loss: 45.5277\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8378 - val_loss: 35.8111\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8178 - val_loss: 35.5448\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7964 - val_loss: 48.6123\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8035 - val_loss: 47.0529\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8002 - val_loss: 52.1660\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7880 - val_loss: 36.2417\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7992 - val_loss: 39.7053\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7906 - val_loss: 37.9058\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7742 - val_loss: 41.1923\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7970 - val_loss: 43.8250\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8019 - val_loss: 34.2726\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7927 - val_loss: 37.0192\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.8157 - val_loss: 36.6437\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7764 - val_loss: 34.7918\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7631 - val_loss: 39.5799\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8244 - val_loss: 39.3901\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8304 - val_loss: 36.2696\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.7994 - val_loss: 34.5867\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7750 - val_loss: 35.4396\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7589 - val_loss: 34.7535\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7351 - val_loss: 37.5204\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8017 - val_loss: 35.0374\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8313 - val_loss: 33.8217\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8058 - val_loss: 35.6525\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7592 - val_loss: 39.9937\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7752 - val_loss: 37.1623\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7916 - val_loss: 37.1808\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.7795 - val_loss: 36.3202\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7791 - val_loss: 39.9471\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7681 - val_loss: 38.5453\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7422 - val_loss: 35.6727\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7514 - val_loss: 34.7924\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7744 - val_loss: 46.0517\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7229 - val_loss: 49.5364\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7430 - val_loss: 35.5961\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6934 - val_loss: 37.9894\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7911 - val_loss: 36.3454\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7158 - val_loss: 35.9240\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7457 - val_loss: 39.1487\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7828 - val_loss: 35.0318\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7315 - val_loss: 34.9032\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7283 - val_loss: 38.6522\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7932 - val_loss: 36.7821\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7263 - val_loss: 39.8137\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7769 - val_loss: 36.8844\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7068 - val_loss: 35.7428\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7637 - val_loss: 35.8424\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7738 - val_loss: 35.4684\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6838 - val_loss: 34.8964\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6941 - val_loss: 44.7864\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7385 - val_loss: 36.2043\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7823 - val_loss: 34.2140\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6673 - val_loss: 37.9942\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6774 - val_loss: 35.9837\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7343 - val_loss: 34.7196\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7012 - val_loss: 39.8952\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6884 - val_loss: 36.4899\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6818 - val_loss: 40.5177\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7215 - val_loss: 36.3167\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7137 - val_loss: 34.9641\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7243 - val_loss: 34.5935\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 390/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6917 - val_loss: 33.2214\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7034 - val_loss: 44.1848\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8312 - val_loss: 34.6740\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7152 - val_loss: 40.6353\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6618 - val_loss: 38.1647\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6881 - val_loss: 37.4231\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7040 - val_loss: 37.3842\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7609 - val_loss: 35.2217\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6987 - val_loss: 35.6239\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6802 - val_loss: 36.3661\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7116 - val_loss: 42.9180\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7177 - val_loss: 36.7563\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6857 - val_loss: 34.8073\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6939 - val_loss: 38.1715\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6517 - val_loss: 50.9415\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6968 - val_loss: 36.7773\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6567 - val_loss: 34.2784\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7321 - val_loss: 36.3671\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6659 - val_loss: 34.4915\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7019 - val_loss: 34.8494\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7194 - val_loss: 36.6416\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6715 - val_loss: 34.7011\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6767 - val_loss: 36.3498\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6725 - val_loss: 35.2081\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6205 - val_loss: 35.4892\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6742 - val_loss: 34.7829\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6488 - val_loss: 34.5130\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6543 - val_loss: 35.2329\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.6437 - val_loss: 36.4463\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6527 - val_loss: 34.8288\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7215 - val_loss: 37.3105\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7423 - val_loss: 35.2048\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6623 - val_loss: 35.0799\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6429 - val_loss: 34.0349\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6145 - val_loss: 34.1577\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6071 - val_loss: 35.3675\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6878 - val_loss: 38.2724\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6461 - val_loss: 37.7269\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7543 - val_loss: 50.0598\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6707 - val_loss: 38.3081\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6532 - val_loss: 34.9624\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6295 - val_loss: 36.7606\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7148 - val_loss: 33.2223\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6604 - val_loss: 35.3520\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7151 - val_loss: 41.9459\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6848 - val_loss: 34.9302\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6067 - val_loss: 39.9972\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.6884 - val_loss: 34.8603\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6462 - val_loss: 34.6330\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6279 - val_loss: 40.0583\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.6512 - val_loss: 40.6198\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6593 - val_loss: 35.0666\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6574 - val_loss: 36.4290\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7051 - val_loss: 36.9075\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6517 - val_loss: 43.2994\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6225 - val_loss: 37.6593\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6233 - val_loss: 37.3263\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6076 - val_loss: 34.9143\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6283 - val_loss: 44.0901\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7137 - val_loss: 38.5765\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6223 - val_loss: 34.9241\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6067 - val_loss: 35.1018\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6544 - val_loss: 37.8755\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6848 - val_loss: 41.8970\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6375 - val_loss: 35.4817\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6402 - val_loss: 38.8582\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7124 - val_loss: 36.1505\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6083 - val_loss: 34.8100\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7055 - val_loss: 40.0379\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6557 - val_loss: 38.9015\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6451 - val_loss: 51.3943\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6618 - val_loss: 40.4723\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7092 - val_loss: 37.5372\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6207 - val_loss: 37.9449\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6872 - val_loss: 35.5156\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7003 - val_loss: 35.7196\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6474 - val_loss: 33.4784\n",
            "Epoch 467/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6757 - val_loss: 36.3779\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7029 - val_loss: 34.5045\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6511 - val_loss: 37.0812\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6560 - val_loss: 34.0656\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6366 - val_loss: 37.1430\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5398 - val_loss: 39.7835\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6081 - val_loss: 47.0567\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6562 - val_loss: 33.8802\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6198 - val_loss: 44.3605\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6467 - val_loss: 35.0923\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6625 - val_loss: 34.4961\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6264 - val_loss: 34.0744\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6474 - val_loss: 45.5363\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6056 - val_loss: 34.5983\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6751 - val_loss: 39.4473\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.6629 - val_loss: 39.3119\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6169 - val_loss: 41.2852\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6323 - val_loss: 35.9436\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5880 - val_loss: 35.4495\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5603 - val_loss: 34.1011\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6232 - val_loss: 34.4107\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5691 - val_loss: 35.0720\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5972 - val_loss: 40.7606\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5583 - val_loss: 34.8958\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6683 - val_loss: 34.3647\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.6599 - val_loss: 36.7025\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5377 - val_loss: 38.3709\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6417 - val_loss: 39.1925\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5982 - val_loss: 35.2347\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5487 - val_loss: 34.9893\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6654 - val_loss: 38.9626\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6040 - val_loss: 35.6858\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5789 - val_loss: 35.0149\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5855 - val_loss: 35.7364\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -1.4814873466459089 \n",
            "MAE:  4.463413812768464 \n",
            "SD:  5.791512425866861\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cip38xZOZnV",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 16)                2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 2,465\n",
            "Trainable params: 2,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 3702.6448 - val_loss: 3655.1450\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 3438.6523 - val_loss: 3327.3694\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 3094.8018 - val_loss: 2739.1909\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 2622.8936 - val_loss: 2071.4656\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2070.2371 - val_loss: 1658.5992\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 1515.5818 - val_loss: 1125.0745\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 1025.7052 - val_loss: 651.1761\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 639.1937 - val_loss: 460.5900\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 366.8456 - val_loss: 256.0389\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 197.3136 - val_loss: 135.7292\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 105.5791 - val_loss: 96.6340\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 58.9241 - val_loss: 100.5760\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 42.5668 - val_loss: 60.5747\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 37.3174 - val_loss: 51.5606\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 35.7413 - val_loss: 47.0264\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 35.0605 - val_loss: 40.6550\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 34.9258 - val_loss: 54.4404\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.6019 - val_loss: 45.5685\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 34.3386 - val_loss: 46.9289\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.0831 - val_loss: 43.4702\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 34.0404 - val_loss: 47.5883\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 33.7998 - val_loss: 47.8874\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.7665 - val_loss: 40.2193\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 33.6023 - val_loss: 38.9394\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.3626 - val_loss: 37.5200\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 33.2030 - val_loss: 40.4178\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 32.9088 - val_loss: 78.9072\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.5788 - val_loss: 41.0417\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.3806 - val_loss: 39.8883\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.7245 - val_loss: 40.7767\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.4718 - val_loss: 36.8634\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3536 - val_loss: 38.5457\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.1437 - val_loss: 42.3609\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.0697 - val_loss: 39.6075\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.0703 - val_loss: 39.4059\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.8979 - val_loss: 45.6924\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7294 - val_loss: 45.2865\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.6745 - val_loss: 52.2255\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.6456 - val_loss: 61.0237\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7534 - val_loss: 35.3552\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.3826 - val_loss: 38.2652\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4577 - val_loss: 37.4339\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.9905 - val_loss: 35.9881\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.9109 - val_loss: 39.9309\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8210 - val_loss: 39.5580\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.9314 - val_loss: 36.2021\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9641 - val_loss: 53.9761\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.5809 - val_loss: 38.1095\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.4584 - val_loss: 43.2806\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.3389 - val_loss: 45.1978\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.4551 - val_loss: 46.5408\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.2857 - val_loss: 43.6480\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 30.2047 - val_loss: 41.7911\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.2256 - val_loss: 37.3511\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.3293 - val_loss: 48.8651\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.3281 - val_loss: 40.3077\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.1632 - val_loss: 38.9370\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 30.0608 - val_loss: 37.2961\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.9805 - val_loss: 39.3681\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.0423 - val_loss: 43.6804\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.8857 - val_loss: 46.7270\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.9641 - val_loss: 36.2246\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 29.59 - 0s 2ms/step - loss: 29.6715 - val_loss: 37.7370\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.8653 - val_loss: 38.8445\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6629 - val_loss: 36.0599\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.6507 - val_loss: 48.2932\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.5793 - val_loss: 37.1394\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.5100 - val_loss: 36.3891\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.5242 - val_loss: 36.2330\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.5401 - val_loss: 39.0969\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.3656 - val_loss: 38.3812\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.4280 - val_loss: 47.8591\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.4140 - val_loss: 49.7099\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.3357 - val_loss: 37.0862\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.4003 - val_loss: 44.5983\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.2816 - val_loss: 41.0654\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.2198 - val_loss: 40.2451\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.1425 - val_loss: 34.8738\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 79/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.1781 - val_loss: 36.1261\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.2760 - val_loss: 37.2314\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.1702 - val_loss: 38.3919\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.3114 - val_loss: 34.5937\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.1244 - val_loss: 43.8380\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.1696 - val_loss: 51.1572\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.0695 - val_loss: 43.9356\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9809 - val_loss: 42.3888\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.0247 - val_loss: 40.8266\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9715 - val_loss: 37.0053\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9824 - val_loss: 43.5445\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9110 - val_loss: 36.0393\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.9099 - val_loss: 41.8162\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.8308 - val_loss: 38.3411\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9190 - val_loss: 39.4663\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7946 - val_loss: 42.0188\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9460 - val_loss: 37.1322\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.8165 - val_loss: 40.3216\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7665 - val_loss: 38.0984\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7769 - val_loss: 44.2957\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.6979 - val_loss: 40.3035\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7725 - val_loss: 39.4033\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7255 - val_loss: 36.5043\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7876 - val_loss: 35.7032\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6780 - val_loss: 35.0206\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5632 - val_loss: 38.8918\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.6187 - val_loss: 39.9966\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6022 - val_loss: 42.5547\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6376 - val_loss: 39.6940\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6630 - val_loss: 50.0858\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6471 - val_loss: 35.2506\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.5062 - val_loss: 36.5293\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6784 - val_loss: 41.2972\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5418 - val_loss: 37.7651\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4221 - val_loss: 33.6982\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4248 - val_loss: 34.1218\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5918 - val_loss: 34.0625\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4234 - val_loss: 37.0687\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4580 - val_loss: 37.3359\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4580 - val_loss: 38.1766\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4163 - val_loss: 38.7183\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3977 - val_loss: 37.6131\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4176 - val_loss: 36.2021\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4685 - val_loss: 35.6396\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3529 - val_loss: 41.5747\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4664 - val_loss: 37.0144\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3478 - val_loss: 36.1031\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4269 - val_loss: 38.8822\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4195 - val_loss: 47.3653\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4548 - val_loss: 38.6163\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2911 - val_loss: 36.7718\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.3978 - val_loss: 35.7200\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2412 - val_loss: 36.2331\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3079 - val_loss: 44.2129\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2455 - val_loss: 34.7995\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3488 - val_loss: 50.1585\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2418 - val_loss: 35.5776\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3142 - val_loss: 41.0602\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3206 - val_loss: 44.9782\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2731 - val_loss: 38.4300\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1871 - val_loss: 37.9483\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2185 - val_loss: 39.9285\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2838 - val_loss: 40.6956\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2169 - val_loss: 42.6990\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3408 - val_loss: 34.9608\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2490 - val_loss: 33.7029\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2964 - val_loss: 59.0586\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1666 - val_loss: 39.4040\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2418 - val_loss: 49.4302\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1496 - val_loss: 35.5108\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1144 - val_loss: 37.2727\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2618 - val_loss: 34.5812\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1260 - val_loss: 34.0507\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1540 - val_loss: 38.0384\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0765 - val_loss: 38.6999\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0577 - val_loss: 39.0464\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0653 - val_loss: 36.2130\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1089 - val_loss: 34.6714\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 157/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1153 - val_loss: 42.1323\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1181 - val_loss: 40.2410\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0821 - val_loss: 37.1845\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0237 - val_loss: 39.7116\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0872 - val_loss: 38.5629\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1748 - val_loss: 34.9497\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1311 - val_loss: 33.6233\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1055 - val_loss: 53.2085\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0990 - val_loss: 39.7113\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0560 - val_loss: 35.3680\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.0549 - val_loss: 42.2124\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.0283 - val_loss: 43.1017\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0478 - val_loss: 41.3327\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9907 - val_loss: 35.3655\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0164 - val_loss: 46.4264\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.0623 - val_loss: 38.4337\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0381 - val_loss: 44.4151\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0375 - val_loss: 34.5091\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0153 - val_loss: 35.4698\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9800 - val_loss: 33.0015\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9830 - val_loss: 35.4447\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9703 - val_loss: 32.9772\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9285 - val_loss: 37.5076\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9277 - val_loss: 37.6073\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9352 - val_loss: 36.1044\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8776 - val_loss: 42.0542\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9549 - val_loss: 44.9971\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8943 - val_loss: 33.6185\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9202 - val_loss: 46.50370s - loss: 27.\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8710 - val_loss: 34.1880\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9008 - val_loss: 36.4738\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9566 - val_loss: 40.6964\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8727 - val_loss: 36.1406\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8345 - val_loss: 33.9262\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9256 - val_loss: 36.6391\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8751 - val_loss: 41.1692\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8545 - val_loss: 36.7038\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9275 - val_loss: 38.2609\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8443 - val_loss: 39.3822\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8708 - val_loss: 35.9827\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8379 - val_loss: 36.3798\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8238 - val_loss: 41.6520\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8421 - val_loss: 43.9303\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7806 - val_loss: 37.4138\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8140 - val_loss: 35.6803\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7335 - val_loss: 38.0985\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7941 - val_loss: 33.6006\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7997 - val_loss: 39.5391\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7515 - val_loss: 37.8637\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8933 - val_loss: 36.3839\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7985 - val_loss: 36.3382\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7839 - val_loss: 40.2799\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7348 - val_loss: 42.3991\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7886 - val_loss: 38.7487\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8371 - val_loss: 34.3737\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6766 - val_loss: 37.9878\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7116 - val_loss: 37.8957\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8045 - val_loss: 45.4125\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8024 - val_loss: 37.8696\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7573 - val_loss: 34.9557\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.7032 - val_loss: 34.9130\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7228 - val_loss: 43.9483\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7215 - val_loss: 43.4724\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7389 - val_loss: 33.8145\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7202 - val_loss: 33.2129\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7390 - val_loss: 34.5176\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6947 - val_loss: 37.1268\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6895 - val_loss: 36.4351\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6019 - val_loss: 49.0072\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6696 - val_loss: 40.4317\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6771 - val_loss: 36.7771\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6393 - val_loss: 33.6322\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6858 - val_loss: 41.5510\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.7316 - val_loss: 36.0584\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6150 - val_loss: 33.7592\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5787 - val_loss: 63.1640\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6117 - val_loss: 33.4433\n",
            "Epoch 234/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6207 - val_loss: 34.9258\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6906 - val_loss: 39.1644\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6627 - val_loss: 32.9757\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5788 - val_loss: 37.4581\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6489 - val_loss: 37.3049\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6439 - val_loss: 39.2156\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6446 - val_loss: 33.9185\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5870 - val_loss: 35.6586\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7270 - val_loss: 35.3594\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6296 - val_loss: 36.0167\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5676 - val_loss: 47.7388\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6129 - val_loss: 40.0105\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5961 - val_loss: 34.5665\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6236 - val_loss: 34.4323\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6394 - val_loss: 43.5953\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6534 - val_loss: 44.7389\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5876 - val_loss: 51.7663\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6010 - val_loss: 35.1152\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5600 - val_loss: 45.2405\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6364 - val_loss: 35.4027\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5006 - val_loss: 35.9981\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5427 - val_loss: 35.9267\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5251 - val_loss: 37.4915\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6013 - val_loss: 34.8628\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5374 - val_loss: 34.6807\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6158 - val_loss: 36.9966\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4996 - val_loss: 37.2222\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5798 - val_loss: 36.0572\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5379 - val_loss: 59.3116\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5211 - val_loss: 46.7708\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5298 - val_loss: 41.3246\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5802 - val_loss: 40.6969\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.5246 - val_loss: 36.0972\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5683 - val_loss: 37.5104\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5074 - val_loss: 38.7129\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4759 - val_loss: 48.3055\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5385 - val_loss: 33.5339\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4815 - val_loss: 36.7126\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4518 - val_loss: 40.5526\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4126 - val_loss: 37.0511\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5561 - val_loss: 36.2301\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5152 - val_loss: 35.0581\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5283 - val_loss: 34.9211\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5694 - val_loss: 33.5867\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4569 - val_loss: 40.6118\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4844 - val_loss: 39.8722\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4592 - val_loss: 35.4173\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4533 - val_loss: 34.8476\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3920 - val_loss: 34.9096\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4646 - val_loss: 45.5576\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4589 - val_loss: 35.9873\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4122 - val_loss: 36.4926\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4521 - val_loss: 48.0804\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4248 - val_loss: 35.7500\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4754 - val_loss: 35.5006\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4191 - val_loss: 34.4108\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4666 - val_loss: 35.9996\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4664 - val_loss: 36.2508\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4080 - val_loss: 40.1086\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5308 - val_loss: 56.8002\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4776 - val_loss: 36.7429\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4372 - val_loss: 38.6949\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4195 - val_loss: 36.1318\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4061 - val_loss: 37.0950\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4551 - val_loss: 36.0493\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4872 - val_loss: 38.4650\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4731 - val_loss: 38.1596\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3817 - val_loss: 36.3654\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3942 - val_loss: 36.4874\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3580 - val_loss: 37.9366\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4555 - val_loss: 35.9611\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3468 - val_loss: 33.5641\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3435 - val_loss: 36.5870\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4288 - val_loss: 35.2700\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.3788 - val_loss: 35.7175\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.4434 - val_loss: 34.5300\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4406 - val_loss: 44.5207\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.4617 - val_loss: 35.0818\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3177 - val_loss: 43.9242\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4408 - val_loss: 37.7671\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3771 - val_loss: 38.1179\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.4192 - val_loss: 36.4866\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3983 - val_loss: 34.0636\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3433 - val_loss: 41.8522\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.4348 - val_loss: 41.1046\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3170 - val_loss: 34.6612\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3222 - val_loss: 37.6465\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4564 - val_loss: 33.2195\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3656 - val_loss: 56.3448\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3736 - val_loss: 36.2722\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3918 - val_loss: 33.6831\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3470 - val_loss: 38.5824\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3855 - val_loss: 37.4803\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2585 - val_loss: 33.7677\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3955 - val_loss: 35.1205\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3289 - val_loss: 34.3803\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3902 - val_loss: 38.5749\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3263 - val_loss: 38.6257\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3460 - val_loss: 37.1684\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3210 - val_loss: 34.8335\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3620 - val_loss: 36.9650\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3855 - val_loss: 33.8829\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3727 - val_loss: 41.4845\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3583 - val_loss: 35.7049\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2872 - val_loss: 37.3193\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2959 - val_loss: 33.5414\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3025 - val_loss: 37.4344\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.4117 - val_loss: 37.3967\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3532 - val_loss: 33.9347\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3552 - val_loss: 54.8854\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3641 - val_loss: 39.1151\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2701 - val_loss: 37.0909\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3216 - val_loss: 36.2005\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3052 - val_loss: 35.0110\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3175 - val_loss: 35.9778\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3340 - val_loss: 37.1996\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2880 - val_loss: 36.7491\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3573 - val_loss: 33.5470\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3094 - val_loss: 33.2044\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3622 - val_loss: 36.3242\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3384 - val_loss: 35.1983\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2431 - val_loss: 35.7974\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3404 - val_loss: 35.7264\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2515 - val_loss: 48.9743\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3347 - val_loss: 37.7922\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2445 - val_loss: 42.6109\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3151 - val_loss: 38.2921\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3685 - val_loss: 35.7685\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3110 - val_loss: 40.4449\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2925 - val_loss: 43.8119\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2434 - val_loss: 35.0348\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2710 - val_loss: 35.1168\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2750 - val_loss: 35.8596\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2398 - val_loss: 44.0934\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3025 - val_loss: 38.1871\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2445 - val_loss: 33.5971\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3220 - val_loss: 34.7783\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2403 - val_loss: 37.2548\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1883 - val_loss: 34.8443\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3867 - val_loss: 40.4981\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2201 - val_loss: 34.6373\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.1972 - val_loss: 34.7726\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2823 - val_loss: 34.8216\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3214 - val_loss: 34.3058\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.3579 - val_loss: 38.0636\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2518 - val_loss: 45.7550\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.2515 - val_loss: 35.3503\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2881 - val_loss: 40.4101\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2634 - val_loss: 38.9547\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2072 - val_loss: 36.9023\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2994 - val_loss: 37.1984\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.2736 - val_loss: 37.9405\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2574 - val_loss: 34.0781\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2737 - val_loss: 34.5399\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2386 - val_loss: 37.6120\n",
            "Epoch 389/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2726 - val_loss: 38.1002\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2598 - val_loss: 39.4904\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2900 - val_loss: 32.8667\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2629 - val_loss: 40.8526\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2051 - val_loss: 33.2176\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2595 - val_loss: 34.8866\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2324 - val_loss: 35.9510\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2593 - val_loss: 34.3773\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3114 - val_loss: 33.3035\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2727 - val_loss: 41.5462\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2235 - val_loss: 40.9475\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2572 - val_loss: 35.5516\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.3105 - val_loss: 33.6052\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.2842 - val_loss: 37.1705\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1886 - val_loss: 35.7659\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1898 - val_loss: 33.1578\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1833 - val_loss: 36.1847\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1951 - val_loss: 35.9295\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2576 - val_loss: 37.4794\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1864 - val_loss: 37.3296\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2605 - val_loss: 36.2581\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2031 - val_loss: 36.6983\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1398 - val_loss: 36.9882\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2327 - val_loss: 35.0128\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2045 - val_loss: 44.5677\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1982 - val_loss: 35.3301\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.2033 - val_loss: 37.3606\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1965 - val_loss: 33.8383\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1164 - val_loss: 34.8835\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2225 - val_loss: 36.1479\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1635 - val_loss: 35.8478\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2254 - val_loss: 40.8802\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2070 - val_loss: 35.1630\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.1636 - val_loss: 33.9990\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.2052 - val_loss: 33.7184\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2352 - val_loss: 43.8638\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2159 - val_loss: 38.7347\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2461 - val_loss: 34.7493\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.2014 - val_loss: 35.5753\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1871 - val_loss: 35.9930\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1880 - val_loss: 36.6757\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1374 - val_loss: 35.3878\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2180 - val_loss: 40.6584\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1648 - val_loss: 36.9939\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2646 - val_loss: 39.9840\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2104 - val_loss: 35.6447\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1360 - val_loss: 37.3325\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2417 - val_loss: 33.3825\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1563 - val_loss: 34.7602\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1724 - val_loss: 36.3073\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2134 - val_loss: 36.0230\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2297 - val_loss: 35.4441\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1680 - val_loss: 38.2318\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1882 - val_loss: 33.9676\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1918 - val_loss: 36.3250\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1586 - val_loss: 34.1682\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2088 - val_loss: 33.2386\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.2310 - val_loss: 40.7405\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1811 - val_loss: 39.9572\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1583 - val_loss: 37.3319\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1062 - val_loss: 39.8502\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1829 - val_loss: 32.4889\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1654 - val_loss: 39.3301\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1707 - val_loss: 35.2305\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1604 - val_loss: 37.9419\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1639 - val_loss: 33.2599\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1249 - val_loss: 35.1158\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1156 - val_loss: 38.2147\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1535 - val_loss: 36.2885\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1835 - val_loss: 33.8845\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1510 - val_loss: 39.4511\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1346 - val_loss: 34.9141\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1211 - val_loss: 38.3462\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.2043 - val_loss: 39.4655\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1513 - val_loss: 38.1258\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1726 - val_loss: 37.6744\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.2103 - val_loss: 36.3903\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1106 - val_loss: 37.9962\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 467/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1201 - val_loss: 34.3364\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.0977 - val_loss: 33.5716\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1691 - val_loss: 35.0799\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1720 - val_loss: 37.6170\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1474 - val_loss: 32.9025\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1587 - val_loss: 35.7971\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1394 - val_loss: 35.4122\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1035 - val_loss: 35.1809\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1399 - val_loss: 37.7384\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1524 - val_loss: 33.9393\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1444 - val_loss: 38.2616\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1346 - val_loss: 34.1779\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1786 - val_loss: 35.4334\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1346 - val_loss: 44.3969\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1325 - val_loss: 35.4901\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1156 - val_loss: 32.6674\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1649 - val_loss: 34.9369\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 27.06 - 2s 10ms/step - loss: 27.0857 - val_loss: 42.1185\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1895 - val_loss: 34.1623\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1901 - val_loss: 41.9504\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1589 - val_loss: 39.9590\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1208 - val_loss: 34.7813\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1068 - val_loss: 35.8129\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.1224 - val_loss: 36.7843\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1250 - val_loss: 42.9439\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 27.16 - 0s 2ms/step - loss: 27.0890 - val_loss: 35.4751\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1242 - val_loss: 34.0277\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.0856 - val_loss: 39.8360\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1210 - val_loss: 39.5387\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1140 - val_loss: 36.5860\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.0900 - val_loss: 36.0597\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.1307 - val_loss: 34.0674\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1131 - val_loss: 45.2670\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.1304 - val_loss: 41.9118\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  2.233998726719733 \n",
            "MAE:  4.802278433061978 \n",
            "SD:  6.076272956767907\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "outputId": "6eb27d39-0cbd-4d30-e47d-82e95f61f8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 16)                2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 2,465\n",
            "Trainable params: 2,401\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "outputId": "b96d94ff-8811-41d2-9d52-2938c3700332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 1s 3ms/step - loss: 3528.2554 - val_loss: 3559.2607\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 3158.3879 - val_loss: 3192.3074\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 2699.4355 - val_loss: 2512.5085\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 2154.3000 - val_loss: 1636.7659\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 1584.6262 - val_loss: 926.4407\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 1058.0571 - val_loss: 617.7585\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 627.4865 - val_loss: 396.6554\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 329.7724 - val_loss: 172.8808\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 159.6567 - val_loss: 63.9267\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 79.3845 - val_loss: 45.6740\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 49.4872 - val_loss: 52.9616\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 39.7473 - val_loss: 46.4744\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.9520 - val_loss: 51.4346\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 36.2503 - val_loss: 49.7155\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 36.0849 - val_loss: 52.9249\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 35.2916 - val_loss: 42.7610\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 34.5361 - val_loss: 43.3379\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 34.2781 - val_loss: 41.8143\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 34.3208 - val_loss: 43.1018\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.7230 - val_loss: 47.6859\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 33.4220 - val_loss: 40.6589\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.4837 - val_loss: 39.5150\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 33.2110 - val_loss: 48.2416\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 33.3543 - val_loss: 37.0723\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 33.1682 - val_loss: 45.6457\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.6171 - val_loss: 44.2277\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.6401 - val_loss: 45.1520\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.4421 - val_loss: 35.6211\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.6245 - val_loss: 41.7223\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 32.3485 - val_loss: 41.4670\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.1954 - val_loss: 39.5417\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.7120 - val_loss: 37.0489\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 32.1028 - val_loss: 41.1924\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.6666 - val_loss: 52.2644\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.4836 - val_loss: 43.6436\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.3330 - val_loss: 49.2788\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.5461 - val_loss: 37.0934\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.5759 - val_loss: 47.4415\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3425 - val_loss: 48.2122\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.4882 - val_loss: 36.6404\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.5283 - val_loss: 46.8059\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.3314 - val_loss: 40.1058\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.2283 - val_loss: 44.6635\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0446 - val_loss: 45.1342\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.1233 - val_loss: 49.1097\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 31.1175 - val_loss: 38.7566\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 31.0510 - val_loss: 57.5221\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.8242 - val_loss: 37.5693\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.8867 - val_loss: 47.5246\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 30.9978 - val_loss: 40.8158\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.8443 - val_loss: 38.2737\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.9178 - val_loss: 52.7939\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.8788 - val_loss: 44.2993\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.6763 - val_loss: 56.9654\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 30.9441 - val_loss: 39.6482\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.8875 - val_loss: 47.4248\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.5389 - val_loss: 37.0093\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 30.5462 - val_loss: 39.0556\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.5718 - val_loss: 38.9876\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.5717 - val_loss: 46.8740\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.3918 - val_loss: 39.3600\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.6805 - val_loss: 51.4853\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.6624 - val_loss: 38.9519\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.4648 - val_loss: 49.0880\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.6156 - val_loss: 38.6477\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.5518 - val_loss: 50.8647\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.4058 - val_loss: 36.0370\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.6543 - val_loss: 40.9247\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.3689 - val_loss: 35.7137\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.3457 - val_loss: 37.2481\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.2695 - val_loss: 36.6157\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.2506 - val_loss: 38.0729\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.4052 - val_loss: 39.7581\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.3491 - val_loss: 55.2648\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.3509 - val_loss: 39.1954\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.1208 - val_loss: 39.2989\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.1739 - val_loss: 39.6409\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.1642 - val_loss: 43.3769\n",
            "Epoch 79/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 2s 10ms/step - loss: 30.2918 - val_loss: 36.0590\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 30.3439 - val_loss: 39.8409\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.1070 - val_loss: 41.1633\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 30.0570 - val_loss: 37.4398\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.1465 - val_loss: 37.6936\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.1217 - val_loss: 53.2076\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.0278 - val_loss: 37.7968\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.0347 - val_loss: 36.7303\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.1784 - val_loss: 50.6683\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 30.0182 - val_loss: 48.8130\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.9526 - val_loss: 37.5677\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.7770 - val_loss: 41.7122\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 30.0075 - val_loss: 33.7725\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.9766 - val_loss: 35.8313\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.9979 - val_loss: 36.9416\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.8052 - val_loss: 39.0941\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.8268 - val_loss: 37.5087\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.7008 - val_loss: 40.8322\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 29.7548 - val_loss: 52.2563\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.7429 - val_loss: 40.5437\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6906 - val_loss: 35.2592\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.7573 - val_loss: 37.5758\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.8340 - val_loss: 52.0817\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.7583 - val_loss: 40.0831\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.5549 - val_loss: 46.6205\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.7061 - val_loss: 38.2084\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.8064 - val_loss: 51.9224\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.5534 - val_loss: 35.3154\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.5289 - val_loss: 38.5560\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.5032 - val_loss: 35.4579\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.6280 - val_loss: 40.3796\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.5286 - val_loss: 51.1087\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.6205 - val_loss: 37.8803\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.5203 - val_loss: 37.9948\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.4416 - val_loss: 36.2518\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.5796 - val_loss: 37.5535\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.4407 - val_loss: 44.1664\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.4890 - val_loss: 42.8890\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.5410 - val_loss: 40.0842\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.4242 - val_loss: 34.5538\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 29.3135 - val_loss: 37.6551\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.4750 - val_loss: 41.0597\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.3354 - val_loss: 43.8786\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.4338 - val_loss: 38.5789\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.4259 - val_loss: 37.4110\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.4346 - val_loss: 42.5298\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.1542 - val_loss: 39.5479\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.4560 - val_loss: 42.1493\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.2699 - val_loss: 42.2943\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.3428 - val_loss: 48.7597\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.4239 - val_loss: 44.9508\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.2555 - val_loss: 35.7407\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.2692 - val_loss: 39.5208\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.2806 - val_loss: 38.8891\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.1614 - val_loss: 38.0706\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 29.1888 - val_loss: 37.1844\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.2891 - val_loss: 47.2324\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.1440 - val_loss: 38.1724\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.0694 - val_loss: 39.5030\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.2137 - val_loss: 40.0962\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.1164 - val_loss: 50.6998\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.1259 - val_loss: 35.2575\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.0030 - val_loss: 52.9829\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.1268 - val_loss: 37.7531\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.1076 - val_loss: 39.7517\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.0606 - val_loss: 33.9306\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9781 - val_loss: 34.4381\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.0062 - val_loss: 37.2002\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.0303 - val_loss: 43.9851\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.0317 - val_loss: 39.5578\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 29.0380 - val_loss: 33.3154\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9730 - val_loss: 34.1020\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.0394 - val_loss: 46.5811\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 29.0416 - val_loss: 38.3537\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.0697 - val_loss: 42.3747\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9448 - val_loss: 42.9565\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 29.0196 - val_loss: 40.0039\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.8783 - val_loss: 45.4499\n",
            "Epoch 157/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9279 - val_loss: 40.6606\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9734 - val_loss: 36.5082\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9771 - val_loss: 38.0961\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.9364 - val_loss: 37.8218\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.8333 - val_loss: 40.7516\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7697 - val_loss: 37.9052\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7726 - val_loss: 34.1981\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9235 - val_loss: 36.0081\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.8293 - val_loss: 35.9146\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.8610 - val_loss: 41.0588\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.8286 - val_loss: 33.5106\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7564 - val_loss: 46.1338\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.8837 - val_loss: 34.6168\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7947 - val_loss: 35.8106\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.8091 - val_loss: 35.1746\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.9244 - val_loss: 35.6820\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7626 - val_loss: 33.3848\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7658 - val_loss: 34.1431\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7503 - val_loss: 37.9868\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7794 - val_loss: 45.4257\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7401 - val_loss: 37.1087\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.8106 - val_loss: 40.9126\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6656 - val_loss: 52.0941\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7378 - val_loss: 34.0933\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.6203 - val_loss: 46.2652\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7728 - val_loss: 44.8092\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7197 - val_loss: 36.1841\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7377 - val_loss: 38.8616\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6979 - val_loss: 33.6958\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7773 - val_loss: 37.7695\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6337 - val_loss: 35.2343\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.7050 - val_loss: 44.0321\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6360 - val_loss: 66.6911\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6674 - val_loss: 46.2151\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6746 - val_loss: 35.9441\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7103 - val_loss: 36.5775\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.5711 - val_loss: 38.0940\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6358 - val_loss: 73.3241\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.7148 - val_loss: 40.6983\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6257 - val_loss: 38.0756\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.6617 - val_loss: 48.2178\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5438 - val_loss: 48.2283\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.6041 - val_loss: 36.0227\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5937 - val_loss: 39.2159\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4696 - val_loss: 43.3889\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5977 - val_loss: 39.0800\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5750 - val_loss: 42.6269\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5404 - val_loss: 36.2309\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4626 - val_loss: 37.5059\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5281 - val_loss: 36.1961\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5985 - val_loss: 33.7272\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.5720 - val_loss: 37.8390\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.5425 - val_loss: 44.6588\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4153 - val_loss: 35.7754\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4299 - val_loss: 37.7639\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4865 - val_loss: 33.9075\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4464 - val_loss: 36.1963\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4489 - val_loss: 39.1294\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.5381 - val_loss: 37.2241\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4644 - val_loss: 40.1506\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4115 - val_loss: 42.3663\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4330 - val_loss: 38.4664\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4420 - val_loss: 35.1884\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4634 - val_loss: 34.0863\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4657 - val_loss: 45.4342\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4793 - val_loss: 35.9048\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3865 - val_loss: 63.2465\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3666 - val_loss: 37.5585\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4404 - val_loss: 36.6354\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3332 - val_loss: 40.6271\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3752 - val_loss: 33.1102\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4211 - val_loss: 36.0714\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4312 - val_loss: 42.3412\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3949 - val_loss: 37.0778\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3611 - val_loss: 33.4185\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4161 - val_loss: 54.8291\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3132 - val_loss: 42.5498\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3221 - val_loss: 52.6203\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3199 - val_loss: 38.7103\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3274 - val_loss: 58.8481\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2904 - val_loss: 33.4635\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.4118 - val_loss: 42.3554\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.4261 - val_loss: 33.2384\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.3933 - val_loss: 37.3309\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.3175 - val_loss: 35.7420\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3294 - val_loss: 42.6319\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.3910 - val_loss: 33.3680\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3065 - val_loss: 46.5138\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.3435 - val_loss: 35.8246\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2296 - val_loss: 34.2966\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3403 - val_loss: 35.3464\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2813 - val_loss: 55.4060\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2647 - val_loss: 55.1855\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1800 - val_loss: 34.2791\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1704 - val_loss: 39.8784\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.3508 - val_loss: 33.0632\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.1900 - val_loss: 37.0723\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2330 - val_loss: 34.9067\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.2915 - val_loss: 52.7168\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2857 - val_loss: 34.8918\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2161 - val_loss: 45.3581\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2305 - val_loss: 41.2748\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2758 - val_loss: 32.9541\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2301 - val_loss: 38.1030\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2809 - val_loss: 38.3601\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1916 - val_loss: 41.4161\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2155 - val_loss: 38.1227\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1581 - val_loss: 54.2111\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2462 - val_loss: 38.2310\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2337 - val_loss: 37.0464\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2104 - val_loss: 32.7255\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1696 - val_loss: 37.6458\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.2491 - val_loss: 35.2141\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2867 - val_loss: 41.3923\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2344 - val_loss: 36.0834\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1979 - val_loss: 38.0712\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1578 - val_loss: 54.9384\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1405 - val_loss: 42.4827\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1028 - val_loss: 44.9533\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1085 - val_loss: 38.9204\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1949 - val_loss: 34.3107\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1633 - val_loss: 34.0627\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0612 - val_loss: 35.8801\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1514 - val_loss: 34.5182\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1318 - val_loss: 44.2752\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1459 - val_loss: 38.4432\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1265 - val_loss: 35.5580\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1084 - val_loss: 35.1571\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2179 - val_loss: 42.7143\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0996 - val_loss: 49.3269\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1665 - val_loss: 47.9039\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.2000 - val_loss: 35.2651\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1013 - val_loss: 34.9113\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.0895 - val_loss: 34.2458\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0616 - val_loss: 37.5759\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.1316 - val_loss: 34.7500\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1314 - val_loss: 35.5623\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1650 - val_loss: 34.5520\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0745 - val_loss: 33.4466\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1312 - val_loss: 37.9425\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.1556 - val_loss: 41.0743\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1061 - val_loss: 38.9975\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1281 - val_loss: 40.3938\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0841 - val_loss: 46.5449\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1064 - val_loss: 39.1418\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9694 - val_loss: 36.6989\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0854 - val_loss: 46.3657\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0726 - val_loss: 46.7404\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1179 - val_loss: 33.6051\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0901 - val_loss: 43.3879\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.0204 - val_loss: 41.6679\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0608 - val_loss: 33.6048\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0817 - val_loss: 39.1950\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0625 - val_loss: 42.7809\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.1085 - val_loss: 40.2923\n",
            "Epoch 312/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0559 - val_loss: 33.1554\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0981 - val_loss: 52.2240\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0557 - val_loss: 43.4275\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.9984 - val_loss: 37.2346\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0558 - val_loss: 33.1798\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9887 - val_loss: 33.0371\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9705 - val_loss: 33.2076\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.0615 - val_loss: 48.7113\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.1244 - val_loss: 34.3688\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0934 - val_loss: 41.3838\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0692 - val_loss: 36.4132\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0190 - val_loss: 34.5094\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.0063 - val_loss: 39.9488\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0223 - val_loss: 33.6024\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0946 - val_loss: 33.5171\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9346 - val_loss: 36.2537\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9506 - val_loss: 33.1587\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9591 - val_loss: 32.5564\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0156 - val_loss: 43.4039\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9929 - val_loss: 36.2119\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 28.0651 - val_loss: 44.6490\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0064 - val_loss: 44.1257\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.9425 - val_loss: 34.7494\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9186 - val_loss: 40.7180\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9635 - val_loss: 37.7405\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9325 - val_loss: 46.0688\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9671 - val_loss: 37.4479\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9512 - val_loss: 33.7288\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9366 - val_loss: 35.2115\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 28.0046 - val_loss: 35.9474\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.0440 - val_loss: 37.0443\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9655 - val_loss: 34.0138\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 28.0147 - val_loss: 44.1855\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9053 - val_loss: 43.2571\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9565 - val_loss: 48.5637\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9243 - val_loss: 38.9040\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9651 - val_loss: 37.2540\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9609 - val_loss: 34.5951\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9276 - val_loss: 36.0240\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9587 - val_loss: 35.6306\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9555 - val_loss: 35.9316\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9690 - val_loss: 40.6828\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.9398 - val_loss: 36.4150\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8564 - val_loss: 42.5455\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9079 - val_loss: 35.9637\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8470 - val_loss: 33.4202\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8754 - val_loss: 39.0563\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8916 - val_loss: 36.4505\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9591 - val_loss: 51.0304\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9879 - val_loss: 44.7636\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9313 - val_loss: 37.5612\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8860 - val_loss: 34.9710\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9058 - val_loss: 42.6552\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8786 - val_loss: 35.1068\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8753 - val_loss: 35.1813\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8695 - val_loss: 37.3314\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9064 - val_loss: 34.7156\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9086 - val_loss: 34.1926\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8908 - val_loss: 42.7471\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9501 - val_loss: 36.9293\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9046 - val_loss: 33.3236\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9096 - val_loss: 35.0602\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8409 - val_loss: 33.9819\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8714 - val_loss: 36.9439\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8499 - val_loss: 33.3547\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7955 - val_loss: 35.5511\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8853 - val_loss: 37.4041\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8362 - val_loss: 36.5992\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8636 - val_loss: 33.3667\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8690 - val_loss: 36.2012\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.8301 - val_loss: 33.9523\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.9415 - val_loss: 38.7286\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8391 - val_loss: 35.6183\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8854 - val_loss: 36.5301\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8347 - val_loss: 42.3488\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8074 - val_loss: 41.7347\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7981 - val_loss: 41.4368\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.8789 - val_loss: 35.1808\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8662 - val_loss: 40.1671\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.7705 - val_loss: 38.6658\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.9015 - val_loss: 46.8358\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8531 - val_loss: 35.7594\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8492 - val_loss: 35.3840\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8048 - val_loss: 41.2547\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.8186 - val_loss: 36.4545\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7869 - val_loss: 36.6314\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8138 - val_loss: 37.0781\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7767 - val_loss: 37.7034\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8248 - val_loss: 36.0444\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7070 - val_loss: 35.4423\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7953 - val_loss: 42.3964\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7386 - val_loss: 34.5562\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.8133 - val_loss: 35.3764\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7735 - val_loss: 34.5321\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.7017 - val_loss: 33.0303\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7199 - val_loss: 40.0860\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7540 - val_loss: 38.1423\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.7802 - val_loss: 36.4922\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7297 - val_loss: 34.9021\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7923 - val_loss: 34.2332\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8355 - val_loss: 42.7544\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7718 - val_loss: 35.1385\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7202 - val_loss: 33.7642\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8191 - val_loss: 34.7070\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - ETA: 0s - loss: 27.72 - 0s 2ms/step - loss: 27.7786 - val_loss: 34.2424\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7600 - val_loss: 39.1492\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.7479 - val_loss: 47.1446\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7935 - val_loss: 34.7965\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.8162 - val_loss: 50.6255\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7868 - val_loss: 43.3502\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7863 - val_loss: 43.2522\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7358 - val_loss: 33.8279\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7744 - val_loss: 33.1633\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6910 - val_loss: 36.4604\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7049 - val_loss: 37.3594\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7563 - val_loss: 33.8328\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.8099 - val_loss: 35.8628\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7395 - val_loss: 36.2115\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6893 - val_loss: 33.6761\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.7115 - val_loss: 34.8802\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7264 - val_loss: 37.4973\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7056 - val_loss: 32.5729\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6914 - val_loss: 44.6024\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6865 - val_loss: 36.3928\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6734 - val_loss: 34.6666\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7263 - val_loss: 33.4201\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7434 - val_loss: 38.9988\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7399 - val_loss: 34.8260\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7921 - val_loss: 35.3124\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7172 - val_loss: 34.8443\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7216 - val_loss: 35.0362\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7350 - val_loss: 33.8198\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7151 - val_loss: 40.7357\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6989 - val_loss: 43.1836\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.6612 - val_loss: 33.8628\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6511 - val_loss: 35.8213\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6614 - val_loss: 40.1169\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7291 - val_loss: 34.5960\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7184 - val_loss: 34.2355\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6917 - val_loss: 36.6114\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7391 - val_loss: 33.9638\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7494 - val_loss: 32.8162\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6793 - val_loss: 43.2044\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6116 - val_loss: 34.0176\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7836 - val_loss: 37.6004\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6930 - val_loss: 41.5958\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6936 - val_loss: 40.8229\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.7520 - val_loss: 36.8771\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6445 - val_loss: 39.7011\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6825 - val_loss: 35.7180\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5647 - val_loss: 33.6845\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7709 - val_loss: 42.6937\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6700 - val_loss: 38.6231\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6802 - val_loss: 33.8881\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6551 - val_loss: 36.2252\n",
            "Epoch 467/500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6699 - val_loss: 42.0197\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.7240 - val_loss: 34.4035\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5849 - val_loss: 33.3509\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6171 - val_loss: 37.8121\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6017 - val_loss: 35.3594\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6445 - val_loss: 34.5697\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6294 - val_loss: 33.2500\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5628 - val_loss: 33.1037\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6200 - val_loss: 35.8926\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6221 - val_loss: 35.3065\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6126 - val_loss: 36.8508\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6507 - val_loss: 37.4562\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6065 - val_loss: 35.0028\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6872 - val_loss: 33.7486\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 0s 3ms/step - loss: 27.6252 - val_loss: 34.9304\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5943 - val_loss: 38.1538\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6248 - val_loss: 33.0220\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5758 - val_loss: 33.4327\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6195 - val_loss: 36.2070\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6174 - val_loss: 32.8660\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6678 - val_loss: 46.0379\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5839 - val_loss: 40.0590\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6485 - val_loss: 34.8299\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6316 - val_loss: 40.7854\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6327 - val_loss: 34.5054\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6411 - val_loss: 43.7562\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6529 - val_loss: 38.8836\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.6177 - val_loss: 35.0482\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5918 - val_loss: 35.5980\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5852 - val_loss: 34.0406\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5790 - val_loss: 38.0097\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.6222 - val_loss: 36.9682\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 0s 2ms/step - loss: 27.5472 - val_loss: 40.5093\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 27.5974 - val_loss: 52.7753\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "outputId": "cf17f313-e200-42cf-b57f-9dc4de402706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ME:  -4.402740282333585 \n",
            "MAE:  5.852789251455497 \n",
            "SD:  5.778513258431223\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "outputId": "8ba7846b-0740-4069-bae5-5fe469ad5510"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "outputId": "b4d9e052-da3c-4251-f8f9-5baebc10edda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ensemble_me:  -1.2167429674199202 \n",
            "Ensemble_std:  5.882099547021997\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "_BP_hv3_7(1).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}