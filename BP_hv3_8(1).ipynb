{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/%08BP_hv3_8(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YTF6cMiY1Hw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiiiBla2-j1S"
      },
      "source": [
        "# batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCoux5AOZnK",
        "outputId": "b5f9a726-4f88-412c-e393-299157b971de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "TensorFlow version :  2.8.2\n",
            "Keras version :  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlHICkovd809",
        "outputId": "e277346e-61f1-4234-9282-668bd6eba1e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러z오기\n",
        "train_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "30bde099-715f-4a99-d8f3-b0a7988884ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "b6f450a2-e760-4647-81aa-3a958ea11ee5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0214804f-a48c-45c4-9d27-c7b867ea6cc9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0214804f-a48c-45c4-9d27-c7b867ea6cc9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0214804f-a48c-45c4-9d27-c7b867ea6cc9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0214804f-a48c-45c4-9d27-c7b867ea6cc9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "5c937364-5f08-4f8c-c134-b7f151ef51e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b42e883-40cb-4d12-8053-d346f3c1dcaa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b42e883-40cb-4d12-8053-d346f3c1dcaa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b42e883-40cb-4d12-8053-d346f3c1dcaa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b42e883-40cb-4d12-8053-d346f3c1dcaa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "outputs": [],
      "source": [
        "#parameter\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptBRJtSOZnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI8SHBwBOZnO",
        "outputId": "21a5bfe9-9696-4318-f367-153034347582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 16)               64        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,137\n",
            "Trainable params: 3,009\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGT6-7NcOZnO",
        "scrolled": true,
        "outputId": "5d934c87-ad62-41c2-e884-3183dbb76e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 5s 6ms/step - loss: 11998.7324 - val_loss: 12059.1641\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11282.1973 - val_loss: 11225.2598\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 9ms/step - loss: 10383.9238 - val_loss: 9828.3447\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 9143.9170 - val_loss: 8416.2422\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 7717.6826 - val_loss: 8217.4355\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6264.2876 - val_loss: 5290.8721\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 4850.5210 - val_loss: 3631.0413\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 3561.8818 - val_loss: 4337.8838\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2478.3059 - val_loss: 2035.2660\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1634.5093 - val_loss: 1022.4335\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1022.6032 - val_loss: 1291.8081\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 616.7618 - val_loss: 540.4155\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 364.9829 - val_loss: 465.2955\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 226.1614 - val_loss: 344.5844\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 156.1675 - val_loss: 247.2838\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 124.1142 - val_loss: 125.3294\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 111.3551 - val_loss: 133.8172\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 105.0894 - val_loss: 205.2181\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 102.5911 - val_loss: 116.1750\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.7306 - val_loss: 158.2489\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 100.1022 - val_loss: 123.4349\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.7353 - val_loss: 108.2070\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 97.8812 - val_loss: 114.6511\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.7669 - val_loss: 111.2011\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.6122 - val_loss: 112.7384\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.8120 - val_loss: 107.5881\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.2607 - val_loss: 116.9264\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.7540 - val_loss: 109.4718\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.7511 - val_loss: 103.9546\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.0450 - val_loss: 136.8043\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.7894 - val_loss: 113.9674\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.2477 - val_loss: 147.9032\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.6076 - val_loss: 132.8744\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.8278 - val_loss: 104.3673\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.3906 - val_loss: 105.4431\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.8319 - val_loss: 137.0282\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.4238 - val_loss: 104.6710\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.2462 - val_loss: 108.1650\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.5237 - val_loss: 106.4336\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.5170 - val_loss: 103.2709\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.2138 - val_loss: 116.6488\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 86.3316 - val_loss: 105.2925\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.0240 - val_loss: 104.1103\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.6355 - val_loss: 111.2620\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4001 - val_loss: 124.2990\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.2596 - val_loss: 111.8856\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.6860 - val_loss: 107.6357\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.7968 - val_loss: 99.7669\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.1158 - val_loss: 103.4055\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.0298 - val_loss: 150.0191\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.6238 - val_loss: 105.9419\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.4941 - val_loss: 116.1557\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.2039 - val_loss: 98.1502\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1385 - val_loss: 99.5222\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.9882 - val_loss: 98.5279\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.8179 - val_loss: 105.9418\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.3124 - val_loss: 108.7293\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.3811 - val_loss: 153.9420\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.0462 - val_loss: 105.4398\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.7361 - val_loss: 100.3693\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.3331 - val_loss: 103.8026\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.5707 - val_loss: 112.8752\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.1069 - val_loss: 94.4571\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.3896 - val_loss: 116.4189\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.0598 - val_loss: 133.6373\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.9474 - val_loss: 97.6926\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 80.5298 - val_loss: 103.5997\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 80.5771 - val_loss: 93.8007\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.7858 - val_loss: 114.4407\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.5589 - val_loss: 99.6119\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.3479 - val_loss: 104.0114\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.9478 - val_loss: 94.1519\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.0252 - val_loss: 95.1301\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.8382 - val_loss: 98.8838\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.8721 - val_loss: 96.1196\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.9119 - val_loss: 96.7369\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.7409 - val_loss: 110.6567\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4651 - val_loss: 102.1823\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.3674 - val_loss: 101.8840\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.0528 - val_loss: 116.4619\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 79.4519 - val_loss: 94.9736\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 79.2954 - val_loss: 100.3806\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.9570 - val_loss: 94.7690\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.9718 - val_loss: 98.0499\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.9941 - val_loss: 94.3318\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.8320 - val_loss: 103.1227\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7530 - val_loss: 117.6694\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.4225 - val_loss: 117.9973\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.7171 - val_loss: 105.9568\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.3751 - val_loss: 109.8091\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.5828 - val_loss: 92.1375\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.3078 - val_loss: 95.4411\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.0144 - val_loss: 110.0850\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.1623 - val_loss: 102.9900\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0275 - val_loss: 100.4841\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.9031 - val_loss: 97.5283\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.9872 - val_loss: 94.2200\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6292 - val_loss: 104.1424\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.7784 - val_loss: 109.6760\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4868 - val_loss: 150.0471\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5863 - val_loss: 146.0611\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3906 - val_loss: 94.9952\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4700 - val_loss: 119.8512\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.3445 - val_loss: 99.1965\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.2303 - val_loss: 111.5905\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9329 - val_loss: 115.2743\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.1560 - val_loss: 99.3284\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.0235 - val_loss: 94.9254\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.0088 - val_loss: 95.1859\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.7488 - val_loss: 95.0409\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5220 - val_loss: 107.9460\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.6208 - val_loss: 103.9428\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.4350 - val_loss: 106.2646\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5613 - val_loss: 108.8529\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5512 - val_loss: 95.3825\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3552 - val_loss: 104.1576\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2522 - val_loss: 101.9615\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0049 - val_loss: 97.2027\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2827 - val_loss: 125.4737\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.1896 - val_loss: 92.5406\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.2435 - val_loss: 107.5758\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0259 - val_loss: 101.4915\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.8595 - val_loss: 101.9900\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9771 - val_loss: 89.2618\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.8484 - val_loss: 92.6909\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.7478 - val_loss: 102.7554\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.6829 - val_loss: 110.4945\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.8481 - val_loss: 95.0619\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5367 - val_loss: 95.0168\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.6036 - val_loss: 110.8671\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.6655 - val_loss: 113.2596\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.3900 - val_loss: 112.0023\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.3834 - val_loss: 110.6940\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5204 - val_loss: 89.8397\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.3637 - val_loss: 101.5371\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.1059 - val_loss: 87.9110\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.1068 - val_loss: 103.2614\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.1437 - val_loss: 97.6126\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.2676 - val_loss: 101.6556\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.1168 - val_loss: 99.8469\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.2289 - val_loss: 100.8206\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.8957 - val_loss: 110.8447\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.9115 - val_loss: 92.7567\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9965 - val_loss: 104.3129\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.3949 - val_loss: 99.9118\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.7378 - val_loss: 96.7943\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.8266 - val_loss: 92.1505\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.8029 - val_loss: 88.2714\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.5926 - val_loss: 89.6572\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.6015 - val_loss: 87.1395\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5337 - val_loss: 91.6589\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6240 - val_loss: 146.0724\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5181 - val_loss: 90.7314\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.8175 - val_loss: 87.5969\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5770 - val_loss: 92.9391\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.6122 - val_loss: 98.2487\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6772 - val_loss: 91.7254\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.3510 - val_loss: 108.0348\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.4154 - val_loss: 96.7368\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2932 - val_loss: 119.8491\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2351 - val_loss: 97.9747\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1104 - val_loss: 105.6388\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2439 - val_loss: 114.1922\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2239 - val_loss: 117.8389\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1453 - val_loss: 90.6165\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.3206 - val_loss: 101.6999\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1133 - val_loss: 118.2488\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0735 - val_loss: 89.5131\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1979 - val_loss: 95.7760\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9248 - val_loss: 92.9894\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9735 - val_loss: 102.3054\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8637 - val_loss: 92.3283\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.8481 - val_loss: 105.3809\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8999 - val_loss: 114.9449\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1402 - val_loss: 91.5487\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8384 - val_loss: 94.4347\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7559 - val_loss: 91.6232\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6702 - val_loss: 103.1984\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8910 - val_loss: 90.9645\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.7581 - val_loss: 99.9298\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6941 - val_loss: 94.0449\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3969 - val_loss: 106.9695\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7056 - val_loss: 96.6340\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4780 - val_loss: 99.2199\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.5824 - val_loss: 90.3858\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5055 - val_loss: 117.4916\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.4955 - val_loss: 90.8210\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5493 - val_loss: 89.4950\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3390 - val_loss: 94.4708\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2856 - val_loss: 95.9754\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3763 - val_loss: 87.2517\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3039 - val_loss: 100.6334\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4221 - val_loss: 114.2264\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3509 - val_loss: 116.8314\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1855 - val_loss: 91.3015\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.0522 - val_loss: 95.8886\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3933 - val_loss: 97.7711\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2607 - val_loss: 101.2263\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2211 - val_loss: 93.8305\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0139 - val_loss: 91.4087\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0091 - val_loss: 90.0107\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.8929 - val_loss: 87.0654\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.9291 - val_loss: 86.5127\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0618 - val_loss: 102.3796\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8558 - val_loss: 90.1080\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0451 - val_loss: 114.2732\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0047 - val_loss: 88.1595\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0638 - val_loss: 109.7085\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7696 - val_loss: 89.1645\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8866 - val_loss: 107.3292\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6299 - val_loss: 124.9530\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7807 - val_loss: 90.3981\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.5507 - val_loss: 92.5111\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7209 - val_loss: 98.0316\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.9183 - val_loss: 93.6553\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7501 - val_loss: 105.1696\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7651 - val_loss: 107.7079\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.7602 - val_loss: 91.4978\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5337 - val_loss: 92.5539\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4896 - val_loss: 99.0559\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5299 - val_loss: 87.4706\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6426 - val_loss: 91.4780\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4320 - val_loss: 101.0684\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6264 - val_loss: 95.8931\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3486 - val_loss: 90.5098\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.5628 - val_loss: 93.5533\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4916 - val_loss: 95.7255\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.4468 - val_loss: 97.4705\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4046 - val_loss: 101.1635\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3531 - val_loss: 110.3429\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4219 - val_loss: 101.2625\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5437 - val_loss: 92.1492\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2144 - val_loss: 90.2715\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2978 - val_loss: 95.6013\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.3508 - val_loss: 89.8021\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.3833 - val_loss: 94.7214\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1958 - val_loss: 93.0257\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4948 - val_loss: 86.9909\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2790 - val_loss: 89.9346\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2946 - val_loss: 95.3489\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2200 - val_loss: 99.6264\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2903 - val_loss: 90.7770\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2178 - val_loss: 97.9316\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2136 - val_loss: 106.6770\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1299 - val_loss: 96.4204\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.1297 - val_loss: 108.1233\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1900 - val_loss: 100.2464\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.1956 - val_loss: 92.7273\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1291 - val_loss: 116.4131\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1216 - val_loss: 97.2555\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0258 - val_loss: 92.5893\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.0082 - val_loss: 91.7715\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2052 - val_loss: 99.2344\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0196 - val_loss: 118.3262\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9501 - val_loss: 108.9077\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9778 - val_loss: 93.2352\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9699 - val_loss: 109.9904\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9748 - val_loss: 94.0972\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.0535 - val_loss: 95.1656\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 71.9214 - val_loss: 102.0342\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 71.9577 - val_loss: 97.3562\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 71.7457 - val_loss: 89.7248\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8632 - val_loss: 95.6922\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7932 - val_loss: 93.4305\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0972 - val_loss: 140.4221\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9073 - val_loss: 102.4342\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8108 - val_loss: 91.5157\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7063 - val_loss: 91.2357\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0084 - val_loss: 95.2527\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8544 - val_loss: 98.3094\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6036 - val_loss: 89.8540\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8575 - val_loss: 107.9904\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8103 - val_loss: 98.7978\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7914 - val_loss: 108.4113\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7523 - val_loss: 92.9007\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9257 - val_loss: 117.5513\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6417 - val_loss: 97.7864\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6754 - val_loss: 103.0573\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8156 - val_loss: 117.8056\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5360 - val_loss: 104.2963\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6428 - val_loss: 107.8478\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5467 - val_loss: 94.1768\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8211 - val_loss: 86.4188\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4882 - val_loss: 96.9958\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7388 - val_loss: 90.1406\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5859 - val_loss: 94.1231\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5016 - val_loss: 95.1409\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6264 - val_loss: 91.0518\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5195 - val_loss: 98.9169\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7200 - val_loss: 89.2685\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4821 - val_loss: 114.1933\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4104 - val_loss: 112.3574\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7218 - val_loss: 98.5691\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4672 - val_loss: 90.9735\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5198 - val_loss: 93.7054\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4463 - val_loss: 99.4696\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5676 - val_loss: 90.8465\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5781 - val_loss: 90.7274\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6184 - val_loss: 116.8317\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6880 - val_loss: 91.9920\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6254 - val_loss: 90.0425\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2646 - val_loss: 101.6791\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5725 - val_loss: 89.8271\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.3081 - val_loss: 97.3543\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.3718 - val_loss: 111.8694\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2706 - val_loss: 101.1307\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4738 - val_loss: 92.5929\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4176 - val_loss: 94.6721\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3525 - val_loss: 90.3222\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5072 - val_loss: 97.3979\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3606 - val_loss: 91.0031\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4636 - val_loss: 89.2132\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.3073 - val_loss: 92.5412\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3466 - val_loss: 116.2537\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4074 - val_loss: 92.1618\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3102 - val_loss: 90.9578\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2928 - val_loss: 106.8502\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1784 - val_loss: 97.5764\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.3468 - val_loss: 90.5139\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4161 - val_loss: 97.6013\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4252 - val_loss: 93.3977\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1661 - val_loss: 97.0827\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2208 - val_loss: 89.6162\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1958 - val_loss: 90.8069\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2737 - val_loss: 105.5139\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3532 - val_loss: 93.8540\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2068 - val_loss: 89.9420\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2768 - val_loss: 101.4847\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1723 - val_loss: 103.3191\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3291 - val_loss: 87.6908\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1795 - val_loss: 97.3571\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1180 - val_loss: 103.5735\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2241 - val_loss: 90.6917\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1752 - val_loss: 93.6773\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1867 - val_loss: 99.3141\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1672 - val_loss: 100.4523\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1140 - val_loss: 107.4960\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1540 - val_loss: 93.3236\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1813 - val_loss: 90.3148\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1484 - val_loss: 98.8308\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2014 - val_loss: 96.3734\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2206 - val_loss: 108.9785\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1915 - val_loss: 93.9706\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1249 - val_loss: 87.8474\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2231 - val_loss: 91.8547\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1453 - val_loss: 91.4762\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2098 - val_loss: 91.3792\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1695 - val_loss: 113.8228\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0623 - val_loss: 106.3640\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9561 - val_loss: 135.0475\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9643 - val_loss: 108.5137\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0338 - val_loss: 101.5863\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1398 - val_loss: 104.8605\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9066 - val_loss: 102.2617\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0687 - val_loss: 92.1053\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9587 - val_loss: 109.6764\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1009 - val_loss: 101.0413\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1137 - val_loss: 107.3421\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0896 - val_loss: 89.4581\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0312 - val_loss: 90.2530\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9541 - val_loss: 100.0718\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9975 - val_loss: 97.6398\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9047 - val_loss: 91.4105\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9402 - val_loss: 90.7924\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7862 - val_loss: 100.8874\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8802 - val_loss: 129.0086\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0563 - val_loss: 96.5017\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9961 - val_loss: 108.6736\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7751 - val_loss: 102.1768\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8263 - val_loss: 91.9695\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9325 - val_loss: 102.9073\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8933 - val_loss: 107.5951\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8394 - val_loss: 89.7797\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7864 - val_loss: 94.2799\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9346 - val_loss: 100.9123\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8790 - val_loss: 113.8829\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0387 - val_loss: 96.2527\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8338 - val_loss: 125.8677\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8053 - val_loss: 92.9303\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8881 - val_loss: 97.9479\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8154 - val_loss: 96.1994\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8830 - val_loss: 97.8153\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0084 - val_loss: 88.6434\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7696 - val_loss: 104.7288\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7855 - val_loss: 91.6661\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9066 - val_loss: 98.9862\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9901 - val_loss: 89.0576\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7576 - val_loss: 91.3095\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7033 - val_loss: 95.0374\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8539 - val_loss: 94.5062\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8118 - val_loss: 92.4360\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7612 - val_loss: 121.4907\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8649 - val_loss: 102.7641\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6949 - val_loss: 97.1194\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7563 - val_loss: 93.9853\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6137 - val_loss: 96.9286\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5829 - val_loss: 93.2454\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7540 - val_loss: 91.0887\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7291 - val_loss: 104.4958\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7655 - val_loss: 97.5328\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6861 - val_loss: 92.3754\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6126 - val_loss: 91.5229\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7185 - val_loss: 121.4747\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7036 - val_loss: 90.6599\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6720 - val_loss: 97.0868\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7185 - val_loss: 89.8351\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6442 - val_loss: 106.8138\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7372 - val_loss: 106.9547\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5596 - val_loss: 108.8922\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6781 - val_loss: 92.5226\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7041 - val_loss: 91.2949\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7636 - val_loss: 102.1672\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6879 - val_loss: 88.9187\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4796 - val_loss: 87.6382\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.4447 - val_loss: 102.9290\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5400 - val_loss: 99.5159\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5807 - val_loss: 93.8892\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5710 - val_loss: 91.1665\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6026 - val_loss: 127.6964\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5882 - val_loss: 92.5878\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6976 - val_loss: 91.4508\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.4749 - val_loss: 131.2800\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8566 - val_loss: 114.4456\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7538 - val_loss: 93.8621\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5274 - val_loss: 88.3546\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6566 - val_loss: 91.8925\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5021 - val_loss: 97.5646\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5841 - val_loss: 91.1204\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7033 - val_loss: 91.5865\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3416 - val_loss: 89.9245\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5024 - val_loss: 91.1230\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6476 - val_loss: 101.5508\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4374 - val_loss: 102.1593\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4688 - val_loss: 92.9095\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.3513 - val_loss: 107.4573\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4295 - val_loss: 91.0559\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.4281 - val_loss: 108.9823\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.3393 - val_loss: 90.9292\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4067 - val_loss: 97.8089\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.3980 - val_loss: 89.3292\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3100 - val_loss: 94.2601\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5848 - val_loss: 93.2115\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6494 - val_loss: 95.7295\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1891 - val_loss: 93.1566\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5017 - val_loss: 91.5342\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5098 - val_loss: 88.2025\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3545 - val_loss: 90.1912\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4484 - val_loss: 105.2623\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3980 - val_loss: 102.4852\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3512 - val_loss: 107.8645\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2848 - val_loss: 102.5056\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3924 - val_loss: 89.6668\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4450 - val_loss: 105.9701\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5404 - val_loss: 131.8550\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4756 - val_loss: 117.7060\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4707 - val_loss: 100.0499\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4217 - val_loss: 93.5155\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4018 - val_loss: 91.9203\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3030 - val_loss: 86.0510\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.3894 - val_loss: 93.7166\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.2592 - val_loss: 89.2025\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4765 - val_loss: 93.0139\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.2886 - val_loss: 99.0080\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2972 - val_loss: 102.3975\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2464 - val_loss: 109.8470\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3767 - val_loss: 114.1769\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3213 - val_loss: 90.2939\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3123 - val_loss: 87.8939\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4164 - val_loss: 93.8319\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4391 - val_loss: 103.2645\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3528 - val_loss: 97.6764\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0471 - val_loss: 109.9394\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2758 - val_loss: 96.1865\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2058 - val_loss: 93.2365\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2766 - val_loss: 93.5738\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1633 - val_loss: 98.7858\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1461 - val_loss: 97.9130\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 69.9854 - val_loss: 95.1538\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3990 - val_loss: 101.0199\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2676 - val_loss: 90.6483\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1723 - val_loss: 98.2139\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4385 - val_loss: 91.5698\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.3533 - val_loss: 100.3696\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.2281 - val_loss: 104.1346\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1877 - val_loss: 112.2303\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3495 - val_loss: 90.5395\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1936 - val_loss: 106.0358\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.2665 - val_loss: 91.8730\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0756 - val_loss: 91.4068\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2099 - val_loss: 92.7672\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1551 - val_loss: 93.6527\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0189 - val_loss: 103.0185\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1252 - val_loss: 97.8301\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1204 - val_loss: 97.5542\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2445 - val_loss: 99.3755\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3312 - val_loss: 121.5417\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0375 - val_loss: 88.6552\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2753 - val_loss: 104.6270\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1879 - val_loss: 98.7894\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1698 - val_loss: 90.9047\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6Dc0xVwOZnO",
        "outputId": "21c1b465-f5dc-4e0a-8795-dae5e6548ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.4600684592615314 \n",
            "MAE:  7.111317227294242 \n",
            "SD:  9.523287937454482\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "qQZLKCzHOZnO",
        "outputId": "bad260df-ceec-4909-f436-4c8d62ce516c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su2Sj5jZCSwH",
        "outputId": "2036da2f-edec-4563-cfc6-61f16faf7737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,137\n",
            "Trainable params: 3,009\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPRh6v-mCSwH",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c33c65-b9e8-48be-f1d9-012122333552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 7ms/step - loss: 12441.4043 - val_loss: 12292.8281\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 12000.6650 - val_loss: 11778.4980\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11546.3242 - val_loss: 11037.6113\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10917.2734 - val_loss: 10005.6309\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10058.8633 - val_loss: 8838.7607\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 8843.6523 - val_loss: 8304.9512\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 7486.9561 - val_loss: 7267.4106\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6128.4131 - val_loss: 6011.9170\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4856.3335 - val_loss: 4575.8726\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 3710.1416 - val_loss: 3474.4492\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2716.7896 - val_loss: 2355.1587\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1904.5846 - val_loss: 1444.7708\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1280.6022 - val_loss: 846.5628\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 827.1281 - val_loss: 684.1441\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 517.4561 - val_loss: 295.2077\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 322.3818 - val_loss: 273.6269\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 205.9523 - val_loss: 137.9407\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 146.9734 - val_loss: 181.8773\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 117.8332 - val_loss: 139.2489\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 104.2687 - val_loss: 138.9444\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 98.5120 - val_loss: 146.2367\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 95.5089 - val_loss: 119.0728\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.3502 - val_loss: 111.0745\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 93.1584 - val_loss: 128.1506\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 92.1684 - val_loss: 115.1049\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 91.4112 - val_loss: 120.8114\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.1789 - val_loss: 118.8304\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.5148 - val_loss: 113.4963\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 90.7335 - val_loss: 124.7424\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.3271 - val_loss: 113.9513\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.0436 - val_loss: 110.2487\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 88.0348 - val_loss: 157.0974\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.8965 - val_loss: 123.2392\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 87.1227 - val_loss: 162.5288\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.0512 - val_loss: 128.4279\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.2391 - val_loss: 126.6484\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.8077 - val_loss: 98.0465\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.4329 - val_loss: 112.3865\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.2265 - val_loss: 117.2202\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 84.8364 - val_loss: 133.3998\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.5264 - val_loss: 102.5287\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.9120 - val_loss: 165.2116\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.8166 - val_loss: 126.8465\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.4232 - val_loss: 102.4688\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1252 - val_loss: 124.4588\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.0677 - val_loss: 109.9723\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.6452 - val_loss: 116.0671\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.7893 - val_loss: 131.2659\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.5807 - val_loss: 105.5390\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.1268 - val_loss: 141.4705\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.7848 - val_loss: 110.9788\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.6186 - val_loss: 112.2919\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.5773 - val_loss: 127.1577\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.4682 - val_loss: 105.0412\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 81.1229 - val_loss: 119.5376\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.8730 - val_loss: 101.9935\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.5800 - val_loss: 108.6407\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.8883 - val_loss: 108.3581\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 80.5459 - val_loss: 175.8075\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 80.4172 - val_loss: 96.8859\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 79.9310 - val_loss: 103.8711\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 79.8277 - val_loss: 103.7556\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 79.5693 - val_loss: 100.5056\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.5380 - val_loss: 104.9908\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 79.2747 - val_loss: 94.7344\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.1654 - val_loss: 103.6222\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.1212 - val_loss: 100.4886\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.9635 - val_loss: 103.7553\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.7254 - val_loss: 98.0462\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.4798 - val_loss: 104.8833\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.5961 - val_loss: 136.2700\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.3763 - val_loss: 113.9692\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0080 - val_loss: 103.3130\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.9900 - val_loss: 109.3787\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.8430 - val_loss: 101.5502\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.7720 - val_loss: 101.9728\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.7784 - val_loss: 89.2953\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.6870 - val_loss: 96.1233\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.6377 - val_loss: 97.6347\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.6367 - val_loss: 113.0592\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3472 - val_loss: 135.1487\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4695 - val_loss: 133.2319\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.5440 - val_loss: 102.0304\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.1816 - val_loss: 100.4887\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.1185 - val_loss: 96.8541\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.8234 - val_loss: 110.8688\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.7253 - val_loss: 97.5846\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.6670 - val_loss: 98.2504\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5852 - val_loss: 114.8816\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.4908 - val_loss: 109.3222\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.7510 - val_loss: 109.2285\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.6862 - val_loss: 99.6206\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.3097 - val_loss: 107.5443\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.5570 - val_loss: 147.1039\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5083 - val_loss: 108.1728\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3178 - val_loss: 100.4064\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1822 - val_loss: 100.6736\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0174 - val_loss: 88.5297\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0499 - val_loss: 101.2825\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1032 - val_loss: 104.1167\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9310 - val_loss: 115.1498\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.6930 - val_loss: 103.4499\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5814 - val_loss: 106.9299\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5367 - val_loss: 124.3823\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5961 - val_loss: 89.6614\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.7144 - val_loss: 113.7524\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.6642 - val_loss: 113.4250\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.3549 - val_loss: 98.4783\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5779 - val_loss: 90.5438\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.2983 - val_loss: 101.1638\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.2664 - val_loss: 119.4737\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.2898 - val_loss: 117.6334\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.1511 - val_loss: 93.7341\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.2585 - val_loss: 111.2462\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.1060 - val_loss: 122.7942\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.9826 - val_loss: 89.1535\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.9197 - val_loss: 102.4867\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9215 - val_loss: 122.8878\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9292 - val_loss: 116.1406\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.8159 - val_loss: 102.7117\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.0071 - val_loss: 116.8139\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.7740 - val_loss: 111.7072\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.6648 - val_loss: 100.6357\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6622 - val_loss: 205.9272\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5316 - val_loss: 104.8767\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.5316 - val_loss: 94.9176\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.5852 - val_loss: 125.1202\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5402 - val_loss: 162.3174\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.5669 - val_loss: 99.2755\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4225 - val_loss: 117.7579\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3791 - val_loss: 146.0490\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4514 - val_loss: 119.8263\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.3259 - val_loss: 118.0209\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.4477 - val_loss: 118.0501\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.3429 - val_loss: 119.1962\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1888 - val_loss: 108.2619\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2084 - val_loss: 89.2838\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2082 - val_loss: 96.1228\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.4854 - val_loss: 97.2550\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1358 - val_loss: 100.2944\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.2522 - val_loss: 98.2138\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.3016 - val_loss: 92.3162\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.1648 - val_loss: 119.8208\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9241 - val_loss: 90.8617\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0273 - val_loss: 108.6223\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.0508 - val_loss: 105.9674\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.0041 - val_loss: 107.2329\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8018 - val_loss: 104.8718\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8213 - val_loss: 107.2829\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.6966 - val_loss: 102.4003\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.9106 - val_loss: 126.4540\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.7911 - val_loss: 105.7958\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9283 - val_loss: 93.0494\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.6556 - val_loss: 108.5737\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8239 - val_loss: 93.2085\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8565 - val_loss: 100.9135\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.5513 - val_loss: 93.1210\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.5544 - val_loss: 88.5749\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5622 - val_loss: 110.2048\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5073 - val_loss: 99.9287\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3840 - val_loss: 101.5503\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2824 - val_loss: 123.5719\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.4174 - val_loss: 93.3059\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2941 - val_loss: 97.1674\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3867 - val_loss: 93.0018\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3247 - val_loss: 92.5126\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3427 - val_loss: 138.8400\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2982 - val_loss: 104.7835\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2199 - val_loss: 103.3641\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2426 - val_loss: 94.1756\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2197 - val_loss: 90.4898\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2383 - val_loss: 99.0730\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1828 - val_loss: 99.3823\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2938 - val_loss: 97.2034\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.9694 - val_loss: 107.2153\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.1117 - val_loss: 99.0421\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0294 - val_loss: 98.0024\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8865 - val_loss: 103.2164\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8067 - val_loss: 99.1422\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.9465 - val_loss: 117.2366\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8440 - val_loss: 125.8543\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1327 - val_loss: 121.5161\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8463 - val_loss: 113.9998\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0345 - val_loss: 98.8602\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8779 - val_loss: 111.6431\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7625 - val_loss: 98.2425\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.7242 - val_loss: 94.2462\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6268 - val_loss: 119.7431\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.7125 - val_loss: 93.5932\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.5459 - val_loss: 97.5934\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6999 - val_loss: 91.9387\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.7643 - val_loss: 91.4579\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5169 - val_loss: 104.4210\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.5883 - val_loss: 96.1312\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5807 - val_loss: 96.3485\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5945 - val_loss: 128.4468\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6928 - val_loss: 121.0291\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5484 - val_loss: 95.5685\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6035 - val_loss: 94.1145\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.5863 - val_loss: 110.8175\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6906 - val_loss: 118.5868\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4555 - val_loss: 95.3374\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4343 - val_loss: 92.7820\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.4124 - val_loss: 108.6451\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5294 - val_loss: 96.8710\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3405 - val_loss: 108.9543\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5571 - val_loss: 93.9405\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4075 - val_loss: 91.0351\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2614 - val_loss: 125.8225\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3316 - val_loss: 105.1058\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2241 - val_loss: 111.7038\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.4146 - val_loss: 143.0666\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.4651 - val_loss: 93.6955\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3158 - val_loss: 101.3369\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4149 - val_loss: 106.3067\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.4268 - val_loss: 122.2046\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2637 - val_loss: 92.2101\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5027 - val_loss: 99.3324\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2779 - val_loss: 93.1893\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.3024 - val_loss: 99.6620\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1475 - val_loss: 93.2154\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2108 - val_loss: 91.0529\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2853 - val_loss: 95.8170\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4531 - val_loss: 132.6177\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.1293 - val_loss: 96.1899\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9694 - val_loss: 115.9982\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0442 - val_loss: 93.6962\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.3832 - val_loss: 106.8531\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9981 - val_loss: 105.5571\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2956 - val_loss: 109.0460\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9626 - val_loss: 96.3839\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2012 - val_loss: 92.6735\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6616 - val_loss: 102.2087\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2417 - val_loss: 100.5927\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1923 - val_loss: 96.7208\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8582 - val_loss: 123.7765\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9531 - val_loss: 91.9096\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8681 - val_loss: 100.0624\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9225 - val_loss: 95.2201\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9080 - val_loss: 108.8014\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8983 - val_loss: 90.9345\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8545 - val_loss: 93.1787\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9929 - val_loss: 88.1576\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7013 - val_loss: 96.7552\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8416 - val_loss: 119.1226\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6873 - val_loss: 105.6440\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8514 - val_loss: 100.9121\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7917 - val_loss: 106.9077\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7236 - val_loss: 98.8973\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7723 - val_loss: 109.1239\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8116 - val_loss: 98.5634\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8027 - val_loss: 89.0961\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8019 - val_loss: 93.2744\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 10ms/step - loss: 71.8004 - val_loss: 99.6260\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 71.7026 - val_loss: 122.5080\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 12ms/step - loss: 71.8179 - val_loss: 100.7824\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 11ms/step - loss: 71.9767 - val_loss: 135.6032\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8216 - val_loss: 93.8709\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8842 - val_loss: 106.2333\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6147 - val_loss: 93.6445\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7613 - val_loss: 88.8995\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6987 - val_loss: 99.8193\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7001 - val_loss: 104.0928\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7299 - val_loss: 108.4698\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5539 - val_loss: 119.6158\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5844 - val_loss: 123.8042\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7849 - val_loss: 103.9572\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7783 - val_loss: 96.6309\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4868 - val_loss: 99.9820\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4714 - val_loss: 109.4161\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7823 - val_loss: 89.5361\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6802 - val_loss: 100.6586\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5217 - val_loss: 103.3989\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4627 - val_loss: 89.0515\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7636 - val_loss: 105.8663\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6077 - val_loss: 89.4117\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4616 - val_loss: 108.9288\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5177 - val_loss: 129.8245\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6652 - val_loss: 90.7941\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4005 - val_loss: 95.4646\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6115 - val_loss: 97.0657\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6084 - val_loss: 103.1432\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5442 - val_loss: 123.1495\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5178 - val_loss: 99.4053\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5418 - val_loss: 108.9833\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3515 - val_loss: 110.8103\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5269 - val_loss: 100.9750\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5304 - val_loss: 114.6896\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3639 - val_loss: 92.7792\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5167 - val_loss: 91.1907\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4383 - val_loss: 90.0149\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4517 - val_loss: 105.5601\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5149 - val_loss: 96.8148\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4870 - val_loss: 94.2644\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2628 - val_loss: 92.2525\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3587 - val_loss: 94.4615\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2928 - val_loss: 117.8238\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6328 - val_loss: 106.5798\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.3576 - val_loss: 97.2155\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3656 - val_loss: 107.4527\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3083 - val_loss: 101.5952\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2338 - val_loss: 96.1176\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1920 - val_loss: 101.3484\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3625 - val_loss: 90.4535\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2450 - val_loss: 100.4578\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2161 - val_loss: 125.6512\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2729 - val_loss: 92.5687\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1390 - val_loss: 92.6181\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1799 - val_loss: 95.5965\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1380 - val_loss: 98.0052\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9783 - val_loss: 113.5298\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0626 - val_loss: 96.4679\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2044 - val_loss: 92.4491\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2780 - val_loss: 112.4233\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1408 - val_loss: 134.4609\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9661 - val_loss: 152.8435\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.3095 - val_loss: 101.0045\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0649 - val_loss: 103.7678\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2159 - val_loss: 101.2288\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0390 - val_loss: 108.0739\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1894 - val_loss: 88.0832\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3389 - val_loss: 89.0705\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0547 - val_loss: 140.6098\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0120 - val_loss: 88.7113\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1494 - val_loss: 93.9253\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0742 - val_loss: 101.2614\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9912 - val_loss: 96.6906\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9411 - val_loss: 136.5268\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1221 - val_loss: 92.8415\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2560 - val_loss: 95.9155\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9420 - val_loss: 94.2577\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0815 - val_loss: 95.2843\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1592 - val_loss: 109.0556\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9756 - val_loss: 105.2031\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8691 - val_loss: 110.9706\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 71.0303 - val_loss: 107.9784\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 71.3263 - val_loss: 93.3543\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8424 - val_loss: 90.5650\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1907 - val_loss: 91.5705\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9934 - val_loss: 89.6817\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0302 - val_loss: 102.3293\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8475 - val_loss: 107.4590\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9757 - val_loss: 95.9925\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0071 - val_loss: 94.5056\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7621 - val_loss: 90.2778\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1977 - val_loss: 118.0762\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9736 - val_loss: 92.3048\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0284 - val_loss: 91.5989\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0959 - val_loss: 94.9038\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7847 - val_loss: 97.1049\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9011 - val_loss: 101.9424\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8998 - val_loss: 105.2173\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0473 - val_loss: 91.3586\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9622 - val_loss: 91.4539\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8427 - val_loss: 104.3238\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8326 - val_loss: 95.6861\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7272 - val_loss: 94.4541\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8447 - val_loss: 96.4889\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8752 - val_loss: 99.1073\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7731 - val_loss: 90.6816\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8844 - val_loss: 105.6893\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8041 - val_loss: 91.9862\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8201 - val_loss: 89.1778\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8088 - val_loss: 112.1479\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7458 - val_loss: 97.9352\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8298 - val_loss: 89.3855\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7109 - val_loss: 90.8583\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5785 - val_loss: 113.7263\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7690 - val_loss: 98.0868\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6849 - val_loss: 115.4722\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6929 - val_loss: 87.6952\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0567 - val_loss: 93.4208\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6411 - val_loss: 106.6115\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5020 - val_loss: 103.1760\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5556 - val_loss: 97.2914\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8216 - val_loss: 106.3413\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6187 - val_loss: 94.2610\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6382 - val_loss: 92.2168\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4983 - val_loss: 91.4154\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6077 - val_loss: 96.5678\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6926 - val_loss: 105.1386\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5811 - val_loss: 97.7903\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.4903 - val_loss: 101.8229\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5364 - val_loss: 94.9654\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4534 - val_loss: 89.1517\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6157 - val_loss: 94.9558\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5329 - val_loss: 125.5046\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6540 - val_loss: 100.0971\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4911 - val_loss: 92.1868\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5099 - val_loss: 95.4503\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.4743 - val_loss: 109.4531\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7493 - val_loss: 100.4372\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7735 - val_loss: 105.3598\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5009 - val_loss: 91.2332\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3657 - val_loss: 93.7065\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4999 - val_loss: 99.4202\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5639 - val_loss: 98.6081\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5015 - val_loss: 98.4343\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5264 - val_loss: 94.3529\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5786 - val_loss: 127.9872\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4438 - val_loss: 104.0332\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4452 - val_loss: 93.0894\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3922 - val_loss: 115.9086\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4546 - val_loss: 95.9851\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5758 - val_loss: 110.5048\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 70.4474 - val_loss: 92.0525\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 70.5682 - val_loss: 111.9319\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 70.3575 - val_loss: 90.3947\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3888 - val_loss: 125.9397\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5169 - val_loss: 96.9246\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5030 - val_loss: 96.0578\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3398 - val_loss: 96.8578\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4504 - val_loss: 91.3435\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4195 - val_loss: 99.4867\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4537 - val_loss: 95.4371\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5984 - val_loss: 100.7798\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5911 - val_loss: 95.9773\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.4661 - val_loss: 99.7616\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4492 - val_loss: 104.5779\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3711 - val_loss: 92.0564\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3935 - val_loss: 118.6011\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2183 - val_loss: 96.4502\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3374 - val_loss: 106.6378\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3983 - val_loss: 113.7291\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2203 - val_loss: 100.5860\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4596 - val_loss: 104.7219\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4168 - val_loss: 98.4598\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2615 - val_loss: 94.7306\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4364 - val_loss: 94.5388\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4341 - val_loss: 107.2041\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2162 - val_loss: 86.8747\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4346 - val_loss: 138.3684\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.4109 - val_loss: 92.3528\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2722 - val_loss: 101.8708\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1076 - val_loss: 97.0471\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3027 - val_loss: 90.2905\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.2959 - val_loss: 98.0301\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2591 - val_loss: 97.9576\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2807 - val_loss: 98.2312\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3859 - val_loss: 104.0341\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3506 - val_loss: 115.2558\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3008 - val_loss: 99.4802\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.3449 - val_loss: 146.2397\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.3395 - val_loss: 87.0653\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.0860 - val_loss: 90.9192\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.2967 - val_loss: 96.4777\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.1104 - val_loss: 108.6051\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3051 - val_loss: 118.3313\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2553 - val_loss: 86.4096\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2148 - val_loss: 87.9893\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1863 - val_loss: 114.3384\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.1412 - val_loss: 91.0660\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2303 - val_loss: 97.0459\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1793 - val_loss: 98.5905\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1286 - val_loss: 88.6214\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2316 - val_loss: 84.4804\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2201 - val_loss: 91.5626\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2763 - val_loss: 92.6193\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1751 - val_loss: 86.5678\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.2190 - val_loss: 105.6297\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3070 - val_loss: 106.5087\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2792 - val_loss: 110.9235\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1962 - val_loss: 96.6883\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2342 - val_loss: 88.4785\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1842 - val_loss: 94.1576\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2925 - val_loss: 93.1432\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2660 - val_loss: 116.8850\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.2075 - val_loss: 90.6270\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1094 - val_loss: 94.0801\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1175 - val_loss: 121.8191\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1131 - val_loss: 107.4734\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1855 - val_loss: 101.5313\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2081 - val_loss: 89.0522\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0773 - val_loss: 92.2097\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2272 - val_loss: 108.3162\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2079 - val_loss: 110.3430\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1829 - val_loss: 92.6650\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0767 - val_loss: 90.3512\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1696 - val_loss: 91.1770\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0741 - val_loss: 99.7554\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1145 - val_loss: 89.2871\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 69.9688 - val_loss: 88.2693\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0232 - val_loss: 127.3567\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3360 - val_loss: 108.0816\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0946 - val_loss: 90.6328\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1308 - val_loss: 101.1721\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1768 - val_loss: 90.8123\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1181 - val_loss: 88.0900\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 69.9341 - val_loss: 95.9846\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0984 - val_loss: 88.1457\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1883 - val_loss: 96.4048\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1015 - val_loss: 128.5844\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1352 - val_loss: 90.2800\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1951 - val_loss: 91.3506\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0262 - val_loss: 92.0188\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1790 - val_loss: 103.5986\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0622 - val_loss: 98.6843\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2346 - val_loss: 93.4153\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1550 - val_loss: 95.4909\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0548 - val_loss: 93.2336\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYDcggm8CSwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dad5150-4c29-4f95-dfa8-ca6a505d36cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.7932198230852109 \n",
            "MAE:  7.326233704357188 \n",
            "SD:  9.623116858425345\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpKjAxdPCSwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "c9ee6c18-a71e-43c8-ba20-ac9f8226b5ab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vAhaD0CTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6fdb9c3-bd6f-41f0-e17a-83b4ba5e5fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,137\n",
            "Trainable params: 3,009\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dcXAOqd2CTsa",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd68119-891c-484a-fc66-3e2382f0a9dc"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 7ms/step - loss: 12126.6816 - val_loss: 12118.5361\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 11404.5264 - val_loss: 11123.5654\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 10428.0850 - val_loss: 9996.8047\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 9192.0420 - val_loss: 8295.7598\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 7763.4448 - val_loss: 6432.7036\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 6251.0752 - val_loss: 5387.9033\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 4794.1211 - val_loss: 4053.5732\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3495.0168 - val_loss: 2788.9009\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2418.2979 - val_loss: 1681.1582\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1584.8522 - val_loss: 1287.5992\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 985.0938 - val_loss: 605.7980\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 587.6036 - val_loss: 596.3711\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 343.7651 - val_loss: 273.1871\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 207.9041 - val_loss: 155.7008\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 140.2964 - val_loss: 152.5791\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 109.2019 - val_loss: 118.0961\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 96.5730 - val_loss: 112.9062\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 91.5869 - val_loss: 106.1158\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 89.3404 - val_loss: 141.2331\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 88.0862 - val_loss: 104.6663\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 87.4517 - val_loss: 194.4678\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 86.6162 - val_loss: 135.3882\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 85.8845 - val_loss: 168.8436\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.7184 - val_loss: 101.7389\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 85.1040 - val_loss: 106.3642\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.4649 - val_loss: 107.4359\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 84.4951 - val_loss: 115.8022\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.8002 - val_loss: 115.6466\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 83.5711 - val_loss: 104.9631\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 83.1398 - val_loss: 105.7226\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.9619 - val_loss: 120.9203\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 82.5774 - val_loss: 115.3290\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.4252 - val_loss: 136.5460\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.2485 - val_loss: 106.7849\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 82.0017 - val_loss: 148.4709\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.6918 - val_loss: 99.2449\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.5227 - val_loss: 114.9320\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.5199 - val_loss: 99.2007\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 81.2787 - val_loss: 103.5984\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.6772 - val_loss: 152.9102\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.7131 - val_loss: 119.6996\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 80.5822 - val_loss: 106.5356\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 80.5090 - val_loss: 126.1635\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 80.0369 - val_loss: 98.8378\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.8316 - val_loss: 100.9672\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.7906 - val_loss: 109.6723\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 79.4074 - val_loss: 120.5210\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4421 - val_loss: 97.6230\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 79.4028 - val_loss: 96.9278\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.9171 - val_loss: 96.7908\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.8584 - val_loss: 113.7476\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.6955 - val_loss: 95.7587\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.8898 - val_loss: 108.6059\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.2602 - val_loss: 101.1770\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.1877 - val_loss: 140.3638\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 78.2544 - val_loss: 96.5514\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 78.0640 - val_loss: 145.5477\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.8948 - val_loss: 97.5225\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.7319 - val_loss: 96.0865\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.6551 - val_loss: 96.4744\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.4911 - val_loss: 114.8410\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4708 - val_loss: 100.3738\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.4597 - val_loss: 103.6717\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.3918 - val_loss: 100.0069\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.1279 - val_loss: 97.2049\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 77.0886 - val_loss: 124.4641\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.2010 - val_loss: 174.5046\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 77.1048 - val_loss: 104.6580\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.9555 - val_loss: 99.2323\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.6874 - val_loss: 105.3738\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.7515 - val_loss: 102.0425\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.7081 - val_loss: 92.6419\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.6907 - val_loss: 88.9151\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.5684 - val_loss: 102.5645\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.2383 - val_loss: 111.0478\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.4744 - val_loss: 104.0547\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.1773 - val_loss: 92.4362\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.3135 - val_loss: 97.7079\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 76.0122 - val_loss: 98.7744\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 76.0308 - val_loss: 92.9549\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9602 - val_loss: 103.2607\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.9046 - val_loss: 92.5883\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.5245 - val_loss: 91.0404\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.7301 - val_loss: 99.1222\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.7246 - val_loss: 100.5356\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.6891 - val_loss: 163.2509\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.7990 - val_loss: 121.9219\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.6808 - val_loss: 107.8476\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.4372 - val_loss: 95.3859\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.6366 - val_loss: 106.3672\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.3657 - val_loss: 108.0087\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.4404 - val_loss: 107.2309\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.2406 - val_loss: 107.3568\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.2838 - val_loss: 105.4482\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.0565 - val_loss: 94.2666\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.1069 - val_loss: 128.5192\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.1874 - val_loss: 94.9546\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.0185 - val_loss: 100.4308\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 75.1600 - val_loss: 100.4923\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.0369 - val_loss: 95.9197\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.8095 - val_loss: 89.0016\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 75.0173 - val_loss: 103.4327\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.9905 - val_loss: 90.4390\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.8097 - val_loss: 131.4366\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6530 - val_loss: 133.1213\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6934 - val_loss: 101.1181\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.8271 - val_loss: 98.9200\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6936 - val_loss: 100.6659\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.4835 - val_loss: 91.9065\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.6108 - val_loss: 95.6926\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.6030 - val_loss: 103.6146\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.5117 - val_loss: 102.1533\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3002 - val_loss: 100.5081\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4583 - val_loss: 99.8716\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.4110 - val_loss: 125.8184\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.0880 - val_loss: 102.0474\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1446 - val_loss: 98.3484\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.0537 - val_loss: 95.5350\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.3012 - val_loss: 91.1457\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1487 - val_loss: 94.6761\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.0930 - val_loss: 88.9592\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 74.1311 - val_loss: 100.0927\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.0485 - val_loss: 173.0184\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 74.0403 - val_loss: 112.8192\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.8389 - val_loss: 96.4811\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9444 - val_loss: 102.0672\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6714 - val_loss: 142.8117\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.9320 - val_loss: 119.5018\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.7532 - val_loss: 160.4382\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.6517 - val_loss: 99.3878\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5404 - val_loss: 113.2341\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7503 - val_loss: 139.3648\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5697 - val_loss: 92.8929\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.5235 - val_loss: 94.8782\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5128 - val_loss: 93.3392\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.7874 - val_loss: 87.3558\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.4908 - val_loss: 104.5190\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2718 - val_loss: 149.9137\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.7342 - val_loss: 96.5839\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2066 - val_loss: 103.8369\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.4585 - val_loss: 163.8995\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.5573 - val_loss: 87.6323\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3329 - val_loss: 92.0601\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.6176 - val_loss: 113.2298\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3358 - val_loss: 105.9884\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.4206 - val_loss: 93.1493\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.2770 - val_loss: 113.8721\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.0887 - val_loss: 102.2327\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.3245 - val_loss: 97.6980\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2409 - val_loss: 100.1068\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.1630 - val_loss: 91.9858\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0329 - val_loss: 89.1995\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.0282 - val_loss: 96.6936\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.9770 - val_loss: 121.1722\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.3006 - val_loss: 99.4194\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.0541 - val_loss: 92.2690\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 73.2505 - val_loss: 94.5864\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 73.0006 - val_loss: 92.6126\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.9880 - val_loss: 93.2869\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.9333 - val_loss: 96.8115\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.8081 - val_loss: 100.2949\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.9374 - val_loss: 93.7801\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.7101 - val_loss: 87.9632\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.9119 - val_loss: 136.4496\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8811 - val_loss: 93.5073\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6370 - val_loss: 114.9471\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6825 - val_loss: 93.1873\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8655 - val_loss: 101.8063\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.8315 - val_loss: 93.4690\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.6502 - val_loss: 174.6542\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.8779 - val_loss: 97.4644\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.8141 - val_loss: 94.1816\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7197 - val_loss: 106.4317\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.7266 - val_loss: 123.2770\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5406 - val_loss: 128.6400\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.8538 - val_loss: 95.9688\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6867 - val_loss: 134.2035\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.5647 - val_loss: 98.6267\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6359 - val_loss: 122.1038\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5521 - val_loss: 95.7729\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 72.4099 - val_loss: 92.7508\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 72.4666 - val_loss: 116.2151\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 72.7053 - val_loss: 117.0361\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5372 - val_loss: 95.1986\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4695 - val_loss: 138.7486\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5683 - val_loss: 115.9649\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.4445 - val_loss: 95.0119\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.5704 - val_loss: 104.7889\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.5290 - val_loss: 88.7938\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.5010 - val_loss: 95.0179\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.6128 - val_loss: 107.3736\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4944 - val_loss: 108.8043\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2525 - val_loss: 103.2844\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.5853 - val_loss: 93.6180\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.4192 - val_loss: 96.7043\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2668 - val_loss: 93.4121\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2732 - val_loss: 111.0222\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.3383 - val_loss: 95.0920\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.2960 - val_loss: 109.5002\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.3221 - val_loss: 93.1567\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3268 - val_loss: 93.2582\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3526 - val_loss: 122.1696\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.3600 - val_loss: 114.0188\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.4180 - val_loss: 102.3414\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1903 - val_loss: 107.4103\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.1167 - val_loss: 118.2406\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.3448 - val_loss: 106.1483\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2054 - val_loss: 97.6073\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.1209 - val_loss: 89.1211\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.0319 - val_loss: 95.9395\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.1215 - val_loss: 89.7986\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0513 - val_loss: 103.5847\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9626 - val_loss: 104.1892\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.2134 - val_loss: 94.6497\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9551 - val_loss: 95.2078\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0645 - val_loss: 98.6034\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.1177 - val_loss: 86.4786\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.0227 - val_loss: 94.9233\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.1695 - val_loss: 94.2348\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8511 - val_loss: 90.5018\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9766 - val_loss: 103.2203\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9040 - val_loss: 103.2001\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9365 - val_loss: 90.8765\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8385 - val_loss: 97.1202\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7598 - val_loss: 96.1439\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0715 - val_loss: 117.6020\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8879 - val_loss: 106.1301\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 72.0632 - val_loss: 124.8401\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9165 - val_loss: 125.1201\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8801 - val_loss: 89.0329\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.9269 - val_loss: 101.2266\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.0692 - val_loss: 112.3421\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 72.0269 - val_loss: 149.1343\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8334 - val_loss: 97.1233\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8696 - val_loss: 93.8003\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6293 - val_loss: 114.3004\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7605 - val_loss: 122.1631\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7814 - val_loss: 92.2581\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7625 - val_loss: 91.2087\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7502 - val_loss: 93.6684\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7685 - val_loss: 93.0379\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7519 - val_loss: 91.8957\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7016 - val_loss: 112.7633\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7730 - val_loss: 95.0692\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6739 - val_loss: 121.7699\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7758 - val_loss: 95.1912\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.9619 - val_loss: 96.3573\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7093 - val_loss: 100.4055\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7675 - val_loss: 89.8352\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5879 - val_loss: 108.3319\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4999 - val_loss: 127.4212\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.8770 - val_loss: 95.7194\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8004 - val_loss: 123.0716\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7347 - val_loss: 136.8506\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.8506 - val_loss: 132.7379\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5988 - val_loss: 103.6826\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4841 - val_loss: 102.6870\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7445 - val_loss: 100.5630\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5187 - val_loss: 86.9358\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4992 - val_loss: 138.6118\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.7084 - val_loss: 91.6552\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5489 - val_loss: 86.6682\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6240 - val_loss: 92.3881\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5611 - val_loss: 99.1684\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4736 - val_loss: 92.7373\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7293 - val_loss: 105.2795\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5685 - val_loss: 104.0193\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7122 - val_loss: 102.2720\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4528 - val_loss: 93.3135\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6853 - val_loss: 91.4869\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4273 - val_loss: 135.4270\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6813 - val_loss: 93.8476\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4938 - val_loss: 95.9889\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.6758 - val_loss: 96.5018\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2841 - val_loss: 110.7356\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5461 - val_loss: 97.1313\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4820 - val_loss: 127.8629\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.7352 - val_loss: 99.0385\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2985 - val_loss: 125.0398\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.5099 - val_loss: 111.5360\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.6715 - val_loss: 106.6785\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4750 - val_loss: 91.7367\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3026 - val_loss: 94.5483\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4798 - val_loss: 88.5995\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.3960 - val_loss: 99.1368\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4908 - val_loss: 103.2062\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5087 - val_loss: 89.1443\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4598 - val_loss: 92.3362\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3921 - val_loss: 114.2220\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4249 - val_loss: 91.8469\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.4177 - val_loss: 103.4266\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4361 - val_loss: 86.6576\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3486 - val_loss: 97.4157\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.5520 - val_loss: 111.3239\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4235 - val_loss: 112.8597\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1186 - val_loss: 91.3445\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2117 - val_loss: 132.4299\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.3734 - val_loss: 104.3298\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1992 - val_loss: 102.1824\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.3532 - val_loss: 87.1612\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3069 - val_loss: 129.3337\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2912 - val_loss: 132.6126\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3252 - val_loss: 124.8134\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1386 - val_loss: 99.1739\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2750 - val_loss: 96.6832\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4234 - val_loss: 92.3516\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3288 - val_loss: 91.8417\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.3813 - val_loss: 96.6022\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3149 - val_loss: 106.6350\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4257 - val_loss: 95.5706\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.4111 - val_loss: 113.2314\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.3040 - val_loss: 109.4158\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0432 - val_loss: 86.6466\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2009 - val_loss: 94.2179\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.3123 - val_loss: 85.2726\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1360 - val_loss: 90.0693\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2204 - val_loss: 98.1443\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1988 - val_loss: 105.3457\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1676 - val_loss: 96.3878\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2058 - val_loss: 104.0691\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1352 - val_loss: 90.5746\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9970 - val_loss: 97.9178\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1101 - val_loss: 96.0413\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1377 - val_loss: 86.0965\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0590 - val_loss: 99.2668\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1742 - val_loss: 100.7579\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1944 - val_loss: 94.2586\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9677 - val_loss: 99.5856\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2067 - val_loss: 129.9564\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0308 - val_loss: 157.9580\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1283 - val_loss: 85.6864\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.2309 - val_loss: 90.6567\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1945 - val_loss: 110.3233\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.1526 - val_loss: 93.5068\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0065 - val_loss: 96.5144\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0585 - val_loss: 95.2464\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0861 - val_loss: 87.6756\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0714 - val_loss: 88.7937\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0811 - val_loss: 90.5538\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1499 - val_loss: 111.1231\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0037 - val_loss: 88.0149\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0664 - val_loss: 108.0405\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0022 - val_loss: 91.6926\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9214 - val_loss: 101.5147\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0464 - val_loss: 101.6516\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1698 - val_loss: 99.6638\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8852 - val_loss: 94.6322\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9257 - val_loss: 93.1046\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8432 - val_loss: 93.0121\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2467 - val_loss: 104.6545\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9997 - val_loss: 99.2798\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9902 - val_loss: 93.4521\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.1284 - val_loss: 98.1150\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0592 - val_loss: 86.8893\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8308 - val_loss: 114.3340\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9376 - val_loss: 94.0679\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8599 - val_loss: 92.2768\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0402 - val_loss: 90.3743\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.0131 - val_loss: 96.1491\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9769 - val_loss: 110.9657\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8083 - val_loss: 112.3429\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7190 - val_loss: 92.8903\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9443 - val_loss: 94.6966\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9409 - val_loss: 104.2331\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7919 - val_loss: 93.4875\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9280 - val_loss: 94.0788\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 71.0399 - val_loss: 92.1107\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9572 - val_loss: 96.1676\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7146 - val_loss: 99.1535\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8543 - val_loss: 106.9232\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7889 - val_loss: 101.6039\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8790 - val_loss: 98.5309\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8221 - val_loss: 100.6384\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8992 - val_loss: 96.7210\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7376 - val_loss: 96.4882\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8391 - val_loss: 97.1685\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8521 - val_loss: 109.3410\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7377 - val_loss: 95.7403\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5624 - val_loss: 100.3102\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.9608 - val_loss: 127.0419\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7757 - val_loss: 140.3410\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8213 - val_loss: 95.6533\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8677 - val_loss: 96.5609\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7889 - val_loss: 98.9712\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.8144 - val_loss: 101.5601\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7134 - val_loss: 101.0538\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.9328 - val_loss: 94.3649\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7721 - val_loss: 93.6463\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6272 - val_loss: 95.5655\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.8463 - val_loss: 93.3594\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7659 - val_loss: 116.3397\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6862 - val_loss: 94.2926\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7448 - val_loss: 95.4938\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7510 - val_loss: 95.7661\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5456 - val_loss: 91.7325\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5514 - val_loss: 93.4221\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6755 - val_loss: 98.6167\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6439 - val_loss: 90.4929\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6257 - val_loss: 101.8212\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6180 - val_loss: 90.2753\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4369 - val_loss: 117.4339\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.7869 - val_loss: 92.6306\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.7644 - val_loss: 96.3551\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5725 - val_loss: 88.6679\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4100 - val_loss: 114.0600\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6723 - val_loss: 99.5522\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5971 - val_loss: 90.7685\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6628 - val_loss: 88.0038\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.5979 - val_loss: 89.4046\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5870 - val_loss: 92.8271\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5287 - val_loss: 98.5779\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 70.6536 - val_loss: 98.7621\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5352 - val_loss: 96.2875\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5349 - val_loss: 93.5529\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5809 - val_loss: 91.8276\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5013 - val_loss: 114.9674\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5717 - val_loss: 95.7250\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4742 - val_loss: 95.2581\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6723 - val_loss: 97.4539\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6515 - val_loss: 88.7188\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4966 - val_loss: 107.4691\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4950 - val_loss: 86.5630\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5160 - val_loss: 141.1397\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4984 - val_loss: 91.4413\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4625 - val_loss: 91.3898\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5557 - val_loss: 93.0104\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5635 - val_loss: 104.6955\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3025 - val_loss: 98.4702\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6049 - val_loss: 102.2238\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5254 - val_loss: 88.8319\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4900 - val_loss: 102.3826\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3973 - val_loss: 88.2972\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4916 - val_loss: 91.9198\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3963 - val_loss: 95.1926\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5177 - val_loss: 90.4691\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3367 - val_loss: 94.5207\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4468 - val_loss: 125.7657\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3897 - val_loss: 103.0562\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3865 - val_loss: 93.4984\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6348 - val_loss: 93.7845\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5322 - val_loss: 90.5545\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5200 - val_loss: 90.4858\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3377 - val_loss: 91.8594\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2985 - val_loss: 88.0642\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4982 - val_loss: 137.3652\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3770 - val_loss: 115.9020\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3613 - val_loss: 90.1822\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3247 - val_loss: 94.3560\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3342 - val_loss: 92.7289\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4075 - val_loss: 93.5123\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4431 - val_loss: 105.7193\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5260 - val_loss: 91.8869\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4118 - val_loss: 90.3349\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3447 - val_loss: 104.9891\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4179 - val_loss: 104.9732\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4039 - val_loss: 94.1797\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4436 - val_loss: 100.8790\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3003 - val_loss: 97.5372\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.6167 - val_loss: 87.4706\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3667 - val_loss: 99.1620\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2649 - val_loss: 95.8880\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2329 - val_loss: 93.7618\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3129 - val_loss: 87.9069\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3124 - val_loss: 91.3299\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2902 - val_loss: 106.8242\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3600 - val_loss: 98.2083\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4715 - val_loss: 140.3833\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3014 - val_loss: 87.7227\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2391 - val_loss: 98.0641\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4185 - val_loss: 131.7332\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2651 - val_loss: 95.4762\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3479 - val_loss: 87.6337\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4094 - val_loss: 101.4917\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2798 - val_loss: 97.8094\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.5216 - val_loss: 97.8412\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3023 - val_loss: 87.7412\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2986 - val_loss: 101.6955\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2358 - val_loss: 99.9441\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3836 - val_loss: 102.7234\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1288 - val_loss: 91.6699\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2739 - val_loss: 117.4684\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3616 - val_loss: 89.7618\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3883 - val_loss: 89.6460\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1988 - val_loss: 96.4846\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0976 - val_loss: 122.9856\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2275 - val_loss: 106.3647\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0415 - val_loss: 91.6482\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1894 - val_loss: 89.6791\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3194 - val_loss: 92.5870\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1820 - val_loss: 102.0246\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2341 - val_loss: 124.7135\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2864 - val_loss: 103.1883\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0864 - val_loss: 89.3393\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3776 - val_loss: 89.7419\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.4574 - val_loss: 113.6214\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.0773 - val_loss: 98.3354\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1909 - val_loss: 90.9099\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.3155 - val_loss: 105.4287\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.2225 - val_loss: 92.9782\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 70.1455 - val_loss: 90.2040\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "696v_fuFCTsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae32ea9f-d713-4c6e-848c-e8b549b659ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.8624343299766387 \n",
            "MAE:  7.097682969840845 \n",
            "SD:  9.458340645443378\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mULwm5BdCTsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "efa24a1f-ede1-441f-d436-d12c95b1a696"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mdZF2osWCUQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596d8b00-c290-44a0-c5ae-02b3f4b5e8ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -0.7052408707744604 \n",
            "Ensemble_std:  9.534915147107734\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SkLVnvKbOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49650e11-95ba-4b08-ee52-bafb69f60d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 16)               64        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,137\n",
            "Trainable params: 3,009\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TnNzIg0iOZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65b3c8d-5309-4ee9-c12a-2eb6662cda21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 5s 7ms/step - loss: 3611.4075 - val_loss: 3612.3416\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3278.0071 - val_loss: 3235.5022\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2815.6685 - val_loss: 2249.8242\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2223.1108 - val_loss: 1744.1191\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1585.8759 - val_loss: 1481.7054\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 997.1841 - val_loss: 981.8547\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 551.8247 - val_loss: 443.7229\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 269.3801 - val_loss: 176.8384\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 123.0305 - val_loss: 104.3381\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 62.9243 - val_loss: 50.1334\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 42.7516 - val_loss: 47.3433\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.3959 - val_loss: 50.2977\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 35.9188 - val_loss: 50.5929\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.0544 - val_loss: 49.9156\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.9686 - val_loss: 75.7132\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5050 - val_loss: 47.9357\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0441 - val_loss: 41.8620\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.9933 - val_loss: 40.5417\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6030 - val_loss: 38.5897\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.3761 - val_loss: 38.4773\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.1669 - val_loss: 51.0433\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0265 - val_loss: 46.7204\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7737 - val_loss: 41.7694\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.6166 - val_loss: 44.0336\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.6459 - val_loss: 46.3590\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.4270 - val_loss: 40.6262\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1952 - val_loss: 40.6522\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1970 - val_loss: 43.6407\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.0335 - val_loss: 45.9031\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.9219 - val_loss: 41.7026\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.9023 - val_loss: 42.2128\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7164 - val_loss: 42.1967\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.7052 - val_loss: 40.9200\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 31.5992 - val_loss: 45.8808\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 31.4537 - val_loss: 53.4353\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 31.3814 - val_loss: 46.9527\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2451 - val_loss: 39.7668\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2303 - val_loss: 48.6799\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.0633 - val_loss: 38.2320\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.9392 - val_loss: 41.1164\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8651 - val_loss: 40.7344\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7553 - val_loss: 37.4030\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7308 - val_loss: 42.0420\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6472 - val_loss: 41.9920\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6141 - val_loss: 36.2965\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5736 - val_loss: 46.8569\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.4829 - val_loss: 37.5821\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.3865 - val_loss: 39.4811\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2123 - val_loss: 41.7095\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2262 - val_loss: 40.7033\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2323 - val_loss: 35.9821\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0348 - val_loss: 38.4041\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0075 - val_loss: 36.1910\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0055 - val_loss: 41.0079\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9060 - val_loss: 37.7807\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9607 - val_loss: 39.4387\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8768 - val_loss: 40.8168\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.7355 - val_loss: 37.6140\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7006 - val_loss: 35.5679\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7296 - val_loss: 37.8705\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.6184 - val_loss: 36.8139\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.6441 - val_loss: 43.4058\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5535 - val_loss: 39.1932\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4364 - val_loss: 37.2877\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.4020 - val_loss: 39.7193\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.4895 - val_loss: 37.5166\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.3083 - val_loss: 38.4291\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2869 - val_loss: 36.0181\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2993 - val_loss: 37.3243\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2792 - val_loss: 36.7944\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1665 - val_loss: 41.2963\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1459 - val_loss: 36.4032\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2111 - val_loss: 40.7762\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1403 - val_loss: 40.3723\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0387 - val_loss: 39.5374\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9886 - val_loss: 39.7419\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9536 - val_loss: 51.8205\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9220 - val_loss: 45.8240\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0105 - val_loss: 48.1690\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9262 - val_loss: 41.0813\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8582 - val_loss: 37.6180\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7857 - val_loss: 38.3530\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8804 - val_loss: 35.2495\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8104 - val_loss: 35.9741\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7519 - val_loss: 36.6904\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7291 - val_loss: 48.3123\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6763 - val_loss: 38.9224\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7225 - val_loss: 39.9112\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6488 - val_loss: 39.8536\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6535 - val_loss: 40.5027\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6483 - val_loss: 35.8786\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6682 - val_loss: 40.4336\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5628 - val_loss: 36.3751\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5876 - val_loss: 43.6816\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5848 - val_loss: 34.9910\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5335 - val_loss: 35.8153\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4789 - val_loss: 37.2658\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4767 - val_loss: 39.3914\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3713 - val_loss: 40.8265\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4261 - val_loss: 34.4475\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4254 - val_loss: 38.4203\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4013 - val_loss: 37.5016\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3131 - val_loss: 35.6525\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3285 - val_loss: 36.5324\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3411 - val_loss: 37.8820\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3433 - val_loss: 37.1647\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2437 - val_loss: 35.7900\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2413 - val_loss: 34.4432\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2476 - val_loss: 44.4471\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2644 - val_loss: 37.2506\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2620 - val_loss: 35.2233\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2240 - val_loss: 34.4246\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1354 - val_loss: 35.3637\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1857 - val_loss: 55.2755\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1361 - val_loss: 38.6926\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1409 - val_loss: 39.5898\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0230 - val_loss: 40.4016\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0689 - val_loss: 42.4550\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0562 - val_loss: 34.6240\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0624 - val_loss: 33.7991\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0239 - val_loss: 41.2648\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9655 - val_loss: 35.3657\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9855 - val_loss: 38.5657\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0223 - val_loss: 36.9600\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9457 - val_loss: 34.4906\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0563 - val_loss: 45.8572\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9206 - val_loss: 35.6997\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9108 - val_loss: 38.3903\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8539 - val_loss: 37.1497\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8602 - val_loss: 39.0747\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8834 - val_loss: 34.0597\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8489 - val_loss: 33.2554\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8864 - val_loss: 34.9913\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8268 - val_loss: 36.3992\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7617 - val_loss: 36.6935\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8442 - val_loss: 39.6739\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8283 - val_loss: 34.8153\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7418 - val_loss: 37.2392\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6852 - val_loss: 38.5388\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8123 - val_loss: 34.6255\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7010 - val_loss: 39.5587\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8070 - val_loss: 34.7623\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7045 - val_loss: 37.8918\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6318 - val_loss: 37.6961\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7230 - val_loss: 36.0573\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6633 - val_loss: 36.7677\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7146 - val_loss: 33.6114\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7221 - val_loss: 46.8375\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7406 - val_loss: 38.2234\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6622 - val_loss: 42.6251\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6862 - val_loss: 37.6829\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6903 - val_loss: 40.9061\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6165 - val_loss: 37.7857\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6474 - val_loss: 36.3270\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6263 - val_loss: 42.4799\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5722 - val_loss: 37.7931\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6100 - val_loss: 38.9914\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6165 - val_loss: 36.2724\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6044 - val_loss: 35.5656\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5581 - val_loss: 34.4489\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5846 - val_loss: 35.9362\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5189 - val_loss: 35.0764\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5732 - val_loss: 37.5404\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5718 - val_loss: 41.0900\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4996 - val_loss: 36.7888\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6212 - val_loss: 36.7839\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4690 - val_loss: 35.8011\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4930 - val_loss: 34.8161\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5022 - val_loss: 37.8661\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5743 - val_loss: 37.2517\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5682 - val_loss: 35.3104\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4128 - val_loss: 42.7351\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4864 - val_loss: 34.4967\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4556 - val_loss: 36.2571\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4587 - val_loss: 39.4970\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5063 - val_loss: 36.2609\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4512 - val_loss: 36.4366\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4224 - val_loss: 35.4087\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4034 - val_loss: 41.6571\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4147 - val_loss: 37.0690\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4532 - val_loss: 49.0612\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3929 - val_loss: 38.7426\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3682 - val_loss: 34.4488\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3881 - val_loss: 36.9394\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3495 - val_loss: 44.0321\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4241 - val_loss: 35.1939\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4094 - val_loss: 33.4227\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3725 - val_loss: 35.5428\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3689 - val_loss: 36.4244\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3840 - val_loss: 41.6407\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4028 - val_loss: 37.3030\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3028 - val_loss: 38.2566\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3382 - val_loss: 47.2872\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3779 - val_loss: 33.6761\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3255 - val_loss: 36.2414\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2935 - val_loss: 37.3200\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2887 - val_loss: 33.9638\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3481 - val_loss: 37.7653\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2849 - val_loss: 33.9491\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3310 - val_loss: 38.0017\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3123 - val_loss: 38.1527\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3877 - val_loss: 35.9130\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3737 - val_loss: 34.6830\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2888 - val_loss: 33.5358\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3082 - val_loss: 34.8955\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3092 - val_loss: 39.2207\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3390 - val_loss: 36.8008\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3405 - val_loss: 38.7800\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2512 - val_loss: 35.6460\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2595 - val_loss: 42.5381\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2915 - val_loss: 37.7744\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2781 - val_loss: 36.8517\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2875 - val_loss: 37.6466\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2549 - val_loss: 35.1144\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2154 - val_loss: 44.2296\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2852 - val_loss: 35.6456\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3104 - val_loss: 35.4918\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1938 - val_loss: 38.1467\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2304 - val_loss: 35.4071\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2643 - val_loss: 38.5534\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2510 - val_loss: 34.1282\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2130 - val_loss: 36.3040\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2147 - val_loss: 41.4036\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2098 - val_loss: 36.2040\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1974 - val_loss: 34.9254\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1711 - val_loss: 34.9634\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1478 - val_loss: 36.1405\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2490 - val_loss: 39.1174\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1659 - val_loss: 36.4632\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2039 - val_loss: 38.6034\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2394 - val_loss: 37.9401\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1409 - val_loss: 35.5429\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1483 - val_loss: 35.4224\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1235 - val_loss: 38.5338\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1273 - val_loss: 36.4281\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1944 - val_loss: 35.0679\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1651 - val_loss: 38.7684\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1152 - val_loss: 33.8167\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2003 - val_loss: 38.0071\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1440 - val_loss: 34.0849\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1414 - val_loss: 37.7227\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1362 - val_loss: 38.9207\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1049 - val_loss: 34.5976\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1714 - val_loss: 42.6682\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1684 - val_loss: 34.7691\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0905 - val_loss: 38.9753\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0741 - val_loss: 33.7152\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1584 - val_loss: 34.4632\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0923 - val_loss: 37.4763\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1059 - val_loss: 34.3593\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0603 - val_loss: 38.2255\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0920 - val_loss: 36.9883\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1213 - val_loss: 41.5940\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1022 - val_loss: 36.3645\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1123 - val_loss: 35.9240\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0310 - val_loss: 33.9155\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1020 - val_loss: 38.1058\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0820 - val_loss: 36.4910\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1232 - val_loss: 34.1233\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0678 - val_loss: 41.2065\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0283 - val_loss: 34.7879\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0231 - val_loss: 36.9612\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9712 - val_loss: 36.9946\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1098 - val_loss: 36.8146\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0455 - val_loss: 48.1920\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0010 - val_loss: 34.7364\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0544 - val_loss: 34.1500\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0004 - val_loss: 34.4794\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0033 - val_loss: 37.5479\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9773 - val_loss: 35.6813\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0008 - val_loss: 38.4499\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0038 - val_loss: 34.9330\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0091 - val_loss: 39.1792\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9396 - val_loss: 43.6562\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9979 - val_loss: 35.8958\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9675 - val_loss: 39.2022\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9962 - val_loss: 41.2564\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9270 - val_loss: 34.1510\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9473 - val_loss: 34.1437\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9847 - val_loss: 44.7495\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0077 - val_loss: 35.9833\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 26.9652 - val_loss: 35.4374\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 26.9494 - val_loss: 42.6041\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 26.9771 - val_loss: 37.0633\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9857 - val_loss: 37.9524\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9251 - val_loss: 34.7252\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9593 - val_loss: 51.1803\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9484 - val_loss: 37.5023\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8890 - val_loss: 38.6903\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9687 - val_loss: 34.4835\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9406 - val_loss: 42.1075\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9250 - val_loss: 33.8882\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9187 - val_loss: 38.6292\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9651 - val_loss: 36.7096\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9119 - val_loss: 43.5206\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9281 - val_loss: 36.9287\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9037 - val_loss: 34.3724\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8936 - val_loss: 38.0348\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9911 - val_loss: 34.0495\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9215 - val_loss: 37.3040\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8346 - val_loss: 49.4760\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9564 - val_loss: 35.4800\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9891 - val_loss: 42.2451\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8568 - val_loss: 34.3686\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9256 - val_loss: 37.0779\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8886 - val_loss: 39.5678\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8813 - val_loss: 38.8574\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8833 - val_loss: 34.1494\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8425 - val_loss: 35.8990\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8876 - val_loss: 34.3582\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9040 - val_loss: 35.2625\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8995 - val_loss: 37.1566\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8069 - val_loss: 35.8041\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8792 - val_loss: 34.5917\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8574 - val_loss: 37.1167\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8329 - val_loss: 59.2517\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8020 - val_loss: 35.9989\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7734 - val_loss: 37.3669\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8873 - val_loss: 33.4722\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8349 - val_loss: 35.1357\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8525 - val_loss: 34.9464\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8227 - val_loss: 35.0137\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8637 - val_loss: 36.3714\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8406 - val_loss: 36.3091\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8220 - val_loss: 34.9684\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8030 - val_loss: 36.9356\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8482 - val_loss: 51.8442\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9171 - val_loss: 35.1661\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8958 - val_loss: 35.1353\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8152 - val_loss: 34.2396\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8000 - val_loss: 42.7739\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8394 - val_loss: 38.5224\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8218 - val_loss: 34.5197\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8367 - val_loss: 39.1423\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8000 - val_loss: 38.0725\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7865 - val_loss: 36.0104\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7745 - val_loss: 35.9066\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8250 - val_loss: 35.0926\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8178 - val_loss: 40.3068\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7757 - val_loss: 37.0605\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8464 - val_loss: 38.7415\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7939 - val_loss: 37.5983\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7751 - val_loss: 33.8046\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7986 - val_loss: 40.3766\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8106 - val_loss: 34.3991\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7390 - val_loss: 38.1289\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7982 - val_loss: 39.2235\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7848 - val_loss: 49.3734\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7213 - val_loss: 35.1049\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8228 - val_loss: 34.9570\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7542 - val_loss: 36.9431\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7385 - val_loss: 36.0556\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7766 - val_loss: 34.4909\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7748 - val_loss: 38.2550\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8256 - val_loss: 36.2118\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7479 - val_loss: 37.8800\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7167 - val_loss: 39.6875\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7842 - val_loss: 36.0156\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7646 - val_loss: 35.2469\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7095 - val_loss: 40.1221\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7668 - val_loss: 35.7749\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7115 - val_loss: 40.0547\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6569 - val_loss: 41.6283\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7281 - val_loss: 37.7292\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8358 - val_loss: 38.2098\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7744 - val_loss: 37.0618\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7612 - val_loss: 37.9786\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7276 - val_loss: 34.3846\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7400 - val_loss: 35.5198\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7682 - val_loss: 33.5524\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7731 - val_loss: 35.5407\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7287 - val_loss: 34.9562\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7591 - val_loss: 36.7080\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8161 - val_loss: 42.2801\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6573 - val_loss: 36.4729\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7706 - val_loss: 35.4601\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7643 - val_loss: 38.0947\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7061 - val_loss: 47.7998\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6799 - val_loss: 38.5470\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7165 - val_loss: 33.4245\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6784 - val_loss: 34.2923\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6767 - val_loss: 36.8556\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7374 - val_loss: 36.9355\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6548 - val_loss: 37.4075\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7172 - val_loss: 36.4304\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6509 - val_loss: 32.6457\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6934 - val_loss: 34.3520\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6768 - val_loss: 38.0441\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7314 - val_loss: 38.6361\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6809 - val_loss: 38.6224\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6800 - val_loss: 34.4746\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6918 - val_loss: 36.7261\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6750 - val_loss: 36.2198\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6409 - val_loss: 38.1186\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6052 - val_loss: 34.7246\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6465 - val_loss: 42.4549\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6968 - val_loss: 34.2952\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6726 - val_loss: 34.7376\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6477 - val_loss: 39.3912\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6403 - val_loss: 35.0390\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6264 - val_loss: 35.8555\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6717 - val_loss: 39.2285\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6670 - val_loss: 36.1587\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6657 - val_loss: 35.5141\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6148 - val_loss: 35.1231\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6129 - val_loss: 42.6740\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7024 - val_loss: 35.7841\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5623 - val_loss: 35.3220\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6485 - val_loss: 44.0379\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7084 - val_loss: 34.5860\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6313 - val_loss: 41.5620\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6525 - val_loss: 36.9304\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6668 - val_loss: 34.8480\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5869 - val_loss: 39.1406\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6734 - val_loss: 34.0640\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7052 - val_loss: 35.8201\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6070 - val_loss: 36.0198\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6258 - val_loss: 36.7925\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5924 - val_loss: 35.2738\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6377 - val_loss: 36.6665\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6180 - val_loss: 35.1401\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6214 - val_loss: 35.8380\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6225 - val_loss: 38.7390\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6487 - val_loss: 33.7534\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6246 - val_loss: 35.4248\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5989 - val_loss: 36.1622\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5618 - val_loss: 36.2971\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6220 - val_loss: 36.8871\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6487 - val_loss: 34.7292\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5833 - val_loss: 35.1046\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6363 - val_loss: 36.3848\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5955 - val_loss: 34.0039\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5684 - val_loss: 34.7741\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6907 - val_loss: 35.9887\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6164 - val_loss: 36.3598\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5775 - val_loss: 35.3828\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6220 - val_loss: 39.6450\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6251 - val_loss: 35.2133\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5614 - val_loss: 36.1495\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5602 - val_loss: 33.7395\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5886 - val_loss: 34.2665\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6130 - val_loss: 34.3811\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5908 - val_loss: 49.6815\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5181 - val_loss: 34.3415\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5916 - val_loss: 37.5283\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5476 - val_loss: 37.4821\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5726 - val_loss: 34.6404\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5359 - val_loss: 34.0009\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5383 - val_loss: 37.4399\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6025 - val_loss: 34.8500\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6234 - val_loss: 37.8163\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5936 - val_loss: 35.6377\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5689 - val_loss: 38.9823\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5750 - val_loss: 33.8158\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5478 - val_loss: 38.2187\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5517 - val_loss: 35.3249\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5548 - val_loss: 35.2134\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5377 - val_loss: 35.7052\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5363 - val_loss: 42.4023\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5518 - val_loss: 37.1840\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5806 - val_loss: 35.2317\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5752 - val_loss: 35.5704\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5484 - val_loss: 42.4746\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5512 - val_loss: 35.4419\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5713 - val_loss: 37.9257\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5366 - val_loss: 46.6497\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5713 - val_loss: 37.0362\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5819 - val_loss: 40.0372\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5371 - val_loss: 35.1033\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5461 - val_loss: 34.0861\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5419 - val_loss: 34.7432\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5320 - val_loss: 37.0396\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4996 - val_loss: 35.8605\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5104 - val_loss: 37.1138\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5542 - val_loss: 34.1160\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4923 - val_loss: 36.0374\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5103 - val_loss: 38.1337\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5477 - val_loss: 36.6616\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5078 - val_loss: 47.8487\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5302 - val_loss: 43.4451\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5309 - val_loss: 34.4447\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5232 - val_loss: 35.0380\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6254 - val_loss: 34.4415\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5009 - val_loss: 34.5539\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5045 - val_loss: 36.1522\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5192 - val_loss: 36.0498\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5126 - val_loss: 36.0038\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5559 - val_loss: 34.4961\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5128 - val_loss: 34.5734\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5075 - val_loss: 34.3036\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5421 - val_loss: 48.6935\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5628 - val_loss: 35.5927\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4705 - val_loss: 35.4937\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4941 - val_loss: 35.9446\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4698 - val_loss: 35.9519\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4798 - val_loss: 37.9748\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5110 - val_loss: 42.2770\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5210 - val_loss: 35.7852\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4821 - val_loss: 34.9281\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5663 - val_loss: 34.3823\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "c1TqXgfDOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "199c5432-0f68-455d-851f-656d79effd5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -0.001841733560347194 \n",
            "MAE:  4.270234173230926 \n",
            "SD:  5.863639183270013\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0cip38xZOZnV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "7b1c48f8-cccb-4eb9-fca6-3d78c5f31747"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAFBCAYAAAAsfIegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwU5f0H8M93k5hwhENARKCCiqIYCBQQRK0Fi6ItXrWoaBFRbKUFtVURD7St9vAslqJUaVGxgidYsYCUH0iV23BfkYIkAgkgNwGy+/398cxkJskm2U1ms5Px83699rWzcz+b2c8888wRUVUQEZF3QsleASKioGGwEhF5jMFKROQxBisRkccYrEREHmOwEhF5LGHBKiIZIrJERFaKyFoRecLq315EFotIrohMFZGTrP7p1udca3i7RK0bEVEiJbLGegxAX1XtAiAbwBUi0gvAHwE8r6pnAfgGwDBr/GEAvrH6P2+NR0RU5yQsWNU4ZH1Ms14KoC+Ad6z+kwFcY3VfbX2GNbyfiEii1o+IKFES2sYqIikikgOgAMAcAF8C2KeqxdYoeQBaW92tAWwHAGv4fgDNErl+RESJkJrImatqGEC2iDQB8D6AjjWdp4gMBzAcABo0aPDdjh3NLHNXHMCJlAyc2+Wkmi6CiL7lli9fvltVW1R3+oQGq01V94nIPAC9ATQRkVSrVtoGQL41Wj6AtgDyRCQVQGMAe6LMayKAiQDQvXt3XbZsGQDghxlzsCPzbCxbdnrCy0NEwSYi22oyfSKvCmhh1VQhIvUA/ADAegDzAPzYGm0IgOlW9wzrM6zh/9E4nhDDxlgi8otE1lhbAZgsIikwAT5NVf8lIusAvCUivwPwBYBXrfFfBfC6iOQC2AvgxngXyAd1EZEfJCxYVXUVgK5R+m8B0DNK/yIAN1R3eQKmKhH5Q620sdYWVTYIkL+dOHECeXl5KCoqSvaqEICMjAy0adMGaWlpns43MMEqwhor+V9eXh4yMzPRrl078DLt5FJV7NmzB3l5eWjfvr2n8w7QswLYGED+V1RUhGbNmjFUfUBE0KxZs4QcPQQmWBmrVFcwVP0jUX+LwAQrhG2sROQPgQlWU2NlrZWormrYsGGFw7Zu3Yrzzz+/FtemZgITrACgvE2AiHwgMMEq4A0CRLHYunUrOnbsiNtuuw1nn302Bg8ejE8++QR9+vRBhw4dsGTJEsyfPx/Z2dnIzs5G165dcfDgQQDA008/jR49eqBz584YO3ZshcsYPXo0xo8fX/L58ccfxzPPPINDhw6hX79+6NatG7KysjB9+vQK51GRoqIiDB06FFlZWejatSvmzZsHAFi7di169uyJ7OxsdO7cGZs3b8bhw4dx1VVXoUuXLjj//PMxderUuJdXHbzciihZ7rkHyMnxdp7Z2cALL1Q5Wm5uLt5++21MmjQJPXr0wJtvvomFCxdixowZeOqppxAOhzF+/Hj06dMHhw4dQkZGBmbPno3NmzdjyZIlUFUMHDgQCxYswCWXXFJu/oMGDcI999yDESNGAACmTZuGWbNmISMjA++//z4aNWqE3bt3o1evXhg4cGBcJ5HGjx8PEcHq1auxYcMG9O/fH5s2bcJLL72EUaNGYfDgwTh+/DjC4TBmzpyJ0047DR999BEAYP/+/TEvpyYCU2MF2BRAFKv27dsjKysLoVAInTp1Qr9+/SAiyMrKwtatW9GnTx/cd999GDduHPbt24fU1FTMnj0bs2fPRteuXdGtWzds2LABmzdvjjr/rl27oqCgAF9//TVWrlyJpk2bom3btlBVjBkzBp07d8Zll12G/Px87Nq1K651X7hwIW655RYAQMeOHXH66adj06ZN6N27N5566in88Y9/xLZt21CvXj1kZWVhzpw5ePDBB/Hpp5+icePGNf7uYhGcGivAc1dUt8RQs0yU9PT0ku5QKFTyORQKobi4GKNHj8ZVV12FmTNnok+fPpg1axZUFQ899BDuuuuumJZxww034J133sHOnTsxaNAgAMCUKVNQWFiI5cuXIy0tDe3atfPsOtKbb74ZF1xwAT766CNceeWVePnll9G3b1+sWLECM2fOxCOPPIJ+/frhscce82R5lQlMsALMVSKvfPnll8jKykJWVhaWLl2KDRs24PLLL8ejjz6KwYMHo2HDhsjPz0daWhpOOeWUqPMYNGgQ7rzzTuzevRvz588HYA7FTznlFKSlpWHevHnYti3+p/NdfPHFmDJlCvr27YtNmzbhq6++wjnnnIMtW7bgjDPOwMiRI/HVV19h1apV6NixI04++WTccsstaNKkCV555ZUafS+xCkywso2VyDsvvPAC5s2bV9JUMGDAAKSnp2P9+vXo3bs3AHN51BtvvFFhsHbq1AkHDx5E69at0apVKwDA4MGD8aMf/QhZWVno3r077AfVx+Puu+/Gz3/+c2RlZSE1NRX/+Mc/kJ6ejmnTpuH1119HWloaTj31VIwZMwZLly7F/fffj1AohLS0NEyYMKH6X0ocJI5HnvqO+0HXNzScibUpXbBuf+sqpiJKnvXr1+Pcc89N9mqQS7S/iYgsV9Xu1Z1nYE5e8bQVEflFYJoCALaxEtW2PXv2oF+/fuX6z507F82axf+/QFevXo1bb721VL/09HQsXry42uuYDIEJVraxEtW+Zs2aIcfDa3GzsrI8nV+yBKYpABA+hIWIfCEwwcqHsBCRXwQmWCG884qI/CEwwSpQNgUQkS8EKFiJyE8qe75q0AUmWAG2sBKRPwTqciuNsN5KdUeynhq4detWXHHFFejVqxc+++wz9OjRA0OHDsXYsWNRUFCAKVOm4OjRoxg1ahQA83+hFixYgMzMTDz99NOYNm0ajh07hmuvvRZPPPFEleukqnjggQfw8ccfQ0TwyCOPYNCgQdixYwcGDRqEAwcOoLi4GBMmTMCFF16IYcOGYdmyZRAR3H777bj33nu9+GpqVXCCFayxEsUq0c9jdXvvvfeQk5ODlStXYvfu3ejRowcuueQSvPnmm7j88svx8MMPIxwO48iRI8jJyUF+fj7WrFkDANi3b19tfB2eC1CwKq8KoDoliU8NLHkeK4Coz2O98cYbcd9992Hw4MG47rrr0KZNm1LPYwWAQ4cOYfPmzVUG68KFC3HTTTchJSUFLVu2xPe+9z0sXboUPXr0wO23344TJ07gmmuuQXZ2Ns444wxs2bIFv/zlL3HVVVehf//+Cf8uEiEwbawivCqAKFaxPI/1lVdewdGjR9GnTx9s2LCh5HmsOTk5yMnJQW5uLoYNG1btdbjkkkuwYMECtG7dGrfddhtee+01NG3aFCtXrsSll16Kl156CXfccUeNy5oMwQlWsCmAyCv281gffPBB9OjRo+R5rJMmTcKhQ4cAAPn5+SgoKKhyXhdffDGmTp2KcDiMwsJCLFiwAD179sS2bdvQsmVL3HnnnbjjjjuwYsUK7N69G5FIBNdffz1+97vfYcWKFYkuakIEpymANVYiz3jxPFbbtddei88//xxdunSBiOBPf/oTTj31VEyePBlPP/000tLS0LBhQ7z22mvIz8/H0KFDEYlEAAC///3vE17WRAjM81iHNv0Ac4/1wVdHWiR5rYgqxuex+g+fx1oJnrwiIr9gUwARVZvXz2MNisAEawjKk1dEtczr57EGRXCaAkQRYY2V6oC6fF4jaBL1twhOsIKPDST/y8jIwJ49exiuPqCq2LNnDzIyMjyfd2CaAtjGSnVBmzZtkJeXh8LCwmSvCsHs6Nq0aeP5fBMWrCLSFsBrAFrCXLs/UVX/LCKPA7gTgL1ljVHVmdY0DwEYBiAMYKSqzop5eeANAuR/aWlpaN++fbJXgxIskTXWYgC/UtUVIpIJYLmIzLGGPa+qz7hHFpHzANwIoBOA0wB8IiJnq2o4loXxcisi8ouEtbGq6g5VXWF1HwSwHkDrSia5GsBbqnpMVf8HIBdAz1iXJwI2BRCRL9TKySsRaQegKwD7n4P/QkRWicgkEWlq9WsNYLtrsjxUHsRllsHLrYjIHxIerCLSEMC7AO5R1QMAJgA4E0A2gB0Ano1zfsNFZJmILHOfAGBTABH5RUKDVUTSYEJ1iqq+BwCquktVw6oaAfA3OIf7+QDauiZvY/UrRVUnqmp3Ve3eooXzXAABmwKIyB8SFqwiIgBeBbBeVZ9z9W/lGu1aAGus7hkAbhSRdBFpD6ADgCWxL49NAUTkD4m8KqAPgFsBrBYR+563MQBuEpFsmKujtgK4CwBUda2ITAOwDuaKghGxXhEA8AYBIvKPhAWrqi5E9P9KPbOSaZ4E8GR1lheSCJsCiMgXAnVLa4Q1ViLygeAEq7ApgIj8ITjBCgWfa0FEfhCcYGWNlYh8IjjByhsEiMgnghOswqYAIvKH4AQr2BRARP4QnGAVNgUQkT8EJ1jBGisR+UNwgpVtrETkE8EJVrDGSkT+EJxgFUCDUxwiqsMCk0QhRACAzQFElHSBCVaxWgEYrESUbMEJVusx1wxWIkq24AQra6xE5BMMViIijwUnWNkUQEQ+EZxgZY2ViHwiOMHKGisR+URwgpU1ViLyieAEK2usROQTwQlW1liJyCcYrEREHgtOsLIpgIh8IjDBGhIGKxH5Q2CC1a6xRiJJXhEi+tYLTrCyjZWIfILBSkTkseAEK09eEZFPBCdYWWMlIp8ITrCyxkpEPhGcYGWNlYh8gsFKROSx4AQrmwKIyCeCE6yssRKRTwQoWFljJSJ/SFiwikhbEZknIutEZK2IjLL6nywic0Rks/Xe1OovIjJORHJFZJWIdItveeadwUpEyZbIGmsxgF+p6nkAegEYISLnARgNYK6qdgAw1/oMAAMAdLBewwFMiGdhIQYrEflEwoJVVXeo6gqr+yCA9QBaA7gawGRrtMkArrG6rwbwmhqLADQRkVaxLo8PYSEiv6iVNlYRaQegK4DFAFqq6g5r0E4ALa3u1gC2uybLs/rFuAzzzhorESVbwoNVRBoCeBfAPap6wD1MVRVAXFEoIsNFZJmILCssLHT683IrIvKJhAariKTBhOoUVX3P6r3LPsS33gus/vkA2romb2P1K0VVJ6pqd1Xt3qJFC9ey7OEeF4KIKE6JvCpAALwKYL2qPucaNAPAEKt7CIDprv4/ta4O6AVgv6vJIIblmXcGKxElW2oC590HwK0AVotIjtVvDIA/AJgmIsMAbAPwE2vYTABXAsgFcATA0HgWxqYAIvKLhAWrqi4EIBUM7hdlfAUworrLY42ViPwiQHdemXcGKxElG4OViMhjwQlWtrESkU8EJ1hZYyUinwhMsIb4dCsi8onABKv92EA+K4CIki1AwWreWWMlomQLTrBa7wxWIkq24AQra6xE5BMMViIijwUnWHkdKxH5RHCClTVWIvKJAAUra6xE5A8BClZTZWWwElGyBShYzTuDlYiSLTjBypNXROQTwQnWEJsCiMgfAhOsoRQGKxH5Q2CC1W5j5UNYiCjZghOsbAogIp9gsBIReYzBSkTkMQYrEZHHGKxERB5jsBIReYzBSkTkMQYrEZHHGKxERB4LTrCmmKIwWIko2YITrKyxEpFPBCZYQ1ZJGKxElGwxBauINBCRkNV9togMFJG0xK5afOwaKx/CQkTJFmuNdQGADBFpDWA2gFsB/CNRK1UdbAogIr+INVhFVY8AuA7AX1X1BgCdErda8SsJ1jCrrESUXDEHq4j0BjAYwEdWv5TErFL1lFwVEGGVlYiSK9ZgvQfAQwDeV9W1InIGgHmJW634scZKRH4RU7Cq6nxVHaiqf7ROYu1W1ZGVTSMik0SkQETWuPo9LiL5IpJjva50DXtIRHJFZKOIXB5vQUqClTVWIkqyWK8KeFNEGolIAwBrAKwTkfurmOwfAK6I0v95Vc22XjOt+Z8H4EaYdtsrAPxVROJqamBTABH5RaxNAeep6gEA1wD4GEB7mCsDKqSqCwDsjXH+VwN4S1WPqer/AOQC6BnjtADYFEBE/hFrsKZZ161eA2CGqp4AUN2q4S9EZJXVVNDU6tcawHbXOHlWv5gxWInIL2IN1pcBbAXQAMACETkdwIFqLG8CgDMBZAPYAeDZeGcgIsNFZJmILCssLHT6symAiHwi1pNX41S1tapeqcY2AN+Pd2GquktVw6oaAfA3OIf7+QDaukZtY/WLNo+JqtpdVbu3aNGipD9PXhGRX8R68qqxiDxn1xRF5FmY2mtcRKSV6+O1MCfCAGAGgBtFJF1E2gPoAGBJXPNmUwAR+URqjONNggnBn1ifbwXwd5g7saISkX8CuBRAcxHJAzAWwKUikg3TPrsVwF0AYF0bOw3AOgDFAEaoajiegoRSWGMlIn+INVjPVNXrXZ+fEJGcyiZQ1Zui9H61kvGfBPBkjOtTTslDWIpZYyWi5Ir15NVREbnI/iAifQAcTcwqVQ9PXhGRX8RaY/0ZgNdEpLH1+RsAQxKzStXDYCUiv4gpWFV1JYAuItLI+nxARO4BsCqRKxcPXhVARH4R138QUNUD1h1YAHBfAtan2nhVABH5RU3+NYt4thYeYFMAEflFTYLVVwnmNAWwxkpEyVVpG6uIHET0ABUA9RKyRtVUUmMN+yrviehbqNJgVdXM2lqRmhLeIEBEPhGYf38tIbaxEpE/BChYWWMlIn8ITLDyWQFE5BeBCVb75FWE17ESUZIFLlhZYyWiZGOwEhF5LDjBypNXROQTwQlW1liJyCeCE6wlD2FhsBJRcgUnWFNTALDGSkTJF5xgZRsrEflEYII1lGpdx8pgJaIkC06wWndeReL6365ERN4LTLCmpJpgDTNYiSjJAhOsJU0BvKWViJIsMMGakmaKEi5O8ooQ0bdecILVagrgf2YhomQLTLDaTQFsYyWiZAtMsPLkFRH5RXCCNc0+ecXrWIkouQITrPZDWFhjJaJkC0ywIiUFKShmsBJR0gUnWEMhhBDhVQFElHSBCtYUhFljJaKkC1ywssZKRMkWqGANIYIwrwogoiQLTrCmpFhNAZLsNSGib7ngBCubAojIJxIWrCIySUQKRGSNq9/JIjJHRDZb702t/iIi40QkV0RWiUi3uBdY0hTgYSGIiKohkTXWfwC4oky/0QDmqmoHAHOtzwAwAEAH6zUcwIS4l1bSFFDd1SUi8kbCglVVFwDYW6b31QAmW92TAVzj6v+aGosANBGRVnEtMCWF17ESkS/UdhtrS1XdYXXvBNDS6m4NYLtrvDyrX+x4HSsR+UTSTl6pqgKI+9ooERkuIstEZFlhYaEzwGoKYI2ViJKttoN1l32Ib70XWP3zAbR1jdfG6leOqk5U1e6q2r1FixbOAKspgP+ZhYiSrbaDdQaAIVb3EADTXf1/al0d0AvAfleTQWx4HSsR+URqomYsIv8EcCmA5iKSB2AsgD8AmCYiwwBsA/ATa/SZAK4EkAvgCIChcS+wpCmAd14RUXIlLFhV9aYKBvWLMq4CGFGjBZZcx8oaKxElV3DuvAJMU0CEwUpEyRWoYA2J8qoAIkq6QAVrCq8KICIfCFawShgRNgUQUZIFKlhDoqyxElHSBSpYTVNAoIpERHVQoFIoRfgQFiJKvkAFa0gUYWUbKxElV6CCNUUivI6ViJIucMHKqwKIKNkCFazmqgAGKxElV6CCNSUUAZ/BQkTJFqhgNTXWQBWJiOqgQKVQCq8KICIfCFawhiKIMFiJKMkCFawhAZsCiCjpApVCKRJxmgI+/xxYuDC5K0RE30oJ+w8CyZASUqcp4MILzbvyMgEiql2BqrGaW1oDVSQiqoMClUKlaqxEREkSqGANhVhjJaLkC1QKpTBYicgHApVCbAogIj8IVLCGBKyxElHSBSqFUlLYFEBEyReoFEoJKSLBKhIR1UGBSqFQiE0BRJR8gUqhlBB48oqIki5QwRoSRRgpQDjs9HR3ExHVgkAFa3pqGMc1DXrsuNPz+PGKJyAiSoBABWtGWjEUIZw47ArTEyeSt0JE9K0UqGBNTzWH/UUHXWHKGiv50YcfAnPmJHstKEEC9djADFewNrJ7MljJjwYONO98rGUgBarGmpFmgvXYIVeNlU0BRFTLAhmsRYeKnZ6ssRJRLWOw1kW//z0gwsNIIp8KVLCmp0UAVCNYd+8G1qxJ0FolwJgx5v3YseSuBxFFlZSTVyKyFcBBAGEAxaraXUROBjAVQDsAWwH8RFW/iWe+GSdZwXrYdVNALG2sF1wAbNlSd2qAoRAQiQBHjgAZGcleGyIqI5k11u+raraqdrc+jwYwV1U7AJhrfY6LHazHjkacnsePmxAqLKx4wi1bzHsiaoAHDgBnnw0sXuzdPMW6bffoUe/mSfHZtAk4eND7+Y4dC1x5pffzpVrlp6aAqwFMtronA7gm3hmUtLE+8Uen5/HjwPTpwHe+U3m4AkBBQbyLrNqyZcDmzcCDD3o3z5D1ZztyxLt5UnzOOQe47LLqTVvZkdFvfgN8/HH15ku+kaxgVQCzRWS5iAy3+rVU1R1W904ALeOdaUZmGgCg6GCZO6+2bgWKioANGyqfwa5d8S6yaiedZN4PHPBunn4N1nnzzPccdBHriGjJkujDi4qAW28Ftm2LPry4OHr/6njxReDzz72bnx+cey7wxBPJXosaSVawXqSq3QAMADBCRC5xD1RVhQnfckRkuIgsE5FlhWVqoBldzgEAFCEDGDfO9Dx+HDh0yHTbh/wV6dEDuOOOuAtTKXvZX3wB/O9/pru4GPjlLyv+4VXFDlY/NQVs2AD07QuMHJnsNYnfsWPAb38b+/dZ1XizZwNvvFHxd+Flk9PIkcCFF9ZsHl9+6a/zCxs2AI8/7t388vLMCeqyduwwzWpvvundsixJCVZVzbfeCwC8D6AngF0i0goArPeox+WqOlFVu6tq9xYtWpQalt6zCwDg2EmNzI8cAA4fdtrCvvyy/AzLPv3q1Ved7oMHgQUL4itcWe52uNtuM++LFwN/+QswdGj15ulljXXvXmD58prPZ98+856TU37Y6tXAT3/qbU3NS3/9K/DYY8ALL8Q2flXBatdopYJHWLqDtaJAiyXovLj5ZdEi4KyzgL/9rebz8kJ1A37nzoqfZNe2LdCqVfn+69aZ9wSUvdaDVUQaiEim3Q2gP4A1AGYAGGKNNgTA9HjnndGmOQCg6N7Rzhe5c6dTa1y0CMjKMj/+994ze8a9eyue4ZAhwPe+V7O2V3ewploXYdT05FNl0y9aBKxaFfu8+vcHuneveY3FDpNIpPyw668HXn/dtDX7kd1ME+uOqqrx7O8gVMHPyx2sFe1sYqnVetG8tH69ef/005rPywvVue68oMD83seOrXicaN+z/TtKQG09GTXWlgAWishKAEsAfKSq/wbwBwA/EJHNAC6zPsfFvvKoqMV3gKZNgfR04OuvnXCbM8dcr/rxx+bHfuml5nCgIitWmPeabMDuYG1kPcHA/tHE+6zY/HzzXlmNtXdvoEuX2Odp11Zreobbnj5asPr9Jo2qghAA5s83VwIAVQer/SOOJVgrCtBYdrr2UUJNVFbmZKjOUZh9buT99ysfL9q2CQQjWFV1i6p2sV6dVPVJq/8eVe2nqh1U9TJVraQqGV1JsBbB7I1OO80Eq11jtb3xhnnftavydk57o6tsAz561Jwcq4gdOBdd5FyVYAe1ey+6ahUwd27F83n7baBNG9M0kYjLrfbsqdn0djnDYeAXvwAmT3aG2Yeshw/HNq+dO00ZP/ywZusUq2jBGg4DkyY5O4VLLzVXAgDRv/f9+80VIICzvcXSFFBRsMYSMPv3Vz1OVeyde03D5a67gL//vebrE+s24lbZ9+0O07KVKPs7DkKwJlJqqvltlJyYtoO1bG3MblsBzImGaMJh5w9VWXPBLbcA7dtX3N518KCpObdp4+xZ3SFk69Kl8st3/vtf8750qRMAhw6Z8KnJhpFmrqTA735n5rNtW8Vnu23hMPDQQ6U3VHeNdfx4pz0ZcMKpqh/NnDlmJ2W30774YqylKO/vfwfWro1t3GjBumABMGxY+e3jxRdL74RUzWvgQHPy8/hx57soKIi+XcQSrGvWAL/+deVHNTWpsRYXm+/ankcs29CsWeXXd9Uq811PnAjcfnv118cWa431iy/MjmXdOue3Ea327d7m7JPHZYfZf/89e4Bv4ronqUKBClYRoHFj13Z/2mlm46nsMPcvfzHvKSlOv7w805RgX0Xw1FPmBAdgNkj3Rjjdagp211oPHHAOsQ8cADIzgVNOcdpqo9VYq5Kebt6PHXMC/9FHzQ/6jTfMYWp1Ata+HGzSJNPeds455k40t+PHSzeHLFgA/OEPwM9/bj4fPuz8qNyXW0UiwMMPO2dkyx45lNW/P3Deec5nd3k2bqw68N3T3X57+SaRiROd5h3AnMw8dswJP3cI2of9ubmlb3ceORJ44AHn8/nnA6ef7pzkLChwtrcFC4BRo8qvnzuc3M0k7trVP/8JPPusKXdFYqmxfv65mU9ZY8aYCoF9CWJVzVJLlwJXXAGMLnPfTpcupQP1k0+qXqfKxBKsxcVAt27AgAFAp07A/feb/tFqrO7ttmwFqeyOvnlzc727BwIVrID5XrZvtz707Wv2UkuWmO6f/cz8eAHgRz8CBg1yJrQDBjBnEd1hPH8+MGKE2cjT0sy8Onc2h6yZmWYc94mZ6683J4Sys03wZmYCLVuaP7L7KoX9+4EnnzThYwuHzQb85z+XLphdszx+3Nkz2xvKT39qArE615C6y92pk/Ojd4fgj35k9lg2ezn2j8B92L9zp9O9bp3ZKdkqq7Hayzt61Pl+3MHasWP5wK+IHTjusCguNoer3/2us5yzzjKhYC/bXebcXPN+773mhKebHbqAKWPJBgdTfve2M3Vq+fVz/53cIesOFbs93b2sTz4xy1I1Owl7nIqsXGkuxfr1r8tvG3aFwK7VVxXSdqXAPtlVkR/8oPLhVYmlKcBuUit7/W5hYfkatbtcZWuj9rLsow6g6p1/jAIXrG3burbzIUOcE0bf+Q4wYYK5fnTAAHOd3FtvmcPOzz4rXVOqyAcfmPf/+z9zCdH8+c78r7rKPHUKcPbaK1eaw+XMTBMMgPkh2nvR/HzgkUdKh89HH5lDrnvuKf0QCSgAABJuSURBVL1suzZ14EDFtQv3db07d5p1BEwNvFs35zB7yBCntmzXhMtyHwLb3fZGZ6//qlWmRuZeH/eGWfaGi2gbraqpcbuvMbZ/ANFq4JGIKcPXX0dfb8ApN+D8sNzhBzhB8cEHTnncP2o7WKOp7MdfNlj37jVHFe4QrKgpwB2seXnmffp0U55IxIRWt26mLf6uu8y2XJaqqTBcd53ZsdvcAQ04tWO7Wayy5i73eubmAk8/bUJt5cro486ZE99JqO3bgT59zG+loulUndp7RTfy7NhRugkKiK3GqlrzcwxlqWqdfX33u9/Vsu6+W7VpU1ePG280+6Nf/KLcuKXs3KnasKG974rt9bOfle/33HOlPw8apPrBB6qbN5vPJ52keuedFc/z8sud7mefVT12THXrVtXhw02/wYNV69ePfR0jEdWHHzbd3/mO0//111WLilTbtq142kOHzPT255wc1cJC1d//vvR4t90WffpXXy39+c9/dsrz3HOqmzapfvpp+el+8xvzfu65qqNHq158sTNsxw6n+/bbVd9+23zHjRqp9uih+v77ped1+ummDAMHOv1WrVIdN850Z2Q4w26+2WwLY8bEtx24XyNHql50Ufn+117rbGsffuj0X7LE6b9li9O/7La4bp3T3aBB6WEipozLl5vlR1uv3r1V//MfZ1lnnll6eIcOpX8P69apnn22arduZjvp3Tu+72HYMGde33xj+k2eHP2396tfmeG//a35rdjzsJ04oXr//c7f6L33Kl922UCw+z/ySOlhjzxi+vfsqfrZZ854J04ogGWq1c+mak/oh1e0YLV/8/v2WT3WrDE//M8+KzduOStXmo0plg3nlFNiG88WDjv9Tj9d9eSTVb/8UvW66yqfvnNn837qqeb9sstUQyHVrCwTTI0aVT79nj0mgKIN69Gj8mnnzTMhaH9+6inV888vP17LltGnLxtQZce78UbVN9+M7Xu0X/GO7/7uqnpddZUJmHjnH8srK8uE35/+5Ow4ANV+/ZxtZM2aiqcfMiR6/3r1zPuUKWYnUdV6LF9uluXeyQImrBctMjupZ59VveUWZ9iAAfGXt3t31fx8U2Gxd3bdupkd66efmu/Cdu+9ZvgTT5T++7Zqpfrkk+Xn3apVxctt0kT1889Vn35a9a9/LT1sxIjSv3d7uZ06qY4f74yXn68M1jLsCtDEieUGxe7f/y79RW/YUPoP9N//On8UwNSGr722fC1ApPR8X3/dGWav+7XXlt84WrSoeMPJzDTvv/2tmf6MMyrfwMtuXNFe992n+sUXqrfeqiUBA6iOGqXav3/V01f06tPH2aCjDW/eXPXxx+Obp7vm6fWroh2EV68JE6L3j0RUr7++evP84Q+rHmfZMmcHPG6c6q5dqmlpzvCmTctP06hRfEdGsbyaN3dCccAAE+LPPFN6HLsWWZNlROvfoIHqTTepLl6sesMNppIwbJgz3F3W5cuVwVpGJGIyq0ED1b/8xVQUq+1Pf1Jdvdp0P/mkqRH86lfm86xZzoZQXGz6FRSoZmebQ71//lN17dry87T/eLNnm8///rf5/P3vO8Pcf3Ag+qHpM8+Y6bt2jb4h2dOkpVXdxGHvyY8cMdPt3m02vrI/zmg/QLsG26hR6cNV+/XQQ6XLHW2Db95ctW9f83nfvqp/PE2amB2fu9kklldOTvQa/qRJpZsb7Ffjxk6N0H61b+90u482WrWq+Cjme9+LXm67+7TTKl9v986zsLD0sCeeKD/+Cy843Wlp5vsvLi4/3umnO39/u98DDzjdN98c3/cby8uuGFT0cgd+Za+yf5cHHjBhGW3cRYucsjZtqpqaGn08u2IxcqQyWKNYu9bkG2B+9y+/rHrgQNRRa2bx4viTe9o0c2hUVlGRCYpZs1RffNGs/Ntvm8Opw4fN57PPNk0IgGmzVXUCoXdvZ8P49FMTjvYGM2qUeXXpUnojstuz+vcvvz52uxhgDqtUVd95R7V1a9Xp050fwOHDppklJ0f16FHTz13j3r3bTFt2Iz7zTCekfvhDc4i4bZsZ9803zTLffbd0e9qECart2jnf31dfmUOTTz6J7ccYiZhaS9n+hw+r/vznprthQ7Nj7NpVde9es172eB98oLpihTlCsdfHHjZwoOodd5T+zu3yuWtlF15omibuvtvZOQPmKGHUKNXHHjM1Off6zZtndsB3313+u9y4UbVXL+dw/YUXzDhvvGHKUVTk/E3d0513nvl7vfuu6tKlTv/8fKd70iRn3AsucPo3bGiGrV9vKgEzZzoVg0succb7739N26j7nMKMGeUD+913ox/yu2uRf/iD03333aZtetIk0277zjuqx4+bALXHeecd8263HbvbvR97zGxHgNmpT51qmixUVX/5S1WAwVqRSET1pZdM8wmgmp6ues01qo8+atrw7d+7L4XDpp3IbeNGc+Jm1y6nFq1qaonnn29qlOGwaSdWNV+A/cN2j79vnzkZoGrmBaj+7W/R12PjRvNDi2b/fnOypayVK81eDDA1PlunTqpXXGEOeb//fdPWN3WqGc99UiWad981P97KrFtn2g0HDzbf0+7dptzNmpnwXbzYjLd9uwmhDz9UfeUVZ/qDB81JvrLfu6ppc3zsMaddMBJxXgsWmNr2hg0mqN56y4T0/v0mmOfPN0cCY8eaYD582NQ6jxxR/fprU/4HHyy/zDfeUM3NNeUq6777zMZs/61tK1c6f9uKvqN27Uxguds49+4163HxxeazO2Q3bjTbTCRi2ttnzCg9rS0ScbaVRYucpirblCmmYnD8uNlOi4vNjmj+fGeccePM4ebUqebvs3OnaevdsMEMHzHC1EwrM3682V4iEdOWbVdAtm0zQT9njln+1q2mkuM+eWiX4/nnaxysoqreXmZQi7p3767L7NsIKxCJmKuppk0ztxLbV7EAQOvW5tGPZ55pLuE891xzOWjbtgle8dqyb5+5JOqSSyoeJxwufXOEVwoKzAXbZZ5AVs7u3ebC7G+r7dvNXXkV3f7qtePHzaV29euX7r9xo/khpKaa6163bTO3UX9Lichydf67SfzTBz1YyzpyBPj3v81lk6tXm9e2baUvcWva1IRrmzbm3X7Zn08+2WyX9ep5XCAi8oWaBmtS/plgMtWvb66ddlM11xZv2mQqeBs2mJrt9u3mpq1oz8gFgGbNTNiefDJw6qnmJqazzjI3KTVq5LyaNDHjZ2aaaVTNNH57sBAReeNbF6zR2A/COu008xCjso4eNTfObN9uAnfvXnPThv35m2/MY1APHar632rZUlJM6J50krlbNTXVvDdoYAI4M9MMs19NmpidQigENGxoasvp6aZ/cbHprl/fhPZJJznztudftrtePdMKEA6bedqvjAwGPlFNMVhjUK+eqYmedVbV4x47Zu5oPHjQ3E23f79p6oxEzOc9e0xwFRaa/idOmGA8ccK8Dh824+3aZZrD7Nc335iAj0S8eXB8ZUIhE/ypqc572SbA5s3N96JqxklJcaYTcR5pYM8j2quoyLynp0dvYhQxy7HLm55udgrFxc6zS+wdUEqK2UkcOWLWq7jYTG8vy70e7nKlpDjfaWZm6fWwu6P1S011nqbmLnsoZP5+zZqZv1mTJuY7sqe1X2U/u1/2c34aNXLKUa+eWUdVZ9n2zvj4cTMsLc1sP5mZZj1EzI46JcXZ1sqWp0EDU/569cx87HWwy2OXyZ6f+xlEZctS0fcW63fqt3nVBIPVY+np5pXI8zFHj5ofweHDppacmmo+HzliwqWoyBnH/tG53+1pU1LMj1HV/LiKi82OobjYvMJhp9tN1dyqbz9Z0a752i/AtFNHIk4/e97u8dLTzTgVPTkvHDZNMfZzYo4dM69QyAn1gwdNueyad/36TmDbQeBeB6LawGCtg+rVMy/3A6coNvYOxA5bO5Tr1y/9jBi7VuY+t+vud+KEs+Mo+16/vmkuatzY1F7t/8hjX8fk7i77ApyLNA4eNNOqmh1lWlrpnYV9tBMKmZ1UcbGpgR444Mzv8GEzbuPGZhz3suydaEqKmb89D3v97DK5X3YtvWxZKvreKhsW7/i1NS9V82ykmmCw0rdKKFT6SYluZa9Aom+vmgYrT1MQEXmMwUpE5DEGKxGRxxisREQeY7ASEXmMwUpE5DEGKxGRxxisREQeY7ASEXmMwUpE5DEGKxGRxxisREQeY7ASEXmMwUpE5DEGKxGRxxisREQeY7ASEXmMwUpE5DHfBauIXCEiG0UkV0RGJ3t9iIji5atgFZEUAOMBDABwHoCbROS85K4VEVF8fBWsAHoCyFXVLap6HMBbAK5O8joREcXFb8HaGsB21+c8qx8RUZ1R5/79tYgMBzDc+nhMRNYkc30SrDmA3cleiQRi+equIJcNAM6pycR+C9Z8AG1dn9tY/Uqo6kQAEwFARJapavfaW73axfLVbUEuX5DLBpjy1WR6vzUFLAXQQUTai8hJAG4EMCPJ60REFBdf1VhVtVhEfgFgFoAUAJNUdW2SV4uIKC6+ClYAUNWZAGbGOPrERK6LD7B8dVuQyxfksgE1LJ+oqlcrQkRE8F8bKxFRnVdngzUIt76KyCQRKXBfMiYiJ4vIHBHZbL03tfqLiIyzyrtKRLolb82rJiJtRWSeiKwTkbUiMsrqH5TyZYjIEhFZaZXvCat/exFZbJVjqnUSFiKSbn3OtYa3S+b6x0JEUkTkCxH5l/U5MGUDABHZKiKrRSTHvgrAq+2zTgZrgG59/QeAK8r0Gw1grqp2ADDX+gyYsnawXsMBTKildayuYgC/UtXzAPQCMML6GwWlfMcA9FXVLgCyAVwhIr0A/BHA86p6FoBvAAyzxh8G4Bur//PWeH43CsB61+cglc32fVXNdl065s32qap17gWgN4BZrs8PAXgo2etVzbK0A7DG9XkjgFZWdysAG63ulwHcFG28uvACMB3AD4JYPgD1AawAcAHMRfOpVv+S7RTmSpfeVneqNZ4ke90rKVMbK1j6AvgXAAlK2Vxl3AqgeZl+nmyfdbLGimDf+tpSVXdY3TsBtLS662yZrUPDrgAWI0Dlsw6VcwAUAJgD4EsA+1S12BrFXYaS8lnD9wNoVrtrHJcXADwAIGJ9bobglM2mAGaLyHLrjk7Ao+3Td5dbkUNVVUTq9GUbItIQwLsA7lHVAyJSMqyul09VwwCyRaQJgPcBdEzyKnlCRH4IoEBVl4vIpclenwS6SFXzReQUAHNEZIN7YE22z7paY63y1tc6bJeItAIA673A6l/nyiwiaTChOkVV37N6B6Z8NlXdB2AezOFxExGxKyzuMpSUzxreGMCeWl7VWPUBMFBEtsI8Ya4vgD8jGGUroar51nsBzI6xJzzaPutqsAb51tcZAIZY3UNg2ibt/j+1zk72ArDfdcjiO2Kqpq8CWK+qz7kGBaV8LayaKkSkHkz78XqYgP2xNVrZ8tnl/jGA/6jVWOc3qvqQqrZR1XYwv63/qOpgBKBsNhFpICKZdjeA/gDWwKvtM9kNyDVoeL4SwCaYdq2Hk70+1SzDPwHsAHACps1mGEzb1FwAmwF8AuBka1yBuRLiSwCrAXRP9vpXUbaLYNqwVgHIsV5XBqh8nQF8YZVvDYDHrP5nAFgCIBfA2wDSrf4Z1udca/gZyS5DjOW8FMC/glY2qywrrddaO0O82j555xURkcfqalMAEZFvMViJiDzGYCUi8hiDlYjIYwxWIiKPMViJLCJyqf0kJ6KaYLASEXmMwUp1jojcYj0LNUdEXrYehnJIRJ63no06V0RaWONmi8gi6xma77uer3mWiHxiPU91hYicac2+oYi8IyIbRGSKuB9uQBQjBivVKSJyLoBBAPqoajaAMIDBABoAWKaqnQDMBzDWmuQ1AA+qameYO2bs/lMAjFfzPNULYe6AA8xTuO6Bec7vGTD3zRPFhU+3orqmH4DvAlhqVSbrwTwoIwJgqjXOGwDeE5HGAJqo6nyr/2QAb1v3iLdW1fcBQFWLAMCa3xJVzbM+58A8L3dh4otFQcJgpbpGAExW1YdK9RR5tMx41b1X+5irOwz+Rqga2BRAdc1cAD+2nqFp/4+i02G2ZfvJSzcDWKiq+wF8IyIXW/1vBTBfVQ8CyBORa6x5pItI/VotBQUa98ZUp6jqOhF5BObJ7yGYJ4ONAHAYQE9rWAFMOyxgHv32khWcWwAMtfrfCuBlEfmNNY8barEYFHB8uhUFgogcUtWGyV4PIoBNAUREnmONlYjIY6yxEhF5jMFKROQxBisRkccYrEREHmOwEhF5jMFKROSx/wdVHd5gpnDnNgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JHn9Tl2zD3we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e1da6f-1ccc-490a-b756-0857dbfcae2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,137\n",
            "Trainable params: 3,009\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Pd6ThmMkD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6f161d-18cb-41b3-ee25-61a97307bd3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 6ms/step - loss: 3624.6265 - val_loss: 3539.8391\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3265.2278 - val_loss: 3362.7090\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 2785.9653 - val_loss: 2615.0259\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 2207.5818 - val_loss: 1772.2808\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1590.3557 - val_loss: 1371.9055\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1028.4991 - val_loss: 887.7119\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 590.5450 - val_loss: 438.4668\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 301.3616 - val_loss: 292.7720\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 142.3763 - val_loss: 127.7292\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 71.2635 - val_loss: 74.0698\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 45.4827 - val_loss: 60.5461\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 37.5955 - val_loss: 61.2194\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 35.4817 - val_loss: 47.3387\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.5749 - val_loss: 49.3185\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 34.0465 - val_loss: 42.4213\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.6921 - val_loss: 40.2163\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3501 - val_loss: 50.2350\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.0279 - val_loss: 44.4221\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.9271 - val_loss: 38.6701\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.6841 - val_loss: 60.7469\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3274 - val_loss: 45.5171\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1898 - val_loss: 45.7916\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0169 - val_loss: 42.9724\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8583 - val_loss: 47.1592\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.7590 - val_loss: 58.6381\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.6076 - val_loss: 38.4997\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2829 - val_loss: 40.0308\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.2211 - val_loss: 43.7336\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1759 - val_loss: 42.3728\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.9108 - val_loss: 48.9202\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7769 - val_loss: 40.6360\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.6964 - val_loss: 48.3823\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.6126 - val_loss: 36.3507\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.4795 - val_loss: 42.3092\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3812 - val_loss: 46.0937\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.3202 - val_loss: 43.1381\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1484 - val_loss: 36.1247\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.1104 - val_loss: 52.7298\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1942 - val_loss: 43.6940\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9798 - val_loss: 43.4094\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8690 - val_loss: 40.8164\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8621 - val_loss: 48.6941\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7891 - val_loss: 56.7995\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.8633 - val_loss: 36.6141\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7302 - val_loss: 35.2596\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5716 - val_loss: 39.9696\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5541 - val_loss: 35.9524\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5325 - val_loss: 53.5187\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.4337 - val_loss: 37.0878\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.4073 - val_loss: 35.3549\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.3510 - val_loss: 33.6520\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.3377 - val_loss: 34.0580\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.2952 - val_loss: 38.4985\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2950 - val_loss: 38.4544\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3141 - val_loss: 38.1373\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1768 - val_loss: 35.6983\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1138 - val_loss: 35.2427\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.1326 - val_loss: 56.4195\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0925 - val_loss: 36.8099\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0451 - val_loss: 34.2213\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0770 - val_loss: 37.7566\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0005 - val_loss: 38.5758\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0344 - val_loss: 36.4944\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9036 - val_loss: 35.7614\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9668 - val_loss: 44.7367\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9370 - val_loss: 39.4033\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8090 - val_loss: 38.9400\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8012 - val_loss: 37.7096\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7716 - val_loss: 39.4958\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8486 - val_loss: 39.1412\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7578 - val_loss: 41.3241\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7507 - val_loss: 35.9209\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7558 - val_loss: 47.7842\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7045 - val_loss: 34.0621\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7328 - val_loss: 33.3333\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6610 - val_loss: 41.9782\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6820 - val_loss: 37.7069\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.5640 - val_loss: 42.7236\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 28.5070 - val_loss: 34.6178\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 28.5561 - val_loss: 34.9241\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4911 - val_loss: 58.6107\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4733 - val_loss: 37.2012\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.5626 - val_loss: 36.6932\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4411 - val_loss: 36.8506\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4582 - val_loss: 38.0093\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3922 - val_loss: 34.4435\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4421 - val_loss: 35.2804\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3528 - val_loss: 59.6014\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3988 - val_loss: 37.0260\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3598 - val_loss: 37.4660\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3283 - val_loss: 37.6408\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2593 - val_loss: 39.5439\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3164 - val_loss: 41.9911\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2766 - val_loss: 41.6453\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1977 - val_loss: 49.9095\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3572 - val_loss: 36.9554\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2142 - val_loss: 34.5210\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.3095 - val_loss: 34.2116\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2201 - val_loss: 50.1158\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1670 - val_loss: 35.6422\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1576 - val_loss: 37.3136\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1327 - val_loss: 37.1389\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0896 - val_loss: 35.5956\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1481 - val_loss: 37.0382\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0995 - val_loss: 43.9078\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1117 - val_loss: 36.3923\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0644 - val_loss: 39.3118\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0858 - val_loss: 36.3181\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0548 - val_loss: 36.1624\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0760 - val_loss: 48.2965\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9291 - val_loss: 38.0139\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9295 - val_loss: 38.3888\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9656 - val_loss: 37.6219\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9703 - val_loss: 37.0767\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9232 - val_loss: 34.6252\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8487 - val_loss: 38.6351\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9741 - val_loss: 35.7453\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9852 - val_loss: 35.8610\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9640 - val_loss: 33.7513\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8859 - val_loss: 35.3938\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8438 - val_loss: 35.7452\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9072 - val_loss: 35.8083\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8919 - val_loss: 44.1543\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9119 - val_loss: 55.6427\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8487 - val_loss: 35.1360\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8035 - val_loss: 42.3279\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8258 - val_loss: 54.5378\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8080 - val_loss: 35.6152\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7947 - val_loss: 42.3765\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7536 - val_loss: 35.0030\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7715 - val_loss: 35.4920\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8157 - val_loss: 35.3015\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7808 - val_loss: 40.8368\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7675 - val_loss: 36.8913\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7202 - val_loss: 35.5442\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7303 - val_loss: 34.0477\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7216 - val_loss: 42.2222\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7225 - val_loss: 33.6858\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7045 - val_loss: 33.7489\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6820 - val_loss: 42.7124\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7016 - val_loss: 35.8836\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6886 - val_loss: 40.9061\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6735 - val_loss: 37.6160\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6334 - val_loss: 36.1841\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6722 - val_loss: 45.7502\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6317 - val_loss: 40.4064\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6103 - val_loss: 39.5586\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6527 - val_loss: 36.1128\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5641 - val_loss: 34.4519\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6611 - val_loss: 44.1902\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5584 - val_loss: 36.3935\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6404 - val_loss: 37.8453\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6301 - val_loss: 34.1858\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5971 - val_loss: 37.1839\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5999 - val_loss: 39.9626\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6150 - val_loss: 34.1315\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5893 - val_loss: 36.0782\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5278 - val_loss: 50.4501\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5740 - val_loss: 46.4559\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4837 - val_loss: 47.3045\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5749 - val_loss: 35.5560\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5114 - val_loss: 42.4733\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4677 - val_loss: 37.2779\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5288 - val_loss: 37.6773\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5648 - val_loss: 33.2710\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4749 - val_loss: 46.3298\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5997 - val_loss: 37.8666\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4823 - val_loss: 36.0369\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5253 - val_loss: 39.0691\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4582 - val_loss: 51.8281\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5123 - val_loss: 34.0275\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4565 - val_loss: 35.3468\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3693 - val_loss: 38.8355\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3885 - val_loss: 34.8275\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4427 - val_loss: 52.2771\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4374 - val_loss: 41.0537\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3707 - val_loss: 36.7745\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4153 - val_loss: 38.6504\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3831 - val_loss: 38.2388\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4581 - val_loss: 39.2357\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4031 - val_loss: 35.9089\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3827 - val_loss: 38.1927\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3741 - val_loss: 36.2215\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4181 - val_loss: 34.8218\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4088 - val_loss: 35.9182\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3997 - val_loss: 36.7402\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3585 - val_loss: 34.5669\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3953 - val_loss: 36.1225\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3976 - val_loss: 40.7240\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4038 - val_loss: 37.7960\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3241 - val_loss: 35.8476\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3234 - val_loss: 42.9510\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3310 - val_loss: 36.4752\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3535 - val_loss: 40.8293\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3713 - val_loss: 34.2983\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3402 - val_loss: 34.6389\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3296 - val_loss: 35.9830\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3208 - val_loss: 44.3146\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3155 - val_loss: 38.3573\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3959 - val_loss: 36.5867\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2893 - val_loss: 39.8920\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2742 - val_loss: 35.4144\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2953 - val_loss: 39.9325\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3149 - val_loss: 36.0956\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2592 - val_loss: 38.6972\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2372 - val_loss: 33.5613\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2573 - val_loss: 35.6218\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2043 - val_loss: 35.2795\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2520 - val_loss: 35.9141\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3005 - val_loss: 38.1435\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2506 - val_loss: 34.6190\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2608 - val_loss: 35.8359\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2312 - val_loss: 41.6001\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2238 - val_loss: 33.5370\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2141 - val_loss: 34.1841\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2092 - val_loss: 33.7801\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2404 - val_loss: 35.1733\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2181 - val_loss: 37.8307\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2115 - val_loss: 33.3771\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1945 - val_loss: 33.2604\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1254 - val_loss: 39.8339\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1813 - val_loss: 37.2208\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1826 - val_loss: 34.9575\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2050 - val_loss: 32.9389\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1702 - val_loss: 34.2206\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2466 - val_loss: 36.0105\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1913 - val_loss: 36.7404\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1308 - val_loss: 34.9053\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1120 - val_loss: 34.6191\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1409 - val_loss: 43.1469\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1865 - val_loss: 33.7463\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1317 - val_loss: 34.1308\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1379 - val_loss: 36.0200\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1344 - val_loss: 35.4430\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1488 - val_loss: 37.0306\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1575 - val_loss: 36.4072\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1419 - val_loss: 33.5616\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0873 - val_loss: 37.3835\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1238 - val_loss: 33.8507\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0859 - val_loss: 40.4388\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1796 - val_loss: 64.1329\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1532 - val_loss: 37.8104\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1603 - val_loss: 34.6043\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1921 - val_loss: 34.7088\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0916 - val_loss: 36.9308\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1226 - val_loss: 39.1222\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1454 - val_loss: 39.1428\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0825 - val_loss: 51.4834\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1211 - val_loss: 35.0066\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0060 - val_loss: 34.3072\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0147 - val_loss: 35.1617\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1120 - val_loss: 34.0830\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0787 - val_loss: 38.5704\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0307 - val_loss: 37.5576\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0507 - val_loss: 36.7557\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0163 - val_loss: 33.5334\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0257 - val_loss: 37.4823\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0390 - val_loss: 37.2389\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0364 - val_loss: 33.7977\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0835 - val_loss: 33.0218\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0347 - val_loss: 35.4612\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0586 - val_loss: 42.0158\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0198 - val_loss: 33.4268\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9842 - val_loss: 35.9086\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0976 - val_loss: 35.0334\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9918 - val_loss: 42.2612\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9567 - val_loss: 33.9299\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9990 - val_loss: 34.0252\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9928 - val_loss: 36.4168\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0235 - val_loss: 32.9740\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9099 - val_loss: 36.5694\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9927 - val_loss: 39.0475\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9815 - val_loss: 36.8234\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0421 - val_loss: 33.2840\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0358 - val_loss: 36.1165\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0414 - val_loss: 36.4240\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0280 - val_loss: 35.8459\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9603 - val_loss: 36.8108\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0294 - val_loss: 36.1555\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0242 - val_loss: 39.7165\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0154 - val_loss: 36.4098\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9716 - val_loss: 35.5664\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9182 - val_loss: 40.8567\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9651 - val_loss: 39.2660\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8802 - val_loss: 40.3063\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9497 - val_loss: 35.2476\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8783 - val_loss: 38.1270\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0705 - val_loss: 36.6023\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9228 - val_loss: 41.5282\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9387 - val_loss: 32.9864\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9491 - val_loss: 38.5673\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9138 - val_loss: 38.8933\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9336 - val_loss: 33.6332\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8995 - val_loss: 35.7283\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9021 - val_loss: 40.6899\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8776 - val_loss: 36.5184\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9446 - val_loss: 34.4247\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8304 - val_loss: 38.2928\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8982 - val_loss: 33.9122\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9235 - val_loss: 34.1452\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9059 - val_loss: 33.6801\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9678 - val_loss: 33.6915\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8501 - val_loss: 35.0703\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8870 - val_loss: 37.8201\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8464 - val_loss: 33.1851\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0328 - val_loss: 43.5772\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9589 - val_loss: 37.5243\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8104 - val_loss: 43.3845\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9036 - val_loss: 34.4447\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8820 - val_loss: 36.4164\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8246 - val_loss: 37.8981\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8420 - val_loss: 33.9867\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8659 - val_loss: 34.4680\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8872 - val_loss: 33.7225\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8845 - val_loss: 37.6927\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9103 - val_loss: 41.9224\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8755 - val_loss: 35.5804\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8935 - val_loss: 36.3253\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8820 - val_loss: 41.7439\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8893 - val_loss: 35.4416\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8380 - val_loss: 38.6013\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8430 - val_loss: 33.9080\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7944 - val_loss: 36.7975\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8928 - val_loss: 34.7762\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8882 - val_loss: 32.6180\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8700 - val_loss: 41.7171\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8376 - val_loss: 36.7972\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8754 - val_loss: 36.3744\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8409 - val_loss: 36.4069\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7925 - val_loss: 34.8726\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8775 - val_loss: 45.4679\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8478 - val_loss: 36.7326\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8510 - val_loss: 35.1011\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7656 - val_loss: 35.1444\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8006 - val_loss: 34.6813\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8290 - val_loss: 37.5715\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7744 - val_loss: 33.9923\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7889 - val_loss: 43.4925\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8419 - val_loss: 46.3258\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8878 - val_loss: 34.9508\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7764 - val_loss: 36.3896\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7546 - val_loss: 33.9300\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8832 - val_loss: 34.1816\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8253 - val_loss: 37.9503\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7800 - val_loss: 33.9970\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7561 - val_loss: 34.9866\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7961 - val_loss: 35.4881\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7431 - val_loss: 35.9981\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8206 - val_loss: 33.5475\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7722 - val_loss: 33.6504\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7782 - val_loss: 35.0264\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8109 - val_loss: 39.5424\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7862 - val_loss: 35.7326\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6706 - val_loss: 35.8776\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8507 - val_loss: 44.8533\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7631 - val_loss: 34.1200\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7701 - val_loss: 33.3734\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7998 - val_loss: 41.3557\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7525 - val_loss: 40.1478\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7567 - val_loss: 39.2371\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7748 - val_loss: 37.2999\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7608 - val_loss: 34.4486\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7779 - val_loss: 36.3301\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7625 - val_loss: 52.5655\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6903 - val_loss: 37.2999\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7464 - val_loss: 33.6094\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6864 - val_loss: 44.5926\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8149 - val_loss: 34.6857\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7395 - val_loss: 36.8436\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7650 - val_loss: 37.6679\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7647 - val_loss: 38.6236\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7384 - val_loss: 35.1063\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7484 - val_loss: 34.6411\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7165 - val_loss: 35.6380\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7342 - val_loss: 33.4956\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7771 - val_loss: 35.6834\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7747 - val_loss: 42.2821\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6894 - val_loss: 34.1424\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6768 - val_loss: 35.2148\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6287 - val_loss: 35.2754\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7126 - val_loss: 38.4787\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7796 - val_loss: 34.1839\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6625 - val_loss: 35.8096\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7405 - val_loss: 36.4487\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6733 - val_loss: 35.4740\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7056 - val_loss: 34.4304\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6749 - val_loss: 41.1157\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7532 - val_loss: 43.3801\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6303 - val_loss: 35.3000\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6682 - val_loss: 34.8093\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6741 - val_loss: 35.4928\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6783 - val_loss: 34.9218\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6974 - val_loss: 34.4402\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6678 - val_loss: 33.8818\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6659 - val_loss: 33.0429\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6247 - val_loss: 34.9540\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6989 - val_loss: 38.0055\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6845 - val_loss: 34.2772\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6601 - val_loss: 41.7080\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6119 - val_loss: 33.6548\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6769 - val_loss: 39.9836\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6363 - val_loss: 35.7460\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6878 - val_loss: 38.5896\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6337 - val_loss: 44.9780\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6855 - val_loss: 38.3297\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6776 - val_loss: 35.0417\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6780 - val_loss: 36.6174\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6772 - val_loss: 35.1643\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6823 - val_loss: 38.4921\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6090 - val_loss: 33.8872\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6768 - val_loss: 34.5521\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5709 - val_loss: 34.7162\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6432 - val_loss: 36.0053\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5413 - val_loss: 36.7936\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5968 - val_loss: 37.5599\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6212 - val_loss: 36.2826\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6500 - val_loss: 36.6578\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5907 - val_loss: 39.4791\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5850 - val_loss: 33.3434\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6846 - val_loss: 33.0227\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 26.6003 - val_loss: 33.7039\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 26.6213 - val_loss: 36.2514\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 26.6539 - val_loss: 34.7360\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6245 - val_loss: 32.6109\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6029 - val_loss: 37.6989\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6008 - val_loss: 33.3713\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5456 - val_loss: 35.3572\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5962 - val_loss: 36.5724\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6136 - val_loss: 36.9171\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5260 - val_loss: 34.1716\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6300 - val_loss: 33.6418\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5610 - val_loss: 41.6798\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5987 - val_loss: 36.9126\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6125 - val_loss: 34.4901\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6310 - val_loss: 35.9339\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6009 - val_loss: 34.1594\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5762 - val_loss: 68.1147\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5723 - val_loss: 33.8657\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5729 - val_loss: 35.8179\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5626 - val_loss: 33.6306\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5957 - val_loss: 33.0968\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6077 - val_loss: 36.3513\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5094 - val_loss: 35.7437\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5349 - val_loss: 34.3063\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6056 - val_loss: 33.4868\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5791 - val_loss: 39.9835\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5466 - val_loss: 34.1053\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5075 - val_loss: 35.1771\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5620 - val_loss: 42.9044\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6075 - val_loss: 39.0097\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5804 - val_loss: 33.8983\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5036 - val_loss: 32.8098\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5845 - val_loss: 33.5090\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5875 - val_loss: 36.3752\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5668 - val_loss: 36.5735\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5604 - val_loss: 37.2593\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6100 - val_loss: 34.3726\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5496 - val_loss: 35.6104\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4775 - val_loss: 49.5622\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5007 - val_loss: 35.6490\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5550 - val_loss: 40.7675\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4958 - val_loss: 35.8530\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5834 - val_loss: 34.2987\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5435 - val_loss: 35.9638\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4541 - val_loss: 38.5614\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4936 - val_loss: 46.4273\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5264 - val_loss: 36.3132\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5362 - val_loss: 45.1994\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5424 - val_loss: 46.2816\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5300 - val_loss: 34.9638\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4810 - val_loss: 36.9840\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4925 - val_loss: 34.9509\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5103 - val_loss: 39.1293\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5934 - val_loss: 35.2009\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4946 - val_loss: 32.5803\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5636 - val_loss: 34.4347\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5018 - val_loss: 37.1782\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4796 - val_loss: 35.7910\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5380 - val_loss: 36.6699\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4724 - val_loss: 33.9018\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5118 - val_loss: 35.4043\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5277 - val_loss: 33.2765\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4347 - val_loss: 34.2216\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5029 - val_loss: 32.1970\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4620 - val_loss: 36.9740\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4507 - val_loss: 33.5544\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4443 - val_loss: 34.7187\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5421 - val_loss: 33.4934\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4601 - val_loss: 33.5908\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4752 - val_loss: 34.7309\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4734 - val_loss: 34.0066\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4991 - val_loss: 34.6360\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4806 - val_loss: 38.1351\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4664 - val_loss: 46.4224\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5575 - val_loss: 34.9825\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5659 - val_loss: 44.4598\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4737 - val_loss: 36.7403\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4787 - val_loss: 33.5756\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4563 - val_loss: 38.1608\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4389 - val_loss: 33.2019\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nroUKm9cD3wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1debd7b-44d7-4458-c34a-ba744a7c57d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.5231062741240288 \n",
            "MAE:  4.225422251059371 \n",
            "SD:  5.738312709374269\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kS--HwX9D3wf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "4829b874-0e88-449f-e160-7dd76866242d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tfHNI3w7D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feb2cb58-d529-45a2-b7a9-8918b3216381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,137\n",
            "Trainable params: 3,009\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fNNzFsx-D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d435b1-918a-4408-fe84-c35e2c66077e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 2s 7ms/step - loss: 3723.0024 - val_loss: 3670.9529\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3442.6826 - val_loss: 3327.6196\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 3106.5884 - val_loss: 2645.8311\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 2662.2190 - val_loss: 2269.4424\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 2152.3843 - val_loss: 2172.0205\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 1634.7240 - val_loss: 1495.4821\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 1126.5156 - val_loss: 929.6401\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 687.5409 - val_loss: 898.5430\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 375.8309 - val_loss: 317.7516\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 188.6568 - val_loss: 120.9879\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 94.5225 - val_loss: 107.9831\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 55.2413 - val_loss: 63.6110\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 41.0515 - val_loss: 47.2966\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 36.6831 - val_loss: 41.5236\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.7597 - val_loss: 59.1648\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 34.0136 - val_loss: 40.7612\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 33.6722 - val_loss: 62.1722\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 33.3684 - val_loss: 39.2354\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.8458 - val_loss: 43.0515\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.7949 - val_loss: 39.0153\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.5248 - val_loss: 44.9966\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.3969 - val_loss: 37.1129\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1606 - val_loss: 48.1622\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 32.1852 - val_loss: 38.9344\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 32.0002 - val_loss: 41.2670\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.8672 - val_loss: 41.0343\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.6539 - val_loss: 35.7666\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.5359 - val_loss: 41.1902\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.4295 - val_loss: 41.2799\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3862 - val_loss: 39.2723\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.3085 - val_loss: 39.3099\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1661 - val_loss: 37.8926\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 31.1031 - val_loss: 45.9370\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 31.0572 - val_loss: 39.9626\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.8893 - val_loss: 43.7155\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.8011 - val_loss: 37.7764\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.7933 - val_loss: 35.4151\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.7077 - val_loss: 39.7304\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5987 - val_loss: 38.4373\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.5830 - val_loss: 39.5017\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.4118 - val_loss: 39.7914\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.3960 - val_loss: 42.4563\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3034 - val_loss: 36.5425\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3599 - val_loss: 38.9680\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.3055 - val_loss: 37.7425\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.2418 - val_loss: 44.5827\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.2331 - val_loss: 55.6450\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1718 - val_loss: 35.2195\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.1186 - val_loss: 42.3326\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9984 - val_loss: 43.3897\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 30.0149 - val_loss: 43.9008\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9568 - val_loss: 40.1861\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 30.0035 - val_loss: 35.7802\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9874 - val_loss: 43.7729\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9136 - val_loss: 37.9465\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.9364 - val_loss: 38.2683\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7885 - val_loss: 36.4017\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7253 - val_loss: 41.5883\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.7286 - val_loss: 36.1608\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6779 - val_loss: 36.2393\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6272 - val_loss: 40.4613\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5654 - val_loss: 40.3245\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.6571 - val_loss: 72.0962\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.5294 - val_loss: 36.1225\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.5090 - val_loss: 45.7423\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4731 - val_loss: 46.4281\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.4912 - val_loss: 36.4772\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.4331 - val_loss: 35.0936\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.3570 - val_loss: 44.5221\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.3243 - val_loss: 35.6453\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1612 - val_loss: 41.0718\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 29.2657 - val_loss: 42.9535\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1675 - val_loss: 36.7317\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1395 - val_loss: 40.5410\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.1698 - val_loss: 35.0848\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0421 - val_loss: 48.6942\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 29.0723 - val_loss: 37.5479\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9805 - val_loss: 35.4425\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9689 - val_loss: 42.8396\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.9947 - val_loss: 42.7953\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.9029 - val_loss: 36.8235\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8661 - val_loss: 43.0017\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.8907 - val_loss: 35.0526\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8803 - val_loss: 38.7487\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.8048 - val_loss: 35.8215\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6920 - val_loss: 43.1423\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7688 - val_loss: 43.1798\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.7101 - val_loss: 37.0378\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6925 - val_loss: 37.1646\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.7239 - val_loss: 38.2189\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6055 - val_loss: 41.4102\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6303 - val_loss: 52.0672\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.6790 - val_loss: 37.3671\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5792 - val_loss: 41.5664\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.6304 - val_loss: 44.9724\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.5662 - val_loss: 34.8043\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4727 - val_loss: 34.0886\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4349 - val_loss: 35.2597\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3815 - val_loss: 37.1956\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4666 - val_loss: 38.6327\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.4518 - val_loss: 35.5998\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4379 - val_loss: 34.3428\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.4114 - val_loss: 34.0843\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3145 - val_loss: 40.6581\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3064 - val_loss: 38.2785\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.3126 - val_loss: 37.0280\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2317 - val_loss: 40.4371\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2800 - val_loss: 35.6498\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2618 - val_loss: 36.1384\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2859 - val_loss: 36.7107\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.2956 - val_loss: 37.6918\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1040 - val_loss: 36.3252\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.2214 - val_loss: 34.4188\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1352 - val_loss: 40.2923\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1805 - val_loss: 35.5360\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0855 - val_loss: 34.0463\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0979 - val_loss: 36.4571\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1354 - val_loss: 41.3631\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.1052 - val_loss: 36.1713\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.0113 - val_loss: 34.7813\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 28.1365 - val_loss: 40.8236\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0546 - val_loss: 36.1983\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9634 - val_loss: 36.4597\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0550 - val_loss: 34.3400\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 28.0057 - val_loss: 34.1827\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9471 - val_loss: 37.2812\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.9070 - val_loss: 39.2760\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9742 - val_loss: 35.3831\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8922 - val_loss: 37.0244\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8872 - val_loss: 34.3938\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9406 - val_loss: 36.5147\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9276 - val_loss: 37.1201\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.9186 - val_loss: 34.1291\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8834 - val_loss: 34.8496\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7896 - val_loss: 33.0345\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8340 - val_loss: 33.7962\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8433 - val_loss: 36.5405\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7913 - val_loss: 34.4673\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7222 - val_loss: 37.8860\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7652 - val_loss: 35.0178\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.8140 - val_loss: 40.2141\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7817 - val_loss: 57.3102\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7662 - val_loss: 34.5050\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.8160 - val_loss: 37.6057\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.7311 - val_loss: 34.3269\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6791 - val_loss: 33.5112\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.7127 - val_loss: 39.1479\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6344 - val_loss: 34.7408\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6198 - val_loss: 34.8864\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6467 - val_loss: 34.3300\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6155 - val_loss: 36.8274\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5576 - val_loss: 40.8804\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6146 - val_loss: 35.5320\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.6125 - val_loss: 37.6199\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5802 - val_loss: 39.7315\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5404 - val_loss: 34.9362\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.6170 - val_loss: 34.8592\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5445 - val_loss: 33.8767\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.5804 - val_loss: 34.1280\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5224 - val_loss: 33.1639\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5112 - val_loss: 37.1505\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.5956 - val_loss: 36.0329\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4910 - val_loss: 39.9188\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4446 - val_loss: 37.2526\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4413 - val_loss: 37.1027\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4801 - val_loss: 34.6532\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4468 - val_loss: 33.4407\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4897 - val_loss: 33.0352\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4495 - val_loss: 43.5009\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4250 - val_loss: 35.5469\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4317 - val_loss: 37.4663\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.4326 - val_loss: 33.6167\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3733 - val_loss: 38.7346\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3832 - val_loss: 35.0924\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3500 - val_loss: 44.0564\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4238 - val_loss: 37.6536\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3447 - val_loss: 34.5636\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3127 - val_loss: 39.4767\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.4095 - val_loss: 60.1304\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3779 - val_loss: 46.0817\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2845 - val_loss: 34.1228\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.3413 - val_loss: 39.9567\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2727 - val_loss: 33.5050\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.3310 - val_loss: 34.3532\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2912 - val_loss: 36.8540\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2946 - val_loss: 35.6583\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2939 - val_loss: 37.0891\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2714 - val_loss: 46.8083\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2787 - val_loss: 44.8348\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2759 - val_loss: 35.6076\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2290 - val_loss: 41.6290\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2206 - val_loss: 45.6716\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.2300 - val_loss: 34.5410\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2938 - val_loss: 41.5547\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2236 - val_loss: 35.3798\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1644 - val_loss: 34.4231\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1986 - val_loss: 35.3471\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1974 - val_loss: 32.5543\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1823 - val_loss: 35.5084\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.2379 - val_loss: 40.5628\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1676 - val_loss: 34.7118\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1265 - val_loss: 38.1220\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0852 - val_loss: 34.4755\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0766 - val_loss: 35.5158\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1444 - val_loss: 38.2680\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0972 - val_loss: 44.6333\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1455 - val_loss: 35.6795\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1529 - val_loss: 36.4347\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1297 - val_loss: 36.0751\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1257 - val_loss: 33.3019\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1143 - val_loss: 37.1301\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0573 - val_loss: 36.3998\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1119 - val_loss: 37.5414\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1050 - val_loss: 35.2605\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1329 - val_loss: 34.4141\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.1151 - val_loss: 44.5198\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0979 - val_loss: 32.8426\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0119 - val_loss: 35.8737\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0504 - val_loss: 38.9144\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.1234 - val_loss: 36.4124\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0567 - val_loss: 36.7463\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0407 - val_loss: 36.9017\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9956 - val_loss: 33.6522\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9962 - val_loss: 33.8496\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0279 - val_loss: 36.5237\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9643 - val_loss: 34.7355\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0154 - val_loss: 34.1489\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9693 - val_loss: 38.3765\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9181 - val_loss: 34.3289\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9250 - val_loss: 34.5241\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9431 - val_loss: 39.8124\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9559 - val_loss: 56.6079\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.9781 - val_loss: 35.5319\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9657 - val_loss: 36.2370\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9462 - val_loss: 36.3697\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 27.0168 - val_loss: 40.8281\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 27.0412 - val_loss: 34.2439\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9339 - val_loss: 74.1384\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8918 - val_loss: 38.4023\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.9276 - val_loss: 33.3276\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8726 - val_loss: 41.2537\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8775 - val_loss: 37.9475\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8684 - val_loss: 33.8023\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8610 - val_loss: 35.5539\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8930 - val_loss: 35.8600\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8320 - val_loss: 32.4002\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8892 - val_loss: 46.7261\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8940 - val_loss: 34.4529\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8361 - val_loss: 34.0925\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8541 - val_loss: 33.9309\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8099 - val_loss: 33.6864\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8962 - val_loss: 35.3527\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8378 - val_loss: 39.6114\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8787 - val_loss: 33.5956\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.8264 - val_loss: 35.8167\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7875 - val_loss: 34.1269\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8247 - val_loss: 35.4022\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7676 - val_loss: 35.3827\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8145 - val_loss: 45.1021\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8338 - val_loss: 38.5819\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8245 - val_loss: 35.9948\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8234 - val_loss: 34.6736\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7751 - val_loss: 34.6554\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7550 - val_loss: 37.5679\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.8190 - val_loss: 35.4112\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7560 - val_loss: 34.1625\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7491 - val_loss: 35.9140\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7341 - val_loss: 32.8573\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7952 - val_loss: 37.9752\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7154 - val_loss: 32.9850\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7505 - val_loss: 41.8753\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7642 - val_loss: 34.9203\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7539 - val_loss: 50.1153\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7446 - val_loss: 42.2751\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7108 - val_loss: 42.8446\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7413 - val_loss: 33.9121\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7462 - val_loss: 38.5581\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7093 - val_loss: 37.1920\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7409 - val_loss: 33.8832\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6736 - val_loss: 32.9944\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6698 - val_loss: 40.0197\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7505 - val_loss: 33.9195\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6764 - val_loss: 35.7537\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7003 - val_loss: 33.0493\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.7460 - val_loss: 38.2809\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6276 - val_loss: 33.3657\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7136 - val_loss: 35.5156\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6506 - val_loss: 33.4824\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6690 - val_loss: 33.6373\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6177 - val_loss: 33.0043\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6520 - val_loss: 35.2373\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6908 - val_loss: 37.8499\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6414 - val_loss: 47.7851\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6934 - val_loss: 34.1070\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.7076 - val_loss: 34.3315\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6900 - val_loss: 36.2242\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6485 - val_loss: 45.5403\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6106 - val_loss: 33.0856\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5762 - val_loss: 34.7695\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6381 - val_loss: 39.2124\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6259 - val_loss: 32.9441\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6075 - val_loss: 33.6630\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6289 - val_loss: 33.1013\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6321 - val_loss: 35.2621\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5960 - val_loss: 35.9881\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6402 - val_loss: 34.9578\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5627 - val_loss: 33.9905\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6243 - val_loss: 37.8769\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6357 - val_loss: 34.4777\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5714 - val_loss: 35.2478\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6030 - val_loss: 41.8512\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5500 - val_loss: 36.4011\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5522 - val_loss: 34.1602\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6293 - val_loss: 37.0657\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5477 - val_loss: 34.3249\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.6079 - val_loss: 37.3229\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5859 - val_loss: 35.6490\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5986 - val_loss: 42.9322\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6079 - val_loss: 46.2067\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5452 - val_loss: 42.8111\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6619 - val_loss: 33.5506\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5383 - val_loss: 32.8480\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5837 - val_loss: 34.4697\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6070 - val_loss: 39.4223\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5035 - val_loss: 34.8517\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5375 - val_loss: 49.4670\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5840 - val_loss: 35.7315\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5391 - val_loss: 36.1754\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5434 - val_loss: 37.8862\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5242 - val_loss: 33.5095\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5020 - val_loss: 39.5129\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5076 - val_loss: 33.0566\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5165 - val_loss: 40.9286\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5715 - val_loss: 41.4254\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4819 - val_loss: 38.0155\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4920 - val_loss: 34.9155\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5014 - val_loss: 33.1275\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5024 - val_loss: 39.8709\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4985 - val_loss: 36.8115\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4882 - val_loss: 39.7367\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4602 - val_loss: 40.9535\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4693 - val_loss: 35.7469\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5069 - val_loss: 33.4089\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4798 - val_loss: 37.4565\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4870 - val_loss: 33.0334\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5645 - val_loss: 42.2108\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4259 - val_loss: 35.3716\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4838 - val_loss: 40.3613\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4610 - val_loss: 35.9177\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.6029 - val_loss: 35.3849\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4482 - val_loss: 34.4881\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.5485 - val_loss: 40.7195\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5179 - val_loss: 37.8262\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4179 - val_loss: 34.9909\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4279 - val_loss: 49.0070\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4295 - val_loss: 38.5644\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4720 - val_loss: 36.3942\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4543 - val_loss: 40.5978\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4958 - val_loss: 34.8157\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4261 - val_loss: 37.6472\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.5005 - val_loss: 34.1063\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4403 - val_loss: 35.4066\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4351 - val_loss: 33.7601\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4642 - val_loss: 33.7602\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4636 - val_loss: 33.0819\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4037 - val_loss: 48.1919\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4002 - val_loss: 32.0305\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4514 - val_loss: 36.2319\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.3841 - val_loss: 35.9527\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4697 - val_loss: 34.0494\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4218 - val_loss: 36.8453\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4044 - val_loss: 34.1337\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4219 - val_loss: 36.2305\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4184 - val_loss: 34.5495\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.3830 - val_loss: 33.2145\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4188 - val_loss: 33.3106\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4016 - val_loss: 39.3003\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4048 - val_loss: 31.8975\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4039 - val_loss: 32.3320\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4243 - val_loss: 35.8438\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3537 - val_loss: 39.5889\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4322 - val_loss: 36.2815\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3846 - val_loss: 35.5230\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4152 - val_loss: 33.1065\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4339 - val_loss: 35.0326\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4226 - val_loss: 34.3693\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4292 - val_loss: 32.9471\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.3824 - val_loss: 37.4967\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3840 - val_loss: 34.8418\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4019 - val_loss: 35.7683\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3775 - val_loss: 36.0549\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4244 - val_loss: 41.0149\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3593 - val_loss: 33.7683\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3485 - val_loss: 35.2575\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3288 - val_loss: 38.3037\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.3582 - val_loss: 38.6882\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4338 - val_loss: 34.3202\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3844 - val_loss: 33.5155\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3735 - val_loss: 36.2839\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4389 - val_loss: 32.9066\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3029 - val_loss: 33.2277\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.3429 - val_loss: 35.0536\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4569 - val_loss: 35.4713\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3659 - val_loss: 35.2239\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3266 - val_loss: 33.9132\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3645 - val_loss: 36.1858\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.3456 - val_loss: 34.2416\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3172 - val_loss: 37.2810\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3253 - val_loss: 32.5731\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.4139 - val_loss: 33.2301\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2672 - val_loss: 42.2991\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3617 - val_loss: 35.4407\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3245 - val_loss: 40.1403\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3648 - val_loss: 34.9008\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3456 - val_loss: 44.5483\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.4189 - val_loss: 40.9876\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2837 - val_loss: 35.0456\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3499 - val_loss: 41.4032\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2930 - val_loss: 32.7459\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3654 - val_loss: 35.6961\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2902 - val_loss: 37.4656\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3392 - val_loss: 32.3012\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3378 - val_loss: 34.1089\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3755 - val_loss: 44.4298\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3252 - val_loss: 34.2176\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2872 - val_loss: 33.5425\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3208 - val_loss: 33.4055\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3036 - val_loss: 33.2479\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2665 - val_loss: 42.0278\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2081 - val_loss: 36.9085\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3918 - val_loss: 34.3298\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 1s 7ms/step - loss: 26.2987 - val_loss: 33.7410\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 26.2925 - val_loss: 33.1125\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 26.2572 - val_loss: 35.2896\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3392 - val_loss: 39.4271\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3075 - val_loss: 32.7347\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2946 - val_loss: 33.7483\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2484 - val_loss: 36.4120\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2594 - val_loss: 35.9565\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3005 - val_loss: 33.5779\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2426 - val_loss: 39.6186\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3137 - val_loss: 33.7141\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2642 - val_loss: 35.0320\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2934 - val_loss: 33.0079\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2695 - val_loss: 34.8559\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1993 - val_loss: 34.5011\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.3351 - val_loss: 35.2282\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2953 - val_loss: 36.4835\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.2906 - val_loss: 32.9967\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2891 - val_loss: 32.9519\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2563 - val_loss: 36.4054\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2120 - val_loss: 37.2884\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2927 - val_loss: 39.5256\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2344 - val_loss: 44.2629\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2758 - val_loss: 39.0061\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2617 - val_loss: 33.9395\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2624 - val_loss: 44.4340\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2047 - val_loss: 44.3002\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1928 - val_loss: 35.9687\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 1s 8ms/step - loss: 26.2306 - val_loss: 31.9265\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 9ms/step - loss: 26.2264 - val_loss: 33.7773\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2537 - val_loss: 34.8493\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2165 - val_loss: 34.7127\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2345 - val_loss: 36.1719\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2348 - val_loss: 33.6561\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2322 - val_loss: 33.3662\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2962 - val_loss: 34.5447\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2007 - val_loss: 37.4325\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2021 - val_loss: 33.3127\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1604 - val_loss: 33.9063\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.1989 - val_loss: 33.6422\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2303 - val_loss: 35.1991\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2836 - val_loss: 33.7701\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2024 - val_loss: 37.1971\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1815 - val_loss: 33.9723\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1839 - val_loss: 35.8990\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2011 - val_loss: 33.3119\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1920 - val_loss: 32.9053\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2062 - val_loss: 34.6947\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2239 - val_loss: 34.7656\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2670 - val_loss: 50.8589\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1992 - val_loss: 36.7360\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1538 - val_loss: 37.9721\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1410 - val_loss: 37.9772\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1340 - val_loss: 45.3776\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1907 - val_loss: 35.4010\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1857 - val_loss: 32.2831\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1837 - val_loss: 35.9307\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1965 - val_loss: 34.1527\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1758 - val_loss: 35.8525\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1775 - val_loss: 38.7735\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1866 - val_loss: 34.6303\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1666 - val_loss: 34.1540\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1546 - val_loss: 33.9776\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.2118 - val_loss: 39.3763\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1712 - val_loss: 39.6247\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.1554 - val_loss: 34.7577\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1599 - val_loss: 34.2750\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 1s 6ms/step - loss: 26.1562 - val_loss: 34.9919\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 1s 5ms/step - loss: 26.1457 - val_loss: 33.2839\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-M4xGsS4D4JT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e73eb54-0717-455c-8939-10cf92b6efa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.9935494694429914 \n",
            "MAE:  4.2159344609839815 \n",
            "SD:  5.683025649811127\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CCaTKbd7D4JU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "3c9b417b-cd6b-4abd-d633-6a489811aa82"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d437a7-48c9-435d-8603-9e4aa355cd9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  0.5049380033355576 \n",
            "Ensemble_std:  5.76165918081847\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GOXHuvZmzx1r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "\bBP_hv3_8(1).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}