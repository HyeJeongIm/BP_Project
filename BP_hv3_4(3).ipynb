{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeJeongIm/BP_Project/blob/main/%08BP_hv3_4(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YTF6cMiY1Hw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiiiBla2-j1S"
      },
      "source": [
        "# batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCoux5AOZnK",
        "outputId": "2c244f85-4dcc-4558-db17-71ec5a2046b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "TensorFlow version :  2.8.2\n",
            "Keras version :  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "# from vis.visualization import visualize_cam, overlay\n",
        "from tensorflow.keras import activations\n",
        "#from vis.utils import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import sys\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras as keras\n",
        "# from tensorflow.python.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta, Nadam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(7)\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('TensorFlow version : ', tf.__version__)\n",
        "print('Keras version : ', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlHICkovd809",
        "outputId": "c89714ee-599b-4c9e-982c-e87f9d45dbc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FtxPSfByeM8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import io\n",
        "\n",
        "# 데이터 파일 불러z오기\n",
        "train_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/train_shuffled_raw_v3.mat')\n",
        "test_data = io.loadmat('/content/gdrive/MyDrive/BP/hz/v3/test_not_shuffled_raw_v3.mat')\n",
        "\n",
        "X_train = train_data['data_shuffled']\n",
        "X_test = test_data['data_not_shuffled']\n",
        "\n",
        "sbp_train = train_data['sbp_total']\n",
        "sbp_test = test_data['sbp_total']\n",
        "dbp_train = train_data['dbp_total']\n",
        "dbp_test = test_data['dbp_total']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75KxLEi8kLbn",
        "outputId": "66bf8426-7aa8-44e4-e6d9-9f796147f1d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168743, 127)\n",
            "(43293, 127)\n",
            "(168743, 1)\n",
            "(43293, 1)\n",
            "(168743, 1)\n",
            "(43293, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape) \n",
        "\n",
        "print(sbp_train.shape)\n",
        "print(sbp_test.shape)\n",
        "print(dbp_train.shape)\n",
        "print(dbp_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "IEfYfZC5qWsR",
        "outputId": "9cef92d0-4fe2-4ab8-ebbf-a4e17a4bc58b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6        7    \\\n",
              "0    0.397525  0.576176  0.782368  0.343816  0.0  0.325039  0.166250  0.58625   \n",
              "1    0.403687  0.576176  0.782368  0.343816  0.0  0.309897  0.166250  0.57500   \n",
              "2    0.405556  0.576176  0.782368  0.343816  0.0  0.317237  0.163750  0.57500   \n",
              "3    0.396543  0.576176  0.782368  0.343816  0.0  0.315348  0.168750  0.58875   \n",
              "4    0.391071  0.576176  0.782368  0.343816  0.0  0.320688  0.170625  0.59125   \n",
              "..        ...       ...       ...       ...  ...       ...       ...      ...   \n",
              "98   0.264083  0.505748  0.826316  0.416961  0.0  0.491736  0.273750  0.84875   \n",
              "99   0.265455  0.505748  0.826316  0.416961  0.0  0.497504  0.325000  0.78750   \n",
              "100  0.258081  0.505748  0.826316  0.416961  0.0  0.498717  0.287500  0.80250   \n",
              "101  0.261381  0.505748  0.826316  0.416961  0.0  0.490427  0.335000  0.77625   \n",
              "102  0.260134  0.505748  0.826316  0.416961  0.0  0.493463  0.340000  0.81000   \n",
              "\n",
              "          8         9    ...      117       118       119       120       121  \\\n",
              "0    0.141250  0.130000  ...  0.21750  0.193750  0.172500  0.151250  0.131250   \n",
              "1    0.140000  0.129375  ...  0.21625  0.195000  0.173750  0.152500  0.132500   \n",
              "2    0.138125  0.127500  ...  0.22375  0.201250  0.180000  0.158750  0.137500   \n",
              "3    0.140000  0.130000  ...  0.22500  0.203125  0.180625  0.158125  0.136875   \n",
              "4    0.143750  0.131875  ...  0.23000  0.207500  0.183750  0.161250  0.138750   \n",
              "..        ...       ...  ...      ...       ...       ...       ...       ...   \n",
              "98   0.238750  0.215000  ...  0.49875  0.351250  0.305000  0.259375  0.200625   \n",
              "99   0.275000  0.255000  ...  0.31875  0.292500  0.265000  0.236250  0.202500   \n",
              "100  0.255000  0.230000  ...  0.31500  0.287500  0.260625  0.230625  0.198750   \n",
              "101  0.291250  0.255000  ...  0.30625  0.280000  0.252500  0.223750  0.192500   \n",
              "102  0.286250  0.251875  ...  0.29750  0.271250  0.243750  0.216250  0.186250   \n",
              "\n",
              "          122      123       124       125       126  \n",
              "0    0.111250  0.08875  0.061250  0.577695  0.334739  \n",
              "1    0.112500  0.08875  0.062500  0.588482  0.335669  \n",
              "2    0.115000  0.09250  0.063750  0.694625  0.386111  \n",
              "3    0.115625  0.09250  0.063125  0.701718  0.390863  \n",
              "4    0.116250  0.09250  0.063750  0.700430  0.381499  \n",
              "..        ...      ...       ...       ...       ...  \n",
              "98   0.148125  0.11000  0.073125  0.668204  0.339492  \n",
              "99   0.166250  0.12875  0.086250  0.535449  0.290942  \n",
              "100  0.163125  0.12625  0.084375  0.531307  0.294047  \n",
              "101  0.158750  0.12375  0.085000  0.550623  0.297881  \n",
              "102  0.155000  0.12250  0.082500  0.537822  0.291545  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b10f0c5d-4f70-4717-9312-4c610abc0f53\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.397525</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.325039</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.58625</td>\n",
              "      <td>0.141250</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.151250</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.061250</td>\n",
              "      <td>0.577695</td>\n",
              "      <td>0.334739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403687</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.309897</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.129375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21625</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.173750</td>\n",
              "      <td>0.152500</td>\n",
              "      <td>0.132500</td>\n",
              "      <td>0.112500</td>\n",
              "      <td>0.08875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.588482</td>\n",
              "      <td>0.335669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.405556</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317237</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>0.57500</td>\n",
              "      <td>0.138125</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22375</td>\n",
              "      <td>0.201250</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.694625</td>\n",
              "      <td>0.386111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.396543</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.315348</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.58875</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.22500</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.180625</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.115625</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063125</td>\n",
              "      <td>0.701718</td>\n",
              "      <td>0.390863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.391071</td>\n",
              "      <td>0.576176</td>\n",
              "      <td>0.782368</td>\n",
              "      <td>0.343816</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.320688</td>\n",
              "      <td>0.170625</td>\n",
              "      <td>0.59125</td>\n",
              "      <td>0.143750</td>\n",
              "      <td>0.131875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>0.183750</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.138750</td>\n",
              "      <td>0.116250</td>\n",
              "      <td>0.09250</td>\n",
              "      <td>0.063750</td>\n",
              "      <td>0.700430</td>\n",
              "      <td>0.381499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.264083</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.491736</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.84875</td>\n",
              "      <td>0.238750</td>\n",
              "      <td>0.215000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.49875</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.200625</td>\n",
              "      <td>0.148125</td>\n",
              "      <td>0.11000</td>\n",
              "      <td>0.073125</td>\n",
              "      <td>0.668204</td>\n",
              "      <td>0.339492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.265455</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.78750</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31875</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.236250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>0.12875</td>\n",
              "      <td>0.086250</td>\n",
              "      <td>0.535449</td>\n",
              "      <td>0.290942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.258081</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.498717</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.80250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31500</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.260625</td>\n",
              "      <td>0.230625</td>\n",
              "      <td>0.198750</td>\n",
              "      <td>0.163125</td>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>0.531307</td>\n",
              "      <td>0.294047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.261381</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.77625</td>\n",
              "      <td>0.291250</td>\n",
              "      <td>0.255000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30625</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.252500</td>\n",
              "      <td>0.223750</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.158750</td>\n",
              "      <td>0.12375</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.550623</td>\n",
              "      <td>0.297881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.260134</td>\n",
              "      <td>0.505748</td>\n",
              "      <td>0.826316</td>\n",
              "      <td>0.416961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493463</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.81000</td>\n",
              "      <td>0.286250</td>\n",
              "      <td>0.251875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29750</td>\n",
              "      <td>0.271250</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.216250</td>\n",
              "      <td>0.186250</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.537822</td>\n",
              "      <td>0.291545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b10f0c5d-4f70-4717-9312-4c610abc0f53')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b10f0c5d-4f70-4717-9312-4c610abc0f53 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b10f0c5d-4f70-4717-9312-4c610abc0f53');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train_raw = pd.DataFrame(X_train)\n",
        "df_train_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "TtAXH0aCrBEF",
        "outputId": "3e045f4c-1a77-4b5d-ca5d-9f3a04474c50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3    4         5         6    \\\n",
              "0    0.409346  0.196754  0.843158  0.327208  0.0  0.334396  0.165625   \n",
              "1    0.412235  0.196754  0.843158  0.327208  0.0  0.312476  0.165625   \n",
              "2    0.407614  0.196754  0.843158  0.327208  0.0  0.326504  0.167500   \n",
              "3    0.407614  0.196754  0.843158  0.327208  0.0  0.356952  0.160000   \n",
              "4    0.401500  0.196754  0.843158  0.327208  0.0  0.341285  0.161250   \n",
              "..        ...       ...       ...       ...  ...       ...       ...   \n",
              "98   0.352657  0.521650  0.867368  0.406007  0.0  0.389110  0.208750   \n",
              "99   0.354369  0.521650  0.867368  0.406007  0.0  0.376453  0.203750   \n",
              "100  0.349282  0.521650  0.867368  0.406007  0.0  0.384221  0.214375   \n",
              "101  0.350962  0.521650  0.867368  0.406007  0.0  0.384311  0.205625   \n",
              "102  0.351807  0.521650  0.867368  0.406007  0.0  0.383750  0.211875   \n",
              "\n",
              "          7         8         9    ...       117      118      119      120  \\\n",
              "0    0.568750  0.136875  0.126875  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "1    0.562500  0.137500  0.125625  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "2    0.568750  0.140000  0.128750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "3    0.577500  0.135000  0.123750  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "4    0.582500  0.136250  0.126250  ...  0.229375  0.18625  0.15875  0.13875   \n",
              "..        ...       ...       ...  ...       ...      ...      ...      ...   \n",
              "98   0.641250  0.174375  0.162500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "99   0.631250  0.170000  0.157500  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "100  0.641875  0.181250  0.166250  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "101  0.646250  0.171250  0.158125  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "102  0.640000  0.178125  0.163750  ...  0.285000  0.26000  0.23125  0.20500   \n",
              "\n",
              "        121      122      123      124       125       126  \n",
              "0    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "1    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "2    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "3    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "4    0.1200  0.10125  0.08125  0.05625  0.764316  0.425633  \n",
              "..      ...      ...      ...      ...       ...       ...  \n",
              "98   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "99   0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "100  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "101  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "102  0.1775  0.14625  0.11125  0.07500  0.675251  0.329698  \n",
              "\n",
              "[103 rows x 127 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-39e4cc13-d4ce-433a-8cf9-d50a6c157cf2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409346</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334396</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.136875</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.412235</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.312476</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.125625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326504</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.128750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.407614</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.356952</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.401500</td>\n",
              "      <td>0.196754</td>\n",
              "      <td>0.843158</td>\n",
              "      <td>0.327208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.341285</td>\n",
              "      <td>0.161250</td>\n",
              "      <td>0.582500</td>\n",
              "      <td>0.136250</td>\n",
              "      <td>0.126250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229375</td>\n",
              "      <td>0.18625</td>\n",
              "      <td>0.15875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.05625</td>\n",
              "      <td>0.764316</td>\n",
              "      <td>0.425633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.352657</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.389110</td>\n",
              "      <td>0.208750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.174375</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.354369</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376453</td>\n",
              "      <td>0.203750</td>\n",
              "      <td>0.631250</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.157500</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.349282</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384221</td>\n",
              "      <td>0.214375</td>\n",
              "      <td>0.641875</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.350962</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384311</td>\n",
              "      <td>0.205625</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>0.158125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.351807</td>\n",
              "      <td>0.521650</td>\n",
              "      <td>0.867368</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.383750</td>\n",
              "      <td>0.211875</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.178125</td>\n",
              "      <td>0.163750</td>\n",
              "      <td>...</td>\n",
              "      <td>0.285000</td>\n",
              "      <td>0.26000</td>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.1775</td>\n",
              "      <td>0.14625</td>\n",
              "      <td>0.11125</td>\n",
              "      <td>0.07500</td>\n",
              "      <td>0.675251</td>\n",
              "      <td>0.329698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39e4cc13-d4ce-433a-8cf9-d50a6c157cf2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-39e4cc13-d4ce-433a-8cf9-d50a6c157cf2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-39e4cc13-d4ce-433a-8cf9-d50a6c157cf2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_test_raw = pd.DataFrame(X_test)\n",
        "df_test_raw.head(103)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G60-qJQROZnM"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nCpydfmAI1AD"
      },
      "outputs": [],
      "source": [
        "#parameter\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "lrate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3V_5euOZnM"
      },
      "source": [
        "# SBP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tFbdpdOZnN"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ptBRJtSOZnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI8SHBwBOZnO",
        "outputId": "fcf667cd-5990-44aa-8755-a41b52936c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 16)               64        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,841\n",
            "Trainable params: 7,265\n",
            "Non-trainable params: 576\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGT6-7NcOZnO",
        "outputId": "9eb7c79c-c425-4303-ee3c-cbf64e10e985",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 9s 17ms/step - loss: 12178.6387 - val_loss: 12160.1768\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 11573.2607 - val_loss: 11322.2969\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 10785.9531 - val_loss: 10461.8643\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 9769.1943 - val_loss: 9538.6982\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 8492.3584 - val_loss: 8457.3418\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 7082.6304 - val_loss: 5991.2695\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 5659.0703 - val_loss: 5756.7031\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 4255.3389 - val_loss: 3380.8411\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 3009.5027 - val_loss: 2400.4214\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2052.0593 - val_loss: 1507.7817\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 1338.7797 - val_loss: 879.3954\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 855.9286 - val_loss: 514.3535\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 551.9042 - val_loss: 331.9485\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 376.4564 - val_loss: 262.9595\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 281.1136 - val_loss: 276.2448\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 235.8167 - val_loss: 228.1034\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 202.5140 - val_loss: 194.9584\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 184.5902 - val_loss: 314.9256\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 172.7867 - val_loss: 181.5632\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 163.0301 - val_loss: 187.8000\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 153.7598 - val_loss: 176.8661\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 145.6035 - val_loss: 175.1218\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 142.3167 - val_loss: 778.8718\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 136.5255 - val_loss: 173.0024\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 134.6864 - val_loss: 1605.9042\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 131.4749 - val_loss: 135.1196\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 128.1058 - val_loss: 154.7605\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 127.3579 - val_loss: 322.5175\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 124.1663 - val_loss: 139.2822\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 124.8955 - val_loss: 131.1987\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 122.3939 - val_loss: 133.7644\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 119.6756 - val_loss: 126.5041\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 118.3738 - val_loss: 132.9316\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 115.5985 - val_loss: 139.5029\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 3s 19ms/step - loss: 115.5956 - val_loss: 119.4661\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 112.8201 - val_loss: 133.6932\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 112.1424 - val_loss: 121.0979\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 3s 19ms/step - loss: 111.8573 - val_loss: 125.8085\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 112.6977 - val_loss: 119.5975\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 110.3044 - val_loss: 132.3785\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 109.8984 - val_loss: 118.5980\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 108.2704 - val_loss: 118.3593\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 106.9371 - val_loss: 146.6209\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 106.7531 - val_loss: 210.8083\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 105.0340 - val_loss: 144.5160\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 107.9875 - val_loss: 161.8929\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 104.2762 - val_loss: 149.1317\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 103.2829 - val_loss: 179.4190\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 103.2507 - val_loss: 112.4167\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 101.6431 - val_loss: 114.7614\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.9924 - val_loss: 135.3057\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 101.0231 - val_loss: 111.8284\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 99.8970 - val_loss: 107.1299\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.8428 - val_loss: 126.3963\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.0957 - val_loss: 123.9457\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.8816 - val_loss: 140.7258\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.5194 - val_loss: 123.8940\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.7705 - val_loss: 137.5388\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 97.9311 - val_loss: 152.4437\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 99.4679 - val_loss: 208.7351\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.2501 - val_loss: 143.4532\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 101.7766 - val_loss: 206.0669\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 103.1664 - val_loss: 153.6364\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 102.9254 - val_loss: 140.1124\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 99.9786 - val_loss: 139.2693\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 98.1549 - val_loss: 299.9129\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.5704 - val_loss: 109.7695\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 97.0961 - val_loss: 109.5412\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 95.5798 - val_loss: 139.1540\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 94.7794 - val_loss: 167.0409\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 94.4872 - val_loss: 111.5518\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 94.3444 - val_loss: 109.6532\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 93.0328 - val_loss: 106.1040\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 93.6993 - val_loss: 137.8668\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 92.6277 - val_loss: 103.9359\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 91.9480 - val_loss: 109.2304\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 92.6697 - val_loss: 99.8230\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 91.6377 - val_loss: 104.1183\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.5904 - val_loss: 117.3465\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.7048 - val_loss: 130.9738\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 91.4383 - val_loss: 117.5741\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.8669 - val_loss: 111.8900\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.0120 - val_loss: 109.3610\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.2386 - val_loss: 107.4567\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 89.0077 - val_loss: 103.0705\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 88.6816 - val_loss: 110.0181\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 88.8838 - val_loss: 113.6575\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 88.4744 - val_loss: 101.9995\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.7870 - val_loss: 120.4035\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.7299 - val_loss: 108.1594\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 87.2522 - val_loss: 105.4259\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.9842 - val_loss: 120.3705\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.0152 - val_loss: 101.3245\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.0558 - val_loss: 107.1968\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 85.5417 - val_loss: 99.5352\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 85.1417 - val_loss: 199.5369\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.7358 - val_loss: 129.8665\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.6155 - val_loss: 100.0043\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.3831 - val_loss: 111.5254\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 85.0810 - val_loss: 98.9993\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.8477 - val_loss: 126.3816\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.5221 - val_loss: 98.1036\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.1035 - val_loss: 154.8854\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.9291 - val_loss: 100.7306\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.6491 - val_loss: 144.2111\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.9111 - val_loss: 116.6219\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.3500 - val_loss: 176.1204\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.5512 - val_loss: 104.0234\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.0885 - val_loss: 95.4161\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.6445 - val_loss: 116.8825\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.3437 - val_loss: 94.0889\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.8194 - val_loss: 100.3040\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.8393 - val_loss: 96.1751\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.3014 - val_loss: 102.5652\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.0566 - val_loss: 148.2616\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.4614 - val_loss: 104.8316\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.7346 - val_loss: 110.2803\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.1293 - val_loss: 103.1820\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.4823 - val_loss: 117.2280\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.8757 - val_loss: 90.0512\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.0056 - val_loss: 96.5260\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.6618 - val_loss: 118.1378\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.9788 - val_loss: 139.8777\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.2700 - val_loss: 273.9289\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.0540 - val_loss: 104.5902\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.7266 - val_loss: 118.2850\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.2400 - val_loss: 105.3558\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.9124 - val_loss: 106.8530\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.2825 - val_loss: 109.7706\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.6617 - val_loss: 129.5517\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.7970 - val_loss: 94.0316\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.1582 - val_loss: 97.9534\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.4988 - val_loss: 97.0840\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.1585 - val_loss: 93.1174\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.2317 - val_loss: 95.1523\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.4595 - val_loss: 104.6614\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.1021 - val_loss: 116.7012\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.8797 - val_loss: 98.7950\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.6006 - val_loss: 99.0314\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.5208 - val_loss: 93.0244\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.8253 - val_loss: 94.3259\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.1601 - val_loss: 144.6678\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.0224 - val_loss: 96.3962\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.7298 - val_loss: 99.8997\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.8890 - val_loss: 95.8710\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.6064 - val_loss: 97.4179\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.3923 - val_loss: 100.1676\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.8600 - val_loss: 186.3515\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.6153 - val_loss: 119.7715\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.1150 - val_loss: 106.9875\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.2930 - val_loss: 97.4877\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.3327 - val_loss: 93.8098\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.5729 - val_loss: 117.5672\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.4012 - val_loss: 103.5100\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.5242 - val_loss: 117.5444\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.3798 - val_loss: 157.8632\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.8072 - val_loss: 239.5005\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 76.5102 - val_loss: 103.8587\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 75.4382 - val_loss: 93.1592\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.0076 - val_loss: 99.7780\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.5959 - val_loss: 97.7836\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.8147 - val_loss: 106.6791\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.3732 - val_loss: 110.1347\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.9750 - val_loss: 102.2955\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.7380 - val_loss: 113.2673\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.2528 - val_loss: 93.4388\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.6714 - val_loss: 131.5778\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.5072 - val_loss: 93.0286\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.4727 - val_loss: 96.9088\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.3458 - val_loss: 106.4115\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.2982 - val_loss: 90.4440\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.4069 - val_loss: 93.6186\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.7615 - val_loss: 97.5472\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.2109 - val_loss: 91.7402\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.1125 - val_loss: 112.8812\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.9512 - val_loss: 133.9531\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.8191 - val_loss: 91.6219\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.6947 - val_loss: 87.1772\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.4671 - val_loss: 98.8815\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.5820 - val_loss: 95.3501\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.0593 - val_loss: 118.1389\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.5612 - val_loss: 87.4407\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.1432 - val_loss: 97.0692\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.2049 - val_loss: 135.2053\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.8300 - val_loss: 89.4453\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.9160 - val_loss: 86.5063\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.2229 - val_loss: 93.7017\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.5973 - val_loss: 110.5766\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.0027 - val_loss: 122.2159\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.2880 - val_loss: 133.0079\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.3174 - val_loss: 96.0791\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.2024 - val_loss: 96.9942\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.8367 - val_loss: 105.6142\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.1040 - val_loss: 96.3883\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.3580 - val_loss: 114.1103\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.0117 - val_loss: 100.0343\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.9935 - val_loss: 143.4656\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.1792 - val_loss: 103.1669\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.8927 - val_loss: 101.2232\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.5848 - val_loss: 98.1832\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.7644 - val_loss: 117.3016\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.8046 - val_loss: 103.9279\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.5132 - val_loss: 88.3638\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.5646 - val_loss: 134.9236\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.7945 - val_loss: 104.1802\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.3233 - val_loss: 105.6429\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.1475 - val_loss: 155.4929\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.5924 - val_loss: 85.3013\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.2592 - val_loss: 142.4526\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.5941 - val_loss: 88.5348\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.2015 - val_loss: 124.6959\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.0188 - val_loss: 90.4723\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.8501 - val_loss: 101.3281\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.4085 - val_loss: 90.7682\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.0102 - val_loss: 103.5179\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.4771 - val_loss: 89.9426\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.8635 - val_loss: 122.0348\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.6675 - val_loss: 92.9708\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.6585 - val_loss: 115.2308\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.8709 - val_loss: 133.8572\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.4225 - val_loss: 97.8948\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.6937 - val_loss: 100.3922\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.7026 - val_loss: 164.4794\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.8969 - val_loss: 116.0749\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.7149 - val_loss: 108.7388\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.2524 - val_loss: 119.2720\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.7004 - val_loss: 93.3957\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.6318 - val_loss: 133.6030\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.1591 - val_loss: 90.4764\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.5318 - val_loss: 117.3030\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.3249 - val_loss: 100.4411\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.5237 - val_loss: 110.8064\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.3079 - val_loss: 99.7042\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.6746 - val_loss: 86.6995\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.3753 - val_loss: 105.1280\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.3411 - val_loss: 112.0568\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.5760 - val_loss: 94.5210\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.4198 - val_loss: 95.2719\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.3017 - val_loss: 104.7204\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.3808 - val_loss: 83.0771\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.1199 - val_loss: 104.1047\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.9176 - val_loss: 128.0957\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.0215 - val_loss: 91.5997\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.5077 - val_loss: 124.0149\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.5222 - val_loss: 102.3910\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.8529 - val_loss: 89.4327\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.8192 - val_loss: 89.5819\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.4959 - val_loss: 87.2280\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.5513 - val_loss: 97.5627\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.5936 - val_loss: 103.6725\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.3761 - val_loss: 90.6257\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.2720 - val_loss: 108.9116\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.6428 - val_loss: 97.2154\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.5763 - val_loss: 101.7054\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.4436 - val_loss: 90.5818\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.3463 - val_loss: 116.3129\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.3542 - val_loss: 88.5190\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.2947 - val_loss: 93.5840\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.0299 - val_loss: 94.0799\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.1311 - val_loss: 89.4975\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.4975 - val_loss: 187.0655\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.9844 - val_loss: 108.8928\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.0427 - val_loss: 110.5502\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.3326 - val_loss: 93.3709\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.1875 - val_loss: 107.6249\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.9847 - val_loss: 131.5530\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.9400 - val_loss: 142.7472\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.8425 - val_loss: 122.7085\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.8073 - val_loss: 93.4501\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.1536 - val_loss: 96.4384\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.7208 - val_loss: 135.6452\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.9078 - val_loss: 83.9531\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.8389 - val_loss: 100.4125\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.1264 - val_loss: 109.5243\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.8668 - val_loss: 113.7056\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.9740 - val_loss: 95.3327\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.8947 - val_loss: 129.4176\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.6930 - val_loss: 95.6059\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.6315 - val_loss: 116.2743\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.6977 - val_loss: 170.6338\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.3535 - val_loss: 91.8097\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.8396 - val_loss: 99.2701\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.6088 - val_loss: 127.6378\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.4352 - val_loss: 102.2128\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.5067 - val_loss: 91.9435\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.1343 - val_loss: 109.7559\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.3845 - val_loss: 94.5317\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.3741 - val_loss: 96.2088\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 3s 19ms/step - loss: 69.4631 - val_loss: 96.1105\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 69.4018 - val_loss: 138.1623\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.5919 - val_loss: 120.0642\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.3343 - val_loss: 89.0974\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.2396 - val_loss: 84.3032\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.0567 - val_loss: 89.0428\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.9485 - val_loss: 100.5919\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.1306 - val_loss: 91.4674\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.8669 - val_loss: 94.9286\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.2182 - val_loss: 109.8705\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.0962 - val_loss: 93.8217\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.0496 - val_loss: 101.0179\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.9391 - val_loss: 90.2727\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.1605 - val_loss: 100.0140\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.9310 - val_loss: 87.1958\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.8163 - val_loss: 102.8598\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.7559 - val_loss: 124.6148\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.0050 - val_loss: 153.2558\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.9903 - val_loss: 102.0521\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.6778 - val_loss: 88.9780\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.5544 - val_loss: 86.7732\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6420 - val_loss: 104.4982\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.7420 - val_loss: 91.3455\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6154 - val_loss: 91.9997\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6288 - val_loss: 153.6654\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.9813 - val_loss: 100.9112\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.7296 - val_loss: 87.0032\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.7544 - val_loss: 122.1411\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.3823 - val_loss: 98.1398\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.4912 - val_loss: 91.4564\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.4003 - val_loss: 88.7508\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6858 - val_loss: 148.9430\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.8840 - val_loss: 174.1394\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.5634 - val_loss: 96.9528\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6219 - val_loss: 97.6051\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.6390 - val_loss: 90.3365\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.4577 - val_loss: 93.9308\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.5364 - val_loss: 90.5348\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6073 - val_loss: 95.4663\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.3826 - val_loss: 94.4398\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.2160 - val_loss: 108.3516\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.8386 - val_loss: 99.7973\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.2708 - val_loss: 90.3637\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.2486 - val_loss: 94.0287\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.6423 - val_loss: 93.1914\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.4052 - val_loss: 124.4180\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.4498 - val_loss: 98.8205\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.0559 - val_loss: 126.5572\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.2684 - val_loss: 138.7600\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.9028 - val_loss: 85.8532\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.3676 - val_loss: 96.9426\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.8619 - val_loss: 113.2151\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.0089 - val_loss: 92.0922\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.9619 - val_loss: 89.6626\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.2602 - val_loss: 83.5280\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.3644 - val_loss: 105.0181\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.9439 - val_loss: 84.1009\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.2800 - val_loss: 113.1888\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.1708 - val_loss: 91.2726\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.7560 - val_loss: 84.1821\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.2673 - val_loss: 88.7620\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.8116 - val_loss: 94.0610\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.0341 - val_loss: 127.2005\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.0290 - val_loss: 91.4682\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.7566 - val_loss: 88.5489\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.7211 - val_loss: 90.4947\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.8564 - val_loss: 96.1849\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.8842 - val_loss: 89.3986\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.9255 - val_loss: 95.4385\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.5895 - val_loss: 103.2502\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.5224 - val_loss: 94.9005\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.5778 - val_loss: 90.9785\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.6100 - val_loss: 145.3945\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.6280 - val_loss: 92.2409\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.5918 - val_loss: 98.4352\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3525 - val_loss: 98.1751\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3942 - val_loss: 85.0809\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.3420 - val_loss: 91.2683\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.2848 - val_loss: 107.3240\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.6047 - val_loss: 128.0408\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.8336 - val_loss: 90.5093\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3784 - val_loss: 97.3495\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.4453 - val_loss: 96.5455\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.4894 - val_loss: 106.5087\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.5315 - val_loss: 96.9166\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.4845 - val_loss: 93.7655\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1922 - val_loss: 87.9102\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3188 - val_loss: 104.3599\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1391 - val_loss: 163.8544\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0933 - val_loss: 97.7963\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1726 - val_loss: 110.5189\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2703 - val_loss: 88.6953\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2784 - val_loss: 89.0468\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3489 - val_loss: 93.5048\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2287 - val_loss: 107.1580\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2381 - val_loss: 117.5381\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2280 - val_loss: 94.4534\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2127 - val_loss: 86.9903\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1718 - val_loss: 97.9630\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.5139 - val_loss: 106.7474\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2192 - val_loss: 95.6349\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0346 - val_loss: 103.5920\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1264 - val_loss: 96.9757\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.9988 - val_loss: 84.4184\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2196 - val_loss: 107.4356\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.1191 - val_loss: 84.9653\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.4806 - val_loss: 87.5634\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0651 - val_loss: 86.7987\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.0711 - val_loss: 105.0165\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.1340 - val_loss: 89.6888\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1076 - val_loss: 90.6448\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.0844 - val_loss: 94.9622\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0290 - val_loss: 116.3369\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8258 - val_loss: 93.6515\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.9893 - val_loss: 96.9564\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.8595 - val_loss: 93.1844\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 67.0229 - val_loss: 86.7618\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 3s 20ms/step - loss: 67.0973 - val_loss: 91.0510\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0532 - val_loss: 93.7091\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.9292 - val_loss: 102.8918\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1362 - val_loss: 86.1976\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8369 - val_loss: 88.8083\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.5904 - val_loss: 89.2288\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8571 - val_loss: 91.2867\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0253 - val_loss: 91.3730\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8981 - val_loss: 97.5407\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8432 - val_loss: 85.5891\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1956 - val_loss: 86.3333\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.7027 - val_loss: 93.9666\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.7284 - val_loss: 86.9659\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.9298 - val_loss: 89.2774\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5120 - val_loss: 87.3886\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5968 - val_loss: 96.0126\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6051 - val_loss: 131.9376\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.7889 - val_loss: 92.2099\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5111 - val_loss: 91.9718\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6710 - val_loss: 101.3676\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6158 - val_loss: 112.1755\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.4977 - val_loss: 88.3405\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6020 - val_loss: 104.3801\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5602 - val_loss: 89.4036\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5784 - val_loss: 106.8545\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.5408 - val_loss: 92.9113\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.9109 - val_loss: 82.8099\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5672 - val_loss: 93.2278\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5067 - val_loss: 122.6515\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6099 - val_loss: 113.8606\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.9458 - val_loss: 89.5009\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5072 - val_loss: 86.3724\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6418 - val_loss: 95.4344\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.4384 - val_loss: 86.9952\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5145 - val_loss: 99.3455\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.4592 - val_loss: 87.4001\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.3928 - val_loss: 104.2466\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.7194 - val_loss: 92.4351\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.4914 - val_loss: 99.9551\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6336 - val_loss: 98.1209\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5441 - val_loss: 92.5497\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.3532 - val_loss: 90.8704\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.3900 - val_loss: 90.2175\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5353 - val_loss: 94.2889\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.3839 - val_loss: 84.2833\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.4877 - val_loss: 86.0941\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5699 - val_loss: 98.0114\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6645 - val_loss: 96.0427\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5556 - val_loss: 125.5319\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5047 - val_loss: 95.6362\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.8455 - val_loss: 86.5505\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.7028 - val_loss: 109.7652\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0712 - val_loss: 125.9709\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.7082 - val_loss: 85.3650\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.3515 - val_loss: 89.3207\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.3827 - val_loss: 110.9052\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.2245 - val_loss: 82.9109\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6658 - val_loss: 86.1535\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.3933 - val_loss: 94.4216\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.4361 - val_loss: 98.8847\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.4717 - val_loss: 100.8354\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.3780 - val_loss: 92.6235\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.4456 - val_loss: 92.8777\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.3727 - val_loss: 100.5971\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.2911 - val_loss: 87.8743\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.0989 - val_loss: 89.5171\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.2898 - val_loss: 102.2197\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.0896 - val_loss: 102.1697\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.3615 - val_loss: 88.3040\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.1719 - val_loss: 97.6312\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.0883 - val_loss: 116.6533\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.1004 - val_loss: 91.8271\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.1332 - val_loss: 99.0464\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 65.8744 - val_loss: 85.2921\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.1082 - val_loss: 92.1776\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.1970 - val_loss: 90.4158\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.1176 - val_loss: 88.9937\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.3391 - val_loss: 90.9487\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5606 - val_loss: 93.4390\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.2536 - val_loss: 96.1492\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.3489 - val_loss: 91.5640\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.1829 - val_loss: 95.7918\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 65.9407 - val_loss: 109.9328\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.1556 - val_loss: 85.1764\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.1496 - val_loss: 113.0025\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 65.9388 - val_loss: 98.2921\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.0012 - val_loss: 91.6528\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 65.8231 - val_loss: 117.7115\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 65.9880 - val_loss: 85.8463\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.2460 - val_loss: 85.6540\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.0418 - val_loss: 111.8369\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.1617 - val_loss: 87.6589\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 66.2812 - val_loss: 95.5114\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 65.9769 - val_loss: 94.4772\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.0475 - val_loss: 102.9447\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6Dc0xVwOZnO",
        "outputId": "90331f43-d028-46bc-db8b-185e91fd4450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  3.3751082650001716 \n",
            "MAE:  7.68418116967053 \n",
            "SD:  9.568349331674897\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "qQZLKCzHOZnO",
        "outputId": "51cabd23-d74f-4a9d-dd16-1bc733664c91"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J-4nO0bgCLWP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gVrTvCSwG"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gJIE2njMCSwH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su2Sj5jZCSwH",
        "outputId": "43c83f4a-19df-442c-f72a-548c4b8a9ce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_19 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_24 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_24 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_25 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_25 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_26 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_26 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_27 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_27 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_28 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_28 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_29 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_29 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_30 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_31 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_32 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_33 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_33 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_34 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_35 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,841\n",
            "Trainable params: 7,265\n",
            "Non-trainable params: 576\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPRh6v-mCSwH",
        "outputId": "a6b05881-e222-418f-b3f5-c84bd12ee717",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 6s 18ms/step - loss: 12238.3486 - val_loss: 12329.6973\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 3s 18ms/step - loss: 11697.7461 - val_loss: 11494.9160\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 11038.2188 - val_loss: 10114.5996\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 10216.0264 - val_loss: 8844.8975\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 9218.4346 - val_loss: 9107.9639\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 8034.7896 - val_loss: 7076.0381\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 6708.5781 - val_loss: 6605.3843\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 5330.4014 - val_loss: 3960.4614\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 4032.5735 - val_loss: 2891.9138\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2920.2563 - val_loss: 2310.9146\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2013.0889 - val_loss: 2168.5137\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 1333.9012 - val_loss: 1077.7214\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 857.7274 - val_loss: 421.3583\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 535.6654 - val_loss: 310.1908\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 348.5587 - val_loss: 321.1343\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 246.3400 - val_loss: 381.1970\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 194.2402 - val_loss: 261.3106\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 170.0787 - val_loss: 176.8717\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 160.6750 - val_loss: 174.6992\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 151.5299 - val_loss: 215.1012\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 144.5130 - val_loss: 208.6514\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 139.2217 - val_loss: 248.8455\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 134.7618 - val_loss: 163.3743\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 132.0289 - val_loss: 146.2513\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 129.4672 - val_loss: 637.2143\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 126.9858 - val_loss: 129.9716\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 123.4461 - val_loss: 149.7398\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 122.1918 - val_loss: 131.9533\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 121.7156 - val_loss: 132.5822\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 119.6199 - val_loss: 162.7505\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 118.5246 - val_loss: 154.1825\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 116.4866 - val_loss: 167.0300\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 116.9278 - val_loss: 148.3833\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 116.6960 - val_loss: 171.1071\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 115.3739 - val_loss: 119.5908\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 112.9534 - val_loss: 128.2217\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 110.9513 - val_loss: 149.8599\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 110.9588 - val_loss: 124.2812\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 109.4265 - val_loss: 158.1561\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 107.9986 - val_loss: 127.7788\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 106.2560 - val_loss: 135.1269\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 106.4293 - val_loss: 159.3280\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 105.8519 - val_loss: 208.8692\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 105.0583 - val_loss: 121.9346\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 105.6970 - val_loss: 235.0135\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 105.1346 - val_loss: 200.8817\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 106.5521 - val_loss: 131.0490\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 105.5606 - val_loss: 179.7587\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 104.2640 - val_loss: 222.5236\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 105.6912 - val_loss: 129.6750\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 103.5153 - val_loss: 230.8277\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 103.5462 - val_loss: 163.8670\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 104.1723 - val_loss: 172.9649\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 118.7002 - val_loss: 508.6116\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 105.9688 - val_loss: 382.0614\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 103.9795 - val_loss: 118.5417\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 103.3517 - val_loss: 405.4626\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 103.6164 - val_loss: 195.0934\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.3146 - val_loss: 156.6318\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 99.8777 - val_loss: 148.0881\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.6844 - val_loss: 130.2158\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.6384 - val_loss: 171.0623\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.4626 - val_loss: 228.1017\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.8630 - val_loss: 157.1577\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.4544 - val_loss: 115.7039\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.2660 - val_loss: 156.7724\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.5645 - val_loss: 235.1650\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.1910 - val_loss: 171.9374\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 96.9561 - val_loss: 139.3997\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 96.3699 - val_loss: 117.7819\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 95.6868 - val_loss: 105.5855\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 95.4416 - val_loss: 149.9824\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 96.3269 - val_loss: 130.7971\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 95.2695 - val_loss: 115.8503\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 94.4730 - val_loss: 112.1542\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 94.1831 - val_loss: 171.6592\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 93.9816 - val_loss: 125.4865\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 94.9241 - val_loss: 141.2096\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 93.5495 - val_loss: 129.9812\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 93.0011 - val_loss: 114.0985\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 92.9987 - val_loss: 135.2565\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 93.2793 - val_loss: 159.6600\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 92.7905 - val_loss: 109.7226\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 93.3693 - val_loss: 127.4586\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 92.0495 - val_loss: 108.2723\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 92.0240 - val_loss: 114.4661\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 91.1575 - val_loss: 208.1823\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.8376 - val_loss: 107.5758\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.2710 - val_loss: 181.8251\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.8718 - val_loss: 145.0451\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 91.0733 - val_loss: 131.1746\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.6961 - val_loss: 138.5392\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.5480 - val_loss: 128.3685\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 88.9746 - val_loss: 110.3268\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 89.7300 - val_loss: 114.9147\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 89.2626 - val_loss: 144.0236\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 88.7785 - val_loss: 115.1284\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.2768 - val_loss: 114.4915\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 88.0436 - val_loss: 152.6979\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 88.1936 - val_loss: 109.6853\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.5077 - val_loss: 126.2990\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.7743 - val_loss: 106.9821\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.0260 - val_loss: 101.7693\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.7273 - val_loss: 104.1682\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.2891 - val_loss: 249.2377\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.3191 - val_loss: 199.3724\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.6351 - val_loss: 154.9798\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 89.1980 - val_loss: 119.9277\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 88.1332 - val_loss: 115.2328\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.6150 - val_loss: 104.4809\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.0946 - val_loss: 142.9655\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.7451 - val_loss: 124.4832\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.5972 - val_loss: 131.8008\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 85.4211 - val_loss: 141.5894\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 3s 19ms/step - loss: 86.2913 - val_loss: 210.1405\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.2512 - val_loss: 113.4790\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.6333 - val_loss: 159.8031\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.3531 - val_loss: 93.6039\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.0107 - val_loss: 159.1560\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.1245 - val_loss: 103.8583\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.3059 - val_loss: 138.9736\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.5588 - val_loss: 110.2815\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.4313 - val_loss: 142.1994\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.3401 - val_loss: 109.4847\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.9671 - val_loss: 97.5839\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.9755 - val_loss: 118.3232\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.2051 - val_loss: 138.2163\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.1650 - val_loss: 101.2530\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.3807 - val_loss: 129.2758\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.0376 - val_loss: 107.2683\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.8856 - val_loss: 107.0939\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.9897 - val_loss: 112.6959\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.0495 - val_loss: 172.3222\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.8665 - val_loss: 124.1293\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.4132 - val_loss: 155.3833\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.4658 - val_loss: 103.1714\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.5109 - val_loss: 108.0520\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.3310 - val_loss: 125.2449\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.3423 - val_loss: 121.2224\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.6555 - val_loss: 151.1473\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.0919 - val_loss: 105.0061\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.5844 - val_loss: 141.1404\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.1497 - val_loss: 100.6304\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.3767 - val_loss: 108.7924\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.4202 - val_loss: 108.4368\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 82.0781 - val_loss: 213.3294\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.7723 - val_loss: 100.2217\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.9466 - val_loss: 825.3818\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.2829 - val_loss: 105.0611\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.6630 - val_loss: 144.7047\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.9095 - val_loss: 163.0783\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.2092 - val_loss: 116.7142\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.9538 - val_loss: 129.5903\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.9805 - val_loss: 107.1523\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.8303 - val_loss: 99.9824\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.1690 - val_loss: 111.5798\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.1186 - val_loss: 127.1027\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.5581 - val_loss: 172.5999\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.6431 - val_loss: 99.8207\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.6798 - val_loss: 106.7974\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.9621 - val_loss: 119.2250\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.0604 - val_loss: 284.0796\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.3751 - val_loss: 214.3366\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.5107 - val_loss: 112.6942\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.4321 - val_loss: 100.7001\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.5036 - val_loss: 112.5898\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.4266 - val_loss: 103.8259\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.1197 - val_loss: 110.2076\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.4353 - val_loss: 112.6483\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.5900 - val_loss: 128.4707\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.9786 - val_loss: 107.0200\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.5083 - val_loss: 92.9551\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.0173 - val_loss: 111.5244\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.5774 - val_loss: 100.3416\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.3699 - val_loss: 176.5099\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.7434 - val_loss: 156.4763\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.1848 - val_loss: 204.2044\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.8741 - val_loss: 104.8009\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.9379 - val_loss: 142.9461\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.1409 - val_loss: 130.4100\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.0035 - val_loss: 100.0879\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.8062 - val_loss: 127.5396\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.5394 - val_loss: 109.9985\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.3440 - val_loss: 124.4221\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.0331 - val_loss: 233.6864\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.9154 - val_loss: 201.0336\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.9748 - val_loss: 142.1802\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.3904 - val_loss: 100.0638\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.1847 - val_loss: 113.5752\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.9924 - val_loss: 109.6212\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.3649 - val_loss: 96.2962\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.4183 - val_loss: 93.0807\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.5543 - val_loss: 90.0738\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.1198 - val_loss: 131.4248\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.3196 - val_loss: 117.7750\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.1191 - val_loss: 107.8554\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.4794 - val_loss: 132.9985\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.4725 - val_loss: 92.7556\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.6287 - val_loss: 203.1587\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.2548 - val_loss: 100.8104\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.1312 - val_loss: 133.1985\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.9530 - val_loss: 102.6268\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.4017 - val_loss: 106.5455\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.7283 - val_loss: 105.5970\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.0153 - val_loss: 95.2275\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 76.9217 - val_loss: 117.4900\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.6510 - val_loss: 108.8077\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.8385 - val_loss: 93.5020\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.8691 - val_loss: 91.3788\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.4404 - val_loss: 196.6820\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 77.6265 - val_loss: 129.9216\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.0032 - val_loss: 157.3328\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.0863 - val_loss: 142.3326\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 77.1735 - val_loss: 257.2599\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.0527 - val_loss: 150.0540\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.8889 - val_loss: 101.9967\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.3984 - val_loss: 103.6587\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.1947 - val_loss: 93.5674\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.4933 - val_loss: 115.2079\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.8331 - val_loss: 91.7364\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.2513 - val_loss: 90.1664\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.9523 - val_loss: 118.7904\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.7688 - val_loss: 91.0891\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 76.8774 - val_loss: 253.4558\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.9182 - val_loss: 97.6184\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.7506 - val_loss: 128.7431\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.6964 - val_loss: 111.9522\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.5984 - val_loss: 96.2510\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.4025 - val_loss: 121.1490\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.1968 - val_loss: 113.2666\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.9122 - val_loss: 137.8379\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.9798 - val_loss: 124.1640\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.5529 - val_loss: 372.3415\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.6075 - val_loss: 94.8423\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.4339 - val_loss: 104.0402\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 3s 20ms/step - loss: 76.4095 - val_loss: 200.4912\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.6300 - val_loss: 94.9485\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.7823 - val_loss: 106.9035\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.2803 - val_loss: 94.7717\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.5258 - val_loss: 121.0554\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.1468 - val_loss: 92.4584\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.0637 - val_loss: 101.8059\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.7369 - val_loss: 100.3174\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.0052 - val_loss: 109.5675\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.5003 - val_loss: 144.4253\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.5752 - val_loss: 115.7478\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.4418 - val_loss: 106.8374\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.2808 - val_loss: 101.0724\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.6216 - val_loss: 185.4782\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.4912 - val_loss: 360.9451\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.7347 - val_loss: 138.2033\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.8944 - val_loss: 90.8060\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.6821 - val_loss: 95.8672\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.9858 - val_loss: 91.4986\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.8785 - val_loss: 102.3326\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.1048 - val_loss: 267.7065\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.9087 - val_loss: 100.5113\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.0784 - val_loss: 125.7264\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.7483 - val_loss: 129.9841\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.8109 - val_loss: 222.1272\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.7282 - val_loss: 270.0307\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.3927 - val_loss: 106.7662\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.1583 - val_loss: 107.3949\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.5528 - val_loss: 100.0402\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.6584 - val_loss: 152.2601\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.2320 - val_loss: 105.1198\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.4383 - val_loss: 182.1443\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.5364 - val_loss: 129.6908\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.1336 - val_loss: 118.4576\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.3545 - val_loss: 119.4088\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.2606 - val_loss: 105.8000\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.9528 - val_loss: 96.5257\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.7690 - val_loss: 123.5712\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.2078 - val_loss: 103.4753\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 74.2141 - val_loss: 117.8687\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.9682 - val_loss: 85.2736\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.7916 - val_loss: 122.3630\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.2824 - val_loss: 97.1459\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.7740 - val_loss: 92.5829\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 74.0205 - val_loss: 89.1780\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.7270 - val_loss: 114.5019\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.4985 - val_loss: 111.2512\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.2756 - val_loss: 91.2745\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.4214 - val_loss: 92.7452\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.2482 - val_loss: 117.0270\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.4520 - val_loss: 139.9781\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.6001 - val_loss: 238.0614\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 74.2257 - val_loss: 140.3290\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.2390 - val_loss: 91.5051\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.3719 - val_loss: 116.8743\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.2140 - val_loss: 100.4898\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.4877 - val_loss: 88.0573\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.1237 - val_loss: 111.4315\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.0671 - val_loss: 93.7555\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.6801 - val_loss: 123.9175\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.6461 - val_loss: 104.7752\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.3774 - val_loss: 110.6432\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.4129 - val_loss: 101.0211\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.1415 - val_loss: 94.0791\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.8787 - val_loss: 123.6013\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.0557 - val_loss: 116.3807\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.8565 - val_loss: 119.5994\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.8479 - val_loss: 116.4826\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.5083 - val_loss: 295.0561\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 75.3573 - val_loss: 103.2742\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.1663 - val_loss: 109.2121\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.0770 - val_loss: 93.4879\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.8163 - val_loss: 131.5830\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.2514 - val_loss: 448.2739\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.4659 - val_loss: 90.2078\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 74.3434 - val_loss: 105.7206\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.4747 - val_loss: 102.7172\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.4659 - val_loss: 95.4004\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.2894 - val_loss: 108.7480\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.1541 - val_loss: 92.1374\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.5686 - val_loss: 106.9893\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.5128 - val_loss: 125.8530\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.2334 - val_loss: 91.7196\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.2474 - val_loss: 106.5573\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.6196 - val_loss: 93.6542\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.5765 - val_loss: 95.6078\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.7886 - val_loss: 90.0753\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.8795 - val_loss: 191.0201\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.7735 - val_loss: 98.9097\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.7831 - val_loss: 88.0560\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.1947 - val_loss: 102.2378\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.0179 - val_loss: 93.6268\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.2678 - val_loss: 177.7327\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.6330 - val_loss: 97.7142\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.3946 - val_loss: 107.4064\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.7559 - val_loss: 112.9753\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 74.0273 - val_loss: 91.5319\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 3s 18ms/step - loss: 78.1119 - val_loss: 100.5678\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.9217 - val_loss: 138.1709\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 75.9290 - val_loss: 114.8247\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.9507 - val_loss: 90.1451\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 74.4404 - val_loss: 87.9220\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.0374 - val_loss: 116.3064\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.3222 - val_loss: 99.0867\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.6509 - val_loss: 103.0535\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 74.5818 - val_loss: 116.0451\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.4777 - val_loss: 116.9776\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.7757 - val_loss: 118.5388\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 77.7346 - val_loss: 103.1099\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 75.6238 - val_loss: 116.2100\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 75.4696 - val_loss: 96.8031\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 74.8562 - val_loss: 101.6488\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.5871 - val_loss: 101.1401\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.4440 - val_loss: 93.9691\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.2283 - val_loss: 89.1201\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.6787 - val_loss: 94.8507\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.7479 - val_loss: 87.3970\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.1721 - val_loss: 114.5180\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.6097 - val_loss: 105.9516\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.1222 - val_loss: 109.9013\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.7278 - val_loss: 85.9533\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.0940 - val_loss: 93.9121\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 72.5687 - val_loss: 128.0261\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.6600 - val_loss: 144.4279\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.9661 - val_loss: 93.4976\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.3579 - val_loss: 102.6311\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 74.6402 - val_loss: 97.2717\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.4887 - val_loss: 147.1158\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 74.4020 - val_loss: 166.4049\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.8434 - val_loss: 98.5048\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.1463 - val_loss: 94.7074\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.7984 - val_loss: 103.9164\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 74.1480 - val_loss: 113.1858\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 72.6738 - val_loss: 108.1453\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 72.7483 - val_loss: 95.2524\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.8973 - val_loss: 91.8176\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.4577 - val_loss: 133.1692\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.9779 - val_loss: 95.4005\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.7200 - val_loss: 109.4659\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.9380 - val_loss: 105.6877\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.4593 - val_loss: 115.2381\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.6237 - val_loss: 125.2548\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.6899 - val_loss: 144.7875\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 72.2058 - val_loss: 94.3358\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.7300 - val_loss: 88.6848\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.8364 - val_loss: 257.8483\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.3978 - val_loss: 111.9895\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.1118 - val_loss: 103.8294\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.7065 - val_loss: 108.9698\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.9708 - val_loss: 386.3416\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.0942 - val_loss: 90.1639\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.4529 - val_loss: 98.9259\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.5574 - val_loss: 237.7558\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.4455 - val_loss: 108.4350\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.6241 - val_loss: 89.7479\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.7628 - val_loss: 100.0527\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.4549 - val_loss: 104.2183\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.4198 - val_loss: 103.6067\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.8246 - val_loss: 94.2096\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.7175 - val_loss: 88.1593\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.0658 - val_loss: 103.2497\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.3206 - val_loss: 117.5117\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.7873 - val_loss: 100.5883\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.6399 - val_loss: 100.5524\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.4089 - val_loss: 89.5742\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.9311 - val_loss: 92.6583\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.2714 - val_loss: 117.8737\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 72.3465 - val_loss: 97.5089\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 71.5697 - val_loss: 125.3253\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 71.5497 - val_loss: 107.9698\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.8131 - val_loss: 93.7717\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 71.2554 - val_loss: 110.3759\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.6819 - val_loss: 93.4492\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.3257 - val_loss: 95.0837\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 73.1528 - val_loss: 119.6969\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.4308 - val_loss: 105.6842\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.6569 - val_loss: 95.6725\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.8102 - val_loss: 93.3924\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.5829 - val_loss: 97.4684\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.8101 - val_loss: 88.7268\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.4438 - val_loss: 116.3983\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.1581 - val_loss: 96.0224\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.8671 - val_loss: 92.8706\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.7163 - val_loss: 123.9958\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.3612 - val_loss: 90.5028\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.8244 - val_loss: 163.7040\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.3292 - val_loss: 155.4209\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.8799 - val_loss: 91.2320\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.4954 - val_loss: 86.2224\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.7056 - val_loss: 112.2602\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.2942 - val_loss: 94.8464\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.1318 - val_loss: 87.6855\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.5635 - val_loss: 94.1877\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.6111 - val_loss: 102.9245\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.4380 - val_loss: 90.1398\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.4962 - val_loss: 93.9239\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.9252 - val_loss: 134.2220\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.4400 - val_loss: 95.6942\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.3383 - val_loss: 150.6490\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.8767 - val_loss: 118.1056\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.1952 - val_loss: 89.1652\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.0939 - val_loss: 89.9679\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.8542 - val_loss: 113.3604\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.0220 - val_loss: 126.6312\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.3973 - val_loss: 93.0974\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.1696 - val_loss: 268.3055\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.4280 - val_loss: 87.7895\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.1470 - val_loss: 111.3927\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.3267 - val_loss: 123.3852\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 70.7345 - val_loss: 128.1114\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.5497 - val_loss: 103.8978\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.3543 - val_loss: 91.3098\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.4941 - val_loss: 96.2663\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.2016 - val_loss: 115.8618\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.0984 - val_loss: 106.1164\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 69.8380 - val_loss: 91.3324\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.6609 - val_loss: 138.8519\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.9903 - val_loss: 105.9246\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 69.8632 - val_loss: 89.7704\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.9139 - val_loss: 109.2090\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.2936 - val_loss: 125.9463\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.7537 - val_loss: 90.9112\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.5749 - val_loss: 108.6299\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.8662 - val_loss: 123.5271\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.0554 - val_loss: 96.3244\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.7542 - val_loss: 160.3507\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.6544 - val_loss: 91.3829\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.6529 - val_loss: 107.1617\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.7072 - val_loss: 106.4224\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.4400 - val_loss: 181.5816\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.2422 - val_loss: 90.7746\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 3s 20ms/step - loss: 70.5922 - val_loss: 121.2760\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.0970 - val_loss: 102.9191\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.1115 - val_loss: 87.7771\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.0420 - val_loss: 96.9557\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.1815 - val_loss: 104.5819\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.0943 - val_loss: 91.7110\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.7783 - val_loss: 95.4597\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 70.4783 - val_loss: 87.5894\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.8828 - val_loss: 104.3571\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 69.9183 - val_loss: 89.5232\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.1533 - val_loss: 106.0528\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.7969 - val_loss: 115.6087\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.2857 - val_loss: 112.4549\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.4540 - val_loss: 89.0696\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.9061 - val_loss: 159.0683\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.3776 - val_loss: 96.9452\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.5288 - val_loss: 104.3903\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.5708 - val_loss: 185.3513\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.2479 - val_loss: 104.0290\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.8868 - val_loss: 101.1299\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.1574 - val_loss: 87.8723\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.0256 - val_loss: 95.6200\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.4923 - val_loss: 96.2015\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 70.1782 - val_loss: 94.5163\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.6745 - val_loss: 89.3617\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.7868 - val_loss: 89.5161\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.1228 - val_loss: 103.1952\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.7616 - val_loss: 92.1619\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.4077 - val_loss: 106.8488\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.7343 - val_loss: 117.1824\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 72.0641 - val_loss: 104.0038\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.7772 - val_loss: 137.6060\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.4074 - val_loss: 103.5049\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 71.1641 - val_loss: 121.6677\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYDcggm8CSwH",
        "outputId": "d382f145-9fef-40dd-c47b-c92e5103c12e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -3.4753961791537753 \n",
            "MAE:  8.541459688976037 \n",
            "SD:  10.468492311648584\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "VpKjAxdPCSwI",
        "outputId": "cd93c888-de77-4c81-8668-933e8985c76b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UKSPwqgYCSwI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhzZWoACTsZ"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T0F7tiaPCTsa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0vAhaD0CTsa",
        "outputId": "d229dc28-74fc-403d-a785-7a8349a9836b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_38 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_36 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_37 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_38 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_38 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_39 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_39 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_40 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_40 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_41 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_41 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_42 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_42 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_43 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_43 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_44 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_44 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_45 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_45 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_46 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_46 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_47 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_47 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_48 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_48 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_49 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_49 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_50 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_50 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_51 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_51 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_52 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_52 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_53 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_53 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,841\n",
            "Trainable params: 7,265\n",
            "Non-trainable params: 576\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcXAOqd2CTsa",
        "outputId": "ad4224f0-0b19-41a5-855e-039d70173416",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 7s 17ms/step - loss: 12344.4258 - val_loss: 12162.0869\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 11814.8730 - val_loss: 11005.1025\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 11062.7012 - val_loss: 10871.1865\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 10059.2334 - val_loss: 8687.6113\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 8857.3018 - val_loss: 6962.5786\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 7437.9331 - val_loss: 6527.2271\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 5987.2163 - val_loss: 5344.7451\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 4588.3120 - val_loss: 3030.0293\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 3291.0647 - val_loss: 2864.1897\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2177.6223 - val_loss: 421.0397\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 1364.6400 - val_loss: 1077.5082\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 785.6948 - val_loss: 431.8059\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 435.4581 - val_loss: 492.7780\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 248.8749 - val_loss: 194.8697\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 177.0749 - val_loss: 194.5255\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 154.0004 - val_loss: 168.1912\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 142.4721 - val_loss: 153.0838\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 135.4998 - val_loss: 193.7420\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 132.0531 - val_loss: 141.6039\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 129.6470 - val_loss: 169.7044\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 128.1185 - val_loss: 139.7405\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 125.3976 - val_loss: 130.2955\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 123.3337 - val_loss: 145.8330\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 121.8658 - val_loss: 176.0977\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 120.2434 - val_loss: 167.0095\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 116.8295 - val_loss: 130.8232\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 115.6750 - val_loss: 125.3137\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 113.8857 - val_loss: 124.9495\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 112.5092 - val_loss: 122.3824\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 111.8622 - val_loss: 149.4051\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 110.3707 - val_loss: 129.2310\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 110.1780 - val_loss: 134.3692\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 107.9174 - val_loss: 120.2313\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 107.3424 - val_loss: 150.0553\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 106.1853 - val_loss: 166.2326\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 105.5121 - val_loss: 117.5726\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 103.9926 - val_loss: 139.1579\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 104.1647 - val_loss: 127.2270\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 104.5502 - val_loss: 137.4915\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 105.8790 - val_loss: 157.0595\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 104.7461 - val_loss: 119.9442\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 105.1444 - val_loss: 120.2879\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 102.0704 - val_loss: 116.5238\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.3023 - val_loss: 149.1449\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.9944 - val_loss: 123.9771\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 105.9115 - val_loss: 175.5250\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 105.2413 - val_loss: 305.0978\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.3704 - val_loss: 119.5583\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 99.9298 - val_loss: 152.5677\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.1182 - val_loss: 151.3749\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.6634 - val_loss: 175.1642\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.3750 - val_loss: 138.7187\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 101.6879 - val_loss: 337.0477\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 99.5512 - val_loss: 251.7096\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 97.2556 - val_loss: 116.0069\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.1898 - val_loss: 280.9644\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.2397 - val_loss: 126.3452\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 95.5675 - val_loss: 134.0945\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 94.1668 - val_loss: 116.1094\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 94.1447 - val_loss: 123.1019\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 93.9771 - val_loss: 109.3805\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 92.9288 - val_loss: 112.2772\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 91.9697 - val_loss: 104.4654\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 91.0560 - val_loss: 102.0652\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.7152 - val_loss: 108.9862\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 90.2313 - val_loss: 117.9842\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 89.9457 - val_loss: 105.4439\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 89.1591 - val_loss: 102.7934\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 3s 18ms/step - loss: 88.7879 - val_loss: 113.1878\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 89.0464 - val_loss: 118.4768\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 88.7071 - val_loss: 101.7401\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.6717 - val_loss: 100.9489\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.2385 - val_loss: 106.7029\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.1969 - val_loss: 113.9830\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.9891 - val_loss: 109.5759\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.0586 - val_loss: 115.6539\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.9920 - val_loss: 156.3364\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.6089 - val_loss: 110.7780\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 86.1576 - val_loss: 181.4436\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 86.5662 - val_loss: 139.4608\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 85.8339 - val_loss: 184.8025\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 85.4126 - val_loss: 122.6217\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.6579 - val_loss: 114.4104\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 85.7506 - val_loss: 107.8195\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.6083 - val_loss: 113.1942\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 84.2185 - val_loss: 116.8720\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.3496 - val_loss: 90.4010\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.6795 - val_loss: 130.9448\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.7134 - val_loss: 107.0420\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.8548 - val_loss: 133.5502\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.3295 - val_loss: 103.4666\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.8360 - val_loss: 108.6059\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.6746 - val_loss: 117.0595\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.4732 - val_loss: 95.4640\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.0980 - val_loss: 97.6806\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.9231 - val_loss: 131.5553\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.5816 - val_loss: 121.2051\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 82.0046 - val_loss: 119.4236\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.4141 - val_loss: 106.2472\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.2948 - val_loss: 150.6456\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.1358 - val_loss: 100.9068\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.1393 - val_loss: 108.3436\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.7691 - val_loss: 136.7151\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.7460 - val_loss: 103.3486\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.6221 - val_loss: 132.2277\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 81.3099 - val_loss: 130.3930\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.4525 - val_loss: 100.2493\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.8030 - val_loss: 111.8199\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.0669 - val_loss: 103.5149\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.1305 - val_loss: 102.2307\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 80.0934 - val_loss: 122.5363\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.9920 - val_loss: 145.6776\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.9123 - val_loss: 95.7425\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.8380 - val_loss: 115.2121\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.7765 - val_loss: 119.8765\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.6462 - val_loss: 113.5647\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.9292 - val_loss: 108.0113\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.7210 - val_loss: 87.2822\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.9591 - val_loss: 107.3197\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.2262 - val_loss: 98.9085\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 79.5372 - val_loss: 101.2948\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.1530 - val_loss: 107.2540\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.1849 - val_loss: 96.4868\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.3607 - val_loss: 120.1062\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.0491 - val_loss: 134.7146\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.0173 - val_loss: 94.5210\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.5792 - val_loss: 113.7146\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.1232 - val_loss: 93.3688\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.0908 - val_loss: 92.7707\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.5007 - val_loss: 95.3029\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.8386 - val_loss: 103.8713\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.5231 - val_loss: 202.2481\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.0507 - val_loss: 93.5351\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.4057 - val_loss: 100.4564\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.1059 - val_loss: 128.1107\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 77.4787 - val_loss: 98.3451\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.4688 - val_loss: 90.7148\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.0039 - val_loss: 98.8812\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.2541 - val_loss: 100.0714\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.2643 - val_loss: 118.8967\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.9952 - val_loss: 159.3944\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.7303 - val_loss: 107.1395\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 76.0084 - val_loss: 102.6513\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.8329 - val_loss: 118.9598\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.2375 - val_loss: 96.1082\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.3678 - val_loss: 110.8115\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.4543 - val_loss: 100.3966\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.1738 - val_loss: 96.2625\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.0502 - val_loss: 106.3760\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.1812 - val_loss: 92.0209\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 75.0101 - val_loss: 118.3354\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 75.1463 - val_loss: 174.1386\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.5602 - val_loss: 111.3071\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.8043 - val_loss: 99.6217\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.6724 - val_loss: 110.5923\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.5552 - val_loss: 93.8263\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.2609 - val_loss: 93.5202\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.1491 - val_loss: 95.3115\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.3721 - val_loss: 108.7476\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.9715 - val_loss: 104.8485\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.2103 - val_loss: 105.5781\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.6534 - val_loss: 119.3477\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.0096 - val_loss: 104.6405\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.0281 - val_loss: 97.6990\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.6231 - val_loss: 121.4384\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.5624 - val_loss: 138.6700\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 74.0724 - val_loss: 92.0638\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.7196 - val_loss: 101.2891\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.9245 - val_loss: 109.3947\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.4650 - val_loss: 92.4732\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.4849 - val_loss: 93.2548\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.5119 - val_loss: 104.4567\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.4380 - val_loss: 91.9826\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.8363 - val_loss: 137.1962\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.7154 - val_loss: 96.2118\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.6798 - val_loss: 100.1540\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.3856 - val_loss: 100.7598\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.2024 - val_loss: 96.0984\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.1104 - val_loss: 110.2920\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.5322 - val_loss: 100.4954\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.8870 - val_loss: 124.4091\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.8390 - val_loss: 111.6690\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.7746 - val_loss: 105.9681\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 3s 20ms/step - loss: 72.8117 - val_loss: 118.3783\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.4629 - val_loss: 106.8201\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.7202 - val_loss: 99.7381\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.7056 - val_loss: 107.4711\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.6385 - val_loss: 91.4552\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.4175 - val_loss: 125.7182\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.1408 - val_loss: 112.8835\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.8746 - val_loss: 148.6566\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.8764 - val_loss: 142.0067\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.4732 - val_loss: 95.7767\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.3087 - val_loss: 89.5054\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.0051 - val_loss: 163.5709\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.7970 - val_loss: 102.3540\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.6971 - val_loss: 166.8521\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.5912 - val_loss: 85.9552\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.6776 - val_loss: 99.8890\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.0479 - val_loss: 93.0122\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.2581 - val_loss: 97.6129\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.2511 - val_loss: 140.2262\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.1707 - val_loss: 100.3941\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.7822 - val_loss: 88.3580\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.6521 - val_loss: 91.8697\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.4871 - val_loss: 93.9820\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.6460 - val_loss: 103.4040\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.6354 - val_loss: 95.1372\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.5572 - val_loss: 100.8457\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.8870 - val_loss: 116.8031\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.9328 - val_loss: 89.5798\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.6404 - val_loss: 99.7827\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.6257 - val_loss: 110.3929\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.3182 - val_loss: 102.8580\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.0207 - val_loss: 121.2742\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.5922 - val_loss: 101.7325\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.7179 - val_loss: 91.7555\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.8654 - val_loss: 89.1749\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.6434 - val_loss: 118.2412\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.4873 - val_loss: 105.7361\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.7763 - val_loss: 258.9246\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.7117 - val_loss: 112.1111\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.4488 - val_loss: 107.1202\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.5175 - val_loss: 95.7851\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.7984 - val_loss: 154.2975\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 72.0489 - val_loss: 92.1390\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.4887 - val_loss: 91.2848\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.7888 - val_loss: 91.1421\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.1668 - val_loss: 92.5711\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.0099 - val_loss: 98.5698\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.0608 - val_loss: 141.8699\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.5630 - val_loss: 96.6973\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.1988 - val_loss: 100.1635\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.0021 - val_loss: 184.3216\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.1938 - val_loss: 107.9360\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.5055 - val_loss: 92.2866\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.5428 - val_loss: 87.4292\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.8042 - val_loss: 93.5184\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.1919 - val_loss: 93.2697\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.6103 - val_loss: 132.1142\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.7351 - val_loss: 107.1969\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.9171 - val_loss: 118.7744\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.5343 - val_loss: 88.0040\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.9467 - val_loss: 94.9067\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.3285 - val_loss: 109.6782\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.0611 - val_loss: 93.3435\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.5137 - val_loss: 91.2934\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.3301 - val_loss: 94.6330\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.1189 - val_loss: 88.3585\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.2732 - val_loss: 93.1896\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.2305 - val_loss: 117.3678\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 71.5500 - val_loss: 123.2564\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.0765 - val_loss: 118.2764\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.7240 - val_loss: 117.9720\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.5115 - val_loss: 90.9704\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.3008 - val_loss: 92.2962\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.3043 - val_loss: 97.5676\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.4886 - val_loss: 139.0023\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.7733 - val_loss: 98.4858\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.1232 - val_loss: 93.9447\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 71.0095 - val_loss: 92.7296\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.4126 - val_loss: 150.9399\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 70.3285 - val_loss: 91.3046\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.1628 - val_loss: 107.0334\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.6486 - val_loss: 86.8931\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.4001 - val_loss: 93.0030\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.7158 - val_loss: 121.2447\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.8829 - val_loss: 105.4933\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.4457 - val_loss: 113.8609\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.2424 - val_loss: 94.6523\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.7930 - val_loss: 90.9653\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.5959 - val_loss: 111.7561\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.9847 - val_loss: 103.4659\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.9729 - val_loss: 105.8643\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.6454 - val_loss: 94.4612\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.8175 - val_loss: 97.4001\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.1670 - val_loss: 122.0571\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.2728 - val_loss: 98.7548\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.1666 - val_loss: 120.9303\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.6979 - val_loss: 93.7193\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.1361 - val_loss: 105.5339\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.6521 - val_loss: 116.2871\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.4484 - val_loss: 96.7158\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.8177 - val_loss: 98.7079\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.7833 - val_loss: 90.9261\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.5973 - val_loss: 97.3929\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.7816 - val_loss: 115.5703\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.4312 - val_loss: 100.0764\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.5471 - val_loss: 87.0927\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.9798 - val_loss: 95.5805\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.5198 - val_loss: 87.0807\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.3835 - val_loss: 103.0370\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.1009 - val_loss: 93.6782\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.1552 - val_loss: 118.7752\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 70.2578 - val_loss: 103.5678\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.3665 - val_loss: 115.4658\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.3285 - val_loss: 92.4924\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.1639 - val_loss: 90.6851\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.0204 - val_loss: 96.0859\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 68.8578 - val_loss: 93.2322\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 3s 18ms/step - loss: 69.0033 - val_loss: 108.6796\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.2069 - val_loss: 88.7833\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 69.0539 - val_loss: 111.4939\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.2440 - val_loss: 95.9374\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.7668 - val_loss: 106.5460\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.0129 - val_loss: 87.2355\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.5880 - val_loss: 92.7553\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.1263 - val_loss: 95.5349\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.9292 - val_loss: 93.9841\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.9096 - val_loss: 90.2896\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.0009 - val_loss: 88.6447\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.3722 - val_loss: 87.5691\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.8763 - val_loss: 116.4340\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6089 - val_loss: 115.1008\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.5016 - val_loss: 105.8652\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.0172 - val_loss: 112.6755\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.9605 - val_loss: 92.3098\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6145 - val_loss: 106.7493\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6877 - val_loss: 93.6916\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.4932 - val_loss: 123.8552\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6447 - val_loss: 95.5514\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6117 - val_loss: 88.6866\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.4214 - val_loss: 103.2379\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.5345 - val_loss: 89.0527\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.5397 - val_loss: 110.4358\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.1161 - val_loss: 92.0991\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.2961 - val_loss: 119.4436\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6450 - val_loss: 149.7975\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.1492 - val_loss: 94.6349\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.9161 - val_loss: 89.2949\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.0865 - val_loss: 126.4334\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.7343 - val_loss: 96.8450\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.6383 - val_loss: 91.1392\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.0402 - val_loss: 149.0726\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.9756 - val_loss: 120.2604\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.8653 - val_loss: 89.6199\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.0905 - val_loss: 102.9058\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.3786 - val_loss: 109.3053\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.1447 - val_loss: 88.2174\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.7714 - val_loss: 85.3624\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.1958 - val_loss: 90.0106\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.9017 - val_loss: 85.1919\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.8370 - val_loss: 87.8000\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.1819 - val_loss: 114.0767\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.9292 - val_loss: 92.1941\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.1220 - val_loss: 178.1290\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.8110 - val_loss: 123.8580\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.1518 - val_loss: 100.7163\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.2945 - val_loss: 89.2334\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.8287 - val_loss: 90.8835\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.3176 - val_loss: 117.7118\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.1848 - val_loss: 99.8849\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.2551 - val_loss: 97.9257\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.2429 - val_loss: 91.0592\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.2468 - val_loss: 95.0274\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.9157 - val_loss: 93.0817\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.4823 - val_loss: 126.8863\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.1244 - val_loss: 142.7138\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.7689 - val_loss: 88.5650\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.5796 - val_loss: 87.9440\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.6177 - val_loss: 113.6454\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.7620 - val_loss: 106.0153\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.2206 - val_loss: 125.1790\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.4338 - val_loss: 90.5473\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 69.0718 - val_loss: 90.9661\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.5977 - val_loss: 90.6266\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.3979 - val_loss: 172.0486\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.5000 - val_loss: 124.8775\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.2194 - val_loss: 90.7569\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.7539 - val_loss: 116.1164\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.4109 - val_loss: 104.5421\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.9075 - val_loss: 128.1331\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.9677 - val_loss: 114.0561\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.8037 - val_loss: 86.4617\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.9796 - val_loss: 89.6182\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.8984 - val_loss: 135.3840\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.5849 - val_loss: 94.3169\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.5187 - val_loss: 106.1907\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.9643 - val_loss: 104.1488\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.4961 - val_loss: 86.6580\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3755 - val_loss: 92.8941\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.6331 - val_loss: 99.8127\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.5364 - val_loss: 88.1651\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.7014 - val_loss: 101.6481\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.4020 - val_loss: 93.7609\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.5757 - val_loss: 93.0204\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1898 - val_loss: 90.6837\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3380 - val_loss: 92.9768\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1774 - val_loss: 107.4149\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.7278 - val_loss: 105.4533\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3779 - val_loss: 91.7758\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3632 - val_loss: 100.3457\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2749 - val_loss: 92.3948\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3779 - val_loss: 111.1631\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3297 - val_loss: 106.0425\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.3425 - val_loss: 96.3056\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3449 - val_loss: 101.3018\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0253 - val_loss: 135.2927\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.7408 - val_loss: 113.3658\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.0849 - val_loss: 100.5480\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.7481 - val_loss: 101.5856\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.4011 - val_loss: 97.2197\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 68.1826 - val_loss: 88.4049\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.7870 - val_loss: 84.9051\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3659 - val_loss: 104.8987\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 67.2921 - val_loss: 88.2817\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.9397 - val_loss: 91.3576\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 67.1383 - val_loss: 100.4459\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 3s 20ms/step - loss: 67.0365 - val_loss: 96.5976\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1862 - val_loss: 98.1988\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1547 - val_loss: 83.9407\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1474 - val_loss: 91.0020\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8135 - val_loss: 93.4066\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8554 - val_loss: 87.1821\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3335 - val_loss: 91.7964\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2493 - val_loss: 92.8474\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.4586 - val_loss: 119.3520\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.6133 - val_loss: 97.5200\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3568 - val_loss: 109.2435\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.5445 - val_loss: 90.7363\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.5559 - val_loss: 145.2234\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.9968 - val_loss: 90.3526\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1661 - val_loss: 90.2141\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.1974 - val_loss: 125.7163\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0192 - val_loss: 107.8924\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.3459 - val_loss: 93.6099\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2030 - val_loss: 98.4388\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.9363 - val_loss: 101.4849\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.4978 - val_loss: 90.9662\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.7397 - val_loss: 89.3256\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.7814 - val_loss: 91.8636\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.9812 - val_loss: 96.4545\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.7484 - val_loss: 85.2023\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.4950 - val_loss: 169.2022\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.2756 - val_loss: 96.1651\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.4875 - val_loss: 103.1450\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0270 - val_loss: 92.1804\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8796 - val_loss: 87.0535\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.9095 - val_loss: 88.6745\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2160 - val_loss: 90.8616\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.8195 - val_loss: 87.3691\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2400 - val_loss: 161.0375\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.0643 - val_loss: 91.5614\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0011 - val_loss: 92.3270\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.2908 - val_loss: 86.9691\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0207 - val_loss: 102.1898\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6950 - val_loss: 99.7597\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.3270 - val_loss: 98.2973\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.4784 - val_loss: 98.2243\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.1782 - val_loss: 99.8158\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.1503 - val_loss: 110.2130\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.6864 - val_loss: 104.3382\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0400 - val_loss: 94.7981\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.4973 - val_loss: 88.6218\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.3508 - val_loss: 99.0181\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.6148 - val_loss: 131.4681\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.6196 - val_loss: 122.8520\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.6056 - val_loss: 106.1377\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6577 - val_loss: 87.2357\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.6312 - val_loss: 87.9879\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6727 - val_loss: 135.8870\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.8587 - val_loss: 180.8209\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.6173 - val_loss: 112.0120\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.5702 - val_loss: 115.9737\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6510 - val_loss: 89.1784\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.4102 - val_loss: 91.3493\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5364 - val_loss: 123.7820\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.7177 - val_loss: 97.3595\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.7814 - val_loss: 91.5013\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5986 - val_loss: 84.8059\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.2709 - val_loss: 91.8279\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8635 - val_loss: 91.2448\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6113 - val_loss: 107.2900\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.9004 - val_loss: 95.8347\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.5664 - val_loss: 99.4944\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.2813 - val_loss: 91.2996\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.9649 - val_loss: 88.3208\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.1424 - val_loss: 101.1231\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.7967 - val_loss: 121.7649\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.8717 - val_loss: 94.4634\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.9158 - val_loss: 91.0619\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.7911 - val_loss: 86.4010\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.7919 - val_loss: 92.4978\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.7777 - val_loss: 93.0297\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.4059 - val_loss: 95.9308\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8001 - val_loss: 90.3808\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.0244 - val_loss: 131.6978\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.0626 - val_loss: 102.9392\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.7707 - val_loss: 93.9749\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6921 - val_loss: 91.1515\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.9107 - val_loss: 121.1988\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8452 - val_loss: 92.4351\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8936 - val_loss: 90.5750\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.5277 - val_loss: 90.8990\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.6529 - val_loss: 92.2269\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 66.7449 - val_loss: 105.3536\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 66.8293 - val_loss: 99.2056\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.4863 - val_loss: 98.1191\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 67.8770 - val_loss: 96.1849\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 67.2530 - val_loss: 118.9081\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "history = model.fit(X_train, sbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, sbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "696v_fuFCTsa",
        "outputId": "4957c9c0-d7b3-4971-8ebf-e53c2419e723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  5.136648249296966 \n",
            "MAE:  8.634299563686909 \n",
            "SD:  9.618887669581797\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = sbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "# 오차의 평균 낮으면 좋은거야 , std 오차들의 표준편차 작으면 좋은거야 \n",
        "# 앙상블 , \n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "mULwm5BdCTsb",
        "outputId": "14f0c472-e342-4d36-ef1a-45dccdace693"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(history.history['val_loss'],color='red')\n",
        "   \n",
        "plt.plot(history.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdZF2osWCUQS",
        "outputId": "a94033a8-e855-436d-be22-ab28207ef094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  1.6787867783811208 \n",
            "Ensemble_std:  9.88524310430176\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXmmunmLOZnU"
      },
      "source": [
        "\n",
        "# DBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MRGXhWIAOZnU"
      },
      "outputs": [],
      "source": [
        "total_me = 0\n",
        "total_std = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeQljB1OZnU"
      },
      "source": [
        "## 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K8erthoaOZnU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkLVnvKbOZnU",
        "outputId": "62b34f1b-629b-4a48-d900-b0030e2fdfc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_57 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_54 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_54 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_55 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_55 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_56 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_56 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_57 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_57 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_58 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_58 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_59 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_59 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_60 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_60 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_61 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_61 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_62 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_62 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_63 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_63 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_64 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_64 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_65 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_65 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_66 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_66 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_70 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_67 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_67 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_68 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_68 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_69 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_69 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_70 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_70 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_71 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_71 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,841\n",
            "Trainable params: 7,265\n",
            "Non-trainable params: 576\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnNzIg0iOZnU",
        "outputId": "912495e4-8021-4c9e-ac4b-86ba9fc569a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 6s 16ms/step - loss: 3608.6221 - val_loss: 3654.9148\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 3297.1235 - val_loss: 3322.2366\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 2928.6235 - val_loss: 2920.9031\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 2491.2927 - val_loss: 1889.1738\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 2013.9375 - val_loss: 1838.8931\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 1486.2534 - val_loss: 1243.7683\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 985.1749 - val_loss: 1113.2753\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 576.2352 - val_loss: 326.7744\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 291.7562 - val_loss: 312.8002\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 143.3843 - val_loss: 92.3346\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 90.9350 - val_loss: 95.8920\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 74.1778 - val_loss: 96.2640\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 67.3115 - val_loss: 116.8043\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 3s 19ms/step - loss: 63.5613 - val_loss: 77.0578\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 59.9850 - val_loss: 60.5020\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 56.9724 - val_loss: 85.0900\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 54.2669 - val_loss: 70.8356\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 52.2771 - val_loss: 55.9619\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 49.8717 - val_loss: 54.6531\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 47.6603 - val_loss: 58.7739\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 46.0760 - val_loss: 59.4205\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 44.9452 - val_loss: 74.5467\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 43.5606 - val_loss: 63.5874\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 42.6468 - val_loss: 92.6316\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 41.5113 - val_loss: 52.0720\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 40.8595 - val_loss: 52.0530\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 40.1672 - val_loss: 52.0429\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 39.4779 - val_loss: 57.4740\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 39.2180 - val_loss: 52.0971\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 38.4849 - val_loss: 52.7865\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 37.7926 - val_loss: 85.1322\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 37.4385 - val_loss: 59.0455\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 37.3081 - val_loss: 56.9284\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 36.9240 - val_loss: 44.5435\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 36.9404 - val_loss: 60.4627\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 37.6404 - val_loss: 132.2598\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 38.8628 - val_loss: 151.7731\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.2117 - val_loss: 95.9777\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 37.0206 - val_loss: 59.7595\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 36.0711 - val_loss: 45.9734\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 36.0101 - val_loss: 53.8251\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 36.2631 - val_loss: 71.1904\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 35.5143 - val_loss: 94.4141\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 35.2521 - val_loss: 51.2215\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 34.5707 - val_loss: 39.9550\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 35.7044 - val_loss: 170.1741\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 36.6283 - val_loss: 143.6040\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 36.5044 - val_loss: 172.0305\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 36.2310 - val_loss: 55.5408\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 36.6985 - val_loss: 116.5016\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 35.2523 - val_loss: 57.7038\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 34.9906 - val_loss: 219.3106\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 36.0543 - val_loss: 72.4330\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 35.8413 - val_loss: 47.5376\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 34.2586 - val_loss: 179.1974\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 34.4395 - val_loss: 43.2046\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.5645 - val_loss: 52.0477\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.6663 - val_loss: 37.9458\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 33.9139 - val_loss: 42.3828\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 33.6568 - val_loss: 41.5631\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 33.5151 - val_loss: 67.6854\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 33.6180 - val_loss: 55.3456\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 33.1879 - val_loss: 44.9518\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.6241 - val_loss: 37.7168\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.5308 - val_loss: 37.4645\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.4173 - val_loss: 40.5393\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.3767 - val_loss: 40.0545\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.6934 - val_loss: 50.0808\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.2692 - val_loss: 45.5429\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.2695 - val_loss: 41.4850\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.0037 - val_loss: 41.5437\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.8905 - val_loss: 43.0493\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.1922 - val_loss: 42.0144\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.2865 - val_loss: 52.6320\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.8870 - val_loss: 40.2143\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.8406 - val_loss: 41.3014\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.6971 - val_loss: 53.0260\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.2473 - val_loss: 126.8016\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.3114 - val_loss: 58.3884\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.9080 - val_loss: 46.8957\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.8757 - val_loss: 43.6865\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.6757 - val_loss: 37.4088\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.9855 - val_loss: 53.9548\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.1606 - val_loss: 43.0842\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.5183 - val_loss: 67.5873\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.3180 - val_loss: 42.8700\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.0065 - val_loss: 41.1116\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.0061 - val_loss: 44.4697\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.0705 - val_loss: 51.6286\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.0807 - val_loss: 48.2358\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.4333 - val_loss: 58.7403\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.3346 - val_loss: 41.4832\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.2982 - val_loss: 39.8001\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.1510 - val_loss: 37.4158\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.5585 - val_loss: 51.8962\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.0093 - val_loss: 53.0339\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.6004 - val_loss: 47.5903\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.4382 - val_loss: 53.8891\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 32.3998 - val_loss: 64.5909\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.2404 - val_loss: 57.4575\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.2603 - val_loss: 41.0752\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.7704 - val_loss: 60.1361\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.6435 - val_loss: 39.4607\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.4341 - val_loss: 65.3232\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.5277 - val_loss: 43.2079\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.3348 - val_loss: 39.2556\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.1943 - val_loss: 39.2978\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.1173 - val_loss: 41.0015\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.0675 - val_loss: 45.4560\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.1086 - val_loss: 38.8537\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.9887 - val_loss: 46.2399\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.1056 - val_loss: 39.5273\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.7686 - val_loss: 35.4415\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.8444 - val_loss: 36.4739\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.8461 - val_loss: 37.5989\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.7279 - val_loss: 73.8135\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.8440 - val_loss: 47.4687\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.8741 - val_loss: 38.5275\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.2034 - val_loss: 44.5011\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.2905 - val_loss: 38.9449\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.8915 - val_loss: 44.1552\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.7701 - val_loss: 39.9614\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.6674 - val_loss: 35.9986\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.5615 - val_loss: 41.6960\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.5457 - val_loss: 36.5410\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.6248 - val_loss: 38.9819\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.5400 - val_loss: 42.6440\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.3762 - val_loss: 36.9986\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.3347 - val_loss: 46.2128\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.2938 - val_loss: 35.3378\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.2964 - val_loss: 43.9392\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.3355 - val_loss: 41.6288\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.2791 - val_loss: 39.8009\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.2716 - val_loss: 37.1612\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.2959 - val_loss: 38.9205\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.2700 - val_loss: 54.8718\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.2737 - val_loss: 37.0208\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.1962 - val_loss: 37.9601\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.0533 - val_loss: 39.8344\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.1324 - val_loss: 45.1666\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 29.4773 - val_loss: 46.2691\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 29.4127 - val_loss: 42.3404\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.1489 - val_loss: 34.9185\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.9896 - val_loss: 35.9288\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.9482 - val_loss: 38.9810\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.8469 - val_loss: 36.2809\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.9172 - val_loss: 34.6060\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.8260 - val_loss: 36.2661\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.8928 - val_loss: 35.8559\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.0648 - val_loss: 44.5804\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.8024 - val_loss: 34.9326\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.6907 - val_loss: 42.3802\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.7595 - val_loss: 44.2498\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.5014 - val_loss: 37.9284\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.7106 - val_loss: 35.8533\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.4930 - val_loss: 36.2490\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.1334 - val_loss: 36.7782\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.6932 - val_loss: 38.8901\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.5411 - val_loss: 34.9193\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.5139 - val_loss: 39.1143\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.5839 - val_loss: 49.5620\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.2923 - val_loss: 37.4958\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.0165 - val_loss: 38.7372\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.8733 - val_loss: 37.5043\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3858 - val_loss: 37.0808\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3646 - val_loss: 39.7586\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3042 - val_loss: 43.9189\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2137 - val_loss: 34.5653\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.2840 - val_loss: 40.7913\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2740 - val_loss: 42.7377\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2353 - val_loss: 36.3102\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1585 - val_loss: 54.2285\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.0829 - val_loss: 37.5233\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.1922 - val_loss: 37.4576\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2362 - val_loss: 35.3170\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1164 - val_loss: 38.9211\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2102 - val_loss: 33.2693\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.0782 - val_loss: 49.9490\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.2238 - val_loss: 38.0043\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.0119 - val_loss: 34.8372\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.0793 - val_loss: 38.0046\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2429 - val_loss: 38.1006\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 28.3043 - val_loss: 41.6561\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9810 - val_loss: 37.1504\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9603 - val_loss: 34.0762\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.9439 - val_loss: 36.9922\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.0117 - val_loss: 39.2037\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.8690 - val_loss: 34.8630\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.8397 - val_loss: 36.8704\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.7976 - val_loss: 33.9804\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8858 - val_loss: 34.6739\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9737 - val_loss: 39.3724\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.7425 - val_loss: 38.6274\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9214 - val_loss: 43.0597\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9852 - val_loss: 37.2488\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.8872 - val_loss: 37.1164\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3972 - val_loss: 38.6879\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.7606 - val_loss: 38.5221\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.7930 - val_loss: 38.2026\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8925 - val_loss: 35.6071\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8163 - val_loss: 46.3806\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.0586 - val_loss: 53.7414\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8870 - val_loss: 34.7835\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6971 - val_loss: 37.6109\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.6353 - val_loss: 34.9538\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6855 - val_loss: 39.5189\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5964 - val_loss: 48.9098\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6282 - val_loss: 34.1738\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.5776 - val_loss: 42.8411\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.6768 - val_loss: 33.6933\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5280 - val_loss: 37.3219\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6135 - val_loss: 37.1799\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4706 - val_loss: 41.8018\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4043 - val_loss: 39.5919\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6357 - val_loss: 57.0664\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6975 - val_loss: 41.3431\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.7970 - val_loss: 39.6214\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5405 - val_loss: 36.3192\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4404 - val_loss: 40.4525\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4940 - val_loss: 39.4184\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6505 - val_loss: 35.1469\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4298 - val_loss: 33.7603\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.2611 - val_loss: 38.2741\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6369 - val_loss: 72.5133\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6515 - val_loss: 35.2330\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.5021 - val_loss: 37.8541\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5389 - val_loss: 39.3923\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4267 - val_loss: 38.4197\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4397 - val_loss: 36.7413\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5376 - val_loss: 39.1573\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3599 - val_loss: 35.9989\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2274 - val_loss: 38.5696\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2532 - val_loss: 38.3353\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2045 - val_loss: 45.8577\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4171 - val_loss: 36.5088\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2644 - val_loss: 34.5622\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4248 - val_loss: 36.2455\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3240 - val_loss: 43.6872\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2541 - val_loss: 34.9229\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2421 - val_loss: 44.7561\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2627 - val_loss: 33.2143\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2311 - val_loss: 37.9927\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1146 - val_loss: 33.9822\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1947 - val_loss: 35.4208\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0531 - val_loss: 36.3187\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1197 - val_loss: 34.7411\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1925 - val_loss: 35.4098\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2119 - val_loss: 40.9187\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2056 - val_loss: 34.4310\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0453 - val_loss: 34.3782\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1322 - val_loss: 48.5362\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3669 - val_loss: 46.7289\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3509 - val_loss: 35.0714\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0557 - val_loss: 37.6373\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1368 - val_loss: 35.7392\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0080 - val_loss: 33.7385\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0975 - val_loss: 36.2439\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9012 - val_loss: 34.7886\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9618 - val_loss: 35.4245\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9364 - val_loss: 37.1504\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9601 - val_loss: 35.6059\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9238 - val_loss: 33.9038\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8432 - val_loss: 35.5994\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 3s 19ms/step - loss: 26.9587 - val_loss: 34.7024\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8602 - val_loss: 33.8573\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9889 - val_loss: 36.1399\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0698 - val_loss: 35.1153\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8055 - val_loss: 33.2931\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9089 - val_loss: 52.7818\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9555 - val_loss: 37.2429\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8639 - val_loss: 36.7322\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8663 - val_loss: 34.2725\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8822 - val_loss: 48.0056\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8151 - val_loss: 34.9360\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8268 - val_loss: 33.8054\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7773 - val_loss: 53.2132\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7530 - val_loss: 37.9038\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8075 - val_loss: 35.9255\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0253 - val_loss: 35.4780\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8794 - val_loss: 43.2718\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8026 - val_loss: 40.0108\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9277 - val_loss: 36.1101\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7387 - val_loss: 33.2436\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8157 - val_loss: 35.3962\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6242 - val_loss: 40.9997\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8233 - val_loss: 36.5369\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7211 - val_loss: 34.7598\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7689 - val_loss: 38.0999\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7047 - val_loss: 33.8084\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7471 - val_loss: 34.6087\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7955 - val_loss: 34.2583\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6855 - val_loss: 35.5204\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6093 - val_loss: 34.0166\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6892 - val_loss: 33.5901\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6814 - val_loss: 40.2902\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7198 - val_loss: 39.1941\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8216 - val_loss: 33.8184\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6236 - val_loss: 34.1756\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6737 - val_loss: 41.1596\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0566 - val_loss: 44.4825\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6532 - val_loss: 37.4334\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8246 - val_loss: 38.8565\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6144 - val_loss: 39.8858\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7884 - val_loss: 34.4975\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7176 - val_loss: 34.0649\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6603 - val_loss: 41.3504\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5420 - val_loss: 33.8469\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5030 - val_loss: 37.5204\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5261 - val_loss: 45.6634\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4338 - val_loss: 33.7602\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5150 - val_loss: 35.6386\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4975 - val_loss: 33.2266\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6116 - val_loss: 36.8070\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4997 - val_loss: 33.8913\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 26.5102 - val_loss: 34.4356\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4189 - val_loss: 49.1318\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4813 - val_loss: 34.7989\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4062 - val_loss: 41.7107\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5288 - val_loss: 33.2977\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4678 - val_loss: 33.7076\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5118 - val_loss: 35.6676\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5532 - val_loss: 38.3030\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4534 - val_loss: 36.3529\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5017 - val_loss: 35.5931\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4977 - val_loss: 40.4818\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5755 - val_loss: 35.7595\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5643 - val_loss: 45.2459\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4347 - val_loss: 40.7592\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4025 - val_loss: 39.0349\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4165 - val_loss: 38.9711\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4087 - val_loss: 43.1114\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3634 - val_loss: 36.3420\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4025 - val_loss: 34.8562\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5718 - val_loss: 36.9835\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6983 - val_loss: 60.0269\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6290 - val_loss: 41.8317\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4135 - val_loss: 35.2578\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3587 - val_loss: 35.2694\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5202 - val_loss: 41.1667\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5484 - val_loss: 36.1486\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2647 - val_loss: 35.7742\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2864 - val_loss: 37.5041\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5125 - val_loss: 34.2504\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4414 - val_loss: 43.7170\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3941 - val_loss: 42.1997\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4210 - val_loss: 37.2897\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4271 - val_loss: 57.6209\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3578 - val_loss: 36.7110\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3758 - val_loss: 34.9317\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7259 - val_loss: 34.3915\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4010 - val_loss: 34.5132\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3795 - val_loss: 34.8571\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3257 - val_loss: 39.3600\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3874 - val_loss: 36.3753\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4745 - val_loss: 36.1534\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5271 - val_loss: 34.5440\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5391 - val_loss: 34.0294\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3405 - val_loss: 35.1818\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4076 - val_loss: 33.6747\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3731 - val_loss: 35.9946\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4580 - val_loss: 38.0655\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3951 - val_loss: 34.2058\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.2899 - val_loss: 36.6123\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2623 - val_loss: 38.2495\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2127 - val_loss: 36.1942\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2028 - val_loss: 32.6000\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1524 - val_loss: 36.2147\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1222 - val_loss: 34.4968\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1520 - val_loss: 47.2810\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1737 - val_loss: 34.2989\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1720 - val_loss: 33.4434\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1617 - val_loss: 33.0420\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2235 - val_loss: 36.2948\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1874 - val_loss: 35.0120\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1727 - val_loss: 33.8896\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2734 - val_loss: 34.8829\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2077 - val_loss: 33.0405\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2070 - val_loss: 41.7009\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1997 - val_loss: 42.1603\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1067 - val_loss: 48.2655\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 3s 19ms/step - loss: 26.0794 - val_loss: 35.4769\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.2336 - val_loss: 35.6487\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1875 - val_loss: 46.6604\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2574 - val_loss: 36.7342\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1081 - val_loss: 37.9052\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1067 - val_loss: 39.8246\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1412 - val_loss: 45.7052\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2399 - val_loss: 41.1333\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0441 - val_loss: 33.6156\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1209 - val_loss: 37.0096\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1192 - val_loss: 38.7709\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2003 - val_loss: 37.6196\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9873 - val_loss: 34.2970\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1413 - val_loss: 34.8256\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0556 - val_loss: 34.4986\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9858 - val_loss: 35.1120\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0744 - val_loss: 36.3837\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0647 - val_loss: 37.2111\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0680 - val_loss: 34.7919\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8436 - val_loss: 35.6429\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9191 - val_loss: 36.4437\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9488 - val_loss: 33.3426\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9604 - val_loss: 34.2401\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0222 - val_loss: 33.1335\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8732 - val_loss: 33.8988\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9164 - val_loss: 33.8032\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8752 - val_loss: 35.1902\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9937 - val_loss: 34.4037\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9926 - val_loss: 32.8665\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8411 - val_loss: 35.1951\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8784 - val_loss: 34.3945\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9429 - val_loss: 36.1750\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8789 - val_loss: 38.5777\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8801 - val_loss: 34.4656\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8269 - val_loss: 34.1457\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8960 - val_loss: 34.2533\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8836 - val_loss: 37.8340\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8707 - val_loss: 38.7526\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9023 - val_loss: 34.3945\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7857 - val_loss: 34.2091\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8454 - val_loss: 33.1295\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8268 - val_loss: 32.8357\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8786 - val_loss: 37.6751\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8450 - val_loss: 34.2864\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8298 - val_loss: 41.6706\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7971 - val_loss: 41.1967\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8451 - val_loss: 33.3843\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7897 - val_loss: 33.4237\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8136 - val_loss: 34.5197\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9308 - val_loss: 36.2536\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7761 - val_loss: 35.8661\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8198 - val_loss: 49.9584\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7191 - val_loss: 37.1871\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8431 - val_loss: 34.2471\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8145 - val_loss: 35.4690\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7841 - val_loss: 33.7499\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8289 - val_loss: 33.9108\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8143 - val_loss: 33.7867\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8455 - val_loss: 33.8374\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7838 - val_loss: 36.6880\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7181 - val_loss: 33.8154\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7160 - val_loss: 36.7512\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7021 - val_loss: 38.1488\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7862 - val_loss: 33.5956\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7798 - val_loss: 34.7110\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7420 - val_loss: 35.2705\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7394 - val_loss: 36.5310\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7208 - val_loss: 33.2097\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7050 - val_loss: 32.5951\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7065 - val_loss: 35.6014\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7800 - val_loss: 35.2450\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6926 - val_loss: 36.3241\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9139 - val_loss: 36.2632\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7637 - val_loss: 33.3346\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7300 - val_loss: 34.8788\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8130 - val_loss: 36.9182\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8363 - val_loss: 35.9696\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7459 - val_loss: 40.6341\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7323 - val_loss: 39.9754\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7695 - val_loss: 33.4430\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7397 - val_loss: 35.3982\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7197 - val_loss: 34.1466\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7043 - val_loss: 36.1530\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8007 - val_loss: 39.0543\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6942 - val_loss: 34.8455\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7050 - val_loss: 34.8812\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7584 - val_loss: 32.8577\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8323 - val_loss: 33.9945\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7295 - val_loss: 35.8084\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6651 - val_loss: 32.8768\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7253 - val_loss: 36.6699\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7351 - val_loss: 34.9300\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7707 - val_loss: 36.1983\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6933 - val_loss: 34.4316\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7074 - val_loss: 39.0059\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7108 - val_loss: 44.7984\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6888 - val_loss: 40.1274\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6832 - val_loss: 34.4164\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6243 - val_loss: 34.7720\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5629 - val_loss: 40.8126\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5900 - val_loss: 32.8808\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6375 - val_loss: 34.1489\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6442 - val_loss: 40.6355\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7032 - val_loss: 37.5299\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6819 - val_loss: 33.5528\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8175 - val_loss: 57.9208\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7486 - val_loss: 34.4191\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7570 - val_loss: 35.0561\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6884 - val_loss: 34.6459\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6558 - val_loss: 37.6758\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6345 - val_loss: 33.8296\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5676 - val_loss: 33.1570\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5428 - val_loss: 34.4675\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5979 - val_loss: 34.1022\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6173 - val_loss: 36.0956\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5303 - val_loss: 36.2326\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5345 - val_loss: 32.2196\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 3s 18ms/step - loss: 25.5385 - val_loss: 37.7222\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 25.5552 - val_loss: 33.3140\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5032 - val_loss: 40.1026\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1TqXgfDOZnV",
        "outputId": "40979682-b69f-47a8-ad53-4eb99698bba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -2.4506905544539084 \n",
            "MAE:  4.8753850346998435 \n",
            "SD:  5.839240810608221\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "0cip38xZOZnV",
        "outputId": "db29420f-6e6a-4bca-e3f5-d4303c30820f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "O6TEeWSqDxwO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH25KGlDD3we"
      },
      "source": [
        "## 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KOSgyzVqD3we"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHn9Tl2zD3we",
        "outputId": "682898f6-1999-4369-a4b0-fa0e22025842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_76 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_72 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_72 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_73 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_73 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_78 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_74 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_74 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_79 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_75 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_75 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_76 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_76 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_77 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_77 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_78 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_78 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_79 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_79 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_80 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_80 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_81 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_81 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_86 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_82 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_82 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_87 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_83 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_83 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_88 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_84 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_84 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_89 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_85 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_85 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_90 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_86 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_86 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_91 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_87 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_87 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_92 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_88 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_88 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_93 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_89 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_89 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_94 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,841\n",
            "Trainable params: 7,265\n",
            "Non-trainable params: 576\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd6ThmMkD3wf",
        "outputId": "62b763c3-6465-49ec-cbb8-2d86db70f9a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 6s 16ms/step - loss: 3697.4460 - val_loss: 3626.2756\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 3387.0952 - val_loss: 3134.8298\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2978.1912 - val_loss: 2986.8103\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2465.2432 - val_loss: 1981.8564\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 1921.0779 - val_loss: 1747.3428\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 1406.8579 - val_loss: 1051.1132\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 952.3029 - val_loss: 1127.2231\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 590.8055 - val_loss: 846.3639\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 328.7301 - val_loss: 275.6851\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 182.8585 - val_loss: 156.6888\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 109.7811 - val_loss: 154.4010\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 78.1181 - val_loss: 75.6201\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 65.4931 - val_loss: 75.4971\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 60.4933 - val_loss: 62.4247\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 56.2719 - val_loss: 68.2769\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 53.6880 - val_loss: 65.7057\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 51.4813 - val_loss: 57.5204\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 49.4310 - val_loss: 60.7020\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 48.2155 - val_loss: 58.8970\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 47.3392 - val_loss: 48.2821\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 46.1542 - val_loss: 101.0513\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 44.3476 - val_loss: 57.7343\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 43.4517 - val_loss: 51.0916\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 43.8432 - val_loss: 89.1219\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 41.9918 - val_loss: 50.6726\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 40.3290 - val_loss: 48.2959\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 39.4833 - val_loss: 48.0011\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 38.6486 - val_loss: 44.4662\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 38.2990 - val_loss: 52.5537\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.6582 - val_loss: 59.9153\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.0458 - val_loss: 71.1313\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 36.6867 - val_loss: 50.1135\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 36.3308 - val_loss: 43.6960\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 36.0276 - val_loss: 40.2296\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 35.8161 - val_loss: 64.7731\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 35.5313 - val_loss: 56.6434\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 35.8421 - val_loss: 63.5869\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 35.7986 - val_loss: 48.6071\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 35.1371 - val_loss: 46.1788\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.8773 - val_loss: 42.6418\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.6339 - val_loss: 57.7670\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 34.7138 - val_loss: 54.1620\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.2850 - val_loss: 39.6223\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.0063 - val_loss: 38.9324\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.7870 - val_loss: 41.9709\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 33.6711 - val_loss: 40.5658\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 33.6021 - val_loss: 42.3465\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.6586 - val_loss: 46.3987\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.5957 - val_loss: 55.8643\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.4574 - val_loss: 42.4789\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.5130 - val_loss: 68.7069\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.1683 - val_loss: 48.4879\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.7949 - val_loss: 47.3899\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.6203 - val_loss: 47.5959\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.5388 - val_loss: 52.1022\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.5117 - val_loss: 51.8579\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.3627 - val_loss: 41.3362\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.4388 - val_loss: 43.6086\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.3871 - val_loss: 40.4326\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.4602 - val_loss: 44.7535\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.4243 - val_loss: 46.3394\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.9400 - val_loss: 62.6608\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.9132 - val_loss: 41.4384\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.7370 - val_loss: 41.9830\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.8918 - val_loss: 54.3732\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.8185 - val_loss: 39.7938\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.3125 - val_loss: 72.4832\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 31.1981 - val_loss: 44.5064\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.1513 - val_loss: 39.9429\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.9952 - val_loss: 50.0370\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.7239 - val_loss: 38.8736\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.7403 - val_loss: 44.3821\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.6759 - val_loss: 37.8464\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.3782 - val_loss: 37.5305\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.3235 - val_loss: 41.7856\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.1719 - val_loss: 49.6370\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.1333 - val_loss: 39.9826\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.2175 - val_loss: 52.5001\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.3828 - val_loss: 40.0214\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.0824 - val_loss: 52.6334\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.8660 - val_loss: 57.7642\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.7962 - val_loss: 50.3483\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.6991 - val_loss: 45.2549\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.6363 - val_loss: 36.5872\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.7327 - val_loss: 43.5685\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.3662 - val_loss: 55.8990\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.3423 - val_loss: 36.8324\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.3860 - val_loss: 38.9893\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.3478 - val_loss: 40.2031\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.2316 - val_loss: 37.2175\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.1580 - val_loss: 43.6945\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.0095 - val_loss: 46.7307\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.9031 - val_loss: 42.4222\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.8880 - val_loss: 55.3990\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.8453 - val_loss: 42.7576\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.7460 - val_loss: 40.7124\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 28.7300 - val_loss: 46.1768\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 28.6594 - val_loss: 40.5968\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.5548 - val_loss: 60.0862\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.6094 - val_loss: 45.6097\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.6613 - val_loss: 36.3951\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3888 - val_loss: 35.1988\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3648 - val_loss: 36.1951\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2853 - val_loss: 35.0290\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2239 - val_loss: 34.8191\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1991 - val_loss: 37.3363\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2065 - val_loss: 41.1234\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3725 - val_loss: 42.8512\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2219 - val_loss: 52.4464\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2796 - val_loss: 41.2234\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1665 - val_loss: 62.9686\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1703 - val_loss: 36.5989\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.0329 - val_loss: 39.0107\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.0297 - val_loss: 37.4250\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.0068 - val_loss: 45.0369\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9182 - val_loss: 47.8134\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8678 - val_loss: 34.7979\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.0724 - val_loss: 73.0237\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8853 - val_loss: 45.5072\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8523 - val_loss: 39.8282\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9312 - val_loss: 77.4492\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.6915 - val_loss: 37.5076\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1610 - val_loss: 64.7384\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.7666 - val_loss: 35.8992\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6199 - val_loss: 55.6607\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6911 - val_loss: 40.4836\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6843 - val_loss: 36.7669\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.7074 - val_loss: 43.4446\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.7979 - val_loss: 39.9093\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5379 - val_loss: 34.4828\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5833 - val_loss: 35.0012\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.5656 - val_loss: 57.3777\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6132 - val_loss: 33.9895\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4226 - val_loss: 39.1747\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4742 - val_loss: 36.7816\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4887 - val_loss: 35.2389\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3412 - val_loss: 60.4486\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3272 - val_loss: 43.3103\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4141 - val_loss: 32.9846\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2993 - val_loss: 71.3160\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3327 - val_loss: 34.2823\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3486 - val_loss: 43.5950\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3239 - val_loss: 39.1424\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2318 - val_loss: 37.4622\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4670 - val_loss: 39.3901\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3003 - val_loss: 37.4438\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2663 - val_loss: 34.3686\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2754 - val_loss: 34.8064\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2811 - val_loss: 35.6194\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1574 - val_loss: 39.6601\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3026 - val_loss: 41.5879\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 27.2052 - val_loss: 46.2259\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2084 - val_loss: 43.5843\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3131 - val_loss: 50.1044\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2514 - val_loss: 37.6381\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1264 - val_loss: 38.9267\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1155 - val_loss: 33.8600\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1888 - val_loss: 57.4083\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2554 - val_loss: 38.4620\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0915 - val_loss: 36.6759\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1550 - val_loss: 51.4603\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1850 - val_loss: 40.1898\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1333 - val_loss: 40.9904\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0955 - val_loss: 54.2191\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1685 - val_loss: 43.4282\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9190 - val_loss: 43.4222\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1067 - val_loss: 36.6212\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9985 - val_loss: 43.4459\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8986 - val_loss: 33.2801\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9496 - val_loss: 40.1604\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0510 - val_loss: 39.8597\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0086 - val_loss: 39.7594\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0914 - val_loss: 40.7878\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9763 - val_loss: 45.7658\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0270 - val_loss: 49.3063\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9249 - val_loss: 38.9057\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9400 - val_loss: 35.9582\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8699 - val_loss: 61.2587\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0440 - val_loss: 49.1140\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7906 - val_loss: 34.8132\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8231 - val_loss: 40.9821\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7703 - val_loss: 37.1141\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9376 - val_loss: 44.7753\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7107 - val_loss: 39.2994\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9003 - val_loss: 41.5075\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8524 - val_loss: 34.1921\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7775 - val_loss: 35.5053\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8405 - val_loss: 38.7880\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8028 - val_loss: 42.4421\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7870 - val_loss: 62.0027\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8079 - val_loss: 35.6317\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9951 - val_loss: 38.0213\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6882 - val_loss: 36.4708\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8724 - val_loss: 40.0509\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7382 - val_loss: 40.2536\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8466 - val_loss: 35.3811\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6759 - val_loss: 49.6226\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6011 - val_loss: 36.2380\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6726 - val_loss: 38.1406\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6852 - val_loss: 35.9381\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6529 - val_loss: 37.3422\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.7440 - val_loss: 38.2007\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5971 - val_loss: 40.1453\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7151 - val_loss: 38.6701\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6433 - val_loss: 48.9986\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6056 - val_loss: 39.2474\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4568 - val_loss: 34.0494\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6466 - val_loss: 36.1057\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5419 - val_loss: 35.3295\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5207 - val_loss: 70.2892\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4703 - val_loss: 33.7449\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5389 - val_loss: 41.2760\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5031 - val_loss: 43.3386\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4079 - val_loss: 34.6794\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4692 - val_loss: 37.8911\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4670 - val_loss: 43.0444\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4799 - val_loss: 35.1187\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.6755 - val_loss: 39.8472\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4836 - val_loss: 34.4409\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5342 - val_loss: 36.7071\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5299 - val_loss: 39.6542\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5405 - val_loss: 35.3494\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4840 - val_loss: 34.3916\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3548 - val_loss: 36.7524\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5278 - val_loss: 34.0047\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4939 - val_loss: 38.1180\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3853 - val_loss: 38.0396\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4402 - val_loss: 35.6948\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4420 - val_loss: 34.4822\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2882 - val_loss: 35.4844\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4429 - val_loss: 41.1662\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3741 - val_loss: 33.6882\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3767 - val_loss: 36.8918\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3704 - val_loss: 44.0627\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4581 - val_loss: 36.3479\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2154 - val_loss: 37.7003\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3366 - val_loss: 39.7645\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2802 - val_loss: 35.3065\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2958 - val_loss: 39.1684\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 3s 18ms/step - loss: 26.2230 - val_loss: 45.0776\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 26.2675 - val_loss: 39.8272\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3826 - val_loss: 35.6514\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2663 - val_loss: 44.9562\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2193 - val_loss: 36.5744\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2037 - val_loss: 48.4314\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3421 - val_loss: 35.3899\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3860 - val_loss: 36.7403\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3456 - val_loss: 40.0866\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2169 - val_loss: 46.7934\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.4127 - val_loss: 36.2381\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2121 - val_loss: 55.9559\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3187 - val_loss: 38.1849\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1939 - val_loss: 36.7912\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2460 - val_loss: 37.8395\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2684 - val_loss: 38.6735\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1738 - val_loss: 41.1664\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2653 - val_loss: 37.6380\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2172 - val_loss: 37.8885\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1918 - val_loss: 33.8853\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0951 - val_loss: 33.9629\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0868 - val_loss: 35.2190\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1537 - val_loss: 33.9841\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2407 - val_loss: 37.1140\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2207 - val_loss: 35.8491\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.0735 - val_loss: 38.8835\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1568 - val_loss: 38.1429\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0865 - val_loss: 35.7059\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1692 - val_loss: 33.9811\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0450 - val_loss: 35.6207\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0724 - val_loss: 34.6068\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0727 - val_loss: 37.4786\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9929 - val_loss: 36.8842\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0304 - val_loss: 34.0018\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0591 - val_loss: 34.4163\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0232 - val_loss: 35.4881\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1817 - val_loss: 35.6457\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0580 - val_loss: 36.6825\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0853 - val_loss: 37.5760\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9818 - val_loss: 37.1194\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0087 - val_loss: 38.7099\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1944 - val_loss: 39.4833\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9904 - val_loss: 34.1364\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.9849 - val_loss: 35.2331\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0185 - val_loss: 39.4134\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9494 - val_loss: 37.0275\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1557 - val_loss: 43.9300\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1097 - val_loss: 34.2820\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0234 - val_loss: 39.2019\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9313 - val_loss: 39.2588\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9064 - val_loss: 32.4613\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9920 - val_loss: 37.8517\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9698 - val_loss: 38.2228\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9923 - val_loss: 36.4146\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9015 - val_loss: 38.7031\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9973 - val_loss: 33.5560\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8648 - val_loss: 40.2655\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8902 - val_loss: 33.8460\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.0270 - val_loss: 35.3493\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.0681 - val_loss: 47.1055\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0306 - val_loss: 75.8871\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0963 - val_loss: 39.3894\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1278 - val_loss: 35.3571\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9168 - val_loss: 38.7393\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8938 - val_loss: 36.3775\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9351 - val_loss: 38.9845\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8615 - val_loss: 35.4241\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8998 - val_loss: 33.2788\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8955 - val_loss: 33.4354\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8733 - val_loss: 36.4507\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8800 - val_loss: 36.2770\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.8839 - val_loss: 39.8745\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.9332 - val_loss: 35.9633\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9097 - val_loss: 35.8879\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8327 - val_loss: 48.2177\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8159 - val_loss: 34.9038\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8481 - val_loss: 36.7907\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.9633 - val_loss: 38.8152\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9140 - val_loss: 34.7919\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.8617 - val_loss: 41.1947\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.7532 - val_loss: 35.1262\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.8171 - val_loss: 34.3043\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7930 - val_loss: 33.6069\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8185 - val_loss: 43.1534\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7556 - val_loss: 45.6641\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7356 - val_loss: 33.2563\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7033 - val_loss: 33.9451\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8252 - val_loss: 35.6598\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7834 - val_loss: 34.9578\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7055 - val_loss: 36.8305\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6994 - val_loss: 35.0991\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6824 - val_loss: 41.8862\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8559 - val_loss: 34.0790\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7474 - val_loss: 34.5598\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7436 - val_loss: 34.4430\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7560 - val_loss: 38.9137\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6789 - val_loss: 59.4201\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7946 - val_loss: 33.6498\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6704 - val_loss: 34.6486\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7179 - val_loss: 41.6654\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.6650 - val_loss: 42.2116\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 25.7058 - val_loss: 36.4427\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.8229 - val_loss: 37.8274\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7807 - val_loss: 36.3060\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7626 - val_loss: 35.9725\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6960 - val_loss: 38.0521\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6495 - val_loss: 34.6478\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7377 - val_loss: 36.0850\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7983 - val_loss: 41.1590\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.7126 - val_loss: 33.3299\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 3s 19ms/step - loss: 25.6200 - val_loss: 43.4088\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7205 - val_loss: 34.9240\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5612 - val_loss: 34.0104\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5670 - val_loss: 34.8018\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6354 - val_loss: 52.2127\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5424 - val_loss: 41.9538\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6381 - val_loss: 34.5483\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5679 - val_loss: 37.1272\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7639 - val_loss: 34.5896\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6779 - val_loss: 36.9639\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5497 - val_loss: 42.0053\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6672 - val_loss: 41.9328\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7066 - val_loss: 47.9711\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5909 - val_loss: 35.2912\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6278 - val_loss: 34.6588\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5654 - val_loss: 42.0058\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6643 - val_loss: 34.3812\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6691 - val_loss: 41.2061\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5323 - val_loss: 36.1378\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6414 - val_loss: 46.4020\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6496 - val_loss: 37.6958\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.9321 - val_loss: 36.4231\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7126 - val_loss: 35.8126\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5994 - val_loss: 34.4389\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5943 - val_loss: 42.1348\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.6047 - val_loss: 38.4100\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6271 - val_loss: 33.8046\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5671 - val_loss: 36.8132\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5946 - val_loss: 32.6223\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5857 - val_loss: 41.0054\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6615 - val_loss: 32.9922\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6545 - val_loss: 32.8924\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6639 - val_loss: 35.2811\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6296 - val_loss: 37.1226\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5875 - val_loss: 49.5971\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5610 - val_loss: 38.8604\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4913 - val_loss: 48.0459\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6568 - val_loss: 35.0123\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.7970 - val_loss: 34.0555\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6281 - val_loss: 34.9479\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5557 - val_loss: 42.2790\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5558 - val_loss: 44.8431\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5789 - val_loss: 40.9719\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5900 - val_loss: 40.6487\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5704 - val_loss: 35.0584\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3985 - val_loss: 38.5968\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5131 - val_loss: 34.9056\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5284 - val_loss: 37.3744\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5622 - val_loss: 35.4840\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5472 - val_loss: 34.7335\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4273 - val_loss: 33.8562\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4846 - val_loss: 37.7353\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5668 - val_loss: 40.0269\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5016 - val_loss: 35.9044\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4381 - val_loss: 34.9198\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.4081 - val_loss: 37.7735\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4419 - val_loss: 41.4538\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.4359 - val_loss: 55.8369\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4445 - val_loss: 57.2063\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4639 - val_loss: 37.1488\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4331 - val_loss: 39.0147\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4039 - val_loss: 33.5831\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4365 - val_loss: 35.1113\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4358 - val_loss: 32.6446\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3718 - val_loss: 35.7160\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3683 - val_loss: 35.2058\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4217 - val_loss: 36.3685\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5396 - val_loss: 35.9069\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4719 - val_loss: 38.2164\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.5125 - val_loss: 34.2142\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.3517 - val_loss: 35.5335\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.4627 - val_loss: 61.1997\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7817 - val_loss: 34.3531\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5794 - val_loss: 38.6403\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4019 - val_loss: 36.0141\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4103 - val_loss: 38.9015\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4695 - val_loss: 35.2202\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3661 - val_loss: 39.2517\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3311 - val_loss: 33.6370\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.7083 - val_loss: 39.3573\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4856 - val_loss: 39.1879\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3558 - val_loss: 38.0625\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.5076 - val_loss: 41.1302\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.4813 - val_loss: 34.3449\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2793 - val_loss: 34.0334\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3538 - val_loss: 34.8067\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3208 - val_loss: 35.9398\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.4617 - val_loss: 34.6915\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4290 - val_loss: 33.7103\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2746 - val_loss: 45.8032\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.4038 - val_loss: 36.8281\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3222 - val_loss: 38.3345\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4273 - val_loss: 35.2967\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3990 - val_loss: 40.2129\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4315 - val_loss: 34.5087\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3939 - val_loss: 37.1585\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3817 - val_loss: 37.1844\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4618 - val_loss: 50.4488\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.3263 - val_loss: 32.8281\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2867 - val_loss: 36.2161\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3935 - val_loss: 44.8235\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3827 - val_loss: 35.3588\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3322 - val_loss: 37.5139\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3377 - val_loss: 36.9061\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.4103 - val_loss: 35.0872\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3735 - val_loss: 36.9492\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2600 - val_loss: 34.3760\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3194 - val_loss: 35.3803\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2592 - val_loss: 35.5737\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2508 - val_loss: 37.7520\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2193 - val_loss: 38.9845\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.3237 - val_loss: 39.3577\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.1899 - val_loss: 34.5579\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2399 - val_loss: 35.5597\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2832 - val_loss: 33.3865\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3813 - val_loss: 42.9883\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.2151 - val_loss: 33.1148\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2437 - val_loss: 41.0398\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2655 - val_loss: 34.1087\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2359 - val_loss: 32.9118\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.4351 - val_loss: 39.7150\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 25.2749 - val_loss: 35.3865\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 3s 20ms/step - loss: 25.2856 - val_loss: 39.4699\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.1962 - val_loss: 34.9855\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2649 - val_loss: 36.3973\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2967 - val_loss: 33.8819\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.1462 - val_loss: 36.5869\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3534 - val_loss: 36.8635\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2614 - val_loss: 37.8565\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2183 - val_loss: 37.6077\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3255 - val_loss: 39.0931\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.6663 - val_loss: 56.9580\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2296 - val_loss: 41.1680\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2783 - val_loss: 36.1101\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2739 - val_loss: 40.4373\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2866 - val_loss: 34.9161\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 25.0880 - val_loss: 35.3226\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.1914 - val_loss: 67.5329\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2254 - val_loss: 35.3492\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.0989 - val_loss: 35.0713\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2143 - val_loss: 35.9075\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.1802 - val_loss: 34.0046\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2447 - val_loss: 35.3500\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3161 - val_loss: 35.8012\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3783 - val_loss: 35.0720\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3207 - val_loss: 36.6196\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2507 - val_loss: 34.5592\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2865 - val_loss: 36.0473\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.3090 - val_loss: 35.9409\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.0858 - val_loss: 41.2339\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.2933 - val_loss: 36.5398\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nroUKm9cD3wf",
        "outputId": "64f372de-58eb-4e46-8be1-cd00e4a334c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  0.43855370924907 \n",
            "MAE:  4.385152255644523 \n",
            "SD:  6.0288844871804175\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "kS--HwX9D3wf",
        "outputId": "9a76be72-2a71-4df6-c3f3-59ef0ddca1a6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rXqq5owqD3wf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbzn89gD4JS"
      },
      "source": [
        "## 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Dy3mnHhtD4JT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D, Dense, MaxPooling2D,MaxPooling1D,GlobalAveragePooling2D,Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad,Adadelta\n",
        "\n",
        "\n",
        "def model1():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, input_shape=(X_train.shape[1],)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(16))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfHNI3w7D4JT",
        "outputId": "9c23e5ec-44fd-4fa4-824f-6401adc733b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_95 (Dense)            (None, 16)                2048      \n",
            "                                                                 \n",
            " batch_normalization_90 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_90 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_96 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_91 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_91 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_97 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_92 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_92 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_98 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_93 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_93 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_99 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_94 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_94 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_100 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_95 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_95 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_101 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_96 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_96 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_102 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_97 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_97 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_103 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_98 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_98 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_104 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_99 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_99 (Activation)  (None, 16)                0         \n",
            "                                                                 \n",
            " dense_105 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_100 (Ba  (None, 16)               64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_100 (Activation)  (None, 16)               0         \n",
            "                                                                 \n",
            " dense_106 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_101 (Ba  (None, 16)               64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_101 (Activation)  (None, 16)               0         \n",
            "                                                                 \n",
            " dense_107 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_102 (Ba  (None, 16)               64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_102 (Activation)  (None, 16)               0         \n",
            "                                                                 \n",
            " dense_108 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_103 (Ba  (None, 16)               64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_103 (Activation)  (None, 16)               0         \n",
            "                                                                 \n",
            " dense_109 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_104 (Ba  (None, 16)               64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_104 (Activation)  (None, 16)               0         \n",
            "                                                                 \n",
            " dense_110 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_105 (Ba  (None, 16)               64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_105 (Activation)  (None, 16)               0         \n",
            "                                                                 \n",
            " dense_111 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_106 (Ba  (None, 16)               64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_106 (Activation)  (None, 16)               0         \n",
            "                                                                 \n",
            " dense_112 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " batch_normalization_107 (Ba  (None, 16)               64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_107 (Activation)  (None, 16)               0         \n",
            "                                                                 \n",
            " dense_113 (Dense)           (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,841\n",
            "Trainable params: 7,265\n",
            "Non-trainable params: 576\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = model1()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNNzFsx-D4JT",
        "outputId": "01914885-d374-48a5-a0ac-746e31d2b49a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "165/165 [==============================] - 7s 16ms/step - loss: 3679.4453 - val_loss: 3672.7612\n",
            "Epoch 2/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 3425.2666 - val_loss: 3342.4019\n",
            "Epoch 3/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 3103.8381 - val_loss: 2906.6787\n",
            "Epoch 4/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2673.6377 - val_loss: 2384.8909\n",
            "Epoch 5/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 2198.5994 - val_loss: 1820.3704\n",
            "Epoch 6/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 1725.2008 - val_loss: 1476.8008\n",
            "Epoch 7/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 1290.6448 - val_loss: 909.2725\n",
            "Epoch 8/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 905.1046 - val_loss: 724.0925\n",
            "Epoch 9/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 593.7485 - val_loss: 468.0482\n",
            "Epoch 10/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 373.3951 - val_loss: 246.2231\n",
            "Epoch 11/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 232.2057 - val_loss: 184.2931\n",
            "Epoch 12/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 159.4210 - val_loss: 122.4511\n",
            "Epoch 13/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 124.8453 - val_loss: 107.6738\n",
            "Epoch 14/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 110.3162 - val_loss: 102.6490\n",
            "Epoch 15/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 104.6702 - val_loss: 101.5294\n",
            "Epoch 16/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 102.2235 - val_loss: 98.2591\n",
            "Epoch 17/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 100.8798 - val_loss: 97.6512\n",
            "Epoch 18/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 98.3702 - val_loss: 97.4499\n",
            "Epoch 19/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 95.8807 - val_loss: 110.2853\n",
            "Epoch 20/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 92.0509 - val_loss: 101.2779\n",
            "Epoch 21/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 87.7162 - val_loss: 97.5959\n",
            "Epoch 22/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 83.3140 - val_loss: 87.3104\n",
            "Epoch 23/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 78.7555 - val_loss: 159.7278\n",
            "Epoch 24/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 73.3764 - val_loss: 81.9767\n",
            "Epoch 25/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 68.8748 - val_loss: 74.1954\n",
            "Epoch 26/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 65.5385 - val_loss: 72.0580\n",
            "Epoch 27/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 62.5444 - val_loss: 81.5216\n",
            "Epoch 28/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 59.0985 - val_loss: 150.4026\n",
            "Epoch 29/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 57.1630 - val_loss: 60.7023\n",
            "Epoch 30/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 55.2077 - val_loss: 72.7934\n",
            "Epoch 31/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 52.6166 - val_loss: 51.9031\n",
            "Epoch 32/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 50.5099 - val_loss: 84.4046\n",
            "Epoch 33/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 49.2277 - val_loss: 46.8945\n",
            "Epoch 34/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 47.6703 - val_loss: 98.1721\n",
            "Epoch 35/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 46.7119 - val_loss: 54.6675\n",
            "Epoch 36/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 45.5044 - val_loss: 66.0265\n",
            "Epoch 37/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 44.5604 - val_loss: 48.0283\n",
            "Epoch 38/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 43.6546 - val_loss: 66.4055\n",
            "Epoch 39/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 42.9014 - val_loss: 44.3498\n",
            "Epoch 40/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 42.0978 - val_loss: 61.2038\n",
            "Epoch 41/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 41.5970 - val_loss: 51.5351\n",
            "Epoch 42/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 40.7225 - val_loss: 42.7990\n",
            "Epoch 43/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 40.1277 - val_loss: 67.8360\n",
            "Epoch 44/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 39.9874 - val_loss: 48.9826\n",
            "Epoch 45/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 39.4836 - val_loss: 48.6033\n",
            "Epoch 46/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 38.8124 - val_loss: 47.7367\n",
            "Epoch 47/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 38.2328 - val_loss: 45.5542\n",
            "Epoch 48/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 38.3186 - val_loss: 42.8886\n",
            "Epoch 49/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.4603 - val_loss: 53.9191\n",
            "Epoch 50/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.7315 - val_loss: 41.9280\n",
            "Epoch 51/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.1069 - val_loss: 74.4191\n",
            "Epoch 52/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 37.1271 - val_loss: 54.0856\n",
            "Epoch 53/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 36.8624 - val_loss: 97.7329\n",
            "Epoch 54/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 36.5296 - val_loss: 40.9132\n",
            "Epoch 55/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 36.3877 - val_loss: 41.4673\n",
            "Epoch 56/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 35.9529 - val_loss: 55.3398\n",
            "Epoch 57/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 35.5760 - val_loss: 53.8199\n",
            "Epoch 58/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 35.4618 - val_loss: 51.8830\n",
            "Epoch 59/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 35.3612 - val_loss: 46.8499\n",
            "Epoch 60/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 35.6109 - val_loss: 43.5007\n",
            "Epoch 61/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 35.1237 - val_loss: 40.2724\n",
            "Epoch 62/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 34.9225 - val_loss: 70.2392\n",
            "Epoch 63/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.7524 - val_loss: 50.8297\n",
            "Epoch 64/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.9716 - val_loss: 55.6889\n",
            "Epoch 65/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 34.8064 - val_loss: 112.4659\n",
            "Epoch 66/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.2995 - val_loss: 58.9488\n",
            "Epoch 67/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.9898 - val_loss: 119.8749\n",
            "Epoch 68/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.8974 - val_loss: 38.5991\n",
            "Epoch 69/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 33.8010 - val_loss: 42.0263\n",
            "Epoch 70/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 34.0309 - val_loss: 39.6956\n",
            "Epoch 71/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 33.5090 - val_loss: 42.6800\n",
            "Epoch 72/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.5947 - val_loss: 61.0303\n",
            "Epoch 73/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.4264 - val_loss: 38.9935\n",
            "Epoch 74/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.1498 - val_loss: 53.6452\n",
            "Epoch 75/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.0434 - val_loss: 63.9203\n",
            "Epoch 76/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 33.2129 - val_loss: 48.6085\n",
            "Epoch 77/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.9521 - val_loss: 54.6940\n",
            "Epoch 78/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.9807 - val_loss: 112.4553\n",
            "Epoch 79/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.7641 - val_loss: 78.5403\n",
            "Epoch 80/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.6954 - val_loss: 119.9792\n",
            "Epoch 81/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.5698 - val_loss: 308.5169\n",
            "Epoch 82/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.6340 - val_loss: 38.0394\n",
            "Epoch 83/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 32.3896 - val_loss: 37.6174\n",
            "Epoch 84/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.8906 - val_loss: 70.9908\n",
            "Epoch 85/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.8955 - val_loss: 47.7818\n",
            "Epoch 86/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.7080 - val_loss: 39.1866\n",
            "Epoch 87/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.5161 - val_loss: 77.7125\n",
            "Epoch 88/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.8040 - val_loss: 101.3531\n",
            "Epoch 89/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.6700 - val_loss: 38.6285\n",
            "Epoch 90/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.3982 - val_loss: 40.2623\n",
            "Epoch 91/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.2221 - val_loss: 60.4215\n",
            "Epoch 92/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.2782 - val_loss: 38.7075\n",
            "Epoch 93/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 31.3198 - val_loss: 44.5471\n",
            "Epoch 94/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 31.0704 - val_loss: 55.1700\n",
            "Epoch 95/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.9513 - val_loss: 36.7128\n",
            "Epoch 96/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.9563 - val_loss: 55.4851\n",
            "Epoch 97/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.9653 - val_loss: 43.5738\n",
            "Epoch 98/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.6753 - val_loss: 38.2974\n",
            "Epoch 99/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.7786 - val_loss: 39.4118\n",
            "Epoch 100/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.8002 - val_loss: 47.2342\n",
            "Epoch 101/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 31.0475 - val_loss: 36.7879\n",
            "Epoch 102/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.5691 - val_loss: 75.1744\n",
            "Epoch 103/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.4842 - val_loss: 43.1678\n",
            "Epoch 104/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.4176 - val_loss: 83.5675\n",
            "Epoch 105/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.7313 - val_loss: 49.1366\n",
            "Epoch 106/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 30.6755 - val_loss: 35.4860\n",
            "Epoch 107/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.3935 - val_loss: 38.2765\n",
            "Epoch 108/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.2386 - val_loss: 35.9590\n",
            "Epoch 109/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.9288 - val_loss: 46.9071\n",
            "Epoch 110/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.0709 - val_loss: 44.3689\n",
            "Epoch 111/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.8559 - val_loss: 40.4767\n",
            "Epoch 112/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.0406 - val_loss: 90.4202\n",
            "Epoch 113/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 30.0160 - val_loss: 48.3232\n",
            "Epoch 114/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.8223 - val_loss: 110.1471\n",
            "Epoch 115/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.9135 - val_loss: 37.5014\n",
            "Epoch 116/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.6228 - val_loss: 63.2551\n",
            "Epoch 117/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.6790 - val_loss: 36.2990\n",
            "Epoch 118/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.4524 - val_loss: 37.6632\n",
            "Epoch 119/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.4301 - val_loss: 53.0227\n",
            "Epoch 120/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.4841 - val_loss: 35.7182\n",
            "Epoch 121/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.4825 - val_loss: 38.9663\n",
            "Epoch 122/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.4744 - val_loss: 34.9953\n",
            "Epoch 123/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.2578 - val_loss: 36.9275\n",
            "Epoch 124/500\n",
            "165/165 [==============================] - 2s 13ms/step - loss: 29.2941 - val_loss: 47.4687\n",
            "Epoch 125/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.3972 - val_loss: 36.2702\n",
            "Epoch 126/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.4876 - val_loss: 43.6781\n",
            "Epoch 127/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.2962 - val_loss: 51.8420\n",
            "Epoch 128/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.2493 - val_loss: 58.4611\n",
            "Epoch 129/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.2479 - val_loss: 46.0133\n",
            "Epoch 130/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.1073 - val_loss: 37.5913\n",
            "Epoch 131/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.2586 - val_loss: 44.8754\n",
            "Epoch 132/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.2292 - val_loss: 43.7537\n",
            "Epoch 133/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.4213 - val_loss: 71.0434\n",
            "Epoch 134/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.0202 - val_loss: 35.9772\n",
            "Epoch 135/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.9715 - val_loss: 67.7010\n",
            "Epoch 136/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.1785 - val_loss: 42.2761\n",
            "Epoch 137/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.9728 - val_loss: 56.3228\n",
            "Epoch 138/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.4268 - val_loss: 47.9247\n",
            "Epoch 139/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.1612 - val_loss: 38.8897\n",
            "Epoch 140/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.8288 - val_loss: 34.4089\n",
            "Epoch 141/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.8372 - val_loss: 37.7926\n",
            "Epoch 142/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.9377 - val_loss: 40.0330\n",
            "Epoch 143/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.9693 - val_loss: 46.1927\n",
            "Epoch 144/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 29.0222 - val_loss: 38.3702\n",
            "Epoch 145/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.7553 - val_loss: 36.7539\n",
            "Epoch 146/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.7266 - val_loss: 73.6290\n",
            "Epoch 147/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.6395 - val_loss: 35.3871\n",
            "Epoch 148/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.9770 - val_loss: 47.6321\n",
            "Epoch 149/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.9560 - val_loss: 38.8234\n",
            "Epoch 150/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.7344 - val_loss: 54.0970\n",
            "Epoch 151/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.8716 - val_loss: 48.6787\n",
            "Epoch 152/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.6790 - val_loss: 44.9366\n",
            "Epoch 153/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.6512 - val_loss: 37.0225\n",
            "Epoch 154/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.6663 - val_loss: 86.7619\n",
            "Epoch 155/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.5088 - val_loss: 44.4346\n",
            "Epoch 156/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.5336 - val_loss: 47.5039\n",
            "Epoch 157/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.4344 - val_loss: 35.2814\n",
            "Epoch 158/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.5359 - val_loss: 56.3815\n",
            "Epoch 159/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.7137 - val_loss: 47.9833\n",
            "Epoch 160/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.5423 - val_loss: 35.1193\n",
            "Epoch 161/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.6002 - val_loss: 42.3760\n",
            "Epoch 162/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.4241 - val_loss: 38.0537\n",
            "Epoch 163/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.4326 - val_loss: 40.1983\n",
            "Epoch 164/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.6329 - val_loss: 35.7900\n",
            "Epoch 165/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.4497 - val_loss: 40.0831\n",
            "Epoch 166/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.4646 - val_loss: 45.6052\n",
            "Epoch 167/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.4328 - val_loss: 43.0632\n",
            "Epoch 168/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.5680 - val_loss: 42.1929\n",
            "Epoch 169/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.4746 - val_loss: 55.8961\n",
            "Epoch 170/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.5797 - val_loss: 38.8625\n",
            "Epoch 171/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3943 - val_loss: 41.8324\n",
            "Epoch 172/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.4154 - val_loss: 39.3825\n",
            "Epoch 173/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3350 - val_loss: 71.3165\n",
            "Epoch 174/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.4650 - val_loss: 39.7493\n",
            "Epoch 175/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3765 - val_loss: 43.6394\n",
            "Epoch 176/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3699 - val_loss: 35.0781\n",
            "Epoch 177/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2494 - val_loss: 37.2550\n",
            "Epoch 178/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3953 - val_loss: 42.8011\n",
            "Epoch 179/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2488 - val_loss: 38.0014\n",
            "Epoch 180/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3441 - val_loss: 45.2724\n",
            "Epoch 181/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2511 - val_loss: 42.0187\n",
            "Epoch 182/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1872 - val_loss: 40.9485\n",
            "Epoch 183/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3499 - val_loss: 34.5815\n",
            "Epoch 184/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.2699 - val_loss: 42.7864\n",
            "Epoch 185/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.0535 - val_loss: 39.6329\n",
            "Epoch 186/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.3391 - val_loss: 36.3122\n",
            "Epoch 187/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1673 - val_loss: 38.9417\n",
            "Epoch 188/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1496 - val_loss: 39.6726\n",
            "Epoch 189/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1614 - val_loss: 37.0069\n",
            "Epoch 190/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1570 - val_loss: 40.2522\n",
            "Epoch 191/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.1322 - val_loss: 51.5558\n",
            "Epoch 192/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.0593 - val_loss: 35.8294\n",
            "Epoch 193/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.0050 - val_loss: 42.0275\n",
            "Epoch 194/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9087 - val_loss: 40.2760\n",
            "Epoch 195/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9234 - val_loss: 38.2398\n",
            "Epoch 196/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8613 - val_loss: 35.5005\n",
            "Epoch 197/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8392 - val_loss: 43.9975\n",
            "Epoch 198/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8461 - val_loss: 46.7800\n",
            "Epoch 199/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9725 - val_loss: 38.4683\n",
            "Epoch 200/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9331 - val_loss: 40.6112\n",
            "Epoch 201/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8506 - val_loss: 36.4364\n",
            "Epoch 202/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6771 - val_loss: 40.5345\n",
            "Epoch 203/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8687 - val_loss: 39.8775\n",
            "Epoch 204/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.7395 - val_loss: 36.4762\n",
            "Epoch 205/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.7476 - val_loss: 38.8721\n",
            "Epoch 206/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9027 - val_loss: 58.4888\n",
            "Epoch 207/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8441 - val_loss: 34.2849\n",
            "Epoch 208/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8440 - val_loss: 35.1658\n",
            "Epoch 209/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.9951 - val_loss: 44.7308\n",
            "Epoch 210/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8216 - val_loss: 45.0877\n",
            "Epoch 211/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8371 - val_loss: 45.0696\n",
            "Epoch 212/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 27.7768 - val_loss: 52.0349\n",
            "Epoch 213/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 28.0575 - val_loss: 34.7527\n",
            "Epoch 214/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6503 - val_loss: 62.3549\n",
            "Epoch 215/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 27.8197 - val_loss: 35.4389\n",
            "Epoch 216/500\n",
            "165/165 [==============================] - 3s 18ms/step - loss: 27.6382 - val_loss: 36.7432\n",
            "Epoch 217/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8265 - val_loss: 39.5917\n",
            "Epoch 218/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8937 - val_loss: 36.4623\n",
            "Epoch 219/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.6190 - val_loss: 35.8934\n",
            "Epoch 220/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8278 - val_loss: 37.8163\n",
            "Epoch 221/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6158 - val_loss: 38.3647\n",
            "Epoch 222/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6530 - val_loss: 36.2414\n",
            "Epoch 223/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8294 - val_loss: 37.9530\n",
            "Epoch 224/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5917 - val_loss: 35.1400\n",
            "Epoch 225/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5242 - val_loss: 37.4806\n",
            "Epoch 226/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6616 - val_loss: 43.1226\n",
            "Epoch 227/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5673 - val_loss: 36.5596\n",
            "Epoch 228/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5127 - val_loss: 51.9415\n",
            "Epoch 229/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6592 - val_loss: 50.1450\n",
            "Epoch 230/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5832 - val_loss: 37.1605\n",
            "Epoch 231/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6991 - val_loss: 33.6651\n",
            "Epoch 232/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6026 - val_loss: 35.9959\n",
            "Epoch 233/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5779 - val_loss: 36.0288\n",
            "Epoch 234/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4845 - val_loss: 66.9414\n",
            "Epoch 235/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5299 - val_loss: 46.7293\n",
            "Epoch 236/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5968 - val_loss: 34.8638\n",
            "Epoch 237/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6700 - val_loss: 54.3127\n",
            "Epoch 238/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8020 - val_loss: 38.0071\n",
            "Epoch 239/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5830 - val_loss: 35.5698\n",
            "Epoch 240/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6214 - val_loss: 37.9621\n",
            "Epoch 241/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3754 - val_loss: 34.3405\n",
            "Epoch 242/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.5651 - val_loss: 42.0212\n",
            "Epoch 243/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6551 - val_loss: 42.9156\n",
            "Epoch 244/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8047 - val_loss: 53.2122\n",
            "Epoch 245/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.8050 - val_loss: 59.9791\n",
            "Epoch 246/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.6413 - val_loss: 35.0531\n",
            "Epoch 247/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4890 - val_loss: 43.4825\n",
            "Epoch 248/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4070 - val_loss: 35.5384\n",
            "Epoch 249/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3138 - val_loss: 33.6158\n",
            "Epoch 250/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2736 - val_loss: 44.4547\n",
            "Epoch 251/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2552 - val_loss: 36.3162\n",
            "Epoch 252/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3250 - val_loss: 41.1141\n",
            "Epoch 253/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3598 - val_loss: 45.1218\n",
            "Epoch 254/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2266 - val_loss: 47.0908\n",
            "Epoch 255/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3580 - val_loss: 35.2965\n",
            "Epoch 256/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1972 - val_loss: 35.1497\n",
            "Epoch 257/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3589 - val_loss: 42.2933\n",
            "Epoch 258/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2874 - val_loss: 38.6941\n",
            "Epoch 259/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.4103 - val_loss: 40.7311\n",
            "Epoch 260/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.3024 - val_loss: 42.4430\n",
            "Epoch 261/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2010 - val_loss: 43.0633\n",
            "Epoch 262/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2570 - val_loss: 35.0899\n",
            "Epoch 263/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2338 - val_loss: 37.6253\n",
            "Epoch 264/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.2383 - val_loss: 34.5143\n",
            "Epoch 265/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1726 - val_loss: 34.4943\n",
            "Epoch 266/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1340 - val_loss: 43.5878\n",
            "Epoch 267/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1628 - val_loss: 46.2178\n",
            "Epoch 268/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1606 - val_loss: 40.2869\n",
            "Epoch 269/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1015 - val_loss: 39.4238\n",
            "Epoch 270/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0308 - val_loss: 37.6978\n",
            "Epoch 271/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1391 - val_loss: 35.8716\n",
            "Epoch 272/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1529 - val_loss: 33.0605\n",
            "Epoch 273/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1067 - val_loss: 40.5349\n",
            "Epoch 274/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0846 - val_loss: 45.1905\n",
            "Epoch 275/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0097 - val_loss: 34.2247\n",
            "Epoch 276/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0990 - val_loss: 34.6644\n",
            "Epoch 277/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0737 - val_loss: 34.6315\n",
            "Epoch 278/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0503 - val_loss: 34.2507\n",
            "Epoch 279/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1299 - val_loss: 40.1939\n",
            "Epoch 280/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.1418 - val_loss: 38.8038\n",
            "Epoch 281/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0718 - val_loss: 33.9840\n",
            "Epoch 282/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9591 - val_loss: 32.9549\n",
            "Epoch 283/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.9634 - val_loss: 46.0366\n",
            "Epoch 284/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0628 - val_loss: 41.6807\n",
            "Epoch 285/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0764 - val_loss: 40.3060\n",
            "Epoch 286/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0793 - val_loss: 39.8425\n",
            "Epoch 287/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0170 - val_loss: 36.6195\n",
            "Epoch 288/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0695 - val_loss: 43.3271\n",
            "Epoch 289/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0628 - val_loss: 35.1862\n",
            "Epoch 290/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8854 - val_loss: 42.7714\n",
            "Epoch 291/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9623 - val_loss: 35.0608\n",
            "Epoch 292/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8875 - val_loss: 34.5195\n",
            "Epoch 293/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9709 - val_loss: 53.2187\n",
            "Epoch 294/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8459 - val_loss: 35.3511\n",
            "Epoch 295/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8370 - val_loss: 40.8885\n",
            "Epoch 296/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8551 - val_loss: 35.0108\n",
            "Epoch 297/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7969 - val_loss: 42.1599\n",
            "Epoch 298/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8832 - val_loss: 41.7206\n",
            "Epoch 299/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9710 - val_loss: 35.1974\n",
            "Epoch 300/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9358 - val_loss: 33.3701\n",
            "Epoch 301/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8538 - val_loss: 35.7960\n",
            "Epoch 302/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.8361 - val_loss: 43.5182\n",
            "Epoch 303/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8187 - val_loss: 36.7502\n",
            "Epoch 304/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8579 - val_loss: 33.9677\n",
            "Epoch 305/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9204 - val_loss: 35.9423\n",
            "Epoch 306/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8978 - val_loss: 42.1204\n",
            "Epoch 307/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8113 - val_loss: 42.2750\n",
            "Epoch 308/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.8751 - val_loss: 38.1542\n",
            "Epoch 309/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9428 - val_loss: 34.5890\n",
            "Epoch 310/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8465 - val_loss: 34.5293\n",
            "Epoch 311/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9448 - val_loss: 37.1140\n",
            "Epoch 312/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8559 - val_loss: 40.1309\n",
            "Epoch 313/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7545 - val_loss: 36.5254\n",
            "Epoch 314/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7889 - val_loss: 40.2564\n",
            "Epoch 315/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7741 - val_loss: 35.0999\n",
            "Epoch 316/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8438 - val_loss: 36.7977\n",
            "Epoch 317/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6947 - val_loss: 35.4557\n",
            "Epoch 318/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.8498 - val_loss: 34.9207\n",
            "Epoch 319/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7908 - val_loss: 38.2373\n",
            "Epoch 320/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8314 - val_loss: 46.7575\n",
            "Epoch 321/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8919 - val_loss: 60.9529\n",
            "Epoch 322/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7978 - val_loss: 34.2468\n",
            "Epoch 323/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6893 - val_loss: 39.5494\n",
            "Epoch 324/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7748 - val_loss: 38.1152\n",
            "Epoch 325/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9109 - val_loss: 34.3093\n",
            "Epoch 326/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.9449 - val_loss: 34.5746\n",
            "Epoch 327/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7082 - val_loss: 35.1558\n",
            "Epoch 328/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6858 - val_loss: 33.2947\n",
            "Epoch 329/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7305 - val_loss: 35.1026\n",
            "Epoch 330/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7843 - val_loss: 36.2219\n",
            "Epoch 331/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6906 - val_loss: 40.1801\n",
            "Epoch 332/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9442 - val_loss: 53.4636\n",
            "Epoch 333/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 26.8760 - val_loss: 59.7594\n",
            "Epoch 334/500\n",
            "165/165 [==============================] - 3s 18ms/step - loss: 26.8399 - val_loss: 39.1870\n",
            "Epoch 335/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8888 - val_loss: 34.6573\n",
            "Epoch 336/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8283 - val_loss: 39.3344\n",
            "Epoch 337/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8509 - val_loss: 42.6248\n",
            "Epoch 338/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 27.0019 - val_loss: 68.0329\n",
            "Epoch 339/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8937 - val_loss: 34.9838\n",
            "Epoch 340/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.8146 - val_loss: 39.1137\n",
            "Epoch 341/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9524 - val_loss: 35.2880\n",
            "Epoch 342/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7854 - val_loss: 41.1918\n",
            "Epoch 343/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.8636 - val_loss: 38.8430\n",
            "Epoch 344/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.7125 - val_loss: 38.9915\n",
            "Epoch 345/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7141 - val_loss: 33.2286\n",
            "Epoch 346/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.6345 - val_loss: 35.8836\n",
            "Epoch 347/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7149 - val_loss: 46.4131\n",
            "Epoch 348/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.9899 - val_loss: 37.6985\n",
            "Epoch 349/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7845 - val_loss: 54.1762\n",
            "Epoch 350/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6943 - val_loss: 35.0449\n",
            "Epoch 351/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6791 - val_loss: 33.7667\n",
            "Epoch 352/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6227 - val_loss: 44.8956\n",
            "Epoch 353/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6242 - val_loss: 36.0985\n",
            "Epoch 354/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7515 - val_loss: 47.6544\n",
            "Epoch 355/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5379 - val_loss: 41.0283\n",
            "Epoch 356/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.6134 - val_loss: 38.0251\n",
            "Epoch 357/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.6270 - val_loss: 51.7244\n",
            "Epoch 358/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5633 - val_loss: 36.1338\n",
            "Epoch 359/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.6354 - val_loss: 39.5704\n",
            "Epoch 360/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7001 - val_loss: 33.9761\n",
            "Epoch 361/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.6614 - val_loss: 36.5552\n",
            "Epoch 362/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5834 - val_loss: 35.7546\n",
            "Epoch 363/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5176 - val_loss: 36.1584\n",
            "Epoch 364/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5187 - val_loss: 41.8050\n",
            "Epoch 365/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5832 - val_loss: 34.6920\n",
            "Epoch 366/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4535 - val_loss: 40.2117\n",
            "Epoch 367/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5730 - val_loss: 35.8477\n",
            "Epoch 368/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.5827 - val_loss: 40.9540\n",
            "Epoch 369/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3713 - val_loss: 53.3898\n",
            "Epoch 370/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 27.7435 - val_loss: 38.5964\n",
            "Epoch 371/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.7024 - val_loss: 34.0925\n",
            "Epoch 372/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.7278 - val_loss: 41.9612\n",
            "Epoch 373/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.6497 - val_loss: 42.7860\n",
            "Epoch 374/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5342 - val_loss: 38.7356\n",
            "Epoch 375/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4708 - val_loss: 36.1373\n",
            "Epoch 376/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5545 - val_loss: 36.8969\n",
            "Epoch 377/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5504 - val_loss: 33.0259\n",
            "Epoch 378/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5649 - val_loss: 52.7968\n",
            "Epoch 379/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5878 - val_loss: 38.3396\n",
            "Epoch 380/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3748 - val_loss: 34.5616\n",
            "Epoch 381/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3378 - val_loss: 35.9746\n",
            "Epoch 382/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3149 - val_loss: 36.7964\n",
            "Epoch 383/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5352 - val_loss: 35.1721\n",
            "Epoch 384/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2996 - val_loss: 33.2434\n",
            "Epoch 385/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3018 - val_loss: 46.3733\n",
            "Epoch 386/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3245 - val_loss: 36.1176\n",
            "Epoch 387/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2373 - val_loss: 33.2165\n",
            "Epoch 388/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3169 - val_loss: 33.3928\n",
            "Epoch 389/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1696 - val_loss: 33.3105\n",
            "Epoch 390/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2815 - val_loss: 38.6913\n",
            "Epoch 391/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2994 - val_loss: 33.4635\n",
            "Epoch 392/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2843 - val_loss: 39.7927\n",
            "Epoch 393/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3757 - val_loss: 34.9113\n",
            "Epoch 394/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2926 - val_loss: 33.8987\n",
            "Epoch 395/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4843 - val_loss: 35.2819\n",
            "Epoch 396/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3588 - val_loss: 35.8031\n",
            "Epoch 397/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3590 - val_loss: 39.3161\n",
            "Epoch 398/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3817 - val_loss: 35.6561\n",
            "Epoch 399/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1937 - val_loss: 36.5740\n",
            "Epoch 400/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1853 - val_loss: 33.0551\n",
            "Epoch 401/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2734 - val_loss: 33.6469\n",
            "Epoch 402/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2639 - val_loss: 39.4685\n",
            "Epoch 403/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2933 - val_loss: 33.7575\n",
            "Epoch 404/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2304 - val_loss: 37.7733\n",
            "Epoch 405/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3462 - val_loss: 38.3086\n",
            "Epoch 406/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3256 - val_loss: 40.7002\n",
            "Epoch 407/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2760 - val_loss: 35.1758\n",
            "Epoch 408/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2740 - val_loss: 35.4375\n",
            "Epoch 409/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2903 - val_loss: 35.4498\n",
            "Epoch 410/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2692 - val_loss: 37.4347\n",
            "Epoch 411/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3403 - val_loss: 34.6064\n",
            "Epoch 412/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1653 - val_loss: 36.2070\n",
            "Epoch 413/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2705 - val_loss: 35.7363\n",
            "Epoch 414/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3549 - val_loss: 36.6843\n",
            "Epoch 415/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2390 - val_loss: 38.8783\n",
            "Epoch 416/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4783 - val_loss: 37.4748\n",
            "Epoch 417/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3625 - val_loss: 42.6235\n",
            "Epoch 418/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1596 - val_loss: 33.5410\n",
            "Epoch 419/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.0982 - val_loss: 39.0123\n",
            "Epoch 420/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2328 - val_loss: 39.4217\n",
            "Epoch 421/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2292 - val_loss: 37.8848\n",
            "Epoch 422/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2542 - val_loss: 37.4721\n",
            "Epoch 423/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3235 - val_loss: 41.1712\n",
            "Epoch 424/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4318 - val_loss: 36.3920\n",
            "Epoch 425/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.7159 - val_loss: 35.0712\n",
            "Epoch 426/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 26.5894 - val_loss: 39.9961\n",
            "Epoch 427/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4761 - val_loss: 37.5996\n",
            "Epoch 428/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5310 - val_loss: 37.5370\n",
            "Epoch 429/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4374 - val_loss: 34.9441\n",
            "Epoch 430/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2991 - val_loss: 36.1990\n",
            "Epoch 431/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2286 - val_loss: 38.2884\n",
            "Epoch 432/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2928 - val_loss: 33.1385\n",
            "Epoch 433/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.3736 - val_loss: 36.1847\n",
            "Epoch 434/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.3244 - val_loss: 35.5687\n",
            "Epoch 435/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3720 - val_loss: 36.7859\n",
            "Epoch 436/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2268 - val_loss: 39.0430\n",
            "Epoch 437/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4377 - val_loss: 34.2906\n",
            "Epoch 438/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2516 - val_loss: 39.9755\n",
            "Epoch 439/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 26.2879 - val_loss: 43.6290\n",
            "Epoch 440/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3294 - val_loss: 38.1535\n",
            "Epoch 441/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.4201 - val_loss: 34.5102\n",
            "Epoch 442/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4300 - val_loss: 35.0673\n",
            "Epoch 443/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3882 - val_loss: 42.9055\n",
            "Epoch 444/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4060 - val_loss: 53.3624\n",
            "Epoch 445/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.3784 - val_loss: 36.4951\n",
            "Epoch 446/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2023 - val_loss: 35.1247\n",
            "Epoch 447/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2070 - val_loss: 34.2723\n",
            "Epoch 448/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3751 - val_loss: 36.9168\n",
            "Epoch 449/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2855 - val_loss: 35.3891\n",
            "Epoch 450/500\n",
            "165/165 [==============================] - 3s 17ms/step - loss: 26.2141 - val_loss: 35.4274\n",
            "Epoch 451/500\n",
            "165/165 [==============================] - 3s 19ms/step - loss: 26.3248 - val_loss: 34.1706\n",
            "Epoch 452/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2400 - val_loss: 33.6009\n",
            "Epoch 453/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2445 - val_loss: 35.7748\n",
            "Epoch 454/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.2186 - val_loss: 34.7960\n",
            "Epoch 455/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1086 - val_loss: 57.1560\n",
            "Epoch 456/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.1923 - val_loss: 40.1458\n",
            "Epoch 457/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.2120 - val_loss: 34.3820\n",
            "Epoch 458/500\n",
            "165/165 [==============================] - 3s 16ms/step - loss: 26.0826 - val_loss: 35.6904\n",
            "Epoch 459/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1021 - val_loss: 33.5075\n",
            "Epoch 460/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1106 - val_loss: 69.1440\n",
            "Epoch 461/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1765 - val_loss: 34.8672\n",
            "Epoch 462/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3426 - val_loss: 36.9262\n",
            "Epoch 463/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2132 - val_loss: 38.6796\n",
            "Epoch 464/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5201 - val_loss: 34.5242\n",
            "Epoch 465/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2024 - val_loss: 33.4106\n",
            "Epoch 466/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.0890 - val_loss: 37.2856\n",
            "Epoch 467/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.0724 - val_loss: 36.6681\n",
            "Epoch 468/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.2461 - val_loss: 34.7863\n",
            "Epoch 469/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.1237 - val_loss: 38.3732\n",
            "Epoch 470/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.1061 - val_loss: 37.7334\n",
            "Epoch 471/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.0178 - val_loss: 35.5759\n",
            "Epoch 472/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2604 - val_loss: 45.0839\n",
            "Epoch 473/500\n",
            "165/165 [==============================] - 3s 15ms/step - loss: 26.2714 - val_loss: 39.4309\n",
            "Epoch 474/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.0337 - val_loss: 38.6381\n",
            "Epoch 475/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9977 - val_loss: 40.5508\n",
            "Epoch 476/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1188 - val_loss: 42.6200\n",
            "Epoch 477/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.0677 - val_loss: 37.7094\n",
            "Epoch 478/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1384 - val_loss: 35.2769\n",
            "Epoch 479/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9937 - val_loss: 44.3288\n",
            "Epoch 480/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.0727 - val_loss: 34.0054\n",
            "Epoch 481/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1910 - val_loss: 67.4916\n",
            "Epoch 482/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2405 - val_loss: 47.8197\n",
            "Epoch 483/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 25.9952 - val_loss: 33.6765\n",
            "Epoch 484/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.0059 - val_loss: 35.7448\n",
            "Epoch 485/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2173 - val_loss: 34.1089\n",
            "Epoch 486/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1428 - val_loss: 36.8352\n",
            "Epoch 487/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1246 - val_loss: 51.6127\n",
            "Epoch 488/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.2871 - val_loss: 36.1881\n",
            "Epoch 489/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.9479 - val_loss: 35.6528\n",
            "Epoch 490/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3198 - val_loss: 38.0591\n",
            "Epoch 491/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.4599 - val_loss: 43.0835\n",
            "Epoch 492/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3163 - val_loss: 34.9971\n",
            "Epoch 493/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1831 - val_loss: 35.3411\n",
            "Epoch 494/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2615 - val_loss: 38.1160\n",
            "Epoch 495/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.6773 - val_loss: 59.0825\n",
            "Epoch 496/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.5041 - val_loss: 36.5466\n",
            "Epoch 497/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.2981 - val_loss: 35.3824\n",
            "Epoch 498/500\n",
            "165/165 [==============================] - 2s 14ms/step - loss: 26.1451 - val_loss: 36.3471\n",
            "Epoch 499/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.3200 - val_loss: 35.4357\n",
            "Epoch 500/500\n",
            "165/165 [==============================] - 2s 15ms/step - loss: 26.1447 - val_loss: 35.7597\n"
          ]
        }
      ],
      "source": [
        "# fit model\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import SGD,Adagrad,Adadelta,Adam\n",
        "\n",
        "model.compile(loss = 'mse', optimizer = Adam(lr=lrate))\n",
        "hist = model.fit(X_train, dbp_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, dbp_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M4xGsS4D4JT",
        "outputId": "9651c793-94dd-4baf-ae76-faee7dcbc683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ME:  -1.4655046213017098 \n",
            "MAE:  4.441584292026395 \n",
            "SD:  5.797587197970584\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(X_test)\n",
        "err = dbp_test - pred\n",
        "me = np.mean(err)\n",
        "mae = np.mean(abs(err))\n",
        "std = np.std(err)\n",
        "\n",
        "total_me = total_me + me\n",
        "total_std = total_std + std\n",
        "\n",
        "print(\"\\nME: \", me, \"\\nMAE: \", mae,\"\\nSD: \", std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "CCaTKbd7D4JU",
        "outputId": "faa070c8-ec0e-49aa-f578-4511a96ff81d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAFBCAYAAAAsfIegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deXwV1fn/P89NQhL2RTbDrgiiwYCAKCoKrtgqWC0iKiKK34oLYlvRWqy12qqtWn/igoriLi4UW7GiSEFUZDOIGpYAAcIWEiAhkITk5vn9ceYwc2/m3tw1dzL3eb9e85qZM2fOnDPLZ555zjLEzBAEQRBihyfRGRAEQXAbIqyCIAgxRoRVEAQhxoiwCoIgxBgRVkEQhBgjwioIghBj4iasRJRBRCuIaC0R/UREDxnhPYnoOyLKJ6L3iKiJEZ5urOcb23vEK2+CIAjxJJ4WaxWAEcx8GoAcAJcQ0VAAjwF4iplPBHAAwCQj/iQAB4zwp4x4giAIjY64CSsryo3VNGNiACMAfGCEzwEw2li+wliHsX0kEVG88icIghAv4upjJaIUIsoFUATgcwCbARxk5hojSiGALGM5C8AOADC2lwJoF8/8CYIgxIPUeCbOzF4AOUTUGsA8AH2jTZOIJgOYDADNmjU7vW9fM8nyDbuwofx49O4NtGwZ7ZHiwOrVan766YnNhyAIQVm9enUxM7ePdP+4CquGmQ8S0WIAZwJoTUSphlXaBcBOI9pOAF0BFBJRKoBWAEps0poFYBYADBo0iFetWnVs27IRf8Q5ix/GzJnAhRfGtUiRoT0bljwLguA8iGhbNPvHs1VAe8NSBRFlArgQQB6AxQCuMqJNADDfWP7YWIex/UsOc4QY7ZKVcWUEQUgk8bRYOwOYQ0QpUAI+l5n/Q0Q/A3iXiP4C4HsArxjxXwHwBhHlA9gP4JpwD6gNQhFWQRASSdyElZl/ADDAJnwLgCE24ZUAro7mmAQ20oomFUEQhOhoEB9rQyEWq+B0qqurUVhYiMrKykRnRQCQkZGBLl26IC0tLabpirAKQgNSWFiIFi1aoEePHpBm2omFmVFSUoLCwkL07Nkzpmm7aqwAEVbB6VRWVqJdu3Yiqg6AiNCuXbu4fD24Slg9JD5WwfmIqDqHeF0LVwmrPke1tYnNhyAIyY0rhVUsVkFofDRv3jzgtoKCApx66qkNmJvoEGEVBEGIMSKsgpBkFBQUoG/fvrjxxhtx0kknYfz48fjiiy8wbNgw9O7dGytWrMCSJUuQk5ODnJwcDBgwAIcOHQIAPPHEExg8eDD69++PBx98MOAxpk+fjpkzZx5b/9Of/oS///3vKC8vx8iRIzFw4EBkZ2dj/vz5AdMIRGVlJSZOnIjs7GwMGDAAixcvBgD89NNPGDJkCHJyctC/f39s2rQJhw8fxmWXXYbTTjsNp556Kt57772wjxcJ0txKEBLF1KlAbm5s08zJAZ5+ut5o+fn5eP/99zF79mwMHjwYb7/9NpYtW4aPP/4Yjz76KLxeL2bOnIlhw4ahvLwcGRkZWLhwITZt2oQVK1aAmXH55Zdj6dKlOPfcc+ukP3bsWEydOhVTpkwBAMydOxefffYZMjIyMG/ePLRs2RLFxcUYOnQoLr/88rAqkWbOnAkiwrp167B+/XpcdNFF2LhxI1544QXcddddGD9+PI4ePQqv14sFCxbg+OOPxyeffAIAKC0tDfk40SAWqyAkIT179kR2djY8Hg9OOeUUjBw5EkSE7OxsFBQUYNiwYZg2bRqeeeYZHDx4EKmpqVi4cCEWLlyIAQMGYODAgVi/fj02bdpkm/6AAQNQVFSEXbt2Ye3atWjTpg26du0KZsb999+P/v3744ILLsDOnTuxd+/esPK+bNkyXHfddQCAvn37onv37ti4cSPOPPNMPProo3jsscewbds2ZGZmIjs7G59//jnuvfdefPXVV2jVqlXU5y4UxGIVhEQRgmUZL9LT048tezyeY+sejwc1NTWYPn06LrvsMixYsADDhg3DZ599BmbGfffdh1tvvTWkY1x99dX44IMPsGfPHowdOxYA8NZbb2Hfvn1YvXo10tLS0KNHj5i1I7322mtxxhln4JNPPsGoUaPw4osvYsSIEVizZg0WLFiABx54ACNHjsSMGTNicrxguEpYpR2rIMSGzZs3Izs7G9nZ2Vi5ciXWr1+Piy++GH/84x8xfvx4NG/eHDt37kRaWho6dOhgm8bYsWNxyy23oLi4GEuWLAGgPsU7dOiAtLQ0LF68GNu2hT863znnnIO33noLI0aMwMaNG7F9+3b06dMHW7ZsQa9evXDnnXdi+/bt+OGHH9C3b1+0bdsW1113HVq3bo2XX345qvMSKq4SVvIok1XasQpCdDz99NNYvHjxMVfBpZdeivT0dOTl5eHMM88EoJpHvfnmmwGF9ZRTTsGhQ4eQlZWFzp07AwDGjx+PX/7yl8jOzsagQYNgHag+VG677Tb85je/QXZ2NlJTU/Haa68hPT0dc+fOxRtvvIG0tDR06tQJ999/P1auXInf/e538Hg8SEtLw/PPPx/5SQkDCnPIU0fhP9B13pV/QL95j+Cdd4Brwh50sAEQX0XSk5eXh5NPPjnR2RAs2F0TIlrNzIMiTVMqrwRBEGKMu1wBThZWR2ZKEKKjpKQEI0eOrBO+aNEitGsX/r9A161bh+uvv94nLD09Hd99913EeUwE7hJWj4N/zeLITAlCdLRr1w65MWyLm52dHdP0EoW7XAFO/oOAIzMlCEI8cJewiitAEAQH4Cph9RilcaSGOTJTgiDEA1cJq3YFSDtWQRASibuEVSqvBMExBBtf1e24S1jFxyoIggNwV3MrEVahEZGoUQMLCgpwySWXYOjQofjmm28wePBgTJw4EQ8++CCKiorw1ltvoaKiAnfddRcA9V+opUuXokWLFnjiiScwd+5cVFVVYcyYMXjooYfqzRMz4/e//z0+/fRTEBEeeOABjB07Frt378bYsWNRVlaGmpoaPP/88zjrrLMwadIkrFq1CkSEm266CXfffXcsTk2DIsLaUDgyU0KyEu/xWK189NFHyM3Nxdq1a1FcXIzBgwfj3HPPxdtvv42LL74Yf/jDH+D1enHkyBHk5uZi586d+PHHHwEABw8ebIjTEXNEWBsKR2ZKSCQJHDXw2HisAGzHY73mmmswbdo0jB8/HldeeSW6dOniMx4rAJSXl2PTpk31CuuyZcswbtw4pKSkoGPHjhg+fDhWrlyJwYMH46abbkJ1dTVGjx6NnJwc9OrVC1u2bMEdd9yByy67DBdddFHcz0U8EB+rICQhoYzH+vLLL6OiogLDhg3D+vXrj43Hmpubi9zcXOTn52PSpEkR5+Hcc8/F0qVLkZWVhRtvvBGvv/462rRpg7Vr1+K8887DCy+8gJtvvjnqsiYCVwmro8djdWSmBMEePR7rvffei8GDBx8bj3X27NkoLy8HAOzcuRNFRUX1pnXOOefgvffeg9frxb59+7B06VIMGTIE27ZtQ8eOHXHLLbfg5ptvxpo1a1BcXIza2lr86le/wl/+8hesWbMm3kWNC650BTiyHasIq9CIiMV4rJoxY8bg22+/xWmnnQYiwuOPP45OnTphzpw5eOKJJ5CWlobmzZvj9ddfx86dOzFx4kTUGg/xX//617iXNR64ajzWvTf/AZ1eeQQzZwK33ZbAjNlx6BDQsqVabsTnXIgOGY/Vech4rPXgaB+rIzMlCEI8cJcrQEa3EoQGJdbjsboFdwmrk7u0CoILifV4rG5BXAENhSMzJSSCxlyv4TbidS3cJaxOtlgdmSmhocnIyEBJSYmIqwNgZpSUlCAjIyPmabvKFeCBaqLhyHvWkZkSGpouXbqgsLAQ+/btS3RWBKgXXZcuXWKebtyElYi6AngdQEcADGAWM/+TiP4E4BYA+s66n5kXGPvcB2ASAC+AO5n5s7COaVis0o5VcCppaWno2bNnorMhxJl4Wqw1AO5h5jVE1ALAaiL63Nj2FDP/3RqZiPoBuAbAKQCOB/AFEZ3EzN5QDyg+VkEQnEDcfKzMvJuZ1xjLhwDkAcgKsssVAN5l5ipm3gogH8CQcI7paGEVBCFpaJDKKyLqAWAAAP1z8NuJ6Acimk1EbYywLAA7LLsVIrgQ2xxHzR0prI7MlCAI8SDuwkpEzQF8CGAqM5cBeB7ACQByAOwG8I8w05tMRKuIaJV/BYAIqyAITiCuwkpEaVCi+hYzfwQAzLyXmb3MXAvgJZif+zsBdLXs3sUI84GZZzHzIGYe1L59e9/jSXMrQRAcQNyElYgIwCsA8pj5SUt4Z0u0MQB+NJY/BnANEaUTUU8AvQGsCOuY0qVVEAQHEM9WAcMAXA9gHRHpPm/3AxhHRDlQTbAKANwKAMz8ExHNBfAzVIuCKeG0CAAAj/GacKSGOTJTgiDEg7gJKzMvA0A2mxYE2ecRAI9EekxKUcoq7VgFQUgk7urSagiraJggCInEncJa60BlFbUXhKTBncLqRBFzYp4EQYgL7hJW3dzK60ARE2EVhKTBXcKamgJAXAGCICQWdwmr+FgFQXAArhJWpKSAUAv2OrG9lSAIyYILhZVRKxarIAgJxF3C6vGAwFJ5JQhCQnGXsBoWq/hYBUFIJO4SVm2xirAKgpBA3CWsKSnwoBa14goQBCGBuEtYPR54UOtMi1UQhKTBXcIqFqsgCA7AXcJqWKwirIIgJJJ4DnTd8GiL1Yn9A0RYBSFpcJfFagirt8aBIibCKghJg7uE1eNBCrziChAEIaG4S1iPVV4lOiOCICQz7hJWqbwSBMEBuEtYpfJKEAQH4C5hFYtVEAQH4C5h1a0CnOhjFWEVhKTBXcIqrQIEQXAA7hJW8bEKguAA3CWs2scqg7AIgpBA3CWsTm7HKharICQN7hRWJ1qsIqyCkDS4S1gNV4DXS4nOSV1EWAUhaXCXsKakqFYBYrEKgpBA3CWsxzoIJDojNoiwCkLS4C5hdXJzK0EQkgZ3CauTm1uJxSoISYO7hNXJFqsIqyAkDe4SVmkVIAiCA3CXsB5rFZDojNggwioISYPrhFVcAYIgJJq4CSsRdSWixUT0MxH9RER3GeFtiehzItpkzNsY4UREzxBRPhH9QEQDwz7oscqrGBdGEAQhDOJpsdYAuIeZ+wEYCmAKEfUDMB3AImbuDWCRsQ4AlwLobUyTATwf9hHFYhUEwQHETViZeTczrzGWDwHIA5AF4AoAc4xocwCMNpavAPA6K5YDaE1EncM6qLZYnahhIqyCkDQ0iI+ViHoAGADgOwAdmXm3sWkPgI7GchaAHZbdCo2w0Dn2BwFpFSAIQuKIu7ASUXMAHwKYysxl1m3MzADCUhwimkxEq4ho1b59+3w36j8IiCtAEIQEEldhJaI0KFF9i5k/MoL36k98Y15khO8E0NWyexcjzAdmnsXMg5h5UPv27X03ah+rEzVMhFUQkoZ4tgogAK8AyGPmJy2bPgYwwVieAGC+JfwGo3XAUAClFpdBaEirAEEQHEBqHNMeBuB6AOuIKNcIux/A3wDMJaJJALYB+LWxbQGAUQDyARwBMDHsIx5rFSA+VkEQEkfchJWZlwEIpHAjbeIzgClRHVRcAYIgOAB39bzSYwWIxSoIQgJxl7DKWAGCIDgAdwnrsQ4CYrEKgpA43CWs0qVVEAQH4C5hdbLFKghC0uAuYdVdWqXyShCEBOIuYSVyrsUqwioISYO7hBVACkkHAUEQEovrhNUDlg4CgiAkFPcJq4fFFSAIQkJxn7ASnCmsgiAkDS4UVpZWAYIgJBRXCqsjLVYRVkFIGlwnrCkkza0EQUgsrhNWj8ehPlYRVkFIGtwnrOIKEAQhwYiwCoIgxBj3CauH4WUHFkssVkFIGhyoQNHh2HasIqyCkDS4TlhTPIxaJxZLhFUQkgYHKlB0KB+rA4slwioISYMDFSg6PEaJHKdjjsuQIAjxwn3CSkrAHPl7FkEQkgL3CatRIq83sfmog1isgpA0uE5YUzwOtVhFWAUhaXCdsGqLVYRVEIRE4T5hdaqPVYRVEJIG9wmrWKyCICQYEdaGQoRVEJIG1wqr41oFCIKQNLhOWFNSAvhYy8uBe+8FqqoaPlOAWKyCkESEJKxE1IyIPMbySUR0ORGlxTdrkeExxl+pY7E++ijw+OPAiy82eJ4AiLAKQhIRqsW6FEAGEWUBWAjgegCvxStT0ZBqWKw1NX4bKivVvLq6YTOkEWEVhKQhVGElZj4C4EoAzzHz1QBOiV+2IictRZmqdYRVQwkaUlCEVRCShpCFlYjOBDAewCdGWEp8shQd2mJNlGEaEBFWQUgaQhXWqQDuAzCPmX8iol4AFscvW5GT5lG1Vo4TVkEQkoaQhJWZlzDz5cz8mFGJVczMdwbbh4hmE1EREf1oCfsTEe0kolxjGmXZdh8R5RPRBiK6ONICBfSxmoWJNOnoEItVEJKGUFsFvE1ELYmoGYAfAfxMRL+rZ7fXAFxiE/4UM+cY0wIj/X4AroHy214C4DkiisjVkJbiUItVhFUQkoZQXQH9mLkMwGgAnwLoCdUyICDMvBTA/hDTvwLAu8xcxcxbAeQDGBLivj6kpdZjsRIp1S0ujiT5yBFhFYSkIVRhTTParY4G8DEzVwOIVCluJ6IfDFdBGyMsC8AOS5xCIyxsUlNDqLy6+Wagffsg6hsHRFgFIWkIVVhfBFAAoBmApUTUHUBZBMd7HsAJAHIA7Abwj3ATIKLJRLSKiFbt27evzva0+nysAPDOO2rekAMKiLAKQtIQauXVM8ycxcyjWLENwPnhHoyZ9zKzl5lrAbwE83N/J4CulqhdjDC7NGYx8yBmHtS+ffs628NqbiViJwhCHAi18qoVET2pLUUi+geU9RoWRNTZsjoGqiIMAD4GcA0RpRNRTwC9AawIN30gBB+rFbFYBUGIA6khxpsNJYK/NtavB/AqVE8sW4joHQDnATiOiAoBPAjgPCLKgfLPFgC4FQCMtrFzAfwMoAbAFGaOaHyqgD5Wq7DpZRFWQRDiQKjCegIz/8qy/hAR5QbbgZnH2QS/EiT+IwAeCTE/AanXx0pkilxDip0IqyAkDaFWXlUQ0dl6hYiGAaiIT5aiI9V4VYTkYxWLVRCEOBCqxfp/AF4nolbG+gEAE+KTpegIyccqrgBBEOJISMLKzGsBnEZELY31MiKaCuCHeGYuEkKyWBMhrIIgJA1h/UGAmcuMHlgAMC0O+YmatFAqr4KFxQuxWAUhaYjm1ywJGtg0ONpiDanySlwBgiDEgWiE1ZFKkWb8MEYqrwRBSBRBfaxEdAj2AkoAMuOSoyip12K1IsIqCEIcCCqszNyioTISK8KyWMXHKghCHHDd76/FYhUEIdG4TlgDWqx2wibNrQRBiAOuE1ZK8SAFNaH9pVUsVkEQ4oDrhBUpKUhFDaqPhiBk4mMVBCEOuE9YPR6koRo11QGEzCpwYrEKghAH3Ces9VmsIqyCIMQZVwprGqpRU+MnZHa9rURYBUGIA+4TVo8nuMWaKGEVBCFpcJ+waos1UAcBq5hK5ZUgCHHAfcJqVF4drXKYxSrCKghJg/uENSUFmahARaD/G4iwCoIQZ9wnrB4PmuEwDh8OsF2EVRCEOOM+Yc3IQFMcweHyEFoFiI9VEIQ44D5hbdrUsFgT0Nxq+XJgyZLYpikIQqMj1J8JNh6aNbN3BWgRjaewnnmmmsuAL4KQ1LjPYjWE9UiF359jtLB5vXXDGgI9KozHfadcEARf3PeUa1dAhV/R7CzWhvR7amElR/4qTBCEGOI+YdWugMoU33BtqSaqVYAWVqnEEgTX41phra7x+A523RA+1mBYXRCCILga9wmr4QoA4FuBlWhhFYtVEJIG9wmrYbECIQhrLEXu9tsDb3v7beDJJ2N3LEEQHI37hDVRFuvMmYG3jR9vLovFKgiux33CmpqKZqlHARjCOn8+cOGFiXcFxJLVq4H77hORFgSH4j5hBdA+sxwAUFQEYPRo4IsvgHIV5gphHTwY+NvfGm/+BcHluFJYuzQ7AAAoLASQYjS7Ki5W80S1Y7US7XH1/mKxCoIjcaWwdm5TCcAQ1rQ0FVhSouZusFg1jT3/guBSXCmsTXpmoWNqsa+w2lmsiRKmWFmaIqyC4EhcKazo3h1duBA7dsAU1gPKPeAIYY0VjT3/guBS4iasRDSbiIqI6EdLWFsi+pyINhnzNkY4EdEzRJRPRD8Q0cCoDt6jB07xrsXqVbXgtCa+29zgY9VIby5BcCTxtFhfA3CJX9h0AIuYuTeARcY6AFwKoLcxTQbwfFRH7t4d5+F/KC7x4Cec4rvNCRaruAIEwdXETViZeSmA/X7BVwCYYyzPATDaEv46K5YDaE1EnSM++AUX4MLmy+GhWrx+5CrfbU4QVkCJ6+23A99+G3kaIqyC4Ega2sfakZl3G8t7AHQ0lrMA7LDEKzTCIqNdO3T51Rm4sskneLX8KtTCMlSfE4SVGaiuVr21zj038nREWAXBkSSs8oqZGUDY38RENJmIVhHRqn379gWOmJODUVUfoZiPw8/oZ4Y7QVgB4KjqHYbUKH7iIMIqCI6koYV1r/7EN+ZFRvhOAF0t8boYYXVg5lnMPIiZB7Vv3z7wkXJyMBzq/1NLMNwM96+8Ki0FunVT/6uqjx07gP/+t/549cEMVFWpZRFWQXAdDS2sHwOYYCxPADDfEn6D0TpgKIBSi8sgMs4+Gz3P7Yau2I4lbceY4f4W6/LlSjBnzKg/zYEDgUsvjSpbx4iFsEqrAEFwJPFsbvUOgG8B9CGiQiKaBOBvAC4kok0ALjDWAWABgC0A8gG8BOC2qDOQmgpa/CWG/6IlltScbfoc/IU1nF+l6E4G/oRby88srgBBcDFx+0srM48LsGmkTVwGMCXmmfB4MOiC1njzP8A+tEcH7AvsY42mCVS4Amd1BegODA1xXEEQGgR39ryy0KePmm+AseAvptpiDUdY/eNG8kkuPlZBcC1JI6wbcZJaiEWrAP/9whVWcQUIgqtxvbB26wakp7O9xWr1sYZjsfoLWiQCJxarILgW1wtrSgpw4glBhDUS/C3USCzWSIX12WcjP64gCA2C64UVAE7q47F3BURaYRULYY3UFXDHHeayWKyC4EiSQlj79AE24wTUICU2roBohRWQVgGC4GKSQlhPOgmoRhMUoEd8XAHRNLdyko+1tla5Gnx+bysIQrgkhbCecIKabz5uKFBTY26orTXFKZrKK3+hDSUtJ7YK+Pe/lath+vT64wqCEJCkENYTT1TzzW0H1RXWjz9Wy7F0BfgLnsfvNDvVYtV/stX/BxMEISKSQlg7dwYyM4HNlVm+wvrSS7617KESrbACoQkrM/Dww8C2baHlI1oi8TcLglCHpBBWIqBXL2BzhZ+wbt1qLkdjsdbXrtXOYg3FFbB5sxocZvRo++2xtljDGTdBEISAJIWwAsrPml9xPFBZaQbGq+dVOK6AYK0CdDqBKpOkVYAgOJKkEtYtFZ3B2y0/KrBaqfH0sdpZguH4WAPlLV7CKq4AQYiKpBLWCm86dh9uYQZaBTGewuqfttUVEIz6Ps3j5QoQYRWEqEgqYQVUR4FjRFr5E66PNSWlbhraYg0mjlrgAgmdVF4JgiNJGmHt2VPNt6G7GRhp99ZwLdaMDGDQIOChh8xjaYs1mDjWJ5xSeSUIjiRphLVbNzUvQA8zMFJhDbfyyusFzjoLaNXKPJZunRBMHHWchvaxCoIQFUkjrJmZQKe2Vb7CGg67dpnL4VqsXq9qGWC1CLVoBrJKN2wA+ve3Ty/QcWKFuAIEISqSRlgBoEdXb+TCmpVlLofrY62t9fWzhmKxLltWf56iFdalS4E33jDXxccqCDEhuYS1b0ZgYY2nj9XrVcIajsVqjRsvV8Dw4cANN0SXhiAIdUguYe3pwXZ0gzfaYkfiY7V2Eti4Eaiutt9XE0pFUiitArZuBQ4cqD+eNb1QXjLr1wMffRRauoKQZMTtL61OpEcPNXzg7hWF6DLkeN+NkVqs+fnA7Nm+2+uzWM86K3BcTaws1l69gK5dge3b648bjrCefHLocQUhyUgui7WHmhdUdY4uIauwDh0KvPaa7/b6fKyB0rISisUaqitgx4764wC+4ygIgpPZtQuYOTPRuQhIUglrr15qnp9vs9Hf8lq0yHdcAStWMbQbYs+alhY/fx+r/3Z/Yu1jLS6uP478Q8u5bNlS9wWezIwZA9x+O1BQEPo+Dfh1lXTC2rQpkJtrs5FZCQsz8PPPwAUX+P5fyko4Dfd1XP/mVqGmFepx6qN9+/rjhOMKcArr1wNz5iQ6F/FnyBBg4sTGdW3iiTYUQn1++vcH2rWLX378SCphTUlR59dWWFesUAOijBkDlJaqsHXr7BOyClp9Vqi+8IFcAaGIY0N1EIhEWBNt5Z52GnDjjYnNQ0Ogv4zEXeNLqPfqunWhV+LGgKQSVgAYMEAJa8DLMX++WYMfSLisYmInmHb/1QrkCoimuVV9ohau6EUikrp1Q6LQXYOTpRdaos+3U3B49+ukE9acHGWQBu0ooC9atMK6fTtw6qlq2e4vAsGOEYqwbt5sH64JZQQtK5FYrE550PWgNm7HKefbKTj0hZqUwgoAucgJHElfrGiFdeZM8y8F4VqsoYxj8Mc/BhfXUIVVHyuSz8x4POgbNwJr14a3T6CKRrchwuqLQ10jSSesp56qjMfvMSBwpPXr1TyUPvp2lmhtLbB3L1BWZoaF2yog1BvGOoaBP6E+hP6dFRJtsfbpY74BQ6UxC+uhQ6G7YRwqJAnDoS+apBPWpk2Bvn3rsVhvu03No7FYO3UCXngheDxACfC0aXVvEOsDFEzogvmaQrVY6+teGwyn3Nj+roCaGvMF6WSYgZYtgVtuCbzdilPOt1Nw6PlIOmEFlDH0fdsLAkeoqFDzWFReaQI1tzp4EHjqKfM33JpYWCaRCmuiLdZI8LdY77lH9Q7buTMx+QkVne9XX6277dNP1X3z009mmFPOd6LRz1K4z0mgZ3rJEt+fi0ZJUgrr4MFA4f6m2IKewSNGKqx2ll8gizXQsUK1WO2YPh0YP75xCGtREdCkCfDNN9Gl4y+s8+er+ZEj0arvM6gAAB/VSURBVKUbb/SPIu3+ffb662r+/fdmmAirL+Gej0CVnOedZ/YgigFJKayXX67m/0KA30prAn0aBxLWJ55Qc7uHOZCPNRCh3jB2IvjYY8Dbb4eeRiKFdelSlcaTT0a2v/Zx+wurbrPo0FrjY2hhtftbry5DmzZmWDIL6+HDwM03A/v3m2Hhno8G8sUnpbD26qU6CszDmOARg1UsWdunaloYPyq0+111oOZWGiL1NtU9SkK1WIPdWP4W6xln2MfTx7IKrMcD/Pa3deN++inw9dehHT8U9HmMtF2iPq/+loiuOAy3yVlDE0xYrQKiSebKq5dfBl55BXj4YTMs3PMhwhpfxowBvsYwFCFIV89AgjZxouryCvgKa9Omah6JxXrvverfWLrrqZ2w2gl9MOHw37ZihX08f4u1pkYd8x//qBt31Cjg7LPN9WiFVZetvhdPIPT5P3jQPi+NRVibNKm7TVus1pdGMlusGuuzEQuLNQ7dhJNWWEeNAhgefIkRgSNpofnnP+tuW7xYWRRWYe3QQc3tLNaMjODCumWLuVxTY/8mtnNN+AuHVXxDfTvPn6/+XXPwoFoP52ZNtMWqz//o0b6Cr2mIjgPV1ZH7coMJq7ZYrdfR/3x/9JE6d3v3hna81audP95Aba1vhZ1GX+va2sgrr+yeiTi8fBMirERUQETriCiXiFYZYW2J6HMi2mTM29SXTjScfjrQCgexCCNVQBubw+mHfupUNU9P991u7f4KmBfe7iE7/vi6YYEoL7e3WO2EdcwY4NxzzXXr56Me88CKbvFg5aGH1A23Zo1aD0ckYm2xWh/6UATA+mKzs8gbwmIdMQJo1iyyfYO5AvT1C2axPvusmtsJkT+ffab+Fjxrlu/xf/1roLAw9DzHm4cfVg3Of/zRN1zfI9bnIBYWaxxevom0WM9n5hxmHmSsTwewiJl7A1hkrMeNlBTgfCw2hdUO/0/v3/zGd33+fF9LS194O4v1+ONDt8rKynyFVd9IdsJaWwt89ZW5vm+fuWw3pKHdQBQtW6p5UZGa2+U/EPXd2GvXAs88E3i7v8Wq/dRA4MrDs84y233atbaw5qkhhDWU/5MFIpiw6vJbxcDfQrOz+O++G/jgg7rpbdig5tbBhT78EHj/feC++8LLdzxZskTN9+zxDbcbwyMSYd25E7jzTvNcxsHv6iRXwBUA9Phvc4D6quyjZ+Rd2diKXsjHCfYR/IXV32LNyzMvTtOm5s1tZ/F1DmNw7UOH7P1IoXz2WIXVrvLDLkwLq7ZaAgmrndDVd2Pn5AB33RXY+tRl0ufOeuxAaX/7rarIqKmx981ae7w5fQyB8nI1txNWTTBXgD6vVmF9+mng6qvrpqPvZ+s5s7MCE41+GfqfE7uvmkhcAZMnA//v/wH/+58Kc5HFygAWEtFqIppshHVk5t3G8h4AHeOdiV/efSJSUIOZmGIfQd/0mowMczk9XQ2yUlWlrNG8PPPC2wlrZmboFqu/sIbTM0pbnYC9xaqF1Xpz6s9Ynb613NZ4duUKt9usP/qm9njqim991ub69fYWq9UF0pCVV5H4LgNZrNaXejBXQH3jWtjFtXNfOVFY/ctk5xIL5f7zr3fQLyod7iKL9WxmHgjgUgBTiOhc60ZmZgQY2Y+IJhPRKiJatc9qnUVA9+7A9XgDL+D/sKe2Q90IZWXAX/5irluF9ZRT1AXZvVs1jO3WLbgrIBwOHfK9YaqqzIG46yNUi9Uq3FYLD/DN/6FD9uGaUIU1kN9W39REdd0UdqJoFa+yMnth1ZVwgdKIF5H4mwMJq/V8hWKxVlUpoQh27zU2YfW/Z/QLJtzKK2sc67nUabjFYmXmnca8CMA8AEMA7CWizgBgzIsC7DuLmQcx86D2oYyKXw/341EcRRP8Y+CbvhsyMoArr1QjSGmsD/Epp5jL+qGwE9ZrrzV9cKFarPn5vv/zYVZW5HvvBd8vO9usaANCF1b/ig/rjWZNw9+CB8IX1uJi33/j6Bvd4/G1tgF7UbRWvpWX27sCrBartSx79gCPPBK/TgN2FYP1oe8V/3JYz3UowlpRoXrbNW8e+Fj6mluPpe9JJwmrLqP/+bQKq3/cUNLzT9NNFisRNSOiFnoZwEUAfgTwMYAJRrQJAOY3RH56n90J12bOw3PfDcK+URPMDS1a1HXoWy9Qdra57C+s1jftWWcBw4ap5VCFdYqNa+LZZ81fxTz3XN3tzKoW1SpGgVwBO3YAV1wRWl70A15VZV/zHK6w9uwJ9O5thltvdP8mQ3bCaj235eX2ghDIFXDJJcADD6hhCe3SHTMmeO360aOqvXGgkegjeUD1+fU/j1Zhtb4crC/EmTPNrsCVlcC77wY/lhZxq9Wv09bnce1adZ/WN9ZvPNHXLJCwhusKsMY5fNgsv76XXGKxdgSwjIjWAlgB4BNm/i+AvwG4kIg2AbjAWI8/X32FB76/CpWVwIM/jzXDmzQBBg70jWt9SAcMMCuzdBtEO4tVdxqww65/OGDvq7N+kttVdFi3a6vlyy/rxtu/X40l8PnngfNlRT/gd96p2or6o2/ae+8FRgZpYaFvYn+rV4tRYWHdCpdQhNXuwfJ3BVRUqJeMHuPVzi3x8cfAv/4FzJhh5qdPH9/2xf/5D/D44/Y90oDILFb9RRBMWK3pWuNZ8/Hpp/UfS98jdm4GLVb6h4X/+lf96cULXcZArgDrObBzBcya5dsSxSqc1mdTp+8Gi5WZtzDzacZ0CjM/YoSXMPNIZu7NzBcws813bHzo00fpxvMFl+JLnK8EMj1dzQP1027d2nQHBLNYre0b/S3WcD5JrWJhJ6xWa69Pn8Dp7N8fnt9RP+CLFtlvf/99NX/8cSXkgXxeR474jh2rxULf1F984esfBuoXVn9ftMZqsf7vf6qd73HH+e7nj24zqTt5vPKKsmxfftmMk5mp5lu2qGvn77qwCmBRUeB/plnRXxX+ZQ3k57aW1+qaetPPlWWHvpbWtP2FVb/sG7rrLLOyuI8cqd9iDfSi0dx6q2qJounUyVx2scXqSB55BDjpJMb4dv/FrnOvMa1Qa/96683frJn525VgPlZrhZc/4Qjr7t3msl2FjbXNX7dugdMJV1gXLlQ3Ypcu9tv/+1/fAVT+9S81ZJ//J/qRI76/KtaCYrUWWrf2/TGgXQ34ySeb63YWq9frey4+/BBYtco3zrZtSvgefFA1B/N6gR9+UNu0KOv8WQVZn7fdu1Uj9o4dgauuMrdXVJgP/YABakAKQAl0IPeBtlj9r4nVYg1FWENBp1NWpq7R11/XFdZYVWbt3es7HnF9fPklMG6cqtMIRVhDqbzavLnucIzWZ3PbNpXOv/9thsWoV5oIq0HTpsCHHxIOVTbBRfnPYe81xhvv1VfNT2ur9dmsmWmx6oFTtLB++60Zz/q5H0m3Td1f3zquqN1DahUTa6WetrI0JSXhCesTT6jG5nb+Ws0995jLd9+thHbDBt+bdORI34GnAwmr9Xz559PforUT1v37lT86mFtiwgQlin/+s3IPlJSYAqetap0/q3hpsdu1y6xI/PBDc/uHH6ob6YUXzHTmz1dfEBMs/nsrgSxWq7Bu22YuV1eb5zGQK8kKs3pZLV5sCuv8+eoaPfxw/CzWK69UHWp03gsKgqepx0LdtcsseyBXQEWFaZQE87H+9rfATTf5hlmFVbvDrC+AGFnqIqwWTj1Vudo2F7fCuCX/p7SsUydzMOrpls5gHToA/fqpZV3hYVdDbX0wIxHW4cPV3PoZbVf5EkhY9aftY48Bv/ylEhCrWyEUnnxS3fi33qp8qcHQnQy+/75u05/Zs83l4cNVCwbr53SbNsGF1b/bpd0vTTZtUg/mDTcEz6eV4mJTWPULTL8sy8pUOb791nzgDx1S7Zb9efRRNbd+luteRLplSF6eCtNlqc9ibdfOtKYBNTbAySereSgW68GDwJw5qtut/31z5IgprPr44fbBnz/f/OOGFS3+5eXq3uzZE7j/fvO8+rNpk3lcu09+wAw/csTMXzBhtfMTHz5s7uvfs8t6jCgRYfVjxAjgxReVa653b+MrISVFiUBGBjBvnroJMjJUjT+gmrkA9sIZilURDG0t79qlWiqcdhpw++114+l+/oAppoApsj17Am3bKgsp3JHSly9XN2TPnuoTNxDWirrXXqvrPrAON1haqga3sVa6tG7tKxZXX20KE1BXWO0s9+3b1dz6Ca8J1LupuNhMa/NmdUz90JWVKT/RWWcpPzBQ/wvS2i5YD1J98KD6PO7XTw2qnJOjrC4trCUlwDvvmPtpS7aDX/vqpUvV/P7768/Hc8/5njP/mv4NG0xh1S9BbSXqMixYoI6zb5/yGc+dq8JffVVZ6KNHA88/7ytw1dVmuWbMUC4XQH39tG9f9/4rKDCvs9UvHUhYrb51/xdAfRVR5eVmWfW9YneMKBFhteGGG9T1PfVUdd+MG2f5Ghs9GjjxRLXctq361Jo4Ua1bP9/0uAI9LX8p0A+CFo9gbQ412v3ADPToAeTm2ldO6dpcnS+NVWDatVNNrexGQgommJoTT/R1h4zxG89Wd40FlAjZDQITjDZtfIW1qEh1vnj8cdUF0SqyRPbtdHWTI7s2zqed5itGuhVCSYkS1ilTlIvg6adNt0Npqbms+9/bjXZmxWoJ5eaqOTPwxhtmeEmJEq/aWlPwr73WtBw3blTXsUcPtX6CX7frDRvqVp75M2VK8MFViorMLyEtNvoePnhQNee67DK1vmqV8hmPHavE7KabfP3L1jqAadPM5Y8+8h30BVDXcdky9QXDrJ6R774zy6UJ5ArYtcsU1r//3TQ08vLsu/JaOXw4+CBDsarIYuZGO51++ukcTw4cYL7oImZ19Zkvv5x54ULm2toAO3i9zE88wVxaqpZ37vTdvnSpSmjIELVeWMj80kvMq1Yxv/KKeaBLLzWXDxwwl4nMtHSY3fT++8wtWqjlK69U87lzmf/5TzNO27a++5SUBE8TYF67VhX+m2/UfO9e3+2/+139aegTaRd+003M06aFlkbHjsxnnRV4+5YtdcPGjTOXTz2VOS/Pd/tf/8p8113MTZqYYWPHMl9zjbnetCnz6tWh5VFPrVureb9+vuGffabmHTqYYV99pa7veeep8v3mN3XviXCm55/3Xc/IsL/+zZoxr1/P/MtfqvXRo5k9HnP7m2+ayz/9VPc4s2YxDx8e2n1knTZtCrxt7Fh1jioq1DkZNcrclpJiLnfrxnzLLb73w7Rp6p73T/P885m7dAl+3zAzgFXMkWtTxDs6YYq3sGrefZd50CDmzEx1xgYPZv73v5WuHDwYZmKLFzP//HPd8G3bzBuCWS0TMR896nvhNe+9F/jmWLCAeetW9RbQwvr++8yffGLGsd503bqpNC+4gHnEiMDpHj7sm2dr3nbvZl6zxjd+06bMAwfWTce/THq6557QxXnIkODbDx2qG/bAA+bynDnMR474bn/hBebly33DLr2U+ZxzzPWOHZkrK0PLo578Rcp/ys42l2fNUi/lDh3Ui+bvf1fh1pcCoIQQMIVQTxddxDxxYuBjDR1qxguW5/PO882z9dxZpx497MPbtTOX581j7ty5/vNkTatZM/OFM26cumeCnUP/6ZNPzOfIOhEF3y8tjXnqVBZhbUAqK9V936uX7/WfPJl5yhTmb7+N8gAbNjBXV6vl0lLmsjK1rA82Y4Zv/DlzmJ95xvfGmDbNTINZWUAA8549yiLR8ZiZv/xSLV98sW+6VgsKYL7qKnMff6zp7d5trt97r2lpAMx33OEb94MPlOXz0kvMv/iFCn/4Yebp08143box9+2rxFaf9AEDmL/4gvm3vw3+gNTW+q537Micn2+uf/qpb/4B9bLy389/0i8h/5dIsOn++5lPPFEtjxnD3L+/7/Zhw3zXTzhBzd99l3n2bLVstZr1jQcwP/usr+CsW8e8eXPgvOivlvPPZ/7VrwLH69rV1yoMNOkXt3Vq2dL3S4tZnW+A+aSTAqdlzc/kyb7bevYM/XwDzF9/rY4b6KtGf0Vo6/W663y2i7AmgMpKZQDOmKFe/Onp6kWYmqo0YvRo5oceUkJbUxPEdRAqy5cz79sXePuePeoGX7So/owDzGeeaYZ98IF6CKxs3sz86qvMzz2nrI2aGuaqKvs0Z8xgnj9fLdfUmDepldpaNeXnq08/f7SVvHy5OmlajLSFXFurjn/bbSoNZuaZM1W80aNVGZo0Yd640fdh/utfTQH6859V2Ny5zBMmqBcXM/ODD5r76M/w7t3txQJQVrjm1VeZc3JU+Pjx6ho8+6y5j/7E+eYbJagA82uvMRcUqPjjxzPffrv5YrG6IFq2VJbrunV8TPQnTFDWmz5HgHIj6fx+8IGZt5df5mPC97e/qeXSUuaVK9XyhRea10tPnToFF6sbblBfQ9aw229X5/bPfzYFy98g0NdwyhTmzz9n/tOfTMsZUC9Uq+h36sS8a5e53Wrtjh7te/zhw83llBTmf/xDlVHj767S0+bN6qtu3jzlmiooUOf1zTeZP/lEhNUJVFcrbbrjDuaTT/a9fkTMxx2nDITp09WzWFiYwMyuXFlXSGPJ3r2BRTgQGzean26hoq3v//3PN3zrVuYVK8z13FwVb+HCwGlt367S8XrV+q9/bT7oc+Yoq1Fb91ooNLW1zD/8oERKo+N98w3zpEkq3bIy9eBavyY0RUXMb72lrGAtghMmmNu1UFk5/ngVr7qa+bvvlCVdUGBuP3BAWbl79pgvJmaVlxkzlOvJmtetWwO7OLp1UzdwQYE6nnXbunW++Tx0yPc8DB1qf871183gwSrN119X53DrVvNLZ8UKdd7z8pRgT5+uwufPV/vedZcqW3p63etiZf9+5R6ZOtUU8nqsnWiFlVQajZNBgwbxKv9eNQ7A61Utsr7/XjV/LCpSFZZ5eaoyMy1Ntd5p3lwN5XrZZaoy/IwzVCU+kYqXmhr5r6AEC15veL2UDh5UTYh+/3vf/WbPVh0uxo0Lvv/u3eoCBusBF4zFi9XNEGyciW3bVJtb3eQvUhYsUPNRo9R89WrV7O2779SNOnCgukmtHU3efRfIygLOOSd42hUV6iYO1MztzTeBCy9ULTHqw+tVD4NuK75/v2qFkpqqrldZWeTn2wYiWs3m303C31+EteGoqVFtvZ97Tv2eqahItfzw77reo4dqUdK9u/o/Xn6+uoevvFIJb1WV2ub/QwNBEGKDCGsjElY7KirMH2d+841qq56Xp9rWf/21ajvdq5caI8R/aIGuXVXTwuOOU4ZVSooS73btVOcGr1cJ9Eknqfi5ueqv3X37quNkZammkhs2qOO1bBn8DyGCkCyIsDZyYQ2V7dtV1+bSUqBVK+VqyMtTolhRoQRVfy0VF4feIzE11Yybmam+Aps2VWPQlJSodvZt2iirunlzZWF7vUrsU1OVmGdmmlNlpdq3tlZ9mTVrpqYWLVTazZqpNuiZmeqF0KSJan/v9aq0O3dWZdV9EUpL1XGbN1dxWrVSZWRW+dYvAq9X5Vd3VNq3T1n2WVniTrFSW6vOlbxAgxOtsEbZ31JoKLp1AyZNCi1udbXq/MOsLFL97snKUn7fI0eUyP38s+qElZ2t3BLFxUpAjxxRU2amErb9+83OXG3aqPQ3bVLpV1ebvxHSnXeOHo3PqHOpqWpKTze7fLdsqUR771613qyZyqPucNS+vSpfq1Zqn4wMJbQVFUpgunVT5UhLU+k2b67SKy1V6TRpol4EVVVqW+vW5k90PR41lERGhkpDd3KrrVXrgO8Pdqur1bbjjjNdlroWKD1dnbfdu9UXRnW1elGUl6v8VFWpl0Xr1iruccep8Px8dfxevcyvFo9HzXWnLj3yJRFw883Kffrpp+ollpqqjqX3jSf796u8R/qn8FhSU6M6gA0frs7B0aPq/MTqhSMWqxBTmJUop6WZYnv4sBLsgwfVtiZNlLBt367itW2rwlJS1MN36JDal0iF6R8F6FH5qquVMDVtqtIsL1cC17y56oZ+4IBZ97NypeodWlam1ktL1UPVqpVKe9cu04VSVWV2Q09L8+3+7vHE748uDUFKSt3xarQIV1er85GVpV6wuhd2ixZqn6oqdf70l0b37upcVFYqQfJ6zZeex6Ou8f79Ks0TTlAviNJS1U3c6zVfBB06qHSPHlXHaN9eXd8jR8x6hLIydV2rqlR+MjPV/hkZ6lgHDihjYdAg8+WQlqYMi/791b3XqpW6rrreYscO9QJbvlzdY8OGqboPZlVX17Il8O9/iysg0dkQXERtrXqIMzPVA15drR7slBQlJFow0tKUSGzbZgrugQPqQbVOWpC1iKWlqS8D62BWREoAmjRRaRcWKssuJUWJ3ZEjarltW3WMqiqVj6oqFV//Lszr9Z08HnWckhIVl0gJ1fnnq/23b1cvlObNVTn27VPHOXLEHHclI0Odi7Q0dazKShU3PV2FN2mijqNdUbW1ZkOAo0fVmOBZWUoU+/VTxyotVePBlJSoNHV+9ReHdkVt2mSKamamOkf666iy0vxiad5cpVVTY76cs7KUm6xVK1XmZs3U8o4dpnWfmamG3di2TVn1HTuqsYzKyoCtW8UVIAgxw+MxP9P9WztlZKiWR1b69m2YfAkNS7R+eRndShAEIcaIsAqCIMQYEVZBEIQYI8IqCIIQY0RYBUEQYowIqyAIQowRYRUEQYgxIqyCIAgxRoRVEAQhxoiwCoIgxBgRVkEQhBgjwioIghBjRFgFQRBijAirIAhCjBFhFQRBiDEirIIgCDFGhFUQBCHGiLAKgiDEGMcJKxFdQkQbiCifiKYnOj+CIAjh4ihhJaIUADMBXAqgH4BxRNQvsbkSBEEID0cJK4AhAPKZeQszHwXwLoArEpwnQRCEsHCasGYB2GFZLzTCBEEQGg2N7vfXRDQZwGRjtYqIfkxkfuLMcQCKE52JOCLla7y4uWwA0CeanZ0mrDsBdLWsdzHCjsHMswDMAgAiWsXMgxouew2LlK9x4+byublsgCpfNPs7zRWwEkBvIupJRE0AXAPg4wTnSRAEISwcZbEycw0R3Q7gMwApAGYz808JzpYgCEJYOEpYAYCZFwBYEGL0WfHMiwOQ8jVu3Fw+N5cNiLJ8xMyxyoggCIIA5/lYBUEQGj2NVljd0PWViGYTUZG1yRgRtSWiz4lokzFvY4QTET1jlPcHIhqYuJzXDxF1JaLFRPQzEf1ERHcZ4W4pXwYRrSCitUb5HjLCexLRd0Y53jMqYUFE6cZ6vrG9RyLzHwpElEJE3xPRf4x115QNAIiogIjWEVGubgUQq/uzUQqri7q+vgbgEr+w6QAWMXNvAIuMdUCVtbcxTQbwfAPlMVJqANzDzP0ADAUwxbhGbilfFYARzHwagBwAlxDRUACPAXiKmU8EcADAJCP+JAAHjPCnjHhO5y4AeZZ1N5VNcz4z51iajsXm/mTmRjcBOBPAZ5b1+wDcl+h8RViWHgB+tKxvANDZWO4MYIOx/CKAcXbxGsMEYD6AC91YPgBNAawBcAZUo/lUI/zYfQrV0uVMYznViEeJznuQMnUxhGUEgP8AILeUzVLGAgDH+YXF5P5slBYr3N31tSMz7zaW9wDoaCw32jIbn4YDAHwHF5XP+FTOBVAE4HMAmwEcZOYaI4q1DMfKZ2wvBdCuYXMcFk8D+D2AWmO9HdxTNg0DWEhEq40enUCM7k/HNbcSTJiZiahRN9sgouYAPgQwlZnLiOjYtsZePmb2AsghotYA5gHom+AsxQQi+gWAImZeTUTnJTo/ceRsZt5JRB0AfE5E660bo7k/G6vFWm/X10bMXiLqDADGvMgIb3RlJqI0KFF9i5k/MoJdUz4NMx8EsBjq87g1EWmDxVqGY+UztrcCUNLAWQ2VYQAuJ6ICqBHmRgD4J9xRtmMw805jXgT1YhyCGN2fjVVY3dz19WMAE4zlCVC+SR1+g1E7ORRAqeWTxXGQMk1fAZDHzE9aNrmlfO0NSxVElAnlP86DEtirjGj+5dPlvgrAl2w465wGM9/HzF2YuQfUs/UlM4+HC8qmIaJmRNRCLwO4CMCPiNX9mWgHchSO51EANkL5tf6Q6PxEWIZ3AOwGUA3ls5kE5ZtaBGATgC8AtDXiElRLiM0A1gEYlOj811O2s6F8WD8AyDWmUS4qX38A3xvl+xHADCO8F4AVAPIBvA8g3QjPMNbzje29El2GEMt5HoD/uK1sRlnWGtNPWkNidX9KzytBEIQY01hdAYIgCI5FhFUQBCHGiLAKgiDEGBFWQRCEGCPCKgiCEGNEWAXBgIjO0yM5CUI0iLAKgiDEGBFWodFBRNcZY6HmEtGLxmAo5UT0lDE26iIiam/EzSGi5cYYmvMs42ueSERfGOOpriGiE4zkmxPRB0S0nojeIuvgBoIQIiKsQqOCiE4GMBbAMGbOAeAFMB5AMwCrmPkUAEsAPGjs8jqAe5m5P1SPGR3+FoCZrMZTPQuqBxygRuGaCjXOby+ofvOCEBYyupXQ2BgJ4HQAKw1jMhNqoIxaAO8Zcd4E8BERtQLQmpmXGOFzALxv9BHPYuZ5AMDMlQBgpLeCmQuN9Vyo8XKXxb9YgpsQYRUaGwRgDjPf5xNI9Ee/eJH21a6yLHshz4gQAeIKEBobiwBcZYyhqf9R1B3qXtYjL10LYBkzlwI4QETnGOHXA1jCzIcAFBLRaCONdCJq2qClEFyNvI2FRgUz/0xED0CN/O6BGhlsCoDDAIYY24qg/LCAGvrtBUM4twCYaIRfD+BFIvqzkcbVDVgMweXI6FaCKyCicmZunuh8CAIgrgBBEISYIxarIAhCjBGLVRAEIcaIsAqCIMQYEVZBEIQYI8IqCIIQY0RYBUEQYowIqyAIQoz5/3d1O210iXd8AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.subplot(111)           \n",
        "plt.plot(hist.history['val_loss'],color='red')\n",
        "plt.legend(['mse_val_loss'])\n",
        "   \n",
        "plt.plot(hist.history['loss'],color='blue')\n",
        "plt.legend(['mse_val_loss', 'mse_loss'])\n",
        "plt.xlabel('epoch',fontsize = 10)\n",
        "plt.ylabel('Loss',fontsize = 10)\n",
        "plt.axis([0, epochs, 0, 300])\n",
        "fig = plt.gcf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "w29yDKafD4JU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "sT_dWNbKD4tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b22191f-3e0e-4b31-bf29-75e671a4f871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble_me:  -1.1592138221688495 \n",
            "Ensemble_std:  5.888570831919741\n"
          ]
        }
      ],
      "source": [
        "Ensemble_me = total_me/3\n",
        "Ensemble_std = total_std/3\n",
        "\n",
        "print(\"\\nEnsemble_me: \", Ensemble_me, \"\\nEnsemble_std: \", Ensemble_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "6Ar3repNHvuu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "\bBP_hv3_4(3).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}